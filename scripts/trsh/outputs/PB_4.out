--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 5018 (also 616 sleepset blocked, 0 schedulings and 4321 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpkg56jpoi/tmpqbvcm344.ll -S -emit-llvm -g -I v3.0 -std=gnu99 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpkg56jpoi/tmpkm0n6miy.ll /tmp/tmpkg56jpoi/tmpqbvcm344.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpkg56jpoi/tmpkm0n6miy.ll
Total wall-clock time: 88.78 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 260 (also 15 sleepset blocked, 0 schedulings and 50 branches were rejected due to the bound) 

 Error detected:
  (<0>,1)
  (<0>,6)
  (<0>,7) litmus_v3.c:109: pthread_t tu, th;
  (<0>,11) rcutree_plugin.h:909: printk(KERN_INFO "Hierarchical RCU implementation.\n");
  (<0>,24)
  (<0>,25) rcutree.c:2030: static void __init rcu_init_one(struct rcu_state *rsp,
  (<0>,26) rcutree.c:2031: struct rcu_data __percpu *rda)
  (<0>,27) rcutree.c:2038: int i;
  (<0>,29) rcutree.c:2046: for (i = 1; i < NUM_RCU_LVLS; i++)
  (<0>,32) rcutree.c:2048: rcu_init_levelspread(rsp);
  (<0>,38)
  (<0>,39) rcutree.c:2012: static void __init rcu_init_levelspread(struct rcu_state *rsp)
  (<0>,40) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,42) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,45) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,47) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,50) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,51) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,52) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,53) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,56) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,59) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,61) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,64) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,65) rcutree.c:2022: cprv = ccur;
  (<0>,66) rcutree.c:2022: cprv = ccur;
  (<0>,68) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,70) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,72) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,76) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,78) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,81) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,83) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,86) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,88) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,90) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,91) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,93) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,96) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,97) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,98) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,100) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,101) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,103) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,106) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,109) rcutree.c:2056: raw_spin_lock_init(&rnp->lock);
  (<0>,113)
  (<0>,114) fake_sync.h:72: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,115) fake_sync.h:74: if (pthread_mutex_init(l, NULL))
  (<0>,121) rcutree.c:2059: rnp->gpnum = 0;
  (<0>,123) rcutree.c:2059: rnp->gpnum = 0;
  (<0>,124) rcutree.c:2060: rnp->qsmask = 0;
  (<0>,126) rcutree.c:2060: rnp->qsmask = 0;
  (<0>,127) rcutree.c:2061: rnp->qsmaskinit = 0;
  (<0>,129) rcutree.c:2061: rnp->qsmaskinit = 0;
  (<0>,130) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,131) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,133) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,135) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,136) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,138) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,141) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,143) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,144) rcutree.c:2064: if (rnp->grphi >= NR_CPUS)
  (<0>,146) rcutree.c:2064: if (rnp->grphi >= NR_CPUS)
  (<0>,149) rcutree.c:2066: if (i == 0) {
  (<0>,152) rcutree.c:2067: rnp->grpnum = 0;
  (<0>,154) rcutree.c:2067: rnp->grpnum = 0;
  (<0>,155) rcutree.c:2068: rnp->grpmask = 0;
  (<0>,157) rcutree.c:2068: rnp->grpmask = 0;
  (<0>,158) rcutree.c:2069: rnp->parent = NULL;
  (<0>,160) rcutree.c:2069: rnp->parent = NULL;
  (<0>,162) rcutree.c:2076: rnp->level = i;
  (<0>,164) rcutree.c:2076: rnp->level = i;
  (<0>,166) rcutree.c:2076: rnp->level = i;
  (<0>,167) rcutree.c:2077: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,171)
  (<0>,172) fake_defs.h:196: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,173) fake_defs.h:198: list->next = list;
  (<0>,175) fake_defs.h:198: list->next = list;
  (<0>,176) fake_defs.h:199: list->prev = list;
  (<0>,177) fake_defs.h:199: list->prev = list;
  (<0>,179) fake_defs.h:199: list->prev = list;
  (<0>,182) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,184) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,185) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,187) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,189) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,190) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,192) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,195) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,199) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,201) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,203) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,206) rcutree.c:2081: rsp->rda = rda;
  (<0>,207) rcutree.c:2081: rsp->rda = rda;
  (<0>,209) rcutree.c:2081: rsp->rda = rda;
  (<0>,210) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,213) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,214) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,215) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,217) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,221) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,222) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,224) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,227) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,228) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,230) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,232) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,235) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,236) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,237) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,245)
  (<0>,246) rcutree.c:1854: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,247) rcutree.c:1854: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,249) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,251) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,253) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,254) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,257)
  (<0>,258) rcutree.c:295: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,262) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,263) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,265) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,269)
  (<0>,270) fake_sync.h:78: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,271) fake_sync.h:78: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,274) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,276) fake_sched.h:43: return __running_cpu;
  (<0>,280) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,282) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,286) fake_sched.h:43: return __running_cpu;
  (<0>,290) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,296) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,297) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,301) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,302) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,304) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,306) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,310) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,312) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,313) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,315) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,316) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,318) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,321) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,323) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,325) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,328) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,330) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,332) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,334) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,337) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,339) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,341) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,344) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,346) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,348) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,350) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,353) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,355) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,357) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,360) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,362) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,364) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,366) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,369) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,371) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,373) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,376) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,378) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,380) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,382) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,385) rcutree.c:1867: rdp->qlen = 0;
  (<0>,387) rcutree.c:1867: rdp->qlen = 0;
  (<0>,388) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,391) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,393) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,394) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,395) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,397) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,398) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,400) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,404)
  (<0>,405) fake_sync.h:86: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,406) fake_sync.h:86: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,407) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,410) fake_sync.h:90: local_irq_restore(flags);
  (<0>,413) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,415) fake_sched.h:43: return __running_cpu;
  (<0>,419) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,421) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,425) fake_sched.h:43: return __running_cpu;
  (<0>,429) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,438) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,440) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,442) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,446) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,447) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,449) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,452) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,453) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,455) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,457) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,460) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,461) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,462) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,470)
  (<0>,471)
  (<0>,472) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,474) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,476) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,478) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,479) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,482)
  (<0>,483) rcutree.c:297: return &rsp->node[0];
  (<0>,487) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,488) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,490) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,494)
  (<0>,495)
  (<0>,496) fake_sync.h:80: local_irq_save(flags);
  (<0>,499) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,501) fake_sched.h:43: return __running_cpu;
  (<0>,505) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,507) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,511) fake_sched.h:43: return __running_cpu;
  (<0>,515) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,521) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,522) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,526) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,527) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,529) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,531) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,535) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,537) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,538) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,540) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,541) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,543) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,546) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,548) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,550) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,553) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,555) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,557) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,559) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,562) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,564) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,566) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,569) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,571) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,573) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,575) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,578) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,580) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,582) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,585) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,587) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,589) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,591) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,594) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,596) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,598) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,601) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,603) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,605) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,607) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,610) rcutree.c:1867: rdp->qlen = 0;
  (<0>,612) rcutree.c:1867: rdp->qlen = 0;
  (<0>,613) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,616) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,618) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,619) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,620) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,622) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,623) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,625) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,629)
  (<0>,630)
  (<0>,631) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,632) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,635) fake_sync.h:90: local_irq_restore(flags);
  (<0>,638) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,640) fake_sched.h:43: return __running_cpu;
  (<0>,644) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,646) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,650) fake_sched.h:43: return __running_cpu;
  (<0>,654) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,663) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,665) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,667) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,678)
  (<0>,679)
  (<0>,680) rcutree.c:2037: int cpustride = 1;
  (<0>,681) rcutree.c:2046: for (i = 1; i < NUM_RCU_LVLS; i++)
  (<0>,683) rcutree.c:2046: for (i = 1; i < NUM_RCU_LVLS; i++)
  (<0>,686) rcutree.c:2048: rcu_init_levelspread(rsp);
  (<0>,692)
  (<0>,693) rcutree.c:2018: cprv = NR_CPUS;
  (<0>,694) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,696) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,699) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,701) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,704) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,705) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,706) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,707) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,710) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,713) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,715) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,718) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,719) rcutree.c:2022: cprv = ccur;
  (<0>,720) rcutree.c:2022: cprv = ccur;
  (<0>,722) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,724) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,726) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,730) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,732) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,735) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,737) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,740) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,742) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,744) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,745) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,747) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,750) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,751) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,752) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,754) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,755) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,757) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,760) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,763) rcutree.c:2056: raw_spin_lock_init(&rnp->lock);
  (<0>,767)
  (<0>,768) fake_sync.h:74: if (pthread_mutex_init(l, NULL))
  (<0>,769) fake_sync.h:74: if (pthread_mutex_init(l, NULL))
  (<0>,775) rcutree.c:2059: rnp->gpnum = 0;
  (<0>,777) rcutree.c:2059: rnp->gpnum = 0;
  (<0>,778) rcutree.c:2060: rnp->qsmask = 0;
  (<0>,780) rcutree.c:2060: rnp->qsmask = 0;
  (<0>,781) rcutree.c:2061: rnp->qsmaskinit = 0;
  (<0>,783) rcutree.c:2061: rnp->qsmaskinit = 0;
  (<0>,784) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,785) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,787) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,789) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,790) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,792) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,795) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,797) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,798) rcutree.c:2064: if (rnp->grphi >= NR_CPUS)
  (<0>,800) rcutree.c:2064: if (rnp->grphi >= NR_CPUS)
  (<0>,803) rcutree.c:2066: if (i == 0) {
  (<0>,806) rcutree.c:2067: rnp->grpnum = 0;
  (<0>,808) rcutree.c:2067: rnp->grpnum = 0;
  (<0>,809) rcutree.c:2068: rnp->grpmask = 0;
  (<0>,811) rcutree.c:2068: rnp->grpmask = 0;
  (<0>,812) rcutree.c:2069: rnp->parent = NULL;
  (<0>,814) rcutree.c:2069: rnp->parent = NULL;
  (<0>,816) rcutree.c:2076: rnp->level = i;
  (<0>,818) rcutree.c:2076: rnp->level = i;
  (<0>,820) rcutree.c:2076: rnp->level = i;
  (<0>,821) rcutree.c:2077: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,825)
  (<0>,826) fake_defs.h:198: list->next = list;
  (<0>,827) fake_defs.h:198: list->next = list;
  (<0>,829) fake_defs.h:198: list->next = list;
  (<0>,830) fake_defs.h:199: list->prev = list;
  (<0>,831) fake_defs.h:199: list->prev = list;
  (<0>,833) fake_defs.h:199: list->prev = list;
  (<0>,836) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,838) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,839) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,841) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,843) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,844) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,846) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,849) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,853) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,855) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,857) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,860) rcutree.c:2081: rsp->rda = rda;
  (<0>,861) rcutree.c:2081: rsp->rda = rda;
  (<0>,863) rcutree.c:2081: rsp->rda = rda;
  (<0>,864) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,867) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,868) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,869) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,871) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,875) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,876) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,878) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,881) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,882) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,884) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,886) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,889) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,890) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,891) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,899)
  (<0>,900)
  (<0>,901) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,903) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,905) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,907) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,908) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,911)
  (<0>,912) rcutree.c:297: return &rsp->node[0];
  (<0>,916) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,917) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,919) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,923)
  (<0>,924)
  (<0>,925) fake_sync.h:80: local_irq_save(flags);
  (<0>,928) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,930) fake_sched.h:43: return __running_cpu;
  (<0>,934) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,936) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,940) fake_sched.h:43: return __running_cpu;
  (<0>,944) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,950) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,951) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,955) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,956) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,958) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,960) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,964) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,966) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,967) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,969) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,970) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,972) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,975) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,977) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,979) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,982) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,984) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,986) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,988) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,991) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,993) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,995) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,998) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1000) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1002) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1004) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1007) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1009) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1011) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1014) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1016) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1018) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1020) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1023) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1025) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1027) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1030) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1032) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1034) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1036) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1039) rcutree.c:1867: rdp->qlen = 0;
  (<0>,1041) rcutree.c:1867: rdp->qlen = 0;
  (<0>,1042) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1045) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1047) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1048) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1049) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1051) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1052) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1054) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1058)
  (<0>,1059)
  (<0>,1060) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1061) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1064) fake_sync.h:90: local_irq_restore(flags);
  (<0>,1067) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1069) fake_sched.h:43: return __running_cpu;
  (<0>,1073) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1075) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1079) fake_sched.h:43: return __running_cpu;
  (<0>,1083) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1092) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1094) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1096) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1100) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,1101) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,1103) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,1106) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,1107) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,1109) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,1111) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,1114) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,1115) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1116) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1124)
  (<0>,1125)
  (<0>,1126) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1128) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1130) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1132) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1133) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1136)
  (<0>,1137) rcutree.c:297: return &rsp->node[0];
  (<0>,1141) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1142) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1144) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1148)
  (<0>,1149)
  (<0>,1150) fake_sync.h:80: local_irq_save(flags);
  (<0>,1153) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1155) fake_sched.h:43: return __running_cpu;
  (<0>,1159) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1161) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1165) fake_sched.h:43: return __running_cpu;
  (<0>,1169) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1175) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1176) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1180) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1181) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1183) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1185) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1189) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1191) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1192) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,1194) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,1195) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1197) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1200) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1202) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1204) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1207) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1209) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1211) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1213) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1216) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1218) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1220) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1223) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1225) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1227) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1229) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1232) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1234) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1236) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1239) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1241) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1243) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1245) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1248) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1250) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1252) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1255) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1257) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1259) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1261) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1264) rcutree.c:1867: rdp->qlen = 0;
  (<0>,1266) rcutree.c:1867: rdp->qlen = 0;
  (<0>,1267) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1270) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1272) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1273) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1274) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1276) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1277) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1279) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1283)
  (<0>,1284)
  (<0>,1285) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1286) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1289) fake_sync.h:90: local_irq_restore(flags);
  (<0>,1292) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1294) fake_sched.h:43: return __running_cpu;
  (<0>,1298) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1300) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1304) fake_sched.h:43: return __running_cpu;
  (<0>,1308) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1317) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1319) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1321) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1331) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,1333) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,1336) rcutree.c:2108: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,1346)
  (<0>,1347) rcutree.c:1938: static int __cpuinit rcu_cpu_notify(struct notifier_block *self,
  (<0>,1348) rcutree.c:1939: unsigned long action, void *hcpu)
  (<0>,1349) rcutree.c:1939: unsigned long action, void *hcpu)
  (<0>,1351) rcutree.c:1941: long cpu = (long)hcpu;
  (<0>,1352) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1353) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1355) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1357) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1358) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1360) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1361) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1362) rcutree.c:1945: switch (action) {
  (<0>,1364) rcutree.c:1948: rcu_prepare_cpu(cpu);
  (<0>,1368)
  (<0>,1369) rcutree.c:1928: static void __cpuinit rcu_prepare_cpu(int cpu)
  (<0>,1378)
  (<0>,1379) rcutree.c:1882: rcu_init_percpu_data(int cpu, struct rcu_state *rsp, int preemptible)
  (<0>,1380) rcutree.c:1882: rcu_init_percpu_data(int cpu, struct rcu_state *rsp, int preemptible)
  (<0>,1381) rcutree.c:1882: rcu_init_percpu_data(int cpu, struct rcu_state *rsp, int preemptible)
  (<0>,1383) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1385) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1387) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1388) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1391)
  (<0>,1392) rcutree.c:297: return &rsp->node[0];
  (<0>,1396) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1397) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1399) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1403)
  (<0>,1404)
  (<0>,1405) fake_sync.h:80: local_irq_save(flags);
  (<0>,1408) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1410) fake_sched.h:43: return __running_cpu;
  (<0>,1414) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1416) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1420) fake_sched.h:43: return __running_cpu;
  (<0>,1424) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1430) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1431) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1435) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1437) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1438) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1440) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1441) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1443) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1444) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1446) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1449) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1450) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1452) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1453) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1455) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1456) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1458) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1459) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1461) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1463) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1464) rcutree.c:1898: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,1468)
  (<0>,1469) fake_sync.h:117: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,1470) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1475) rcutree.c:1906: raw_spin_lock(&rsp->onofflock);		/* irqs already disabled. */
  (<0>,1479) fake_sync.h:112: preempt_disable();
  (<0>,1481) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1482) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1486) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1488) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1489) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1490) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1492) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1493) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1495) rcutree.c:1913: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,1499) fake_sync.h:112: preempt_disable();
  (<0>,1501) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1502) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1506) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1507) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1509) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1511) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1512) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1514) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1515) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1516) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1517) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1519) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1522) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1524) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1525) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1527) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1528) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1530) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1531) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1533) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1534) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1536) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1538) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1540) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1542) rcutree.c:1921: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,1546)
  (<0>,1547) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1548) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1553) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1555) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1556) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1558) rcutree.c:1923: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,1562) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,1564) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,1568)
  (<0>,1569)
  (<0>,1570) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1571) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1574) fake_sync.h:90: local_irq_restore(flags);
  (<0>,1577) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1579) fake_sched.h:43: return __running_cpu;
  (<0>,1583) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1585) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1589) fake_sched.h:43: return __running_cpu;
  (<0>,1593) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1601) rcutree.c:1931: rcu_init_percpu_data(cpu, &rcu_bh_state, 0);
  (<0>,1610)
  (<0>,1611)
  (<0>,1612)
  (<0>,1613) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1615) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1617) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1619) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1620) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1623)
  (<0>,1624) rcutree.c:297: return &rsp->node[0];
  (<0>,1628) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1629) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1631) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1635)
  (<0>,1636)
  (<0>,1637) fake_sync.h:80: local_irq_save(flags);
  (<0>,1640) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1642) fake_sched.h:43: return __running_cpu;
  (<0>,1646) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1648) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1652) fake_sched.h:43: return __running_cpu;
  (<0>,1656) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1662) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1663) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1667) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1669) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1670) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1672) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1673) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1675) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1676) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1678) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1681) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1682) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1684) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1685) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1687) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1688) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1690) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1691) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1693) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1695) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1696) rcutree.c:1898: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,1700)
  (<0>,1701) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1702) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1707) rcutree.c:1906: raw_spin_lock(&rsp->onofflock);		/* irqs already disabled. */
  (<0>,1711) fake_sync.h:112: preempt_disable();
  (<0>,1713) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1714) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1718) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1720) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1721) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1722) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1724) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1725) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1727) rcutree.c:1913: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,1731) fake_sync.h:112: preempt_disable();
  (<0>,1733) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1734) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1738) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1739) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1741) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1743) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1744) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1746) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1747) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1748) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1749) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1751) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1754) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1756) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1757) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1759) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1760) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1762) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1763) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1765) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1766) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1768) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1770) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1772) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1774) rcutree.c:1921: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,1778)
  (<0>,1779) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1780) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1785) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1787) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1788) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1790) rcutree.c:1923: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,1794) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,1796) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,1800)
  (<0>,1801)
  (<0>,1802) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1803) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1806) fake_sync.h:90: local_irq_restore(flags);
  (<0>,1809) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1811) fake_sched.h:43: return __running_cpu;
  (<0>,1815) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1817) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1821) fake_sched.h:43: return __running_cpu;
  (<0>,1825) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1833) rcutree.c:1932: rcu_preempt_init_percpu_data(cpu);
  (<0>,1836) rcutree_plugin.h:1090: }
  (<0>,1839) rcutree.c:1949: rcu_prepare_kthreads(cpu);
  (<0>,1843) rcutree_plugin.h:1763: }
  (<0>,1848) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,1850) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,1852) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,1855) rcutree.c:2108: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,1865)
  (<0>,1866)
  (<0>,1867)
  (<0>,1868) rcutree.c:1941: long cpu = (long)hcpu;
  (<0>,1870) rcutree.c:1941: long cpu = (long)hcpu;
  (<0>,1871) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1872) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1874) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1876) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1877) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1879) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1880) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1881) rcutree.c:1945: switch (action) {
  (<0>,1883) rcutree.c:1948: rcu_prepare_cpu(cpu);
  (<0>,1887)
  (<0>,1888) rcutree.c:1930: rcu_init_percpu_data(cpu, &rcu_sched_state, 0);
  (<0>,1897)
  (<0>,1898)
  (<0>,1899)
  (<0>,1900) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1902) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1904) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1906) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1907) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1910)
  (<0>,1911) rcutree.c:297: return &rsp->node[0];
  (<0>,1915) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1916) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1918) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1922)
  (<0>,1923)
  (<0>,1924) fake_sync.h:80: local_irq_save(flags);
  (<0>,1927) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1929) fake_sched.h:43: return __running_cpu;
  (<0>,1933) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1935) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1939) fake_sched.h:43: return __running_cpu;
  (<0>,1943) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1949) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1950) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1954) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1956) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1957) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1959) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1960) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1962) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1963) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1965) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1968) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1969) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1971) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1972) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1974) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1975) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1977) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1978) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1980) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1982) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1983) rcutree.c:1898: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,1987)
  (<0>,1988) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1989) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1994) rcutree.c:1906: raw_spin_lock(&rsp->onofflock);		/* irqs already disabled. */
  (<0>,1998) fake_sync.h:112: preempt_disable();
  (<0>,2000) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2001) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2005) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2007) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2008) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2009) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2011) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2012) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2014) rcutree.c:1913: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2018) fake_sync.h:112: preempt_disable();
  (<0>,2020) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2021) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2025) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2026) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2028) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2030) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2031) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2033) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2034) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2035) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2036) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2038) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2041) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2043) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2044) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2046) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2047) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2049) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2050) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2052) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2053) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2055) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2057) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2059) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2061) rcutree.c:1921: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2065)
  (<0>,2066) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2067) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2072) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2074) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2075) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2077) rcutree.c:1923: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2081) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,2083) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,2087)
  (<0>,2088)
  (<0>,2089) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,2090) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,2093) fake_sync.h:90: local_irq_restore(flags);
  (<0>,2096) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2098) fake_sched.h:43: return __running_cpu;
  (<0>,2102) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2104) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2108) fake_sched.h:43: return __running_cpu;
  (<0>,2112) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2120) rcutree.c:1931: rcu_init_percpu_data(cpu, &rcu_bh_state, 0);
  (<0>,2129)
  (<0>,2130)
  (<0>,2131)
  (<0>,2132) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2134) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2136) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2138) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2139) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2142)
  (<0>,2143) rcutree.c:297: return &rsp->node[0];
  (<0>,2147) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2148) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2150) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2154)
  (<0>,2155)
  (<0>,2156) fake_sync.h:80: local_irq_save(flags);
  (<0>,2159) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2161) fake_sched.h:43: return __running_cpu;
  (<0>,2165) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2167) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2171) fake_sched.h:43: return __running_cpu;
  (<0>,2175) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2181) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,2182) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,2186) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,2188) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,2189) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,2191) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,2192) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2194) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2195) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,2197) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,2200) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,2201) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,2203) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,2204) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2206) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2207) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2209) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2210) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,2212) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,2214) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,2215) rcutree.c:1898: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2219)
  (<0>,2220) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2221) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2226) rcutree.c:1906: raw_spin_lock(&rsp->onofflock);		/* irqs already disabled. */
  (<0>,2230) fake_sync.h:112: preempt_disable();
  (<0>,2232) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2233) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2237) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2239) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2240) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2241) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2243) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2244) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2246) rcutree.c:1913: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2250) fake_sync.h:112: preempt_disable();
  (<0>,2252) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2253) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2257) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2258) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2260) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2262) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2263) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2265) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2266) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2267) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2268) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2270) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2273) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2275) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2276) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2278) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2279) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2281) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2282) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2284) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2285) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2287) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2289) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2291) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2293) rcutree.c:1921: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2297)
  (<0>,2298) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2299) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2304) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2306) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2307) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2309) rcutree.c:1923: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2313) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,2315) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,2319)
  (<0>,2320)
  (<0>,2321) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,2322) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,2325) fake_sync.h:90: local_irq_restore(flags);
  (<0>,2328) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2330) fake_sched.h:43: return __running_cpu;
  (<0>,2334) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2336) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2340) fake_sched.h:43: return __running_cpu;
  (<0>,2344) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2352) rcutree.c:1932: rcu_preempt_init_percpu_data(cpu);
  (<0>,2355) rcutree_plugin.h:1090: }
  (<0>,2358) rcutree.c:1949: rcu_prepare_kthreads(cpu);
  (<0>,2362) rcutree_plugin.h:1763: }
  (<0>,2367) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,2369) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,2371) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,2379) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2381) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2384) litmus_v3.c:114: set_cpu(i);
  (<0>,2387)
  (<0>,2388) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,2389) fake_sched.h:56: __running_cpu = cpu;
  (<0>,2399) rcutree.c:351: unsigned long flags;
  (<0>,2402) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2404) fake_sched.h:43: return __running_cpu;
  (<0>,2408) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2410) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2414) fake_sched.h:43: return __running_cpu;
  (<0>,2418) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2424) fake_sched.h:43: return __running_cpu;
  (<0>,2428) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,2429) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2431) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2433) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2437) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2440) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2441) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2442) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2444) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2445) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2447) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2450) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2456) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2457) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2460) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2465) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2466) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2467) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2468) rcutree.c:365: local_irq_restore(flags);
  (<0>,2471) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2473) fake_sched.h:43: return __running_cpu;
  (<0>,2477) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2479) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2483) fake_sched.h:43: return __running_cpu;
  (<0>,2487) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2494) fake_sched.h:43: return __running_cpu;
  (<0>,2498) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
  (<0>,2508) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2510) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2512) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2515) litmus_v3.c:114: set_cpu(i);
  (<0>,2518)
  (<0>,2519) fake_sched.h:56: __running_cpu = cpu;
  (<0>,2520) fake_sched.h:56: __running_cpu = cpu;
  (<0>,2530) rcutree.c:354: local_irq_save(flags);
  (<0>,2533) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2535) fake_sched.h:43: return __running_cpu;
  (<0>,2539) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2541) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2545) fake_sched.h:43: return __running_cpu;
  (<0>,2549) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2555) fake_sched.h:43: return __running_cpu;
  (<0>,2559) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,2560) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2562) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2564) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2568) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2571) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2572) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2573) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2575) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2576) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2578) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2581) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2587) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2588) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2591) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2596) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2597) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2598) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2599) rcutree.c:365: local_irq_restore(flags);
  (<0>,2602) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2604) fake_sched.h:43: return __running_cpu;
  (<0>,2608) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2610) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2614) fake_sched.h:43: return __running_cpu;
  (<0>,2618) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2625) fake_sched.h:43: return __running_cpu;
  (<0>,2629) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
  (<0>,2639) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2641) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2643) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2654)
  (<0>,2655) litmus_v3.c:46: void *thread_reader(void *arg)
  (<0>,2658)
  (<0>,2659) fake_sched.h:56: __running_cpu = cpu;
  (<0>,2660) fake_sched.h:56: __running_cpu = cpu;
  (<0>,2663) fake_sched.h:43: return __running_cpu;
  (<0>,2667)
  (<0>,2668) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,2671) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,2682) rcutree.c:383: unsigned long flags;
  (<0>,2685) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2687) fake_sched.h:43: return __running_cpu;
  (<0>,2691) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2693) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2697) fake_sched.h:43: return __running_cpu;
  (<0>,2701) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2707) fake_sched.h:43: return __running_cpu;
  (<0>,2711) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,2712) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2714) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2716) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2720) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2723) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2724) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2725) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2727) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2728) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2730) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2733) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2740) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2741) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2744) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2749) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2750) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2751) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2752) rcutree.c:397: local_irq_restore(flags);
  (<0>,2755) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2757) fake_sched.h:43: return __running_cpu;
  (<0>,2761) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2763) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2767) fake_sched.h:43: return __running_cpu;
  (<0>,2771) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2786) litmus_v3.c:52: r_x = x;
  (<0>,2787) litmus_v3.c:52: r_x = x;
  (<0>,2791) fake_sched.h:43: return __running_cpu;
  (<0>,2795) fake_sched.h:153: if (!local_irq_depth[get_cpu()]) {
  (<0>,2799) fake_sched.h:43: return __running_cpu;
  (<0>,2803) fake_sched.h:154: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2808) fake_sched.h:43: return __running_cpu;
  (<0>,2812) fake_sched.h:157: local_irq_depth[get_cpu()] = 1;
  (<0>,2824) rcutree.c:386: local_irq_save(flags);
  (<0>,2827) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2829) fake_sched.h:43: return __running_cpu;
  (<0>,2833) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2835) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2840) fake_sched.h:43: return __running_cpu;
  (<0>,2844) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,2845) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2847) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2849) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2852) rcutree.c:389: local_irq_restore(flags);
  (<0>,2855) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2857) fake_sched.h:43: return __running_cpu;
  (<0>,2861) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2863) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2872) fake_sched.h:43: return __running_cpu;
  (<0>,2877)
  (<0>,2878) rcutree.c:1290: void rcu_check_callbacks(int cpu, int user)
  (<0>,2879) rcutree.c:1290: void rcu_check_callbacks(int cpu, int user)
  (<0>,2882) rcutree.c:1320: rcu_bh_qs(cpu);
  (<0>,2886)
  (<0>,2887) rcutree.c:172: void rcu_bh_qs(int cpu)
  (<0>,2890) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
  (<0>,2891) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,2893) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,2895) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,2897) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,2899) rcutree.c:178: rdp->passed_quiesc = 1;
  (<0>,2901) rcutree.c:178: rdp->passed_quiesc = 1;
  (<0>,2904) rcutree.c:1322: rcu_preempt_check_callbacks(cpu);
  (<0>,2907) rcutree_plugin.h:1024: }
  (<0>,2909) rcutree.c:1323: if (rcu_pending(cpu))
  (<0>,2912)
  (<0>,2913) rcutree.c:1755: static int rcu_pending(int cpu)
  (<0>,2921)
  (<0>,2922) rcutree.c:1687: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,2923) rcutree.c:1687: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,2925) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,2926) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,2927) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,2929) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,2931) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,2932) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,2933) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,2940)
  (<0>,2941) rcutree.c:613: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,2942) rcutree.c:613: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,2945) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,2946) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,2947) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,2949) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,2950) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,2951) rcutree.c:623: rnp = rdp->mynode;
  (<0>,2953) rcutree.c:623: rnp = rdp->mynode;
  (<0>,2954) rcutree.c:623: rnp = rdp->mynode;
  (<0>,2955) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,2957) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,2958) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,2960) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,2964) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
  (<0>,2967)
  (<0>,2968) rcutree.c:150: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,2970) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,2971) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,2973) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,2981) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,2983) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,2986) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,2988) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,2991) rcutree.c:1704: rdp->n_rp_qs_pending++;
  (<0>,2993) rcutree.c:1704: rdp->n_rp_qs_pending++;
  (<0>,2995) rcutree.c:1704: rdp->n_rp_qs_pending++;
  (<0>,2996) rcutree.c:1705: if (!rdp->preemptible &&
  (<0>,2998) rcutree.c:1705: if (!rdp->preemptible &&
  (<0>,3001) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
  (<0>,3003) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
  (<0>,3005) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
  (<0>,3013) rcutree.c:1715: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,3016)
  (<0>,3017) rcutree.c:278: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,3019) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3022) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3028) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3029) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3033)
  (<0>,3034) rcutree.c:287: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3035) rcutree.c:287: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3038) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3039) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3046) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,3048) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,3049) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,3051) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,3054) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,3056) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,3057) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,3059) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,3062) rcutree.c:1739: if (rcu_gp_in_progress(rsp) &&
  (<0>,3065)
  (<0>,3066) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3068) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3069) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3071) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3077) rcutree.c:1746: rdp->n_rp_need_nothing++;
  (<0>,3079) rcutree.c:1746: rdp->n_rp_need_nothing++;
  (<0>,3081) rcutree.c:1746: rdp->n_rp_need_nothing++;
  (<0>,3082) rcutree.c:1747: return 0;
  (<0>,3084) rcutree.c:1748: }
  (<0>,3088) rcutree.c:1758: __rcu_pending(&rcu_bh_state, &rcu_bh_data[cpu]) ||
  (<0>,3096)
  (<0>,3097)
  (<0>,3098) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,3100) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,3101) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,3102) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,3104) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,3106) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,3107) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,3108) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,3115)
  (<0>,3116)
  (<0>,3117) rcutree.c:619: if (rcu_cpu_stall_suppress)
  (<0>,3120) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,3121) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,3122) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,3124) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,3125) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,3126) rcutree.c:623: rnp = rdp->mynode;
  (<0>,3128) rcutree.c:623: rnp = rdp->mynode;
  (<0>,3129) rcutree.c:623: rnp = rdp->mynode;
  (<0>,3130) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,3132) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,3133) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,3135) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,3139) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
  (<0>,3142)
  (<0>,3143) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3145) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3146) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3148) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3156) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,3158) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,3161) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,3163) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,3166) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,3168) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,3171) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,3173) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,3176) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,3178) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,3180) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,3181) rcutree.c:1711: return 1;
  (<0>,3183) rcutree.c:1748: }
  (<0>,3194) fake_sched.h:43: return __running_cpu;
  (<0>,3198) rcutree.c:1526: raise_softirq(RCU_SOFTIRQ);
  (<0>,3205) fake_sched.h:43: return __running_cpu;
  (<0>,3209) fake_sched.h:162: local_irq_depth[get_cpu()] = 0;
  (<0>,3211) fake_sched.h:43: return __running_cpu;
  (<0>,3215) fake_sched.h:163: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3221) fake_sched.h:43: return __running_cpu;
  (<0>,3225) fake_sched.h:192: if (need_softirq[get_cpu()]) {
  (<0>,3230) rcutree.c:1499: &rcu_sched_data[get_cpu()]);
  (<0>,3232) fake_sched.h:43: return __running_cpu;
  (<0>,3243)
  (<0>,3244) rcutree.c:1460: __rcu_process_callbacks(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3245) rcutree.c:1460: __rcu_process_callbacks(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3247) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3254) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3255) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3257) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3263) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3264) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3265) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3266) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3268) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3269) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3273) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,3274) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,3280)
  (<0>,3281) rcutree.c:781: rcu_process_gp_end(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3282) rcutree.c:781: rcu_process_gp_end(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3285) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3287) fake_sched.h:43: return __running_cpu;
  (<0>,3291) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3293) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3297) fake_sched.h:43: return __running_cpu;
  (<0>,3301) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3306) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3308) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3309) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3310) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3312) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3313) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3315) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3318) rcutree.c:790: local_irq_restore(flags);
  (<0>,3321) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3323) fake_sched.h:43: return __running_cpu;
  (<0>,3327) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3329) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3333) fake_sched.h:43: return __running_cpu;
  (<0>,3337) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3344) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,3345) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,3349)
  (<0>,3350) rcutree.c:1071: rcu_check_quiescent_state(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3351) rcutree.c:1071: rcu_check_quiescent_state(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3352) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,3358)
  (<0>,3359) rcutree.c:721: check_for_new_grace_period(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3360) rcutree.c:721: check_for_new_grace_period(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3361) rcutree.c:726: local_irq_save(flags);
  (<0>,3364) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3366) fake_sched.h:43: return __running_cpu;
  (<0>,3370) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3372) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3376) fake_sched.h:43: return __running_cpu;
  (<0>,3380) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3385) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3387) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3388) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3390) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3393) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,3394) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,3400)
  (<0>,3401) rcutree.c:699: static void note_new_gpnum(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3402) rcutree.c:699: static void note_new_gpnum(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3405) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3407) fake_sched.h:43: return __running_cpu;
  (<0>,3411) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3413) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3417) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3419) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3420) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3421) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3423) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3424) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
    (<0.0>,1)
    (<0.0>,2)
    (<0.0>,3) litmus_v3.c:75: set_cpu(cpu0);
    (<0.0>,6)
    (<0.0>,7) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,8) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,11) fake_sched.h:43: return __running_cpu;
    (<0.0>,15)
    (<0.0>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,30) rcutree.c:386: local_irq_save(flags);
    (<0.0>,33) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,35) fake_sched.h:43: return __running_cpu;
    (<0.0>,39) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,41) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,45) fake_sched.h:43: return __running_cpu;
    (<0.0>,49) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,55) fake_sched.h:43: return __running_cpu;
    (<0.0>,59) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,60) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,62) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,64) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,68) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,71) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,72) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,73) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,75) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,76) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,78) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,81) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,88) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,89) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,92) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,97) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,98) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,99) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,100) rcutree.c:397: local_irq_restore(flags);
    (<0.0>,103) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,105) fake_sched.h:43: return __running_cpu;
    (<0.0>,109) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,111) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,115) fake_sched.h:43: return __running_cpu;
    (<0.0>,119) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,127) litmus_v3.c:78: x = 1;
    (<0.0>,138) rcupdate.h:178: }
    (<0.0>,143)
    (<0.0>,144) fake_sync.h:260: x->done = 0;
    (<0.0>,146) fake_sync.h:260: x->done = 0;
    (<0.0>,154)
    (<0.0>,155)
    (<0.0>,156) rcutree.c:1601: __call_rcu(head, func, &rcu_sched_state);
    (<0.0>,157) rcutree.c:1601: __call_rcu(head, func, &rcu_sched_state);
    (<0.0>,166)
    (<0.0>,167)
    (<0.0>,168)
    (<0.0>,169) rcutree.c:1536: debug_rcu_head_queue(head);
    (<0.0>,172) rcupdate.h:808: }
    (<0.0>,174) rcutree.c:1537: head->func = func;
    (<0.0>,175) rcutree.c:1537: head->func = func;
    (<0.0>,178) rcutree.c:1537: head->func = func;
    (<0.0>,179) rcutree.c:1538: head->next = NULL;
    (<0.0>,181) rcutree.c:1538: head->next = NULL;
    (<0.0>,183) rcutree.c:1548: local_irq_save(flags);
    (<0.0>,186) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,188) fake_sched.h:43: return __running_cpu;
    (<0.0>,192) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,194) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,198) fake_sched.h:43: return __running_cpu;
    (<0.0>,202) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,208) fake_sched.h:43: return __running_cpu;
    (<0.0>,211) rcutree.c:1549: rdp = &rsp->rda[get_cpu()];
    (<0.0>,213) rcutree.c:1549: rdp = &rsp->rda[get_cpu()];
    (<0.0>,215) rcutree.c:1549: rdp = &rsp->rda[get_cpu()];
    (<0.0>,216) rcutree.c:1552: *rdp->nxttail[RCU_NEXT_TAIL] = head;
    (<0.0>,217) rcutree.c:1552: *rdp->nxttail[RCU_NEXT_TAIL] = head;
    (<0.0>,220) rcutree.c:1552: *rdp->nxttail[RCU_NEXT_TAIL] = head;
    (<0.0>,221) rcutree.c:1552: *rdp->nxttail[RCU_NEXT_TAIL] = head;
    (<0.0>,222) rcutree.c:1553: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
    (<0.0>,224) rcutree.c:1553: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
    (<0.0>,227) rcutree.c:1553: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
    (<0.0>,228) rcutree.c:1554: rdp->qlen++;
    (<0.0>,230) rcutree.c:1554: rdp->qlen++;
    (<0.0>,232) rcutree.c:1554: rdp->qlen++;
    (<0.0>,233) rcutree.c:1557: if (irqs_disabled_flags(flags)) {
    (<0.0>,236) fake_sched.h:169: return !!local_irq_depth[get_cpu()];
    (<0.0>,238) fake_sched.h:43: return __running_cpu;
    (<0.0>,242) fake_sched.h:169: return !!local_irq_depth[get_cpu()];
    (<0.0>,250) rcutree.c:1558: local_irq_restore(flags);
    (<0.0>,253) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,255) fake_sched.h:43: return __running_cpu;
    (<0.0>,259) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,261) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,280) fake_sync.h:266: might_sleep();
    (<0.0>,284) fake_sched.h:43: return __running_cpu;
    (<0.0>,288) fake_sched.h:96: rcu_enter_nohz();
    (<0.0>,297) rcutree.c:354: local_irq_save(flags);
    (<0.0>,300) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,302) fake_sched.h:43: return __running_cpu;
    (<0.0>,306) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,308) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,312) fake_sched.h:43: return __running_cpu;
    (<0.0>,316) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,322) fake_sched.h:43: return __running_cpu;
    (<0.0>,326) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,327) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,329) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,331) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,335) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,338) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,339) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,340) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,342) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,343) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,345) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,348) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,354) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,355) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,358) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,363) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,364) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,365) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,366) rcutree.c:365: local_irq_restore(flags);
    (<0.0>,369) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,371) fake_sched.h:43: return __running_cpu;
    (<0.0>,375) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,377) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,381) fake_sched.h:43: return __running_cpu;
    (<0.0>,385) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,392) fake_sched.h:43: return __running_cpu;
    (<0.0>,396) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
    (<0.0>,405) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,408) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,413) fake_sync.h:269: while (!x->done)
      (<0.1>,1)
      (<0.1>,2)
      (<0.1>,3) litmus_v3.c:91: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,30) rcutree.c:386: local_irq_save(flags);
      (<0.1>,33) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:43: return __running_cpu;
      (<0.1>,39) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,41) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,45) fake_sched.h:43: return __running_cpu;
      (<0.1>,49) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,55) fake_sched.h:43: return __running_cpu;
      (<0.1>,59) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,60) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,62) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,64) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,68) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,71) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,72) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,73) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,75) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,76) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,78) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,81) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,88) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,89) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,92) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,97) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,98) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,99) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,100) rcutree.c:397: local_irq_restore(flags);
      (<0.1>,103) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,105) fake_sched.h:43: return __running_cpu;
      (<0.1>,109) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,111) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,115) fake_sched.h:43: return __running_cpu;
      (<0.1>,119) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,130) fake_sched.h:43: return __running_cpu;
      (<0.1>,134) fake_sched.h:153: if (!local_irq_depth[get_cpu()]) {
      (<0.1>,138) fake_sched.h:43: return __running_cpu;
      (<0.1>,142) fake_sched.h:154: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,147) fake_sched.h:43: return __running_cpu;
      (<0.1>,151) fake_sched.h:157: local_irq_depth[get_cpu()] = 1;
      (<0.1>,163) rcutree.c:386: local_irq_save(flags);
      (<0.1>,166) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,168) fake_sched.h:43: return __running_cpu;
      (<0.1>,172) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,174) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,179) fake_sched.h:43: return __running_cpu;
      (<0.1>,183) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,184) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,186) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,188) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,191) rcutree.c:389: local_irq_restore(flags);
      (<0.1>,194) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,196) fake_sched.h:43: return __running_cpu;
      (<0.1>,200) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,202) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,211) fake_sched.h:43: return __running_cpu;
      (<0.1>,216)
      (<0.1>,217)
      (<0.1>,218) rcutree.c:1292: if (user ||
      (<0.1>,221) rcutree.c:1320: rcu_bh_qs(cpu);
      (<0.1>,225)
      (<0.1>,226) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,229) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,230) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,232) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,234) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,236) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,238) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,240) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,243) rcutree.c:1322: rcu_preempt_check_callbacks(cpu);
      (<0.1>,246) rcutree_plugin.h:1024: }
      (<0.1>,248) rcutree.c:1323: if (rcu_pending(cpu))
      (<0.1>,251)
      (<0.1>,252) rcutree.c:1757: return __rcu_pending(&rcu_sched_state, &rcu_sched_data[cpu]) ||
      (<0.1>,260)
      (<0.1>,261)
      (<0.1>,262) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,264) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,265) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,266) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,268) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,270) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,271) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,272) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,279)
      (<0.1>,280)
      (<0.1>,281) rcutree.c:619: if (rcu_cpu_stall_suppress)
      (<0.1>,284) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,285) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,286) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,288) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,289) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,290) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,292) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,293) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,294) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,296) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,297) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,299) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,303) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
      (<0.1>,306)
      (<0.1>,307) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,309) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,310) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,312) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,320) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,322) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,325) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,327) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,330) rcutree.c:1704: rdp->n_rp_qs_pending++;
      (<0.1>,332) rcutree.c:1704: rdp->n_rp_qs_pending++;
      (<0.1>,334) rcutree.c:1704: rdp->n_rp_qs_pending++;
      (<0.1>,335) rcutree.c:1705: if (!rdp->preemptible &&
      (<0.1>,337) rcutree.c:1705: if (!rdp->preemptible &&
      (<0.1>,340) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
      (<0.1>,342) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
      (<0.1>,344) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
      (<0.1>,352) rcutree.c:1715: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
      (<0.1>,355)
      (<0.1>,356) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,358) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,361) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,367) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,368) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,372)
      (<0.1>,373)
      (<0.1>,374) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,377) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,378) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,381) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,384)
      (<0.1>,385) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,387) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,388) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,390) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,401) rcutree.c:1722: rdp->n_rp_cpu_needs_gp++;
      (<0.1>,403) rcutree.c:1722: rdp->n_rp_cpu_needs_gp++;
      (<0.1>,405) rcutree.c:1722: rdp->n_rp_cpu_needs_gp++;
      (<0.1>,406) rcutree.c:1723: return 1;
      (<0.1>,408) rcutree.c:1748: }
      (<0.1>,419) fake_sched.h:43: return __running_cpu;
      (<0.1>,423) rcutree.c:1526: raise_softirq(RCU_SOFTIRQ);
      (<0.1>,430) fake_sched.h:43: return __running_cpu;
      (<0.1>,434) fake_sched.h:162: local_irq_depth[get_cpu()] = 0;
      (<0.1>,436) fake_sched.h:43: return __running_cpu;
      (<0.1>,440) fake_sched.h:163: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,446) fake_sched.h:43: return __running_cpu;
      (<0.1>,450) fake_sched.h:192: if (need_softirq[get_cpu()]) {
      (<0.1>,455) rcutree.c:1499: &rcu_sched_data[get_cpu()]);
      (<0.1>,457) fake_sched.h:43: return __running_cpu;
      (<0.1>,468)
      (<0.1>,469)
      (<0.1>,470) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,472) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,479) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,480) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,482) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,488) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,489) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,490) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,491) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,493) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,494) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,498) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,499) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,505)
      (<0.1>,506)
      (<0.1>,507) rcutree.c:786: local_irq_save(flags);
      (<0.1>,510) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,512) fake_sched.h:43: return __running_cpu;
      (<0.1>,516) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,518) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,522) fake_sched.h:43: return __running_cpu;
      (<0.1>,526) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,531) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,533) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,534) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,535) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,537) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,538) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,540) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,543) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,546) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,548) fake_sched.h:43: return __running_cpu;
      (<0.1>,552) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,554) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,558) fake_sched.h:43: return __running_cpu;
      (<0.1>,562) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,569) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,570) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,574)
      (<0.1>,575)
      (<0.1>,576) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,577) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,583)
      (<0.1>,584)
      (<0.1>,585) rcutree.c:724: int ret = 0;
      (<0.1>,586) rcutree.c:726: local_irq_save(flags);
      (<0.1>,589) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,591) fake_sched.h:43: return __running_cpu;
      (<0.1>,595) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,597) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,601) fake_sched.h:43: return __running_cpu;
      (<0.1>,605) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,610) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,612) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,613) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,615) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,618) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,619) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,625)
      (<0.1>,626)
      (<0.1>,627) rcutree.c:704: local_irq_save(flags);
      (<0.1>,630) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,632) fake_sched.h:43: return __running_cpu;
      (<0.1>,636) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,638) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,642) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,644) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,645) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,646) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,648) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,649) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,651) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,654) rcutree.c:708: local_irq_restore(flags);
      (<0.1>,657) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,659) fake_sched.h:43: return __running_cpu;
      (<0.1>,663) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,665) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,671) rcutree.c:729: ret = 1;
      (<0.1>,673) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,676) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,678) fake_sched.h:43: return __running_cpu;
      (<0.1>,682) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,684) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,688) fake_sched.h:43: return __running_cpu;
      (<0.1>,692) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,697) rcutree.c:732: return ret;
      (<0.1>,703) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,704) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,708)
      (<0.1>,709)
      (<0.1>,710) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,713) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,714) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,717) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,720)
      (<0.1>,721) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,723) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,724) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,726) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,737) rcutree.c:1484: raw_spin_lock_irqsave(&rcu_get_root(rsp)->lock, flags);
      (<0.1>,740)
      (<0.1>,741) rcutree.c:297: return &rsp->node[0];
      (<0.1>,746) rcutree.c:1484: raw_spin_lock_irqsave(&rcu_get_root(rsp)->lock, flags);
      (<0.1>,750)
      (<0.1>,751)
      (<0.1>,752) fake_sync.h:80: local_irq_save(flags);
      (<0.1>,755) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,757) fake_sched.h:43: return __running_cpu;
      (<0.1>,761) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,763) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,767) fake_sched.h:43: return __running_cpu;
      (<0.1>,771) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,777) fake_sync.h:82: if (pthread_mutex_lock(l))
      (<0.1>,778) fake_sync.h:82: if (pthread_mutex_lock(l))
      (<0.1>,782) rcutree.c:1485: rcu_start_gp(rsp, flags);  /* releases above lock */
      (<0.1>,783) rcutree.c:1485: rcu_start_gp(rsp, flags);  /* releases above lock */
      (<0.1>,792)
      (<0.1>,793) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,795) fake_sched.h:43: return __running_cpu;
      (<0.1>,798) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,800) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,802) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,803) rcutree.c:837: struct rcu_node *rnp = rcu_get_root(rsp);
      (<0.1>,806)
      (<0.1>,807) rcutree.c:297: return &rsp->node[0];
      (<0.1>,811) rcutree.c:837: struct rcu_node *rnp = rcu_get_root(rsp);
      (<0.1>,812) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,813) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,817)
      (<0.1>,818)
      (<0.1>,819) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,822) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,823) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,826) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,829)
      (<0.1>,830) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,832) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,833) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,835) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,846) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,848) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,852) rcutree.c:863: rsp->gpnum++;
      (<0.1>,854) rcutree.c:863: rsp->gpnum++;
      (<0.1>,856) rcutree.c:863: rsp->gpnum++;
      (<0.1>,857) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,859) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,865) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,866) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,868) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,873) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,874) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,875) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,876) rcutree.c:865: rsp->signaled = RCU_GP_INIT; /* Hold off force_quiescent_state. */
      (<0.1>,878) rcutree.c:865: rsp->signaled = RCU_GP_INIT; /* Hold off force_quiescent_state. */
      (<0.1>,879) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,881) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,883) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,884) rcutree.c:867: record_gp_stall_check_time(rsp);
      (<0.1>,887)
      (<0.1>,888) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,889) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,891) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,892) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,894) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,896) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,898) rcutree.c:871: rcu_preempt_check_blocked_tasks(rnp);
      (<0.1>,904)
      (<0.1>,905) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,907) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,912) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,913) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,915) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,919) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,920) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,921) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,923) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,925) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,926) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,928) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,929) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,931) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,932) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,934) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,935) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,937) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,938) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,940) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,941) rcutree.c:879: rsp->signaled = RCU_SIGNAL_INIT; /* force_quiescent_state OK. */
      (<0.1>,943) rcutree.c:879: rsp->signaled = RCU_SIGNAL_INIT; /* force_quiescent_state OK. */
      (<0.1>,944) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,945) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,946) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,951)
      (<0.1>,952)
      (<0.1>,953)
      (<0.1>,954) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,955) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,956) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,961)
      (<0.1>,962)
      (<0.1>,963)
      (<0.1>,964) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,966) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,967) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,969) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,972) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,975) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,976) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,979) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,980) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,983) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,984) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,987) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,988) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,991) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,992) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,995) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,996) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,998) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,999) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,1001) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,1002) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,1004) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,1005) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,1007) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,1011) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,1013) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,1014) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,1016) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,1022) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1025) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1026) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1029) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1030) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1033) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1034) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1037) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1038) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,1039) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,1040) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,1045)
      (<0.1>,1046)
      (<0.1>,1047)
      (<0.1>,1048) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,1050) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,1051) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,1053) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,1056) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,1058) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,1059) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,1061) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,1062) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,1064) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,1065) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,1067) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,1071) rcutree.c:688: rdp->qs_pending = 1;
      (<0.1>,1073) rcutree.c:688: rdp->qs_pending = 1;
      (<0.1>,1074) rcutree.c:690: rdp->passed_quiesc = 0;
      (<0.1>,1076) rcutree.c:690: rdp->passed_quiesc = 0;
      (<0.1>,1081) rcutree.c:881: rcu_preempt_boost_start_gp(rnp);
      (<0.1>,1084) rcutree_plugin.h:1736: }
      (<0.1>,1086) rcutree.c:882: raw_spin_unlock_irqrestore(&rnp->lock, flags);
      (<0.1>,1088) rcutree.c:882: raw_spin_unlock_irqrestore(&rnp->lock, flags);
      (<0.1>,1092)
      (<0.1>,1093)
      (<0.1>,1094) fake_sync.h:88: if (pthread_mutex_unlock(l))
      (<0.1>,1095) fake_sync.h:88: if (pthread_mutex_unlock(l))
      (<0.1>,1098) fake_sync.h:90: local_irq_restore(flags);
      (<0.1>,1101) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1103) fake_sched.h:43: return __running_cpu;
      (<0.1>,1107) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1109) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1113) fake_sched.h:43: return __running_cpu;
      (<0.1>,1117) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1127) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,1130)
      (<0.1>,1131) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1133) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1136) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1144) fake_sched.h:43: return __running_cpu;
      (<0.1>,1155)
      (<0.1>,1156)
      (<0.1>,1157) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1159) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1166) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1167) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1169) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1175) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1176) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1177) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1178) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,1180) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,1181) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,1185) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,1186) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,1192)
      (<0.1>,1193)
      (<0.1>,1194) rcutree.c:786: local_irq_save(flags);
      (<0.1>,1197) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1199) fake_sched.h:43: return __running_cpu;
      (<0.1>,1203) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1205) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1209) fake_sched.h:43: return __running_cpu;
      (<0.1>,1213) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1218) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,1220) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,1221) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,1222) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,1224) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,1225) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,1227) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,1230) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,1233) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1235) fake_sched.h:43: return __running_cpu;
      (<0.1>,1239) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1241) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1245) fake_sched.h:43: return __running_cpu;
      (<0.1>,1249) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1256) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,1257) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,1261)
      (<0.1>,1262)
      (<0.1>,1263) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,1264) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,1270)
      (<0.1>,1271)
      (<0.1>,1272) rcutree.c:724: int ret = 0;
      (<0.1>,1273) rcutree.c:726: local_irq_save(flags);
      (<0.1>,1276) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1278) fake_sched.h:43: return __running_cpu;
      (<0.1>,1282) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1284) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1288) fake_sched.h:43: return __running_cpu;
      (<0.1>,1292) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1297) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,1299) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,1300) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,1302) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,1305) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,1306) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,1312)
      (<0.1>,1313)
      (<0.1>,1314) rcutree.c:704: local_irq_save(flags);
      (<0.1>,1317) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1319) fake_sched.h:43: return __running_cpu;
      (<0.1>,1323) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1325) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1329) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,1331) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,1332) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,1333) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,1335) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,1336) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,1338) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,1341) rcutree.c:708: local_irq_restore(flags);
      (<0.1>,1344) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1346) fake_sched.h:43: return __running_cpu;
      (<0.1>,1350) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1352) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1358) rcutree.c:729: ret = 1;
      (<0.1>,1360) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,1363) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1365) fake_sched.h:43: return __running_cpu;
      (<0.1>,1369) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1371) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1375) fake_sched.h:43: return __running_cpu;
      (<0.1>,1379) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1384) rcutree.c:732: return ret;
      (<0.1>,1390) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,1391) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,1395)
      (<0.1>,1396)
      (<0.1>,1397) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,1400) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,1401) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,1408) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,1411)
      (<0.1>,1412) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1414) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1417) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1430) fake_sched.h:43: return __running_cpu;
      (<0.1>,1434) fake_sched.h:194: need_softirq[get_cpu()] = 0;
      (<0.1>,1445) rcutree.c:354: local_irq_save(flags);
      (<0.1>,1448) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1450) fake_sched.h:43: return __running_cpu;
      (<0.1>,1454) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1456) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1460) fake_sched.h:43: return __running_cpu;
      (<0.1>,1464) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1470) fake_sched.h:43: return __running_cpu;
      (<0.1>,1474) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1475) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1477) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1479) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1482) rcutree.c:357: local_irq_restore(flags);
      (<0.1>,1485) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1487) fake_sched.h:43: return __running_cpu;
      (<0.1>,1491) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1493) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1497) fake_sched.h:43: return __running_cpu;
      (<0.1>,1501) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1513) fake_sched.h:43: return __running_cpu;
      (<0.1>,1517)
      (<0.1>,1518) rcutree.c:187: rcu_sched_qs(cpu);
      (<0.1>,1522)
      (<0.1>,1523) rcutree.c:165: struct rcu_data *rdp = &rcu_sched_data[cpu];
      (<0.1>,1526) rcutree.c:165: struct rcu_data *rdp = &rcu_sched_data[cpu];
      (<0.1>,1527) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1529) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1531) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1533) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1535) rcutree.c:169: rdp->passed_quiesc = 1;
      (<0.1>,1537) rcutree.c:169: rdp->passed_quiesc = 1;
      (<0.1>,1539) rcutree.c:188: rcu_preempt_note_context_switch(cpu);
      (<0.1>,1542) rcutree_plugin.h:938: }
      (<0.1>,1546) fake_sched.h:43: return __running_cpu;
      (<0.1>,1550) fake_sched.h:96: rcu_enter_nohz();
      (<0.1>,1559) rcutree.c:354: local_irq_save(flags);
      (<0.1>,1562) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1564) fake_sched.h:43: return __running_cpu;
      (<0.1>,1568) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1570) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1574) fake_sched.h:43: return __running_cpu;
      (<0.1>,1578) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1584) fake_sched.h:43: return __running_cpu;
      (<0.1>,1588) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1589) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1591) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1593) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1597) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1600) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1601) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1602) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1604) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1605) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1607) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1610) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1616) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1617) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1620) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1625) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1626) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1627) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1628) rcutree.c:365: local_irq_restore(flags);
      (<0.1>,1631) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1633) fake_sched.h:43: return __running_cpu;
      (<0.1>,1637) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1639) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1643) fake_sched.h:43: return __running_cpu;
      (<0.1>,1647) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1654) fake_sched.h:43: return __running_cpu;
      (<0.1>,1658) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
      (<0.1>,1667) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1670) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1675) fake_sched.h:43: return __running_cpu;
      (<0.1>,1679)
      (<0.1>,1680) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,1683) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,1694) rcutree.c:386: local_irq_save(flags);
      (<0.1>,1697) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1699) fake_sched.h:43: return __running_cpu;
      (<0.1>,1703) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1705) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1709) fake_sched.h:43: return __running_cpu;
      (<0.1>,1713) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1719) fake_sched.h:43: return __running_cpu;
      (<0.1>,1723) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1724) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1726) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1728) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1732) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1735) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1736) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1737) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1739) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1740) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1742) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1745) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1752) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1753) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1756) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1761) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1762) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1763) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1764) rcutree.c:397: local_irq_restore(flags);
      (<0.1>,1767) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1769) fake_sched.h:43: return __running_cpu;
      (<0.1>,1773) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1775) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1779) fake_sched.h:43: return __running_cpu;
      (<0.1>,1783) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1795) fake_sched.h:43: return __running_cpu;
      (<0.1>,1799) fake_sched.h:153: if (!local_irq_depth[get_cpu()]) {
      (<0.1>,1803) fake_sched.h:43: return __running_cpu;
      (<0.1>,1807) fake_sched.h:154: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1812) fake_sched.h:43: return __running_cpu;
      (<0.1>,1816) fake_sched.h:157: local_irq_depth[get_cpu()] = 1;
      (<0.1>,1828) rcutree.c:386: local_irq_save(flags);
      (<0.1>,1831) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1833) fake_sched.h:43: return __running_cpu;
      (<0.1>,1837) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1839) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1844) fake_sched.h:43: return __running_cpu;
      (<0.1>,1848) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1849) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1851) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1853) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1856) rcutree.c:389: local_irq_restore(flags);
      (<0.1>,1859) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1861) fake_sched.h:43: return __running_cpu;
      (<0.1>,1865) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1867) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1876) fake_sched.h:43: return __running_cpu;
      (<0.1>,1881)
      (<0.1>,1882)
      (<0.1>,1883) rcutree.c:1292: if (user ||
      (<0.1>,1886) rcutree.c:1320: rcu_bh_qs(cpu);
      (<0.1>,1890)
      (<0.1>,1891) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,1894) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,1895) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1897) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1899) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1901) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1903) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,1905) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,1908) rcutree.c:1322: rcu_preempt_check_callbacks(cpu);
      (<0.1>,1911) rcutree_plugin.h:1024: }
      (<0.1>,1913) rcutree.c:1323: if (rcu_pending(cpu))
      (<0.1>,1916)
      (<0.1>,1917) rcutree.c:1757: return __rcu_pending(&rcu_sched_state, &rcu_sched_data[cpu]) ||
      (<0.1>,1925)
      (<0.1>,1926)
      (<0.1>,1927) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,1929) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,1930) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,1931) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,1933) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,1935) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,1936) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,1937) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,1944)
      (<0.1>,1945)
      (<0.1>,1946) rcutree.c:619: if (rcu_cpu_stall_suppress)
      (<0.1>,1949) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,1950) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,1951) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,1953) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,1954) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,1955) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,1957) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,1958) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,1959) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1961) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1962) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1964) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1968) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1969) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1973) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
      (<0.1>,1976)
      (<0.1>,1977) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,1979) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,1980) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,1982) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,1988) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
      (<0.1>,1989) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
      (<0.1>,1996) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,1998) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,2001) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,2003) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,2006) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,2008) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,2011) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,2013) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,2016) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,2018) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,2020) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,2021) rcutree.c:1711: return 1;
      (<0.1>,2023) rcutree.c:1748: }
      (<0.1>,2034) fake_sched.h:43: return __running_cpu;
      (<0.1>,2038) rcutree.c:1526: raise_softirq(RCU_SOFTIRQ);
      (<0.1>,2045) fake_sched.h:43: return __running_cpu;
      (<0.1>,2049) fake_sched.h:162: local_irq_depth[get_cpu()] = 0;
      (<0.1>,2051) fake_sched.h:43: return __running_cpu;
      (<0.1>,2055) fake_sched.h:163: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,2061) fake_sched.h:43: return __running_cpu;
      (<0.1>,2065) fake_sched.h:192: if (need_softirq[get_cpu()]) {
      (<0.1>,2070) rcutree.c:1499: &rcu_sched_data[get_cpu()]);
      (<0.1>,2072) fake_sched.h:43: return __running_cpu;
      (<0.1>,2083)
      (<0.1>,2084)
      (<0.1>,2085) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2087) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2094) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2095) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2097) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2103) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2104) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2105) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2106) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,2108) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,2109) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,2113) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,2114) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,2120)
      (<0.1>,2121)
      (<0.1>,2122) rcutree.c:786: local_irq_save(flags);
      (<0.1>,2125) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2127) fake_sched.h:43: return __running_cpu;
      (<0.1>,2131) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2133) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2137) fake_sched.h:43: return __running_cpu;
      (<0.1>,2141) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,2146) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,2148) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,2149) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,2150) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2152) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2153) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2155) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2158) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,2161) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2163) fake_sched.h:43: return __running_cpu;
      (<0.1>,2167) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2169) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2173) fake_sched.h:43: return __running_cpu;
      (<0.1>,2177) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,2184) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,2185) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,2189)
      (<0.1>,2190)
      (<0.1>,2191) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,2192) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,2198)
      (<0.1>,2199)
      (<0.1>,2200) rcutree.c:724: int ret = 0;
      (<0.1>,2201) rcutree.c:726: local_irq_save(flags);
      (<0.1>,2204) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2206) fake_sched.h:43: return __running_cpu;
      (<0.1>,2210) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2212) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2216) fake_sched.h:43: return __running_cpu;
      (<0.1>,2220) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,2225) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2227) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2228) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2230) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2233) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,2236) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2238) fake_sched.h:43: return __running_cpu;
      (<0.1>,2242) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2244) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2248) fake_sched.h:43: return __running_cpu;
      (<0.1>,2252) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,2257) rcutree.c:732: return ret;
      (<0.1>,2261) rcutree.c:1081: if (!rdp->qs_pending)
      (<0.1>,2263) rcutree.c:1081: if (!rdp->qs_pending)
      (<0.1>,2266) rcutree.c:1088: if (!rdp->passed_quiesc)
      (<0.1>,2268) rcutree.c:1088: if (!rdp->passed_quiesc)
      (<0.1>,2271) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2273) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2274) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2275) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2276) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2278) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2287)
      (<0.1>,2288)
      (<0.1>,2289)
      (<0.1>,2290)
      (<0.1>,2291) rcutree.c:1032: rnp = rdp->mynode;
      (<0.1>,2293) rcutree.c:1032: rnp = rdp->mynode;
      (<0.1>,2294) rcutree.c:1032: rnp = rdp->mynode;
      (<0.1>,2295) rcutree.c:1033: raw_spin_lock_irqsave(&rnp->lock, flags);
      (<0.1>,2297) rcutree.c:1033: raw_spin_lock_irqsave(&rnp->lock, flags);
      (<0.1>,2301)
      (<0.1>,2302)
      (<0.1>,2303) fake_sync.h:80: local_irq_save(flags);
      (<0.1>,2306) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2308) fake_sched.h:43: return __running_cpu;
      (<0.1>,2312) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2314) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2318) fake_sched.h:43: return __running_cpu;
      (<0.1>,2322) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,2328) fake_sync.h:82: if (pthread_mutex_lock(l))
      (<0.1>,2329) fake_sync.h:82: if (pthread_mutex_lock(l))
      (<0.1>,2333) rcutree.c:1034: if (lastcomp != rnp->completed) {
      (<0.1>,2334) rcutree.c:1034: if (lastcomp != rnp->completed) {
      (<0.1>,2336) rcutree.c:1034: if (lastcomp != rnp->completed) {
      (<0.1>,2339) rcutree.c:1048: mask = rdp->grpmask;
      (<0.1>,2341) rcutree.c:1048: mask = rdp->grpmask;
      (<0.1>,2342) rcutree.c:1048: mask = rdp->grpmask;
      (<0.1>,2343) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
      (<0.1>,2345) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
      (<0.1>,2346) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
      (<0.1>,2350) rcutree.c:1052: rdp->qs_pending = 0;
      (<0.1>,2352) rcutree.c:1052: rdp->qs_pending = 0;
      (<0.1>,2353) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2356) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2357) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2360) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2361) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
      (<0.1>,2362) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
      (<0.1>,2363) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
      (<0.1>,2364) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
      (<0.1>,2374)
      (<0.1>,2375)
      (<0.1>,2376)
      (<0.1>,2377) rcutree.c:978: for (;;) {
      (<0.1>,2379) rcutree.c:979: if (!(rnp->qsmask & mask)) {
      (<0.1>,2381) rcutree.c:979: if (!(rnp->qsmask & mask)) {
      (<0.1>,2382) rcutree.c:979: if (!(rnp->qsmask & mask)) {
      (<0.1>,2386) rcutree.c:985: rnp->qsmask &= ~mask;
      (<0.1>,2388) rcutree.c:985: rnp->qsmask &= ~mask;
      (<0.1>,2390) rcutree.c:985: rnp->qsmask &= ~mask;
      (<0.1>,2392) rcutree.c:985: rnp->qsmask &= ~mask;
      (<0.1>,2393) rcutree.c:987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
      (<0.1>,2395) rcutree.c:987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
      (<0.1>,2398) rcutree.c:990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
      (<0.1>,2400) rcutree.c:990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
      (<0.1>,2404)
      (<0.1>,2405)
      (<0.1>,2406) fake_sync.h:88: if (pthread_mutex_unlock(l))
      (<0.1>,2407) fake_sync.h:88: if (pthread_mutex_unlock(l))
      (<0.1>,2410) fake_sync.h:90: local_irq_restore(flags);
      (<0.1>,2413) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2415) fake_sched.h:43: return __running_cpu;
      (<0.1>,2419) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2421) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2425) fake_sched.h:43: return __running_cpu;
      (<0.1>,2429) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,2442) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,2443) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,2447)
      (<0.1>,2448)
      (<0.1>,2449) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2452) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2453) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2456) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2459)
      (<0.1>,2460) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2462) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2463) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2465) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2476) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,2479)
      (<0.1>,2480) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2482) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2485) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2493) fake_sched.h:43: return __running_cpu;
      (<0.1>,2504)
      (<0.1>,2505)
      (<0.1>,2506) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2508) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2515) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2516) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2518) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2524) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2525) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2526) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2527) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,2529) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,2530) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,2534) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,2535) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,2541)
      (<0.1>,2542)
      (<0.1>,2543) rcutree.c:786: local_irq_save(flags);
      (<0.1>,2546) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2548) fake_sched.h:43: return __running_cpu;
      (<0.1>,2552) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2554) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2558) fake_sched.h:43: return __running_cpu;
      (<0.1>,2562) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,2567) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,2569) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,2570) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,2571) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2573) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2574) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2576) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2579) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,2582) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2584) fake_sched.h:43: return __running_cpu;
      (<0.1>,2588) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2590) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2594) fake_sched.h:43: return __running_cpu;
      (<0.1>,2598) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,2605) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,2606) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,2610)
      (<0.1>,2611)
      (<0.1>,2612) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,2613) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,2619)
      (<0.1>,2620)
      (<0.1>,2621) rcutree.c:724: int ret = 0;
      (<0.1>,2622) rcutree.c:726: local_irq_save(flags);
      (<0.1>,2625) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2627) fake_sched.h:43: return __running_cpu;
      (<0.1>,2631) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2633) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2637) fake_sched.h:43: return __running_cpu;
      (<0.1>,2641) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,2646) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2648) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2649) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2651) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2654) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,2655) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,2661)
      (<0.1>,2662)
      (<0.1>,2663) rcutree.c:704: local_irq_save(flags);
      (<0.1>,2666) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2668) fake_sched.h:43: return __running_cpu;
      (<0.1>,2672) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2674) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2678) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,2680) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,2681) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,2682) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,2684) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,2685) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,2687) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,2690) rcutree.c:708: local_irq_restore(flags);
      (<0.1>,2693) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2695) fake_sched.h:43: return __running_cpu;
      (<0.1>,2699) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2701) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2707) rcutree.c:729: ret = 1;
      (<0.1>,2709) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,2712) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2714) fake_sched.h:43: return __running_cpu;
      (<0.1>,2718) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2720) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2724) fake_sched.h:43: return __running_cpu;
      (<0.1>,2728) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,2733) rcutree.c:732: return ret;
      (<0.1>,2739) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,2740) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,2744)
      (<0.1>,2745)
      (<0.1>,2746) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2749) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2750) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2757) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,2760)
      (<0.1>,2761) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2763) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2766) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2779) fake_sched.h:43: return __running_cpu;
      (<0.1>,2783) fake_sched.h:194: need_softirq[get_cpu()] = 0;
      (<0.1>,2794) rcutree.c:354: local_irq_save(flags);
      (<0.1>,2797) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2799) fake_sched.h:43: return __running_cpu;
      (<0.1>,2803) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2805) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2809) fake_sched.h:43: return __running_cpu;
      (<0.1>,2813) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,2819) fake_sched.h:43: return __running_cpu;
      (<0.1>,2823) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,2824) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,2826) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,2828) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,2831) rcutree.c:357: local_irq_restore(flags);
      (<0.1>,2834) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2836) fake_sched.h:43: return __running_cpu;
      (<0.1>,2840) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2842) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2846) fake_sched.h:43: return __running_cpu;
      (<0.1>,2850) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,2863) fake_sched.h:43: return __running_cpu;
      (<0.1>,2867) fake_sched.h:153: if (!local_irq_depth[get_cpu()]) {
      (<0.1>,2871) fake_sched.h:43: return __running_cpu;
      (<0.1>,2875) fake_sched.h:154: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,2880) fake_sched.h:43: return __running_cpu;
      (<0.1>,2884) fake_sched.h:157: local_irq_depth[get_cpu()] = 1;
      (<0.1>,2896) rcutree.c:386: local_irq_save(flags);
      (<0.1>,2899) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2901) fake_sched.h:43: return __running_cpu;
      (<0.1>,2905) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2907) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2912) fake_sched.h:43: return __running_cpu;
      (<0.1>,2916) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,2917) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,2919) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,2921) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,2924) rcutree.c:389: local_irq_restore(flags);
      (<0.1>,2927) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2929) fake_sched.h:43: return __running_cpu;
      (<0.1>,2933) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2935) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2944) fake_sched.h:43: return __running_cpu;
      (<0.1>,2949)
      (<0.1>,2950)
      (<0.1>,2951) rcutree.c:1292: if (user ||
      (<0.1>,2954) rcutree.c:1320: rcu_bh_qs(cpu);
      (<0.1>,2958)
      (<0.1>,2959) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,2962) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,2963) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,2965) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,2967) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,2969) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,2971) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,2973) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,2976) rcutree.c:1322: rcu_preempt_check_callbacks(cpu);
      (<0.1>,2979) rcutree_plugin.h:1024: }
      (<0.1>,2981) rcutree.c:1323: if (rcu_pending(cpu))
      (<0.1>,2984)
      (<0.1>,2985) rcutree.c:1757: return __rcu_pending(&rcu_sched_state, &rcu_sched_data[cpu]) ||
      (<0.1>,2993)
      (<0.1>,2994)
      (<0.1>,2995) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,2997) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,2998) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,2999) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,3001) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,3003) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,3004) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,3005) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,3012)
      (<0.1>,3013)
      (<0.1>,3014) rcutree.c:619: if (rcu_cpu_stall_suppress)
      (<0.1>,3017) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,3018) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,3019) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,3021) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,3022) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,3023) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,3025) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,3026) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,3027) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3029) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3030) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3032) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3036) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
      (<0.1>,3039)
      (<0.1>,3040) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3042) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3043) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3045) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3051) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
      (<0.1>,3052) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
      (<0.1>,3059) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,3061) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,3064) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,3066) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,3070) rcutree.c:1715: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
      (<0.1>,3073)
      (<0.1>,3074) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3076) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3079) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3085) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,3086) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,3090)
      (<0.1>,3091)
      (<0.1>,3092) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3095) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3096) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3099) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3102)
      (<0.1>,3103) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3105) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3106) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3108) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3119) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
      (<0.1>,3121) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
      (<0.1>,3122) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
      (<0.1>,3124) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
      (<0.1>,3127) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,3129) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,3130) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,3132) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,3135) rcutree.c:1739: if (rcu_gp_in_progress(rsp) &&
      (<0.1>,3138)
      (<0.1>,3139) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3141) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3142) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3144) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3150) rcutree.c:1740: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies)) {
      (<0.1>,3152) rcutree.c:1740: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies)) {
      (<0.1>,3153) rcutree.c:1740: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies)) {
      (<0.1>,3157) rcutree.c:1746: rdp->n_rp_need_nothing++;
      (<0.1>,3159) rcutree.c:1746: rdp->n_rp_need_nothing++;
      (<0.1>,3161) rcutree.c:1746: rdp->n_rp_need_nothing++;
      (<0.1>,3162) rcutree.c:1747: return 0;
      (<0.1>,3164) rcutree.c:1748: }
      (<0.1>,3168) rcutree.c:1758: __rcu_pending(&rcu_bh_state, &rcu_bh_data[cpu]) ||
      (<0.1>,3176)
      (<0.1>,3177)
      (<0.1>,3178) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,3180) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,3181) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,3182) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,3184) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,3186) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,3187) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,3188) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,3195)
      (<0.1>,3196)
      (<0.1>,3197) rcutree.c:619: if (rcu_cpu_stall_suppress)
      (<0.1>,3200) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,3201) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,3202) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,3204) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,3205) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,3206) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,3208) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,3209) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,3210) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3212) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3213) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3215) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3219) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
      (<0.1>,3222)
      (<0.1>,3223) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3225) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3226) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3228) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3236) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,3238) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,3241) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,3243) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,3246) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,3248) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,3251) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,3253) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,3256) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,3258) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,3260) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,3261) rcutree.c:1711: return 1;
      (<0.1>,3263) rcutree.c:1748: }
      (<0.1>,3274) fake_sched.h:43: return __running_cpu;
      (<0.1>,3278) rcutree.c:1526: raise_softirq(RCU_SOFTIRQ);
      (<0.1>,3285) fake_sched.h:43: return __running_cpu;
      (<0.1>,3289) fake_sched.h:162: local_irq_depth[get_cpu()] = 0;
      (<0.1>,3291) fake_sched.h:43: return __running_cpu;
      (<0.1>,3295) fake_sched.h:163: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,3301) fake_sched.h:43: return __running_cpu;
      (<0.1>,3305) fake_sched.h:192: if (need_softirq[get_cpu()]) {
      (<0.1>,3310) rcutree.c:1499: &rcu_sched_data[get_cpu()]);
      (<0.1>,3312) fake_sched.h:43: return __running_cpu;
      (<0.1>,3323)
      (<0.1>,3324)
      (<0.1>,3325) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3327) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3334) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3335) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3337) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3343) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3344) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3345) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3346) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,3348) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,3349) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,3353) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,3354) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,3360)
      (<0.1>,3361)
      (<0.1>,3362) rcutree.c:786: local_irq_save(flags);
      (<0.1>,3365) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3367) fake_sched.h:43: return __running_cpu;
      (<0.1>,3371) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3373) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3377) fake_sched.h:43: return __running_cpu;
      (<0.1>,3381) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,3386) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,3388) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,3389) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,3390) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,3392) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,3393) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,3395) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,3398) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,3401) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3403) fake_sched.h:43: return __running_cpu;
      (<0.1>,3407) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3409) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3413) fake_sched.h:43: return __running_cpu;
      (<0.1>,3417) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,3424) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,3425) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,3429)
      (<0.1>,3430)
      (<0.1>,3431) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,3432) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,3438)
      (<0.1>,3439)
      (<0.1>,3440) rcutree.c:724: int ret = 0;
      (<0.1>,3441) rcutree.c:726: local_irq_save(flags);
      (<0.1>,3444) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3446) fake_sched.h:43: return __running_cpu;
      (<0.1>,3450) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3452) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3456) fake_sched.h:43: return __running_cpu;
      (<0.1>,3460) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,3465) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,3467) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,3468) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,3470) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,3473) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,3476) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3478) fake_sched.h:43: return __running_cpu;
      (<0.1>,3482) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3484) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3488) fake_sched.h:43: return __running_cpu;
      (<0.1>,3492) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,3497) rcutree.c:732: return ret;
      (<0.1>,3501) rcutree.c:1081: if (!rdp->qs_pending)
      (<0.1>,3503) rcutree.c:1081: if (!rdp->qs_pending)
      (<0.1>,3508) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,3509) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,3513)
      (<0.1>,3514)
      (<0.1>,3515) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3518) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3519) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3522) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3525)
      (<0.1>,3526) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3426) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3429) rcutree.c:707: !raw_spin_trylock(&rnp->lock)) { /* irqs already off, so later. */
  (<0>,3434) fake_sync.h:126: preempt_disable();
  (<0>,3436) fake_sync.h:127: if (pthread_mutex_trylock(l)) {
  (<0>,3437) fake_sync.h:127: if (pthread_mutex_trylock(l)) {
  (<0>,3440) fake_sync.h:131: return 1;
  (<0>,3442) fake_sync.h:132: }
  (<0>,3446) rcutree.c:711: __note_new_gpnum(rsp, rnp, rdp);
  (<0>,3447) rcutree.c:711: __note_new_gpnum(rsp, rnp, rdp);
  (<0>,3448) rcutree.c:711: __note_new_gpnum(rsp, rnp, rdp);
  (<0>,3453)
  (<0>,3454)
  (<0>,3455)
  (<0>,3456) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
  (<0>,3458) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
  (<0>,3459) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
  (<0>,3461) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
  (<0>,3464) rcutree.c:683: rdp->gpnum = rnp->gpnum;
  (<0>,3466) rcutree.c:683: rdp->gpnum = rnp->gpnum;
  (<0>,3467) rcutree.c:683: rdp->gpnum = rnp->gpnum;
  (<0>,3469) rcutree.c:683: rdp->gpnum = rnp->gpnum;
  (<0>,3470) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
  (<0>,3472) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
  (<0>,3473) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
  (<0>,3475) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
  (<0>,3479) rcutree.c:688: rdp->qs_pending = 1;
  (<0>,3481) rcutree.c:688: rdp->qs_pending = 1;
  (<0>,3482) rcutree.c:690: rdp->passed_quiesc = 0;
  (<0>,3484) rcutree.c:690: rdp->passed_quiesc = 0;
  (<0>,3488) rcutree.c:712: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,3490) rcutree.c:712: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,3494)
  (<0>,3495)
  (<0>,3496) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,3497) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,3500) fake_sync.h:90: local_irq_restore(flags);
  (<0>,3503) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3505) fake_sched.h:43: return __running_cpu;
  (<0>,3509) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3511) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3519) rcutree.c:729: ret = 1;
  (<0>,3521) rcutree.c:731: local_irq_restore(flags);
  (<0>,3524) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3526) fake_sched.h:43: return __running_cpu;
  (<0>,3530) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3532) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3536) fake_sched.h:43: return __running_cpu;
  (<0>,3540) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3545) rcutree.c:732: return ret;
  (<0>,3551) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3552) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3556)
  (<0>,3557)
  (<0>,3558) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3561) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3562) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3569) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,3572)
  (<0>,3573) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3575) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3578) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3586) fake_sched.h:43: return __running_cpu;
  (<0>,3597)
  (<0>,3598)
  (<0>,3599) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3601) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3608) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3609) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3611) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3617) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3618) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3619) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3620) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3622) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3623) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3627) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,3628) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,3634)
  (<0>,3635)
  (<0>,3636) rcutree.c:786: local_irq_save(flags);
  (<0>,3639) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3641) fake_sched.h:43: return __running_cpu;
  (<0>,3645) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3647) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3651) fake_sched.h:43: return __running_cpu;
  (<0>,3655) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3660) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3662) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3663) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3664) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3666) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3667) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3669) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3672) rcutree.c:790: local_irq_restore(flags);
  (<0>,3675) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3677) fake_sched.h:43: return __running_cpu;
  (<0>,3681) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3683) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3687) fake_sched.h:43: return __running_cpu;
  (<0>,3691) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3698) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,3699) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,3703)
  (<0>,3704)
  (<0>,3705) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,3706) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,3712)
  (<0>,3713)
  (<0>,3714) rcutree.c:724: int ret = 0;
  (<0>,3715) rcutree.c:726: local_irq_save(flags);
  (<0>,3718) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3720) fake_sched.h:43: return __running_cpu;
  (<0>,3724) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3726) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3730) fake_sched.h:43: return __running_cpu;
  (<0>,3734) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3739) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3741) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3742) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3744) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3747) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,3748) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,3754)
  (<0>,3755)
  (<0>,3756) rcutree.c:704: local_irq_save(flags);
  (<0>,3759) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3761) fake_sched.h:43: return __running_cpu;
  (<0>,3765) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3767) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3771) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3773) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3774) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3775) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3777) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3778) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3780) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3783) rcutree.c:708: local_irq_restore(flags);
  (<0>,3786) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3788) fake_sched.h:43: return __running_cpu;
  (<0>,3792) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3794) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3800) rcutree.c:729: ret = 1;
  (<0>,3802) rcutree.c:731: local_irq_restore(flags);
  (<0>,3805) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3807) fake_sched.h:43: return __running_cpu;
  (<0>,3811) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3813) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3817) fake_sched.h:43: return __running_cpu;
  (<0>,3821) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3826) rcutree.c:732: return ret;
  (<0>,3832) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3833) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3837)
  (<0>,3838)
  (<0>,3839) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3842) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3843) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3850) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,3853)
  (<0>,3854) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3856) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3859) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3872) fake_sched.h:43: return __running_cpu;
  (<0>,3876) fake_sched.h:194: need_softirq[get_cpu()] = 0;
  (<0>,3887) rcutree.c:354: local_irq_save(flags);
  (<0>,3890) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3892) fake_sched.h:43: return __running_cpu;
  (<0>,3896) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3898) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3902) fake_sched.h:43: return __running_cpu;
  (<0>,3906) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3912) fake_sched.h:43: return __running_cpu;
  (<0>,3916) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3917) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,3919) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,3921) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,3924) rcutree.c:357: local_irq_restore(flags);
  (<0>,3927) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3929) fake_sched.h:43: return __running_cpu;
  (<0>,3933) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3935) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3939) fake_sched.h:43: return __running_cpu;
  (<0>,3943) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3955) fake_sched.h:43: return __running_cpu;
  (<0>,3959)
  (<0>,3960) rcutree.c:187: rcu_sched_qs(cpu);
  (<0>,3964)
  (<0>,3965) rcutree.c:165: struct rcu_data *rdp = &rcu_sched_data[cpu];
  (<0>,3968) rcutree.c:165: struct rcu_data *rdp = &rcu_sched_data[cpu];
  (<0>,3969) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,3971) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,3973) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,3975) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,3977) rcutree.c:169: rdp->passed_quiesc = 1;
  (<0>,3979) rcutree.c:169: rdp->passed_quiesc = 1;
  (<0>,3981) rcutree.c:188: rcu_preempt_note_context_switch(cpu);
  (<0>,3984) rcutree_plugin.h:938: }
  (<0>,3988) fake_sched.h:43: return __running_cpu;
  (<0>,3992) fake_sched.h:96: rcu_enter_nohz();
  (<0>,4001) rcutree.c:354: local_irq_save(flags);
  (<0>,4004) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4006) fake_sched.h:43: return __running_cpu;
  (<0>,4010) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4012) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4016) fake_sched.h:43: return __running_cpu;
  (<0>,4020) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4026) fake_sched.h:43: return __running_cpu;
  (<0>,4030) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4031) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,4033) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,4035) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,4039) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4042) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4043) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4044) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4046) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4047) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4049) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4052) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4058) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4059) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4062) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4067) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4068) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4069) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4070) rcutree.c:365: local_irq_restore(flags);
  (<0>,4073) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4075) fake_sched.h:43: return __running_cpu;
  (<0>,4079) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4081) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4085) fake_sched.h:43: return __running_cpu;
  (<0>,4089) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4096) fake_sched.h:43: return __running_cpu;
  (<0>,4100) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
  (<0>,4109) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,4112) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,4117) fake_sched.h:43: return __running_cpu;
  (<0>,4121)
  (<0>,4122) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,4125) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,4136) rcutree.c:386: local_irq_save(flags);
  (<0>,4139) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4141) fake_sched.h:43: return __running_cpu;
  (<0>,4145) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4147) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4151) fake_sched.h:43: return __running_cpu;
  (<0>,4155) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4161) fake_sched.h:43: return __running_cpu;
  (<0>,4165) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4166) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4168) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4170) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4174) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4177) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4178) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4179) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4181) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4182) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4184) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4187) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4194) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4195) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4198) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4203) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4204) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4205) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4206) rcutree.c:397: local_irq_restore(flags);
  (<0>,4209) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4211) fake_sched.h:43: return __running_cpu;
  (<0>,4215) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4217) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4221) fake_sched.h:43: return __running_cpu;
  (<0>,4225) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4237) fake_sched.h:43: return __running_cpu;
  (<0>,4241) fake_sched.h:153: if (!local_irq_depth[get_cpu()]) {
  (<0>,4245) fake_sched.h:43: return __running_cpu;
  (<0>,4249) fake_sched.h:154: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4254) fake_sched.h:43: return __running_cpu;
  (<0>,4258) fake_sched.h:157: local_irq_depth[get_cpu()] = 1;
  (<0>,4270) rcutree.c:386: local_irq_save(flags);
  (<0>,4273) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4275) fake_sched.h:43: return __running_cpu;
  (<0>,4279) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4281) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4286) fake_sched.h:43: return __running_cpu;
  (<0>,4290) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4291) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4293) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4295) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4298) rcutree.c:389: local_irq_restore(flags);
  (<0>,4301) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4303) fake_sched.h:43: return __running_cpu;
  (<0>,4307) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4309) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4318) fake_sched.h:43: return __running_cpu;
  (<0>,4323)
  (<0>,4324)
  (<0>,4325) rcutree.c:1292: if (user ||
  (<0>,4328) rcutree.c:1320: rcu_bh_qs(cpu);
  (<0>,4332)
  (<0>,4333) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
  (<0>,4336) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
  (<0>,4337) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,4339) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,4341) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,4343) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,4345) rcutree.c:178: rdp->passed_quiesc = 1;
  (<0>,4347) rcutree.c:178: rdp->passed_quiesc = 1;
  (<0>,4350) rcutree.c:1322: rcu_preempt_check_callbacks(cpu);
  (<0>,4353) rcutree_plugin.h:1024: }
  (<0>,4355) rcutree.c:1323: if (rcu_pending(cpu))
  (<0>,4358)
  (<0>,4359) rcutree.c:1757: return __rcu_pending(&rcu_sched_state, &rcu_sched_data[cpu]) ||
  (<0>,4367)
  (<0>,4368)
  (<0>,4369) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,4371) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,4372) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,4373) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,4375) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,4377) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,4378) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,4379) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,4386)
  (<0>,4387)
  (<0>,4388) rcutree.c:619: if (rcu_cpu_stall_suppress)
  (<0>,4391) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,4392) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,4393) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,4395) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,4396) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,4397) rcutree.c:623: rnp = rdp->mynode;
  (<0>,4399) rcutree.c:623: rnp = rdp->mynode;
  (<0>,4400) rcutree.c:623: rnp = rdp->mynode;
  (<0>,4401) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4403) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4404) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4406) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4410) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4411) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4415) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
  (<0>,4418)
  (<0>,4419) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4421) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4422) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4424) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4430) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
  (<0>,4431) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
  (<0>,4438) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,4440) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,4443) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,4445) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,4448) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,4450) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,4453) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,4455) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,4458) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,4460) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,4462) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,4463) rcutree.c:1711: return 1;
  (<0>,4465) rcutree.c:1748: }
  (<0>,4476) fake_sched.h:43: return __running_cpu;
  (<0>,4480) rcutree.c:1526: raise_softirq(RCU_SOFTIRQ);
  (<0>,4487) fake_sched.h:43: return __running_cpu;
  (<0>,4491) fake_sched.h:162: local_irq_depth[get_cpu()] = 0;
  (<0>,4493) fake_sched.h:43: return __running_cpu;
  (<0>,4497) fake_sched.h:163: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4503) fake_sched.h:43: return __running_cpu;
  (<0>,4507) fake_sched.h:192: if (need_softirq[get_cpu()]) {
  (<0>,4512) rcutree.c:1499: &rcu_sched_data[get_cpu()]);
  (<0>,4514) fake_sched.h:43: return __running_cpu;
  (<0>,4525)
  (<0>,4526)
  (<0>,4527) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4529) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4536) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4537) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4539) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4545) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4546) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4547) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4548) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,4550) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,4551) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,4555) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,4556) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,4562)
  (<0>,4563)
  (<0>,4564) rcutree.c:786: local_irq_save(flags);
  (<0>,4567) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4569) fake_sched.h:43: return __running_cpu;
  (<0>,4573) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4575) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4579) fake_sched.h:43: return __running_cpu;
  (<0>,4583) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4588) rcutree.c:787: rnp = rdp->mynode;
  (<0>,4590) rcutree.c:787: rnp = rdp->mynode;
  (<0>,4591) rcutree.c:787: rnp = rdp->mynode;
  (<0>,4592) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,4594) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,4595) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,4597) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,4600) rcutree.c:789: !raw_spin_trylock(&rnp->lock)) { /* irqs already off, so later. */
  (<0>,4605) fake_sync.h:126: preempt_disable();
  (<0>,4607) fake_sync.h:127: if (pthread_mutex_trylock(l)) {
  (<0>,4608) fake_sync.h:127: if (pthread_mutex_trylock(l)) {
  (<0>,4611) fake_sync.h:131: return 1;
  (<0>,4613) fake_sync.h:132: }
  (<0>,4617) rcutree.c:793: __rcu_process_gp_end(rsp, rnp, rdp);
  (<0>,4618) rcutree.c:793: __rcu_process_gp_end(rsp, rnp, rdp);
  (<0>,4619) rcutree.c:793: __rcu_process_gp_end(rsp, rnp, rdp);
  (<0>,4624)
  (<0>,4625)
  (<0>,4626)
  (<0>,4627) rcutree.c:745: if (rdp->completed != rnp->completed) {
  (<0>,4629) rcutree.c:745: if (rdp->completed != rnp->completed) {
  (<0>,4630) rcutree.c:745: if (rdp->completed != rnp->completed) {
  (<0>,4632) rcutree.c:745: if (rdp->completed != rnp->completed) {
  (<0>,4635) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
  (<0>,4638) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
  (<0>,4639) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
  (<0>,4642) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
  (<0>,4643) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
  (<0>,4646) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
  (<0>,4647) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
  (<0>,4650) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
  (<0>,4651) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4654) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4655) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4658) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4659) rcutree.c:753: rdp->completed = rnp->completed;
  (<0>,4661) rcutree.c:753: rdp->completed = rnp->completed;
  (<0>,4662) rcutree.c:753: rdp->completed = rnp->completed;
  (<0>,4664) rcutree.c:753: rdp->completed = rnp->completed;
  (<0>,4665) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
  (<0>,4667) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
  (<0>,4668) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
  (<0>,4670) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
  (<0>,4674) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
  (<0>,4676) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
  (<0>,4677) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
  (<0>,4679) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
  (<0>,4685) rcutree.c:794: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4687) rcutree.c:794: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4691)
  (<0>,4692)
  (<0>,4693) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,4694) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,4697) fake_sync.h:90: local_irq_restore(flags);
  (<0>,4700) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4702) fake_sched.h:43: return __running_cpu;
  (<0>,4706) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4708) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4712) fake_sched.h:43: return __running_cpu;
  (<0>,4716) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4725) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,4726) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,4730)
  (<0>,4731)
  (<0>,4732) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,4733) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,4739)
  (<0>,4740)
  (<0>,4741) rcutree.c:724: int ret = 0;
  (<0>,4742) rcutree.c:726: local_irq_save(flags);
  (<0>,4745) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4747) fake_sched.h:43: return __running_cpu;
  (<0>,4751) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4753) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4757) fake_sched.h:43: return __running_cpu;
  (<0>,4761) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4766) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,4768) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,4769) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,4771) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,4774) rcutree.c:731: local_irq_restore(flags);
  (<0>,4777) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4779) fake_sched.h:43: return __running_cpu;
  (<0>,4783) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4785) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4789) fake_sched.h:43: return __running_cpu;
  (<0>,4793) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4798) rcutree.c:732: return ret;
  (<0>,4802) rcutree.c:1081: if (!rdp->qs_pending)
  (<0>,4804) rcutree.c:1081: if (!rdp->qs_pending)
  (<0>,4807) rcutree.c:1088: if (!rdp->passed_quiesc)
  (<0>,4809) rcutree.c:1088: if (!rdp->passed_quiesc)
  (<0>,4812) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4814) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4815) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4816) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4817) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4819) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4828)
  (<0>,4829)
  (<0>,4830)
  (<0>,4831)
  (<0>,4832) rcutree.c:1032: rnp = rdp->mynode;
  (<0>,4834) rcutree.c:1032: rnp = rdp->mynode;
  (<0>,4835) rcutree.c:1032: rnp = rdp->mynode;
  (<0>,4836) rcutree.c:1033: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4838) rcutree.c:1033: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4842)
  (<0>,4843)
  (<0>,4844) fake_sync.h:80: local_irq_save(flags);
  (<0>,4847) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4849) fake_sched.h:43: return __running_cpu;
  (<0>,4853) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4855) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4859) fake_sched.h:43: return __running_cpu;
  (<0>,4863) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4869) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,4870) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,4874) rcutree.c:1034: if (lastcomp != rnp->completed) {
  (<0>,4875) rcutree.c:1034: if (lastcomp != rnp->completed) {
  (<0>,4877) rcutree.c:1034: if (lastcomp != rnp->completed) {
  (<0>,4880) rcutree.c:1048: mask = rdp->grpmask;
  (<0>,4882) rcutree.c:1048: mask = rdp->grpmask;
  (<0>,4883) rcutree.c:1048: mask = rdp->grpmask;
  (<0>,4884) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
  (<0>,4886) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
  (<0>,4887) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
  (<0>,4891) rcutree.c:1052: rdp->qs_pending = 0;
  (<0>,4893) rcutree.c:1052: rdp->qs_pending = 0;
  (<0>,4894) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4897) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4898) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4901) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4902) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,4903) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,4904) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,4905) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,4915)
  (<0>,4916)
  (<0>,4917)
  (<0>,4918) rcutree.c:978: for (;;) {
  (<0>,4920) rcutree.c:979: if (!(rnp->qsmask & mask)) {
  (<0>,4922) rcutree.c:979: if (!(rnp->qsmask & mask)) {
  (<0>,4923) rcutree.c:979: if (!(rnp->qsmask & mask)) {
  (<0>,4927) rcutree.c:985: rnp->qsmask &= ~mask;
  (<0>,4929) rcutree.c:985: rnp->qsmask &= ~mask;
  (<0>,4931) rcutree.c:985: rnp->qsmask &= ~mask;
  (<0>,4933) rcutree.c:985: rnp->qsmask &= ~mask;
  (<0>,4934) rcutree.c:987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,4936) rcutree.c:987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,4939) rcutree.c:987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,4942) rcutree_plugin.h:946: return 0;
  (<0>,4946) rcutree.c:994: mask = rnp->grpmask;
  (<0>,4948) rcutree.c:994: mask = rnp->grpmask;
  (<0>,4949) rcutree.c:994: mask = rnp->grpmask;
  (<0>,4950) rcutree.c:995: if (rnp->parent == NULL) {
  (<0>,4952) rcutree.c:995: if (rnp->parent == NULL) {
  (<0>,4956) rcutree.c:1013: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
  (<0>,4957) rcutree.c:1013: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
  (<0>,4965)
  (<0>,4966)
  (<0>,4967) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,4970)
  (<0>,4971) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4973) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4974) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4976) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4985) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,4986) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,4989)
  (<0>,4990) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4992) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4993) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4995) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5002) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,5003) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,5004) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,5006) rcutree.c:951: gp_duration = jiffies - rsp->gp_start;
  (<0>,5007) rcutree.c:951: gp_duration = jiffies - rsp->gp_start;
  (<0>,5009) rcutree.c:951: gp_duration = jiffies - rsp->gp_start;
  (<0>,5011) rcutree.c:951: gp_duration = jiffies - rsp->gp_start;
  (<0>,5012) rcutree.c:952: if (gp_duration > rsp->gp_max)
  (<0>,5013) rcutree.c:952: if (gp_duration > rsp->gp_max)
  (<0>,5015) rcutree.c:952: if (gp_duration > rsp->gp_max)
  (<0>,5018) rcutree.c:954: rsp->completed = rsp->gpnum;
  (<0>,5020) rcutree.c:954: rsp->completed = rsp->gpnum;
  (<0>,5021) rcutree.c:954: rsp->completed = rsp->gpnum;
  (<0>,5023) rcutree.c:954: rsp->completed = rsp->gpnum;
  (<0>,5024) rcutree.c:955: rsp->signaled = RCU_GP_IDLE;
  (<0>,5026) rcutree.c:955: rsp->signaled = RCU_GP_IDLE;
  (<0>,5027) rcutree.c:956: rcu_start_gp(rsp, flags);  /* releases root node's rnp->lock. */
  (<0>,5028) rcutree.c:956: rcu_start_gp(rsp, flags);  /* releases root node's rnp->lock. */
  (<0>,5037)
  (<0>,5038) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5040) fake_sched.h:43: return __running_cpu;
  (<0>,5043) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5045) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5047) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5048) rcutree.c:837: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5051)
  (<0>,5052) rcutree.c:297: return &rsp->node[0];
  (<0>,5056) rcutree.c:837: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5057) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
  (<0>,5058) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
  (<0>,5062)
  (<0>,5063)
  (<0>,5064) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5067) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5068) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5075) rcutree.c:840: if (cpu_needs_another_gp(rsp, rdp))
  (<0>,5076) rcutree.c:840: if (cpu_needs_another_gp(rsp, rdp))
  (<0>,5080)
  (<0>,5081)
  (<0>,5082) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5085) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5086) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5093) rcutree.c:842: if (rnp->completed == rsp->completed) {
  (<0>,5095) rcutree.c:842: if (rnp->completed == rsp->completed) {
  (<0>,5096) rcutree.c:842: if (rnp->completed == rsp->completed) {
  (<0>,5098) rcutree.c:842: if (rnp->completed == rsp->completed) {
  (<0>,5101) rcutree.c:846: raw_spin_unlock(&rnp->lock);	 /* irqs remain disabled. */
  (<0>,5105)
  (<0>,5106) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,5107) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,5112) rcutree.c:853: rcu_for_each_node_breadth_first(rsp, rnp) {
  (<0>,5115) rcutree.c:853: rcu_for_each_node_breadth_first(rsp, rnp) {
  (<0>,5117) rcutree.c:853: rcu_for_each_node_breadth_first(rsp, rnp) {
  (<0>,5118) rcutree.c:853: rcu_for_each_node_breadth_first(rsp, rnp) {
  (<0>,5123) rcutree.c:854: raw_spin_lock(&rnp->lock); /* irqs already disabled. */
  (<0>,5127) fake_sync.h:112: preempt_disable();
  (<0>,5129) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,5130) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,5134) rcutree.c:855: rnp->completed = rsp->completed;
  (<0>,5136) rcutree.c:855: rnp->completed = rsp->completed;
  (<0>,5137) rcutree.c:855: rnp->completed = rsp->completed;
  (<0>,5139) rcutree.c:855: rnp->completed = rsp->completed;
  (<0>,5140) rcutree.c:856: raw_spin_unlock(&rnp->lock); /* irqs remain disabled. */
  (<0>,5144)
  (<0>,5145) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,5146) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,5152) rcutree.c:853: rcu_for_each_node_breadth_first(rsp, rnp) {
  (<0>,5154) rcutree.c:853: rcu_for_each_node_breadth_first(rsp, rnp) {
  (<0>,5156) rcutree.c:853: rcu_for_each_node_breadth_first(rsp, rnp) {
  (<0>,5157) rcutree.c:853: rcu_for_each_node_breadth_first(rsp, rnp) {
  (<0>,5162) rcutree.c:858: local_irq_restore(flags);
  (<0>,5165) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5167) fake_sched.h:43: return __running_cpu;
  (<0>,5171) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5173) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5177) fake_sched.h:43: return __running_cpu;
  (<0>,5181) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5195) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5196) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5200)
  (<0>,5201)
  (<0>,5202) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5205) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5206) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5213) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,5216)
  (<0>,5217) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5219) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5222) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5230) fake_sched.h:43: return __running_cpu;
  (<0>,5241)
  (<0>,5242)
  (<0>,5243) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5245) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5252) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5253) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5255) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5261) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5262) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5263) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5264) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,5266) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,5267) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,5271) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,5272) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,5278)
  (<0>,5279)
  (<0>,5280) rcutree.c:786: local_irq_save(flags);
  (<0>,5283) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5285) fake_sched.h:43: return __running_cpu;
  (<0>,5289) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5291) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5295) fake_sched.h:43: return __running_cpu;
  (<0>,5299) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5304) rcutree.c:787: rnp = rdp->mynode;
  (<0>,5306) rcutree.c:787: rnp = rdp->mynode;
  (<0>,5307) rcutree.c:787: rnp = rdp->mynode;
  (<0>,5308) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,5310) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,5311) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,5313) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,5316) rcutree.c:790: local_irq_restore(flags);
  (<0>,5319) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5321) fake_sched.h:43: return __running_cpu;
  (<0>,5325) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5327) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5331) fake_sched.h:43: return __running_cpu;
  (<0>,5335) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5342) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5343) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5347)
  (<0>,5348)
  (<0>,5349) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,5350) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,5356)
  (<0>,5357)
  (<0>,5358) rcutree.c:724: int ret = 0;
  (<0>,5359) rcutree.c:726: local_irq_save(flags);
  (<0>,5362) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5364) fake_sched.h:43: return __running_cpu;
  (<0>,5368) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5370) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5374) fake_sched.h:43: return __running_cpu;
  (<0>,5378) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5383) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,5385) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,5386) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,5388) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,5391) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,5392) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,5398)
  (<0>,5399)
  (<0>,5400) rcutree.c:704: local_irq_save(flags);
  (<0>,5403) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5405) fake_sched.h:43: return __running_cpu;
  (<0>,5409) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5411) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5415) rcutree.c:705: rnp = rdp->mynode;
  (<0>,5417) rcutree.c:705: rnp = rdp->mynode;
  (<0>,5418) rcutree.c:705: rnp = rdp->mynode;
  (<0>,5419) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,5421) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,5422) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,5424) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,5427) rcutree.c:708: local_irq_restore(flags);
  (<0>,5430) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5432) fake_sched.h:43: return __running_cpu;
  (<0>,5436) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5438) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5444) rcutree.c:729: ret = 1;
  (<0>,5446) rcutree.c:731: local_irq_restore(flags);
  (<0>,5449) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5451) fake_sched.h:43: return __running_cpu;
  (<0>,5455) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5457) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5461) fake_sched.h:43: return __running_cpu;
  (<0>,5465) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5470) rcutree.c:732: return ret;
  (<0>,5476) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5477) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5481)
  (<0>,5482)
  (<0>,5483) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5486) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5487) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5494) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,5497)
  (<0>,5498) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5500) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5503) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5516) fake_sched.h:43: return __running_cpu;
  (<0>,5520) fake_sched.h:194: need_softirq[get_cpu()] = 0;
  (<0>,5531) rcutree.c:354: local_irq_save(flags);
  (<0>,5534) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5536) fake_sched.h:43: return __running_cpu;
  (<0>,5540) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5542) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5546) fake_sched.h:43: return __running_cpu;
  (<0>,5550) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5556) fake_sched.h:43: return __running_cpu;
  (<0>,5560) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,5561) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5563) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5565) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5568) rcutree.c:357: local_irq_restore(flags);
  (<0>,5571) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5573) fake_sched.h:43: return __running_cpu;
  (<0>,5577) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5579) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5583) fake_sched.h:43: return __running_cpu;
  (<0>,5587) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,3528) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3529) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3531) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3542) rcutree.c:1484: raw_spin_lock_irqsave(&rcu_get_root(rsp)->lock, flags);
      (<0.1>,3545)
      (<0.1>,3546) rcutree.c:297: return &rsp->node[0];
      (<0.1>,3551) rcutree.c:1484: raw_spin_lock_irqsave(&rcu_get_root(rsp)->lock, flags);
      (<0.1>,3555)
      (<0.1>,3556)
      (<0.1>,3557) fake_sync.h:80: local_irq_save(flags);
      (<0.1>,3560)
      (<0.1>,3562) fake_sched.h:43: return __running_cpu;
      (<0.1>,3566) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3568) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3572) fake_sched.h:43: return __running_cpu;
      (<0.1>,3576) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,3582) fake_sync.h:82: if (pthread_mutex_lock(l))
      (<0.1>,3583) fake_sync.h:82: if (pthread_mutex_lock(l))
      (<0.1>,3587) rcutree.c:1485: rcu_start_gp(rsp, flags);  /* releases above lock */
      (<0.1>,3588) rcutree.c:1485: rcu_start_gp(rsp, flags);  /* releases above lock */
      (<0.1>,3597)
      (<0.1>,3598)
      (<0.1>,3600) fake_sched.h:43: return __running_cpu;
      (<0.1>,3603) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,3605) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,3607) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,3608) rcutree.c:837: struct rcu_node *rnp = rcu_get_root(rsp);
      (<0.1>,3611)
      (<0.1>,3612) rcutree.c:297: return &rsp->node[0];
      (<0.1>,3616) rcutree.c:837: struct rcu_node *rnp = rcu_get_root(rsp);
      (<0.1>,3617) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,3618) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,3622)
      (<0.1>,3623)
      (<0.1>,3624) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3627) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3628) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3631) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3634)
      (<0.1>,3635) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3637) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3638) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3640) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3651) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,3653) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,3657) rcutree.c:863: rsp->gpnum++;
      (<0.1>,3659) rcutree.c:863: rsp->gpnum++;
      (<0.1>,3661) rcutree.c:863: rsp->gpnum++;
      (<0.1>,3662) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,3664) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,3670) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,3671) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,3673) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,3678) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,3679) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,3680) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,3681) rcutree.c:865: rsp->signaled = RCU_GP_INIT; /* Hold off force_quiescent_state. */
      (<0.1>,3683) rcutree.c:865: rsp->signaled = RCU_GP_INIT; /* Hold off force_quiescent_state. */
      (<0.1>,3684) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,3686) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,3688) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,3689) rcutree.c:867: record_gp_stall_check_time(rsp);
      (<0.1>,3692)
      (<0.1>,3693) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,3694) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,3696) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,3697) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,3699) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,3701) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,3703) rcutree.c:871: rcu_preempt_check_blocked_tasks(rnp);
      (<0.1>,3709)
      (<0.1>,3710) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,3712) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,3717) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,3718) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,3720) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,3724) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,3725) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,3726) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,3728) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,3730) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,3731) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,3733) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,3734) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,3736) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,3737) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,3739) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,3740) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,3742) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,3743) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,3745) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,3746) rcutree.c:879: rsp->signaled = RCU_SIGNAL_INIT; /* force_quiescent_state OK. */
      (<0.1>,3748) rcutree.c:879: rsp->signaled = RCU_SIGNAL_INIT; /* force_quiescent_state OK. */
      (<0.1>,3749) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,3750) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,3751) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,3756)
      (<0.1>,3757)
      (<0.1>,3758)
      (<0.1>,3759) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,3760) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,3761) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,3766)
      (<0.1>,3767)
      (<0.1>,3768)
      (<0.1>,3769) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,3771) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,3772) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,3774) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,3777) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,3780) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,3781) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,3784) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,3785) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,3788) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,3789) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,3792) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,3793) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3796) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3797) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3800) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3801) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,3803) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,3804) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,3806) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,3807) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,3809) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,3810) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,3812) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,3816) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,3818) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,3819) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,3821) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,3827) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3830) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3831) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3834) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3835) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3838) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3839) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3842) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,3843) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,3844) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,3845) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,3850)
      (<0.1>,3851)
      (<0.1>,3852)
      (<0.1>,3853) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,3855) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,3856) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,3858) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,3861) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,3863) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,3864) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,3866) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,3867) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,3869) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,3870) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,3872) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,3876) rcutree.c:688: rdp->qs_pending = 1;
      (<0.1>,3878) rcutree.c:688: rdp->qs_pending = 1;
      (<0.1>,3879) rcutree.c:690: rdp->passed_quiesc = 0;
      (<0.1>,3881) rcutree.c:690: rdp->passed_quiesc = 0;
      (<0.1>,3886) rcutree.c:881: rcu_preempt_boost_start_gp(rnp);
      (<0.1>,3889)
      (<0.1>,3891) rcutree.c:882: raw_spin_unlock_irqrestore(&rnp->lock, flags);
      (<0.1>,3893) rcutree.c:882: raw_spin_unlock_irqrestore(&rnp->lock, flags);
      (<0.1>,3897)
      (<0.1>,3898)
      (<0.1>,3899) fake_sync.h:88: if (pthread_mutex_unlock(l))
      (<0.1>,3900) fake_sync.h:88: if (pthread_mutex_unlock(l))
      (<0.1>,3903) fake_sync.h:90: local_irq_restore(flags);
      (<0.1>,3906)
      (<0.1>,3908) fake_sched.h:43: return __running_cpu;
      (<0.1>,3912) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3914) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3918) fake_sched.h:43: return __running_cpu;
      (<0.1>,3922) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,3932) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,3935)
      (<0.1>,3936) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3938) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3941) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3947) rcutree.c:1490: invoke_rcu_callbacks(rsp, rdp);
      (<0.1>,3948) rcutree.c:1490: invoke_rcu_callbacks(rsp, rdp);
      (<0.1>,3952)
      (<0.1>,3953)
      (<0.1>,3954) rcutree.c:1515: if (unlikely(!ACCESS_ONCE(rcu_scheduler_fully_active)))
      (<0.1>,3963) rcutree.c:1517: if (likely(!rsp->boost)) {
      (<0.1>,3965) rcutree.c:1517: if (likely(!rsp->boost)) {
      (<0.1>,3974) rcutree.c:1518: rcu_do_batch(rsp, rdp);
      (<0.1>,3975) rcutree.c:1518: rcu_do_batch(rsp, rdp);
      (<0.1>,3984)
      (<0.1>,3985)
      (<0.1>,3986) rcutree.c:1219: if (!cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,3989)
      (<0.1>,3990) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3992) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3995) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4001) rcutree.c:1226: local_irq_save(flags);
      (<0.1>,4004)
      (<0.1>,4006) fake_sched.h:43: return __running_cpu;
      (<0.1>,4010) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4012) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4016) fake_sched.h:43: return __running_cpu;
      (<0.1>,4020) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4025) rcutree.c:1227: list = rdp->nxtlist;
      (<0.1>,4027) rcutree.c:1227: list = rdp->nxtlist;
      (<0.1>,4028) rcutree.c:1227: list = rdp->nxtlist;
      (<0.1>,4029) rcutree.c:1228: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4032) rcutree.c:1228: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4033) rcutree.c:1228: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4034) rcutree.c:1228: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4036) rcutree.c:1228: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4037) rcutree.c:1229: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
      (<0.1>,4040) rcutree.c:1229: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
      (<0.1>,4041) rcutree.c:1229: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
      (<0.1>,4042) rcutree.c:1230: tail = rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4045) rcutree.c:1230: tail = rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4046) rcutree.c:1230: tail = rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4047) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4049) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4052) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4054) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4057) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4058) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4061) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4064) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4066) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4068) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4071) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4074) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4076) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4078) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4081) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4083) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4086) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4087) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4090) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4093) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4095) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4097) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4100) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4103) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4105) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4107) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4110) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4112) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4115) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4116) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4119) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4122) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4124) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4126) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4129) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4132) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4134) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4136) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4139) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4141) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4144) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4145) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4148) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,4151) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4153) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4155) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4158) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,4161) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4163) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4165) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,4168) rcutree.c:1234: local_irq_restore(flags);
      (<0.1>,4171)
      (<0.1>,4173) fake_sched.h:43: return __running_cpu;
      (<0.1>,4177) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4179) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4183) fake_sched.h:43: return __running_cpu;
      (<0.1>,4187) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4192) rcutree.c:1237: count = 0;
      (<0.1>,4194) rcutree.c:1238: while (list) {
      (<0.1>,4197) rcutree.c:1239: next = list->next;
      (<0.1>,4199) rcutree.c:1239: next = list->next;
      (<0.1>,4200) rcutree.c:1239: next = list->next;
      (<0.1>,4203) rcutree.c:1241: debug_rcu_head_unqueue(list);
      (<0.1>,4206)
      (<0.1>,4208) rcutree.c:1242: __rcu_reclaim(list);
      (<0.1>,4213)
      (<0.1>,4214) rcupdate.h:837: unsigned long offset = (unsigned long)head->func;
      (<0.1>,4217) rcupdate.h:837: unsigned long offset = (unsigned long)head->func;
      (<0.1>,4219) rcupdate.h:837: unsigned long offset = (unsigned long)head->func;
      (<0.1>,4220) rcupdate.h:839: if (__is_kfree_rcu_offset(offset))
      (<0.1>,4221) rcupdate.h:839: if (__is_kfree_rcu_offset(offset))
      (<0.1>,4222) rcupdate.h:817: return offset < 4096;
      (<0.1>,4225) rcupdate.h:842: head->func(head);
      (<0.1>,4228) rcupdate.h:842: head->func(head);
      (<0.1>,4229) rcupdate.h:842: head->func(head);
      (<0.1>,4235)
      (<0.1>,4236) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,4237) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,4238) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,4242) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,4243) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,4244) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,4245) rcupdate.c:106: complete(&rcu->completion);
      (<0.1>,4249)
      (<0.1>,4250) fake_sync.h:276: x->done++;
      (<0.1>,4252) fake_sync.h:276: x->done++;
      (<0.1>,4254) fake_sync.h:276: x->done++;
      (<0.1>,4259) rcutree.c:1243: list = next;
      (<0.1>,4260) rcutree.c:1243: list = next;
      (<0.1>,4261) rcutree.c:1244: if (++count >= rdp->blimit)
      (<0.1>,4263) rcutree.c:1244: if (++count >= rdp->blimit)
      (<0.1>,4265) rcutree.c:1244: if (++count >= rdp->blimit)
      (<0.1>,4267) rcutree.c:1244: if (++count >= rdp->blimit)
      (<0.1>,4271) rcutree.c:1238: while (list) {
      (<0.1>,4274) rcutree.c:1248: local_irq_save(flags);
      (<0.1>,4277)
      (<0.1>,4279) fake_sched.h:43: return __running_cpu;
      (<0.1>,4283) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4285) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4289) fake_sched.h:43: return __running_cpu;
      (<0.1>,4293) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4298) rcutree.c:1251: rdp->qlen -= count;
      (<0.1>,4300) rcutree.c:1251: rdp->qlen -= count;
      (<0.1>,4302) rcutree.c:1251: rdp->qlen -= count;
      (<0.1>,4304) rcutree.c:1251: rdp->qlen -= count;
      (<0.1>,4305) rcutree.c:1252: rdp->n_cbs_invoked += count;
      (<0.1>,4307) rcutree.c:1252: rdp->n_cbs_invoked += count;
      (<0.1>,4309) rcutree.c:1252: rdp->n_cbs_invoked += count;
      (<0.1>,4311) rcutree.c:1252: rdp->n_cbs_invoked += count;
      (<0.1>,4312) rcutree.c:1253: if (list != NULL) {
      (<0.1>,4315) rcutree.c:1264: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
      (<0.1>,4317) rcutree.c:1264: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
      (<0.1>,4320) rcutree.c:1268: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
      (<0.1>,4322) rcutree.c:1268: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
      (<0.1>,4325) rcutree.c:1268: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
      (<0.1>,4327) rcutree.c:1268: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
      (<0.1>,4330) rcutree.c:1271: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
      (<0.1>,4332) rcutree.c:1271: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
      (<0.1>,4333) rcutree.c:1271: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
      (<0.1>,4335) rcutree.c:1271: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
      (<0.1>,4336) rcutree.c:1271: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
      (<0.1>,4342) rcutree.c:1274: local_irq_restore(flags);
      (<0.1>,4345)
      (<0.1>,4347) fake_sched.h:43: return __running_cpu;
      (<0.1>,4351) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4353) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4357) fake_sched.h:43: return __running_cpu;
      (<0.1>,4361) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4366) rcutree.c:1277: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,4369)
      (<0.1>,4370) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4372) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4375) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4387) fake_sched.h:43: return __running_cpu;
      (<0.1>,4398)
      (<0.1>,4399)
      (<0.1>,4400) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4402) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4409) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4410) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4412) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4418) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4419) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4420) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4421) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,4423) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,4424) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,4428) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,4429) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,4435)
      (<0.1>,4436)
      (<0.1>,4437) rcutree.c:786: local_irq_save(flags);
      (<0.1>,4440)
      (<0.1>,4442) fake_sched.h:43: return __running_cpu;
      (<0.1>,4446) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4448) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4452) fake_sched.h:43: return __running_cpu;
      (<0.1>,4456) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4461) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,4463) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,4464) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,4465) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4467) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4468) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4470) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4473) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,4476)
      (<0.1>,4478) fake_sched.h:43: return __running_cpu;
      (<0.1>,4482) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4484) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4488) fake_sched.h:43: return __running_cpu;
      (<0.1>,4492) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4499) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,4500) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,4504)
      (<0.1>,4505)
      (<0.1>,4506) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,4507) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,4513)
      (<0.1>,4514)
      (<0.1>,4515) rcutree.c:724: int ret = 0;
      (<0.1>,4516) rcutree.c:726: local_irq_save(flags);
      (<0.1>,4519)
      (<0.1>,4521) fake_sched.h:43: return __running_cpu;
      (<0.1>,4525) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4527) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4531) fake_sched.h:43: return __running_cpu;
      (<0.1>,4535) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4540) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4542) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4543) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4545) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4548) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,4549) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,4555)
      (<0.1>,4556)
      (<0.1>,4557) rcutree.c:704: local_irq_save(flags);
      (<0.1>,4560)
      (<0.1>,4562) fake_sched.h:43: return __running_cpu;
      (<0.1>,4566) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4568) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4572) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,4574) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,4575) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,4576) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,4578) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,4579) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,4581) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,4584) rcutree.c:708: local_irq_restore(flags);
      (<0.1>,4587)
      (<0.1>,4589) fake_sched.h:43: return __running_cpu;
      (<0.1>,4593) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4595) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4601) rcutree.c:729: ret = 1;
      (<0.1>,4603) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,4606)
      (<0.1>,4608) fake_sched.h:43: return __running_cpu;
      (<0.1>,4612) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4614) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4618) fake_sched.h:43: return __running_cpu;
      (<0.1>,4622) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4627) rcutree.c:732: return ret;
      (<0.1>,4633) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,4634) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,4638)
      (<0.1>,4639)
      (<0.1>,4640) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,4643) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,4644) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,4651) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,4654)
      (<0.1>,4655) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4657) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4660) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4673) fake_sched.h:43: return __running_cpu;
      (<0.1>,4677) fake_sched.h:194: need_softirq[get_cpu()] = 0;
      (<0.1>,4688) rcutree.c:354: local_irq_save(flags);
      (<0.1>,4691)
      (<0.1>,4693) fake_sched.h:43: return __running_cpu;
      (<0.1>,4697) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4699) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4703) fake_sched.h:43: return __running_cpu;
      (<0.1>,4707) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4713) fake_sched.h:43: return __running_cpu;
      (<0.1>,4717) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,4718) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4720) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4722) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4725) rcutree.c:357: local_irq_restore(flags);
      (<0.1>,4728)
      (<0.1>,4730) fake_sched.h:43: return __running_cpu;
      (<0.1>,4734) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4736) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4740) fake_sched.h:43: return __running_cpu;
      (<0.1>,4744) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4755) fake_sched.h:43: return __running_cpu;
      (<0.1>,4759)
      (<0.1>,4768) rcutree.c:354: local_irq_save(flags);
      (<0.1>,4771)
      (<0.1>,4773) fake_sched.h:43: return __running_cpu;
      (<0.1>,4777) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4779) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4783) fake_sched.h:43: return __running_cpu;
      (<0.1>,4787) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4793) fake_sched.h:43: return __running_cpu;
      (<0.1>,4797) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,4798) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4800) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4802) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4806) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4809) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4810) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4811) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4813) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4814) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4816) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4819) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4825) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4826) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4829) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4834) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4835) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4836) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4837) rcutree.c:365: local_irq_restore(flags);
      (<0.1>,4840)
      (<0.1>,4842) fake_sched.h:43: return __running_cpu;
      (<0.1>,4846) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4848) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4852) fake_sched.h:43: return __running_cpu;
      (<0.1>,4856) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4863) fake_sched.h:43: return __running_cpu;
      (<0.1>,4867) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
      (<0.1>,4876) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,4879) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,415) fake_sync.h:269: while (!x->done)
    (<0.0>,422) fake_sched.h:43: return __running_cpu;
    (<0.0>,426)
    (<0.0>,427) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,430) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,441) rcutree.c:386: local_irq_save(flags);
    (<0.0>,444)
    (<0.0>,446) fake_sched.h:43: return __running_cpu;
    (<0.0>,450) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,452) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,456) fake_sched.h:43: return __running_cpu;
    (<0.0>,460) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,466) fake_sched.h:43: return __running_cpu;
    (<0.0>,470) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,471) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,473) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,475) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,479) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,482) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,483) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,484) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,486) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,487) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,489) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,492) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,499) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,500) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,503) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,508) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,509) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,510) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,511) rcutree.c:397: local_irq_restore(flags);
    (<0.0>,514)
    (<0.0>,516) fake_sched.h:43: return __running_cpu;
    (<0.0>,520) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,522) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,526) fake_sched.h:43: return __running_cpu;
    (<0.0>,530) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,542)
    (<0.0>,547) litmus_v3.c:83: y = 1;
    (<0.0>,549) fake_sched.h:43: return __running_cpu;
    (<0.0>,553)
    (<0.0>,562) rcutree.c:354: local_irq_save(flags);
    (<0.0>,565)
    (<0.0>,567) fake_sched.h:43: return __running_cpu;
    (<0.0>,571) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,573) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,577) fake_sched.h:43: return __running_cpu;
    (<0.0>,581) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,587) fake_sched.h:43: return __running_cpu;
    (<0.0>,591) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,592) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,594) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,596) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,600) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,603) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,604) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,605) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,607) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,608) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,610) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,613) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,619) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,620) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,623) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,628) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,629) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,630) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,631) rcutree.c:365: local_irq_restore(flags);
    (<0.0>,634)
    (<0.0>,636) fake_sched.h:43: return __running_cpu;
    (<0.0>,640) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,642) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,646) fake_sched.h:43: return __running_cpu;
    (<0.0>,650) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,657) fake_sched.h:43: return __running_cpu;
    (<0.0>,661) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
    (<0.0>,670) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,673) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,5597) litmus_v3.c:62: r_y = y;
  (<0>,5598) litmus_v3.c:62: r_y = y;
  (<0>,5607) fake_sched.h:43: return __running_cpu;
  (<0>,5611)
  (<0>,5620) rcutree.c:354: local_irq_save(flags);
  (<0>,5623)
  (<0>,5625) fake_sched.h:43: return __running_cpu;
  (<0>,5629) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5631) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5635) fake_sched.h:43: return __running_cpu;
  (<0>,5639) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5645) fake_sched.h:43: return __running_cpu;
  (<0>,5649) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,5650) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5652) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5654) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5658) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5661) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5662) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5663) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5665) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5666) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5668) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5671) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5677) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5678) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5681) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5686) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5687) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5688) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5689) rcutree.c:365: local_irq_restore(flags);
  (<0>,5692)
  (<0>,5694) fake_sched.h:43: return __running_cpu;
  (<0>,5698) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5700) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5704) fake_sched.h:43: return __running_cpu;
  (<0>,5708) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5715) fake_sched.h:43: return __running_cpu;
  (<0>,5719) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
  (<0>,5728) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,5731) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,5736) litmus_v3.c:124: if (pthread_join(tu, NULL))
  (<0>,5740) litmus_v3.c:126: if (pthread_join(th, NULL))
  (<0>,5744) litmus_v3.c:129: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,5747) litmus_v3.c:129: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,5751) litmus_v3.c:129: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,5752) litmus_v3.c:129: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,5753) litmus_v3.c:129: BUG_ON(r_x == 0 && r_y == 1);
             Error: Assertion violation at (<0>,5756): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpu1qo609o/tmpcmvm2h8w.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DFORCE_FAILURE_1 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpu1qo609o/tmprlnm4u6k.ll /tmp/tmpu1qo609o/tmpcmvm2h8w.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=4 --preemption-bounding=PB --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpu1qo609o/tmprlnm4u6k.ll
Total wall-clock time: 4.20 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 1591 (also 36 sleepset blocked, 0 schedulings and 947 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmphsmb3dx4/tmpaa3xe6rx.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DFORCE_FAILURE_3 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmphsmb3dx4/tmpe_ymmyoz.ll /tmp/tmphsmb3dx4/tmpaa3xe6rx.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmphsmb3dx4/tmpe_ymmyoz.ll
Total wall-clock time: 23.09 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 4119 (also 36 sleepset blocked, 0 schedulings and 2082 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpp5lovpi2/tmpqcht280t.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DFORCE_FAILURE_5 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpp5lovpi2/tmpn5r16hqz.ll /tmp/tmpp5lovpi2/tmpqcht280t.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpp5lovpi2/tmpn5r16hqz.ll
Total wall-clock time: 59.47 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 1457 (also 36 sleepset blocked, 0 schedulings and 819 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpe1iv2ak3/tmpmy1k668z.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DLIVENESS_CHECK_1 -DASSERT_0 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpe1iv2ak3/tmppc92o9x3.ll /tmp/tmpe1iv2ak3/tmpmy1k668z.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpe1iv2ak3/tmppc92o9x3.ll
Total wall-clock time: 21.19 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 2061 (also 44 sleepset blocked, 0 schedulings and 1406 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmplvtjirzb/tmpfbyq0djh.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DLIVENESS_CHECK_2 -DASSERT_0 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmplvtjirzb/tmptbn9cau8.ll /tmp/tmplvtjirzb/tmpfbyq0djh.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmplvtjirzb/tmptbn9cau8.ll
Total wall-clock time: 30.50 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 1752 (also 36 sleepset blocked, 0 schedulings and 1015 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmptmygmlxw/tmpxt6p3f94.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DLIVENESS_CHECK_3 -DASSERT_0 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmptmygmlxw/tmpy7m7ald3.ll /tmp/tmptmygmlxw/tmpxt6p3f94.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmptmygmlxw/tmpy7m7ald3.ll
Total wall-clock time: 24.98 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 1843 (also 240 sleepset blocked, 0 schedulings and 1836 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpgxezc948/tmpx45m6f_y.ll -S -emit-llvm -g -I v3.19 -std=gnu99 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpgxezc948/tmpg52eu36v.ll /tmp/tmpgxezc948/tmpx45m6f_y.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpgxezc948/tmpg52eu36v.ll
Total wall-clock time: 60.48 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 171 (also 11 sleepset blocked, 0 schedulings and 98 branches were rejected due to the bound) 

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,73) tree_plugin.h:931: pr_info("Hierarchical RCU implementation.\n");
  (<0>,77) tree_plugin.h:91: if (rcu_fanout_leaf != CONFIG_RCU_FANOUT_LEAF)
  (<0>,89) tree.c:3718: ulong d;
  (<0>,90) tree.c:3722: int rcu_capacity[MAX_RCU_LVLS + 1];
  (<0>,91) tree.c:3732: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,94) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,95) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,97) tree.c:3734: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,100) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,101) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,103) tree.c:3738: if (rcu_fanout_leaf == CONFIG_RCU_FANOUT_LEAF &&
  (<0>,115)
  (<0>,116) tree.c:3628: static void __init rcu_init_one(struct rcu_state *rsp,
  (<0>,117) tree.c:3629: struct rcu_data __percpu *rda)
  (<0>,118) tree.c:3643: int i;
  (<0>,121) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,123) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,124) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,127) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,130) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,133) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,136) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,140) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,142) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,143) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,146) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,149) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,152) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,158)
  (<0>,159) tree.c:3610: static void __init rcu_init_levelspread(struct rcu_state *rsp)
  (<0>,160) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,162) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,164) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,169) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,172) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,173) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,174) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,175) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,183) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,186) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,187) tree.c:3620: cprv = ccur;
  (<0>,188) tree.c:3620: cprv = ccur;
  (<0>,190) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,192) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,194) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,198) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,199) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,201) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,202) tree.c:3661: fl_mask <<= 1;
  (<0>,206) tree.c:3661: fl_mask <<= 1;
  (<0>,207) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,209) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,211) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,219) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,221) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,223) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,224) tree.c:3667: rnp = rsp->level[i];
  (<0>,226) tree.c:3667: rnp = rsp->level[i];
  (<0>,229) tree.c:3667: rnp = rsp->level[i];
  (<0>,230) tree.c:3667: rnp = rsp->level[i];
  (<0>,231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,233) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,236) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,242) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,246)
  (<0>,247) fake_sync.h:68: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,248) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,254) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,258)
  (<0>,259) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,260) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,268) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,269) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,271) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,274) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,275) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,277) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,278) tree.c:3677: rnp->qsmask = 0;
  (<0>,280) tree.c:3677: rnp->qsmask = 0;
  (<0>,281) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,283) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,285) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,287) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,295) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,297) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,298) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,300) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,303) tree.c:3683: if (i == 0) {
  (<0>,306) tree.c:3684: rnp->grpnum = 0;
  (<0>,308) tree.c:3684: rnp->grpnum = 0;
  (<0>,309) tree.c:3685: rnp->grpmask = 0;
  (<0>,311) tree.c:3685: rnp->grpmask = 0;
  (<0>,312) tree.c:3686: rnp->parent = NULL;
  (<0>,314) tree.c:3686: rnp->parent = NULL;
  (<0>,316) tree.c:3693: rnp->level = i;
  (<0>,318) tree.c:3693: rnp->level = i;
  (<0>,320) tree.c:3693: rnp->level = i;
  (<0>,321) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,325)
  (<0>,326) fake_defs.h:273: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,327) fake_defs.h:275: list->next = list;
  (<0>,329) fake_defs.h:275: list->next = list;
  (<0>,330) fake_defs.h:276: list->prev = list;
  (<0>,331) fake_defs.h:276: list->prev = list;
  (<0>,333) fake_defs.h:276: list->prev = list;
  (<0>,335) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,338) tree_plugin.h:2694: }
  (<0>,341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,348) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,351) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,354) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,358) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,360) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,362) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,365) tree.c:3699: rsp->rda = rda;
  (<0>,366) tree.c:3699: rsp->rda = rda;
  (<0>,368) tree.c:3699: rsp->rda = rda;
  (<0>,371) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,374) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,377) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,378) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,379) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,381) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,382) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,389)
  (<0>,390) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,391) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,393) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,394) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,397) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,399) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,400) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,403) fake_defs.h:536: if (offset & mask)
  (<0>,404) fake_defs.h:536: if (offset & mask)
  (<0>,408) fake_defs.h:537: return cpu;
  (<0>,409) fake_defs.h:537: return cpu;
  (<0>,411) fake_defs.h:540: }
  (<0>,413) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,414) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,418) tree.c:3703: while (i > rnp->grphi)
  (<0>,419) tree.c:3703: while (i > rnp->grphi)
  (<0>,421) tree.c:3703: while (i > rnp->grphi)
  (<0>,424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,425) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,429) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,432) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,433) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,434) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,447)
  (<0>,448) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,449) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,451) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,453) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,455) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,456) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,459)
  (<0>,460) tree.c:451: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,464) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,465) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,467) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,471)
  (<0>,472) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,473) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,476) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,478) fake_sched.h:43: return __running_cpu;
  (<0>,482) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,484) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,488) fake_sched.h:43: return __running_cpu;
  (<0>,492) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,498) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,499) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,504) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,506) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,508) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,512) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,514) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,515) tree.c:3412: init_callback_list(rdp);
  (<0>,519)
  (<0>,520) tree.c:1223: static void init_callback_list(struct rcu_data *rdp)
  (<0>,523) tree_plugin.h:2732: return false;
  (<0>,526) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,528) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,529) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,531) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,534) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,538) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,541) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,543) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,545) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,547) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,550) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,554) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,557) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,559) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,561) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,563) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,566) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,570) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,573) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,575) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,577) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,579) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,582) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,586) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,589) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,591) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,593) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,595) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,599) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,601) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,602) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,604) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,608) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,610) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,611) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,613) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,621) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,623) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,629) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,630) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,631) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,634) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,642) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,645) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,652) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,653) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,654) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,655) tree.c:3418: rdp->cpu = cpu;
  (<0>,656) tree.c:3418: rdp->cpu = cpu;
  (<0>,658) tree.c:3418: rdp->cpu = cpu;
  (<0>,659) tree.c:3419: rdp->rsp = rsp;
  (<0>,660) tree.c:3419: rdp->rsp = rsp;
  (<0>,662) tree.c:3419: rdp->rsp = rsp;
  (<0>,663) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,666) tree_plugin.h:2711: }
  (<0>,668) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,670) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,674)
  (<0>,675) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,676) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,677) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,680) fake_sync.h:86: local_irq_restore(flags);
  (<0>,683) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,685) fake_sched.h:43: return __running_cpu;
  (<0>,689) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,691) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,695) fake_sched.h:43: return __running_cpu;
  (<0>,699) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,708) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,709) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,716)
  (<0>,717)
  (<0>,718) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,720) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,721) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,724) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,726) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,727) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,730) fake_defs.h:536: if (offset & mask)
  (<0>,731) fake_defs.h:536: if (offset & mask)
  (<0>,735) fake_defs.h:537: return cpu;
  (<0>,736) fake_defs.h:537: return cpu;
  (<0>,738) fake_defs.h:540: }
  (<0>,740) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,741) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,745) tree.c:3703: while (i > rnp->grphi)
  (<0>,746) tree.c:3703: while (i > rnp->grphi)
  (<0>,748) tree.c:3703: while (i > rnp->grphi)
  (<0>,751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,752) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,756) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,759) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,760) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,761) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,774)
  (<0>,775)
  (<0>,776) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,778) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,780) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,782) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,783) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,786)
  (<0>,787) tree.c:453: return &rsp->node[0];
  (<0>,791) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,792) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,794) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,798)
  (<0>,799)
  (<0>,800) fake_sync.h:76: local_irq_save(flags);
  (<0>,803) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,805) fake_sched.h:43: return __running_cpu;
  (<0>,809) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,811) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,815) fake_sched.h:43: return __running_cpu;
  (<0>,819) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,825) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,826) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,831) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,833) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,835) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,839) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,841) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,842) tree.c:3412: init_callback_list(rdp);
  (<0>,846)
  (<0>,847) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,850) tree_plugin.h:2732: return false;
  (<0>,853) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,855) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,856) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,858) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,861) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,865) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,868) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,870) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,872) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,874) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,877) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,881) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,884) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,886) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,888) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,890) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,893) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,897) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,900) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,902) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,904) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,906) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,909) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,913) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,916) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,918) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,920) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,922) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,926) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,928) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,929) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,931) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,935) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,937) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,938) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,940) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,948) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,950) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,956) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,957) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,958) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,961) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,969) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,972) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,979) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,980) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,981) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,982) tree.c:3418: rdp->cpu = cpu;
  (<0>,983) tree.c:3418: rdp->cpu = cpu;
  (<0>,985) tree.c:3418: rdp->cpu = cpu;
  (<0>,986) tree.c:3419: rdp->rsp = rsp;
  (<0>,987) tree.c:3419: rdp->rsp = rsp;
  (<0>,989) tree.c:3419: rdp->rsp = rsp;
  (<0>,990) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,993) tree_plugin.h:2711: }
  (<0>,995) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,997) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1001)
  (<0>,1002)
  (<0>,1003) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1004) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1007) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1010) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1012) fake_sched.h:43: return __running_cpu;
  (<0>,1016) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1018) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1022) fake_sched.h:43: return __running_cpu;
  (<0>,1026) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1035) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1036) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1043)
  (<0>,1044)
  (<0>,1045) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1047) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1048) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1051) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1053) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1054) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1057) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,1059) fake_defs.h:540: }
  (<0>,1061) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1062) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1065) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1070)
  (<0>,1071) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1072) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1073) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1074) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1076) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1081)
  (<0>,1082) fake_defs.h:279: static inline void __list_add(struct list_head *new,
  (<0>,1083) fake_defs.h:280: struct list_head *prev,
  (<0>,1084) fake_defs.h:281: struct list_head *next)
  (<0>,1085) fake_defs.h:283: next->prev = new;
  (<0>,1087) fake_defs.h:283: next->prev = new;
  (<0>,1088) fake_defs.h:284: new->next = next;
  (<0>,1089) fake_defs.h:284: new->next = next;
  (<0>,1091) fake_defs.h:284: new->next = next;
  (<0>,1092) fake_defs.h:285: new->prev = prev;
  (<0>,1093) fake_defs.h:285: new->prev = prev;
  (<0>,1095) fake_defs.h:285: new->prev = prev;
  (<0>,1096) fake_defs.h:286: prev->next = new;
  (<0>,1097) fake_defs.h:286: prev->next = new;
  (<0>,1099) fake_defs.h:286: prev->next = new;
  (<0>,1110)
  (<0>,1111)
  (<0>,1112) tree.c:3642: int cpustride = 1;
  (<0>,1113) tree.c:3650: if (rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1116) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1118) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1119) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1122) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1125) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1126) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1128) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1133) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1135) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1137) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1141) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1143) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1144) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1147) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,1153)
  (<0>,1154) tree.c:3616: cprv = nr_cpu_ids;
  (<0>,1155) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1157) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1159) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1162) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1164) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1168) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1169) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1170) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1173) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1176) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1182) tree.c:3620: cprv = ccur;
  (<0>,1183) tree.c:3620: cprv = ccur;
  (<0>,1185) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1187) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1189) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1193) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1194) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1196) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1197) tree.c:3661: fl_mask <<= 1;
  (<0>,1201) tree.c:3661: fl_mask <<= 1;
  (<0>,1202) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1204) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1206) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1209) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1211) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1218) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1219) tree.c:3667: rnp = rsp->level[i];
  (<0>,1221) tree.c:3667: rnp = rsp->level[i];
  (<0>,1224) tree.c:3667: rnp = rsp->level[i];
  (<0>,1225) tree.c:3667: rnp = rsp->level[i];
  (<0>,1226) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1228) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1229) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1237) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,1241)
  (<0>,1242) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1243) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1249) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1253)
  (<0>,1254) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1255) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1261) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1263) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1264) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1267) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1269) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1270) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1273) tree.c:3677: rnp->qsmask = 0;
  (<0>,1275) tree.c:3677: rnp->qsmask = 0;
  (<0>,1276) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1278) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1279) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1280) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1282) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1285) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1287) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1293) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1295) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1298) tree.c:3683: if (i == 0) {
  (<0>,1301) tree.c:3684: rnp->grpnum = 0;
  (<0>,1303) tree.c:3684: rnp->grpnum = 0;
  (<0>,1304) tree.c:3685: rnp->grpmask = 0;
  (<0>,1306) tree.c:3685: rnp->grpmask = 0;
  (<0>,1307) tree.c:3686: rnp->parent = NULL;
  (<0>,1309) tree.c:3686: rnp->parent = NULL;
  (<0>,1311) tree.c:3693: rnp->level = i;
  (<0>,1313) tree.c:3693: rnp->level = i;
  (<0>,1315) tree.c:3693: rnp->level = i;
  (<0>,1316) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1320)
  (<0>,1321) fake_defs.h:275: list->next = list;
  (<0>,1322) fake_defs.h:275: list->next = list;
  (<0>,1324) fake_defs.h:275: list->next = list;
  (<0>,1325) fake_defs.h:276: list->prev = list;
  (<0>,1326) fake_defs.h:276: list->prev = list;
  (<0>,1328) fake_defs.h:276: list->prev = list;
  (<0>,1330) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,1333) tree_plugin.h:2694: }
  (<0>,1336) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1338) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1339) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1353) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1355) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1357) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1360) tree.c:3699: rsp->rda = rda;
  (<0>,1361) tree.c:3699: rsp->rda = rda;
  (<0>,1363) tree.c:3699: rsp->rda = rda;
  (<0>,1366) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1369) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1372) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1373) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1374) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1376) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1377) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1384)
  (<0>,1385)
  (<0>,1386) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1388) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1389) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1392) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1394) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1395) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1398) fake_defs.h:536: if (offset & mask)
  (<0>,1399) fake_defs.h:536: if (offset & mask)
  (<0>,1403) fake_defs.h:537: return cpu;
  (<0>,1404) fake_defs.h:537: return cpu;
  (<0>,1406) fake_defs.h:540: }
  (<0>,1408) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1409) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1413) tree.c:3703: while (i > rnp->grphi)
  (<0>,1414) tree.c:3703: while (i > rnp->grphi)
  (<0>,1416) tree.c:3703: while (i > rnp->grphi)
  (<0>,1419) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1420) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1422) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1428) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1429) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1442)
  (<0>,1443)
  (<0>,1444) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1446) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1448) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1450) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1451) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1454)
  (<0>,1455) tree.c:453: return &rsp->node[0];
  (<0>,1459) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1460) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1462) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1466)
  (<0>,1467)
  (<0>,1468) fake_sync.h:76: local_irq_save(flags);
  (<0>,1471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1473) fake_sched.h:43: return __running_cpu;
  (<0>,1477) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1479) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1483) fake_sched.h:43: return __running_cpu;
  (<0>,1487) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1493) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1494) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1498) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1499) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1501) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1507) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1509) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1510) tree.c:3412: init_callback_list(rdp);
  (<0>,1514)
  (<0>,1515) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1518) tree_plugin.h:2732: return false;
  (<0>,1521) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1523) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1529) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1531) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1533) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1538) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1540) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1542) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1545) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1547) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1549) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1554) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1556) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1558) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1561) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1563) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1565) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1570) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1572) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1574) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1577) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1579) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1581) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1586) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1588) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1590) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1594) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1596) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1597) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1599) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1600) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1603) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1606) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1608) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1610) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1616) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1618) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1624) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1626) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1627) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1629) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1638) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1640) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1647) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1649) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1650) tree.c:3418: rdp->cpu = cpu;
  (<0>,1651) tree.c:3418: rdp->cpu = cpu;
  (<0>,1653) tree.c:3418: rdp->cpu = cpu;
  (<0>,1654) tree.c:3419: rdp->rsp = rsp;
  (<0>,1655) tree.c:3419: rdp->rsp = rsp;
  (<0>,1657) tree.c:3419: rdp->rsp = rsp;
  (<0>,1658) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1661) tree_plugin.h:2711: }
  (<0>,1663) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1665) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1669)
  (<0>,1670)
  (<0>,1671) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1672) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1675) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1678) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1680) fake_sched.h:43: return __running_cpu;
  (<0>,1684) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1686) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1690) fake_sched.h:43: return __running_cpu;
  (<0>,1694) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1703) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1704) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1711)
  (<0>,1712)
  (<0>,1713) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1715) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1716) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1719) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1721) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1722) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1725) fake_defs.h:536: if (offset & mask)
  (<0>,1726) fake_defs.h:536: if (offset & mask)
  (<0>,1730) fake_defs.h:537: return cpu;
  (<0>,1731) fake_defs.h:537: return cpu;
  (<0>,1733) fake_defs.h:540: }
  (<0>,1735) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1736) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1740) tree.c:3703: while (i > rnp->grphi)
  (<0>,1741) tree.c:3703: while (i > rnp->grphi)
  (<0>,1743) tree.c:3703: while (i > rnp->grphi)
  (<0>,1746) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1747) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1749) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1755) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1756) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1769)
  (<0>,1770)
  (<0>,1771) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1773) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1775) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1777) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1778) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1781)
  (<0>,1782) tree.c:453: return &rsp->node[0];
  (<0>,1786) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1787) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1789) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1793)
  (<0>,1794)
  (<0>,1795) fake_sync.h:76: local_irq_save(flags);
  (<0>,1798) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1800) fake_sched.h:43: return __running_cpu;
  (<0>,1804) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1806) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1810) fake_sched.h:43: return __running_cpu;
  (<0>,1814) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1820) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1821) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1825) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1826) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1828) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1834) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1836) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1837) tree.c:3412: init_callback_list(rdp);
  (<0>,1841)
  (<0>,1842) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1845) tree_plugin.h:2732: return false;
  (<0>,1848) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1850) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1851) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1853) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1856) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1858) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1860) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1865) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1867) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1869) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1872) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1874) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1876) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1881) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1883) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1885) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1888) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1890) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1892) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1897) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1899) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1901) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1904) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1906) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1908) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1913) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1915) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1917) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1921) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1923) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1924) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1926) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1927) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1930) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1933) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1935) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1937) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1943) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1945) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1951) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1953) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1954) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1956) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1965) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1967) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1974) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1976) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1977) tree.c:3418: rdp->cpu = cpu;
  (<0>,1978) tree.c:3418: rdp->cpu = cpu;
  (<0>,1980) tree.c:3418: rdp->cpu = cpu;
  (<0>,1981) tree.c:3419: rdp->rsp = rsp;
  (<0>,1982) tree.c:3419: rdp->rsp = rsp;
  (<0>,1984) tree.c:3419: rdp->rsp = rsp;
  (<0>,1985) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1988) tree_plugin.h:2711: }
  (<0>,1990) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1992) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1996)
  (<0>,1997)
  (<0>,1998) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1999) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,2002) fake_sync.h:86: local_irq_restore(flags);
  (<0>,2005) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2007) fake_sched.h:43: return __running_cpu;
  (<0>,2011) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2013) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2017) fake_sched.h:43: return __running_cpu;
  (<0>,2021) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2030) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2031) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2038)
  (<0>,2039)
  (<0>,2040) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2042) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2043) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2046) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2048) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2049) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2052) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,2054) fake_defs.h:540: }
  (<0>,2056) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2057) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2060) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,2065)
  (<0>,2066)
  (<0>,2067) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2068) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2069) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2071) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2076)
  (<0>,2077)
  (<0>,2078)
  (<0>,2079) fake_defs.h:283: next->prev = new;
  (<0>,2080) fake_defs.h:283: next->prev = new;
  (<0>,2082) fake_defs.h:283: next->prev = new;
  (<0>,2083) fake_defs.h:284: new->next = next;
  (<0>,2084) fake_defs.h:284: new->next = next;
  (<0>,2086) fake_defs.h:284: new->next = next;
  (<0>,2087) fake_defs.h:285: new->prev = prev;
  (<0>,2088) fake_defs.h:285: new->prev = prev;
  (<0>,2090) fake_defs.h:285: new->prev = prev;
  (<0>,2091) fake_defs.h:286: prev->next = new;
  (<0>,2092) fake_defs.h:286: prev->next = new;
  (<0>,2094) fake_defs.h:286: prev->next = new;
  (<0>,2106) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2108) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2109) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2116)
  (<0>,2117)
  (<0>,2118) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2120) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2121) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2124) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2126) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2127) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2130) fake_defs.h:536: if (offset & mask)
  (<0>,2131) fake_defs.h:536: if (offset & mask)
  (<0>,2135) fake_defs.h:537: return cpu;
  (<0>,2136) fake_defs.h:537: return cpu;
  (<0>,2138) fake_defs.h:540: }
  (<0>,2140) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2141) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2144) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2163)
  (<0>,2164) tree.c:3493: static int rcu_cpu_notify(struct notifier_block *self,
  (<0>,2165) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2166) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2168) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2169) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2170) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2172) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2174) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2175) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2177) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2178) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2181) tree.c:3502: switch (action) {
  (<0>,2183) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2192)
  (<0>,2193) tree.c:3482: static void rcu_prepare_cpu(int cpu)
  (<0>,2194) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2195) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2199) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2200) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2201) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2203) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2207) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2208) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2216)
  (<0>,2217) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2218) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2220) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2222) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2224) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2225) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2228)
  (<0>,2229) tree.c:453: return &rsp->node[0];
  (<0>,2233) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2234) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2238)
  (<0>,2239) fake_sync.h:172: void mutex_lock(struct mutex *l)
  (<0>,2241) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2245) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2247) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2251)
  (<0>,2252)
  (<0>,2253) fake_sync.h:76: local_irq_save(flags);
  (<0>,2256) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2258) fake_sched.h:43: return __running_cpu;
  (<0>,2262) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2264) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2268) fake_sched.h:43: return __running_cpu;
  (<0>,2272) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2278) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2279) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2283) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2285) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2286) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2288) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2289) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2291) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2292) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2294) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2295) tree.c:3446: rdp->blimit = blimit;
  (<0>,2296) tree.c:3446: rdp->blimit = blimit;
  (<0>,2298) tree.c:3446: rdp->blimit = blimit;
  (<0>,2299) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2303)
  (<0>,2304) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2307) tree_plugin.h:2732: return false;
  (<0>,2310) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2312) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2313) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2315) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2318) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2320) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2322) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2325) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2327) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2329) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2331) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2334) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2336) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2338) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2341) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2343) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2345) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2347) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2350) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2352) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2354) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2357) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2359) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2361) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2363) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2366) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2368) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2370) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2373) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2375) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2377) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2379) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2383) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2385) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2387) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2388) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2390) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2393) tree_plugin.h:3164: }
  (<0>,2395) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2397) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2400) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2403) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2405) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2408) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2409) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2413)
  (<0>,2414) fake_sync.h:113: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2415) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2420) tree.c:3455: rnp = rdp->mynode;
  (<0>,2422) tree.c:3455: rnp = rdp->mynode;
  (<0>,2423) tree.c:3455: rnp = rdp->mynode;
  (<0>,2424) tree.c:3456: mask = rdp->grpmask;
  (<0>,2426) tree.c:3456: mask = rdp->grpmask;
  (<0>,2427) tree.c:3456: mask = rdp->grpmask;
  (<0>,2429) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2433) fake_sync.h:108: preempt_disable();
  (<0>,2435) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2436) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2440) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2441) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2443) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2445) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2446) tree.c:3461: mask = rnp->grpmask;
  (<0>,2448) tree.c:3461: mask = rnp->grpmask;
  (<0>,2449) tree.c:3461: mask = rnp->grpmask;
  (<0>,2450) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2451) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2453) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2456) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2458) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2459) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2461) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2462) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2464) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2465) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2467) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2468) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2470) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2471) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2473) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2477) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2481)
  (<0>,2482) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2483) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2488) tree.c:3475: rnp = rnp->parent;
  (<0>,2490) tree.c:3475: rnp = rnp->parent;
  (<0>,2491) tree.c:3475: rnp = rnp->parent;
  (<0>,2493) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2497) tree.c:3477: local_irq_restore(flags);
  (<0>,2500) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2502) fake_sched.h:43: return __running_cpu;
  (<0>,2506) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2508) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2512) fake_sched.h:43: return __running_cpu;
  (<0>,2516) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2521) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2525)
  (<0>,2526) fake_sync.h:178: void mutex_unlock(struct mutex *l)
  (<0>,2528) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2534) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2537) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2538) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2539) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2543) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2544) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2545) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2547) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2551) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2552) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2560)
  (<0>,2561)
  (<0>,2562) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2564) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2566) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2568) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2569) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2572)
  (<0>,2573) tree.c:453: return &rsp->node[0];
  (<0>,2577) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2578) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2582)
  (<0>,2583) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2585) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2589) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2591) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2595)
  (<0>,2596)
  (<0>,2597) fake_sync.h:76: local_irq_save(flags);
  (<0>,2600) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2602) fake_sched.h:43: return __running_cpu;
  (<0>,2606) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2608) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2612) fake_sched.h:43: return __running_cpu;
  (<0>,2616) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2622) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2623) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2627) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2629) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2630) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2632) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2633) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2635) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2636) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2638) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2639) tree.c:3446: rdp->blimit = blimit;
  (<0>,2640) tree.c:3446: rdp->blimit = blimit;
  (<0>,2642) tree.c:3446: rdp->blimit = blimit;
  (<0>,2643) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2647)
  (<0>,2648) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2651) tree_plugin.h:2732: return false;
  (<0>,2654) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2656) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2657) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2659) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2662) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2664) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2666) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2669) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2671) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2673) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2675) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2678) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2680) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2682) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2685) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2687) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2689) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2691) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2694) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2696) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2698) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2701) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2703) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2705) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2707) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2710) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2712) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2714) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2717) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2719) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2721) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2723) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2727) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2729) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2731) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2732) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2734) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2737) tree_plugin.h:3164: }
  (<0>,2739) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2741) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2744) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2747) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2749) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2752) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2753) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2757)
  (<0>,2758) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2759) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2764) tree.c:3455: rnp = rdp->mynode;
  (<0>,2766) tree.c:3455: rnp = rdp->mynode;
  (<0>,2767) tree.c:3455: rnp = rdp->mynode;
  (<0>,2768) tree.c:3456: mask = rdp->grpmask;
  (<0>,2770) tree.c:3456: mask = rdp->grpmask;
  (<0>,2771) tree.c:3456: mask = rdp->grpmask;
  (<0>,2773) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2777) fake_sync.h:108: preempt_disable();
  (<0>,2779) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2780) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2784) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2785) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2787) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2789) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2790) tree.c:3461: mask = rnp->grpmask;
  (<0>,2792) tree.c:3461: mask = rnp->grpmask;
  (<0>,2793) tree.c:3461: mask = rnp->grpmask;
  (<0>,2794) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2795) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2797) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2800) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2802) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2803) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2805) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2806) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2808) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2809) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2811) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2812) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2814) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2815) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2817) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2821) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2825)
  (<0>,2826) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2827) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2832) tree.c:3475: rnp = rnp->parent;
  (<0>,2834) tree.c:3475: rnp = rnp->parent;
  (<0>,2835) tree.c:3475: rnp = rnp->parent;
  (<0>,2837) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2841) tree.c:3477: local_irq_restore(flags);
  (<0>,2844) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2846) fake_sched.h:43: return __running_cpu;
  (<0>,2850) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2852) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2856) fake_sched.h:43: return __running_cpu;
  (<0>,2860) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2865) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2869)
  (<0>,2870) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2872) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2878) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2881) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2882) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2883) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2887) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2888) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2889) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2891) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2896) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,2900) tree_plugin.h:1499: }
  (<0>,2902) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2906) tree_plugin.h:2724: }
  (<0>,2913) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2914) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2921)
  (<0>,2922)
  (<0>,2923) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2925) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2926) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2929) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2931) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2932) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2935) fake_defs.h:536: if (offset & mask)
  (<0>,2936) fake_defs.h:536: if (offset & mask)
  (<0>,2940) fake_defs.h:537: return cpu;
  (<0>,2941) fake_defs.h:537: return cpu;
  (<0>,2943) fake_defs.h:540: }
  (<0>,2945) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2946) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2949) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2968)
  (<0>,2969)
  (<0>,2970)
  (<0>,2971) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2973) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2974) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2975) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2977) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2979) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2980) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2982) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2983) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2986) tree.c:3502: switch (action) {
  (<0>,2988) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2997)
  (<0>,2998) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2999) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3000) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3004) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3005) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3006) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3008) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3012) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3013) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3021)
  (<0>,3022)
  (<0>,3023) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3025) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3027) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3029) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3030) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3033)
  (<0>,3034) tree.c:453: return &rsp->node[0];
  (<0>,3038) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3039) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3043)
  (<0>,3044) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3046) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3050) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3052) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3056)
  (<0>,3057)
  (<0>,3058) fake_sync.h:76: local_irq_save(flags);
  (<0>,3061) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3063) fake_sched.h:43: return __running_cpu;
  (<0>,3067) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3069) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3073) fake_sched.h:43: return __running_cpu;
  (<0>,3077) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3083) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3084) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3088) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3090) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3091) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3093) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3094) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3096) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3097) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3099) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3100) tree.c:3446: rdp->blimit = blimit;
  (<0>,3101) tree.c:3446: rdp->blimit = blimit;
  (<0>,3103) tree.c:3446: rdp->blimit = blimit;
  (<0>,3104) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3108)
  (<0>,3109) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3112) tree_plugin.h:2732: return false;
  (<0>,3115) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3117) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3118) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3120) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3123) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3125) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3127) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3130) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3132) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3134) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3136) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3139) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3141) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3143) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3146) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3148) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3150) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3152) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3155) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3157) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3159) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3162) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3164) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3166) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3168) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3171) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3173) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3175) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3178) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3180) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3182) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3184) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3188) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3190) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3192) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3193) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3195) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3198) tree_plugin.h:3164: }
  (<0>,3200) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3202) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3205) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3208) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3210) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3213) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3214) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3218)
  (<0>,3219) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3220) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3225) tree.c:3455: rnp = rdp->mynode;
  (<0>,3227) tree.c:3455: rnp = rdp->mynode;
  (<0>,3228) tree.c:3455: rnp = rdp->mynode;
  (<0>,3229) tree.c:3456: mask = rdp->grpmask;
  (<0>,3231) tree.c:3456: mask = rdp->grpmask;
  (<0>,3232) tree.c:3456: mask = rdp->grpmask;
  (<0>,3234) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3238) fake_sync.h:108: preempt_disable();
  (<0>,3240) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3241) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3245) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3246) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3248) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3250) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3251) tree.c:3461: mask = rnp->grpmask;
  (<0>,3253) tree.c:3461: mask = rnp->grpmask;
  (<0>,3254) tree.c:3461: mask = rnp->grpmask;
  (<0>,3255) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3256) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3258) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3261) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3263) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3264) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3266) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3267) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3269) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3270) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3272) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3273) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3275) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3276) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3278) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3282) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3286)
  (<0>,3287) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3288) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3293) tree.c:3475: rnp = rnp->parent;
  (<0>,3295) tree.c:3475: rnp = rnp->parent;
  (<0>,3296) tree.c:3475: rnp = rnp->parent;
  (<0>,3298) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3302) tree.c:3477: local_irq_restore(flags);
  (<0>,3305) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3307) fake_sched.h:43: return __running_cpu;
  (<0>,3311) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3313) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3317) fake_sched.h:43: return __running_cpu;
  (<0>,3321) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3326) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3330)
  (<0>,3331) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3333) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3339) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3342) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3343) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3344) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3348) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3349) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3350) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3352) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3356) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3357) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3365)
  (<0>,3366)
  (<0>,3367) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3369) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3371) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3373) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3374) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3377)
  (<0>,3378) tree.c:453: return &rsp->node[0];
  (<0>,3382) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3383) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3387)
  (<0>,3388) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3390) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3394) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3396) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3400)
  (<0>,3401)
  (<0>,3402) fake_sync.h:76: local_irq_save(flags);
  (<0>,3405) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3407) fake_sched.h:43: return __running_cpu;
  (<0>,3411) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3413) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3417) fake_sched.h:43: return __running_cpu;
  (<0>,3421) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3427) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3428) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3432) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3434) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3435) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3437) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3438) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3440) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3441) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3443) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3444) tree.c:3446: rdp->blimit = blimit;
  (<0>,3445) tree.c:3446: rdp->blimit = blimit;
  (<0>,3447) tree.c:3446: rdp->blimit = blimit;
  (<0>,3448) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3452)
  (<0>,3453) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3456) tree_plugin.h:2732: return false;
  (<0>,3459) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3461) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3462) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3464) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3467) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3469) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3471) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3474) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3476) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3478) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3480) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3483) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3485) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3487) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3490) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3492) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3494) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3496) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3499) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3501) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3503) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3506) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3508) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3510) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3512) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3515) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3517) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3519) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3522) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3528) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3532) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3534) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3536) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3537) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3539) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3542) tree_plugin.h:3164: }
  (<0>,3544) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3546) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3549) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3552) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3554) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3557) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3558) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3562)
  (<0>,3563) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3564) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3569) tree.c:3455: rnp = rdp->mynode;
  (<0>,3571) tree.c:3455: rnp = rdp->mynode;
  (<0>,3572) tree.c:3455: rnp = rdp->mynode;
  (<0>,3573) tree.c:3456: mask = rdp->grpmask;
  (<0>,3575) tree.c:3456: mask = rdp->grpmask;
  (<0>,3576) tree.c:3456: mask = rdp->grpmask;
  (<0>,3578) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3582) fake_sync.h:108: preempt_disable();
  (<0>,3584) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3585) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3589) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3590) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3592) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3594) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3595) tree.c:3461: mask = rnp->grpmask;
  (<0>,3597) tree.c:3461: mask = rnp->grpmask;
  (<0>,3598) tree.c:3461: mask = rnp->grpmask;
  (<0>,3599) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3600) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3602) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3605) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3607) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3608) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3610) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3611) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3613) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3614) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3616) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3617) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3619) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3620) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3622) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3626) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3630)
  (<0>,3631) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3632) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3637) tree.c:3475: rnp = rnp->parent;
  (<0>,3639) tree.c:3475: rnp = rnp->parent;
  (<0>,3640) tree.c:3475: rnp = rnp->parent;
  (<0>,3642) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3646) tree.c:3477: local_irq_restore(flags);
  (<0>,3649) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3651) fake_sched.h:43: return __running_cpu;
  (<0>,3655) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3657) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3661) fake_sched.h:43: return __running_cpu;
  (<0>,3665) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3670) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3674)
  (<0>,3675) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3677) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3683) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3686) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3687) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3688) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3692) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3693) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3694) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3696) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3701) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,3705) tree_plugin.h:1499: }
  (<0>,3707) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,3711) tree_plugin.h:2724: }
  (<0>,3718) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3719) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3726)
  (<0>,3727)
  (<0>,3728) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3730) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3731) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3734) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3736) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3737) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3740) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,3742) fake_defs.h:540: }
  (<0>,3744) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3745) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3751) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3753) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3756) litmus.c:123: set_cpu(i);
  (<0>,3759)
  (<0>,3760) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,3761) fake_sched.h:56: __running_cpu = cpu;
  (<0>,3765) tree.c:578: unsigned long flags;
  (<0>,3768) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3770) fake_sched.h:43: return __running_cpu;
  (<0>,3774) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3776) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3780) fake_sched.h:43: return __running_cpu;
  (<0>,3784) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3797) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3799) fake_sched.h:43: return __running_cpu;
  (<0>,3803) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3804) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3806) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3807) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3808) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3814) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3815) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3820) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3821) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3822) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3823) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,3827) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3829) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3830) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3831) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3850)
  (<0>,3852) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3854) fake_sched.h:43: return __running_cpu;
  (<0>,3858) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3861) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,3865) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3866) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3867) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3871) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3872) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3873) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3875) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3880) fake_sched.h:43: return __running_cpu;
  (<0>,3883) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3885) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3887) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3888) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3891) tree_plugin.h:2720: }
  (<0>,3894) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3897) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3898) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3899) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3903) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3904) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3905) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3907) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3912) fake_sched.h:43: return __running_cpu;
  (<0>,3915) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3917) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3919) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3920) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3923) tree_plugin.h:2720: }
  (<0>,3926) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3929) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3930) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3931) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3935) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3936) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3937) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3939) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3946) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3949) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3950) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3951) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3953) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3954) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3956) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3959) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3965) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3966) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3969) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3974) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3975) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3976) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3990) tree_plugin.h:3141: }
  (<0>,3992) tree.c:583: local_irq_restore(flags);
  (<0>,3995) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3997) fake_sched.h:43: return __running_cpu;
  (<0>,4001) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4003) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4007) fake_sched.h:43: return __running_cpu;
  (<0>,4011) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4018) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4020) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4022) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4025) litmus.c:123: set_cpu(i);
  (<0>,4028)
  (<0>,4029) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4030) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4034) tree.c:580: local_irq_save(flags);
  (<0>,4037) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4039) fake_sched.h:43: return __running_cpu;
  (<0>,4043) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4045) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4049) fake_sched.h:43: return __running_cpu;
  (<0>,4053) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4066) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4068) fake_sched.h:43: return __running_cpu;
  (<0>,4072) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4073) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4075) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4076) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4077) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4083) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4084) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4089) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4090) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4091) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4092) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4096) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4098) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4099) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4100) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4119)
  (<0>,4121) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4123) fake_sched.h:43: return __running_cpu;
  (<0>,4127) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4130) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,4134) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4135) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4136) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4140) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4141) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4142) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4144) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4149) fake_sched.h:43: return __running_cpu;
  (<0>,4152) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4154) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4156) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4157) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4160) tree_plugin.h:2720: }
  (<0>,4163) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4166) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4167) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4168) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4172) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4173) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4181) fake_sched.h:43: return __running_cpu;
  (<0>,4184) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4186) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4188) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4189) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4192) tree_plugin.h:2720: }
  (<0>,4195) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4198) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4199) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4200) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4204) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4205) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4206) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4208) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4215) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4218) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4219) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4220) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4222) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4223) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4225) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4228) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4234) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4235) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4238) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4243) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4244) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4245) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4259) tree_plugin.h:3141: }
  (<0>,4261) tree.c:583: local_irq_restore(flags);
  (<0>,4264) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4266) fake_sched.h:43: return __running_cpu;
  (<0>,4270) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4272) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4276) fake_sched.h:43: return __running_cpu;
  (<0>,4280) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4287) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4289) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4291) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4303) tree.c:3561: unsigned long flags;
  (<0>,4304) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4305) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4306) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4310) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4314) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4320) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4322) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4331)
  (<0>,4332) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4333) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4334) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4335) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4336) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4339)
  (<0>,4340)
  (<0>,4347)
  (<0>,4348)
  (<0>,4355)
  (<0>,4356)
  (<0>,4363)
  (<0>,4364)
  (<0>,4371)
  (<0>,4372)
  (<0>,4379)
  (<0>,4380)
  (<0>,4387)
  (<0>,4388)
  (<0>,4395)
  (<0>,4396)
  (<0>,4403)
  (<0>,4404)
  (<0>,4411)
  (<0>,4412) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4421) fake_defs.h:730: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,4427) fake_defs.h:732: task = malloc(sizeof(*task));
  (<0>,4428) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4430) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4432) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4433) fake_defs.h:734: task->tid = t;
  (<0>,4434) fake_defs.h:734: task->tid = t;
  (<0>,4436) fake_defs.h:734: task->tid = t;
  (<0>,4437) fake_defs.h:735: return task;
  (<0>,4438) fake_defs.h:735: return task;
  (<0>,4440) fake_defs.h:738: }
  (<0>,4442) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4443) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4446)
  (<0>,4447) tree.c:453: return &rsp->node[0];
  (<0>,4451) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4452) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4454) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4458)
  (<0>,4459)
  (<0>,4460) fake_sync.h:76: local_irq_save(flags);
  (<0>,4463) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4465) fake_sched.h:43: return __running_cpu;
  (<0>,4469) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4475) fake_sched.h:43: return __running_cpu;
  (<0>,4479) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4485) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4486) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4490) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4491) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4493) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4494) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4496) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4500)
  (<0>,4501)
  (<0>,4502) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4503) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4506) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4509) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4511) fake_sched.h:43: return __running_cpu;
  (<0>,4515) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4517) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4521) fake_sched.h:43: return __running_cpu;
  (<0>,4525) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4533) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4537) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4538) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4542) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4543) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4544) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4546) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4550) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4552) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4554) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4563)
  (<0>,4564)
  (<0>,4565)
  (<0>,4566)
  (<0>,4567) fake_defs.h:727: struct task_struct *task = NULL;
  (<0>,4568) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4571)
  (<0>,4572)
  (<0>,4579)
  (<0>,4580)
  (<0>,4587)
  (<0>,4588)
  (<0>,4595)
  (<0>,4596)
  (<0>,4603)
  (<0>,4604) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4617) fake_defs.h:737: return NULL;
  (<0>,4619) fake_defs.h:738: }
  (<0>,4621) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4622) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4625)
  (<0>,4626) tree.c:453: return &rsp->node[0];
  (<0>,4630) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4631) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4633) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4637)
  (<0>,4638)
  (<0>,4639) fake_sync.h:76: local_irq_save(flags);
  (<0>,4642) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4644) fake_sched.h:43: return __running_cpu;
  (<0>,4648) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4650) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4654) fake_sched.h:43: return __running_cpu;
  (<0>,4658) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4664) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4665) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4669) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4670) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4672) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4673) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4675) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4679)
  (<0>,4680)
  (<0>,4681) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4682) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4685) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4688) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4690) fake_sched.h:43: return __running_cpu;
  (<0>,4694) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4696) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4700) fake_sched.h:43: return __running_cpu;
  (<0>,4704) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4712) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4715) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4716) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4717) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4721) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4722) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4723) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4725) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4739)
  (<0>,4740) litmus.c:51: void *thread_reader(void *arg)
  (<0>,4743)
  (<0>,4744) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4745) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4748) fake_sched.h:43: return __running_cpu;
  (<0>,4752)
  (<0>,4753) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,4756) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,4761) tree.c:702: unsigned long flags;
  (<0>,4764) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4766) fake_sched.h:43: return __running_cpu;
  (<0>,4770) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4772) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4780) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4793) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4795) fake_sched.h:43: return __running_cpu;
  (<0>,4799) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4800) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4802) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4803) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4804) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4809) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4810) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4814) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4815) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4816) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4817) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,4821) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4823) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4824) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4825) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4839)
  (<0>,4840) tree.c:644: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,4842) fake_sched.h:43: return __running_cpu;
  (<0>,4846) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4850) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4853) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4854) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4855) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4857) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4858) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4860) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4870) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4871) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4874) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4879) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4880) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4881) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4886) tree.c:656: if (!user && !is_idle_task(current)) {
  (<0>,4895) tree_plugin.h:3145: }
  (<0>,4897) tree.c:707: local_irq_restore(flags);
  (<0>,4900) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4902) fake_sched.h:43: return __running_cpu;
  (<0>,4906) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4908) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4912) fake_sched.h:43: return __running_cpu;
  (<0>,4916) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4932) litmus.c:57: r_x = x;
  (<0>,4933) litmus.c:57: r_x = x;
  (<0>,4937) fake_sched.h:43: return __running_cpu;
  (<0>,4941) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4949) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4954) fake_sched.h:43: return __running_cpu;
  (<0>,4958) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
  (<0>,4968) tree.c:745: unsigned long flags;
  (<0>,4971) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4973) fake_sched.h:43: return __running_cpu;
  (<0>,4977) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4984) fake_sched.h:43: return __running_cpu;
  (<0>,4988) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4989) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4991) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4992) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4993) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4995) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4997) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4998) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5000) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5005) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5006) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5008) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5012) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5013) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5014) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5015) tree.c:754: if (oldval)
  (<0>,5023) tree_plugin.h:3145: }
  (<0>,5025) tree.c:759: local_irq_restore(flags);
  (<0>,5028) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5030) fake_sched.h:43: return __running_cpu;
  (<0>,5034) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5036) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5044) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,5049) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,5054) fake_sched.h:43: return __running_cpu;
  (<0>,5059) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,5067) fake_sched.h:43: return __running_cpu;
  (<0>,5072) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
  (<0>,5078) fake_sched.h:43: return __running_cpu;
  (<0>,5083) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
  (<0>,5096) tree.c:3185: struct rcu_state *rsp;
  (<0>,5097) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5098) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5102) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5103) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5104) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5106) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5110) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5112) fake_sched.h:43: return __running_cpu;
  (<0>,5115) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5117) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5124)
  (<0>,5125) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5126) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5128) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5129) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5130) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5132) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5134) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5135) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5136) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5146)
  (<0>,5147) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5148) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5153) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
  (<0>,5156) tree_plugin.h:3185: return 0;
  (<0>,5159) tree.c:3135: if (rcu_scheduler_fully_active &&
  (<0>,5162) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5164) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5167) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5169) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5173) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,5176)
  (<0>,5177) tree.c:442: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,5179) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5182) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5189) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5190) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5196)
  (<0>,5197) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5198) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5201)
  (<0>,5202) tree.c:176: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,5204) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5205) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5207) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5213) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5219)
  (<0>,5220) tree.c:461: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,5223)
  (<0>,5224) tree.c:453: return &rsp->node[0];
  (<0>,5228) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5229) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5231) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5235) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5236) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5238) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5241) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5242) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5243) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5247) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5250) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5253) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5256) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5257) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5260) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5262) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5265) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5268) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5271) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5272) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5274) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5277) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5281) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5283) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5285) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5288) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5291) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5294) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5295) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5297) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5300) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5304) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5306) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5308) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5311) tree.c:493: return 0; /* No grace period needed. */
  (<0>,5313) tree.c:494: }
  (<0>,5317) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5319) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5320) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5322) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5325) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,1)
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:704: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,56) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,58) fake_sched.h:43: return __running_cpu;
      (<0.1>,62) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,63) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,67) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,72) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,73) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,77) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,78) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,79) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,80) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,84) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,86) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,87) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,88) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,102)
      (<0.1>,103) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,105) fake_sched.h:43: return __running_cpu;
      (<0.1>,109) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,113) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,116) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,117) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,118) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,120) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,123) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,126) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,133) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,137) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,143) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,144) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,149) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,158) tree_plugin.h:3145: }
      (<0.1>,160) tree.c:707: local_irq_restore(flags);
      (<0.1>,163) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,165) fake_sched.h:43: return __running_cpu;
      (<0.1>,169) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,171) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,175) fake_sched.h:43: return __running_cpu;
      (<0.1>,179) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,186) litmus.c:87: x = 1;
      (<0.1>,196) tree.c:2787: ret = num_online_cpus() <= 1;
      (<0.1>,198) tree.c:2789: return ret;
      (<0.1>,202) tree.c:2841: if (rcu_expedited)
      (<0.1>,208) update.c:223: init_rcu_head_on_stack(&rcu.head);
      (<0.1>,212) rcupdate.h:401: }
      (<0.1>,217)
      (<0.1>,218) fake_sync.h:232: x->done = 0;
      (<0.1>,220) fake_sync.h:232: x->done = 0;
      (<0.1>,224) update.c:226: crf(&rcu.head, wakeme_after_rcu);
      (<0.1>,229)
      (<0.1>,230)
      (<0.1>,231) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,232) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,249)
      (<0.1>,250)
      (<0.1>,251)
      (<0.1>,252)
      (<0.1>,254)
      (<0.1>,255) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,262) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,263) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,269) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,270) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,271) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,272) tree.c:2690: if (debug_rcu_head_queue(head)) {
      (<0.1>,275) rcu.h:92: return 0;
      (<0.1>,279) tree.c:2696: head->func = func;
      (<0.1>,280) tree.c:2696: head->func = func;
      (<0.1>,282) tree.c:2696: head->func = func;
      (<0.1>,283) tree.c:2697: head->next = NULL;
      (<0.1>,285) tree.c:2697: head->next = NULL;
      (<0.1>,286) tree.c:2705: local_irq_save(flags);
      (<0.1>,289) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,291) fake_sched.h:43: return __running_cpu;
      (<0.1>,295) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,297) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,301) fake_sched.h:43: return __running_cpu;
      (<0.1>,305) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,311) fake_sched.h:43: return __running_cpu;
      (<0.1>,314) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,316) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,318) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,319) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,322) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,330) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,333) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,335) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,337) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,339) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,340) tree.c:2721: if (lazy)
      (<0.1>,347) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,348) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,351) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,352) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,353) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,355) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,358) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,359) tree.c:2729: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,366) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,367) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,368) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,369) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,377)
      (<0.1>,378)
      (<0.1>,379)
      (<0.1>,380) tree.c:2628: if (!rcu_is_watching() && cpu_online(smp_processor_id()))
      (<0.1>,386) fake_sched.h:43: return __running_cpu;
      (<0.1>,392) tree.c:815: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,397) tree.c:829: ret = __rcu_is_watching();
      (<0.1>,399) tree.c:831: return ret;
      (<0.1>,403) tree.c:2632: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,406) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,408) fake_sched.h:43: return __running_cpu;
      (<0.1>,412) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,422) tree.c:2737: local_irq_restore(flags);
      (<0.1>,425) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,427) fake_sched.h:43: return __running_cpu;
      (<0.1>,431) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,433) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,437) fake_sched.h:43: return __running_cpu;
      (<0.1>,441) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,452) fake_sync.h:238: might_sleep();
      (<0.1>,456) fake_sched.h:43: return __running_cpu;
      (<0.1>,460) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,463) tree.c:580: local_irq_save(flags);
      (<0.1>,466) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,468) fake_sched.h:43: return __running_cpu;
      (<0.1>,472) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,474) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,478) fake_sched.h:43: return __running_cpu;
      (<0.1>,482) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,495) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,497) fake_sched.h:43: return __running_cpu;
      (<0.1>,501) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,502) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,504) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,505) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,506) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,512) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,513) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,518) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,519) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,520) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,521) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,525) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,527) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,528) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,529) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,548)
      (<0.1>,550) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,552) fake_sched.h:43: return __running_cpu;
      (<0.1>,556) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,559) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,563) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,564) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,565) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,569) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,570) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,571) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,573) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,578) fake_sched.h:43: return __running_cpu;
      (<0.1>,581) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,583) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,585) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,586) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,589) tree_plugin.h:2720: }
      (<0.1>,592) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,595) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,596) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,597) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,601) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,602) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,603) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,605) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,610) fake_sched.h:43: return __running_cpu;
      (<0.1>,613) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,615) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,617) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,618) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,621) tree_plugin.h:2720: }
      (<0.1>,624) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,627) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,628) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,629) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,633) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,635) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,637) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,644) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,647) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,648) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,649) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,651) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,652) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,654) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,657) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,663) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,664) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,667) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,672) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,673) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,674) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,688) tree_plugin.h:3141: }
      (<0.1>,690) tree.c:583: local_irq_restore(flags);
      (<0.1>,693) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,695) fake_sched.h:43: return __running_cpu;
      (<0.1>,699) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,701) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,705) fake_sched.h:43: return __running_cpu;
      (<0.1>,709) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,715) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,718) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,723) fake_sync.h:241: while (!x->done)
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:704: local_irq_save(flags);
    (<0.0>,35) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,64) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,66) fake_sched.h:43: return __running_cpu;
    (<0.0>,70) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,71) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,75) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,80) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,81) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,85) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,86) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,87) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,88) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,92) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,94) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,95) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,96) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,110)
    (<0.0>,111) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,113) fake_sched.h:43: return __running_cpu;
    (<0.0>,117) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,124) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,125) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,126) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,128) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,129) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,131) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,141) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,145) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,150) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,151) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,152) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,157) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,166) tree_plugin.h:3145: }
    (<0.0>,168) tree.c:707: local_irq_restore(flags);
    (<0.0>,171) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,173) fake_sched.h:43: return __running_cpu;
    (<0.0>,177) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,179) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,183) fake_sched.h:43: return __running_cpu;
    (<0.0>,187) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,194) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,206)
    (<0.0>,207) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,209) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,213)
    (<0.0>,214) tree.c:453: return &rsp->node[0];
    (<0.0>,218) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,223) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,225) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) fake_sched.h:43: return __running_cpu;
    (<0.0>,233) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,237) fake_sched.h:43: return __running_cpu;
    (<0.0>,241) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,246) fake_sched.h:43: return __running_cpu;
    (<0.0>,250) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,260) tree.c:749: local_irq_save(flags);
    (<0.0>,263) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,271) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,276) fake_sched.h:43: return __running_cpu;
    (<0.0>,280) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,281) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,283) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,284) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,285) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,287) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,289) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,290) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,292) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,297) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,298) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,300) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,304) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,305) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,306) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,307) tree.c:754: if (oldval)
    (<0.0>,315) tree_plugin.h:3145: }
    (<0.0>,317) tree.c:759: local_irq_restore(flags);
    (<0.0>,320) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,322) fake_sched.h:43: return __running_cpu;
    (<0.0>,326) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,328) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,336) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,341) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,346) fake_sched.h:43: return __running_cpu;
    (<0.0>,351) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,359) fake_sched.h:43: return __running_cpu;
    (<0.0>,364) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,370) fake_sched.h:43: return __running_cpu;
    (<0.0>,375) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,388) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,389) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,390) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,394) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,395) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,396) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,398) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,402) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,404) fake_sched.h:43: return __running_cpu;
    (<0.0>,407) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,409) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,416)
    (<0.0>,417)
    (<0.0>,418) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,420) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,421) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,422) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,424) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,426) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,427) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,428) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,438)
    (<0.0>,439)
    (<0.0>,440) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,445) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,448) tree_plugin.h:3185: return 0;
    (<0.0>,451) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,454) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,456) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,459) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,461) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,465) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,468)
    (<0.0>,469) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,471) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,474) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,481) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,482) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,488)
    (<0.0>,489)
    (<0.0>,490) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,493)
    (<0.0>,494) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,496) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,497) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,499) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,505) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,511)
    (<0.0>,512) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,515)
    (<0.0>,516) tree.c:453: return &rsp->node[0];
    (<0.0>,520) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,521) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,523) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,527) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,528) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,530) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,533) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,534) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,535) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,539) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,542) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,545) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,548) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,549) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,552) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,554) tree.c:494: }
    (<0.0>,558) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,560) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,562) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,563) tree.c:3152: return 1;
    (<0.0>,565) tree.c:3176: }
    (<0.0>,569) tree.c:3189: return 1;
    (<0.0>,571) tree.c:3191: }
    (<0.0>,578) fake_sched.h:43: return __running_cpu;
    (<0.0>,582) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,586) tree.c:2437: if (user)
    (<0.0>,594) fake_sched.h:43: return __running_cpu;
    (<0.0>,598) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,600) fake_sched.h:43: return __running_cpu;
    (<0.0>,604) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,610) fake_sched.h:43: return __running_cpu;
    (<0.0>,614) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,624) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,627) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,628) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,629) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,633) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,634) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,635) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,637) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,641) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,650) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,652) fake_sched.h:43: return __running_cpu;
    (<0.0>,655) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,657) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,659) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,660) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,662) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,669) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,670) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,672) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,678) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,679) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,680) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,681) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,682) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,686)
    (<0.0>,687)
    (<0.0>,688) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,689) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,696)
    (<0.0>,697)
    (<0.0>,698) tree.c:1584: local_irq_save(flags);
    (<0.0>,701) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,703) fake_sched.h:43: return __running_cpu;
    (<0.0>,707) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,709) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,713) fake_sched.h:43: return __running_cpu;
    (<0.0>,717) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,722) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,724) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,725) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,726) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,728) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,729) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,731) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,734) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,736) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,737) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,739) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,742) tree.c:1589: local_irq_restore(flags);
    (<0.0>,745) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,747) fake_sched.h:43: return __running_cpu;
    (<0.0>,751) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,753) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,757) fake_sched.h:43: return __running_cpu;
    (<0.0>,761) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,768) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,770) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,775) tree.c:2558: local_irq_save(flags);
    (<0.0>,778) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,780) fake_sched.h:43: return __running_cpu;
    (<0.0>,784) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,786) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,790) fake_sched.h:43: return __running_cpu;
    (<0.0>,794) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,799) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,800) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,806)
    (<0.0>,807)
    (<0.0>,808) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,811)
    (<0.0>,812) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,814) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,815) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,817) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,823) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,829)
    (<0.0>,830) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,833)
    (<0.0>,834) tree.c:453: return &rsp->node[0];
    (<0.0>,838) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,839) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,841) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,845) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,846) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,848) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,851) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,852) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,853) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,857) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,860) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,863) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,866) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,867) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,870) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,872) tree.c:494: }
    (<0.0>,876) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,879)
    (<0.0>,880) tree.c:453: return &rsp->node[0];
    (<0.0>,887) fake_sync.h:108: preempt_disable();
    (<0.0>,889) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,890) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,894) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,900) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,902) fake_sched.h:43: return __running_cpu;
    (<0.0>,905) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,907) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,909) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,910) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,913)
    (<0.0>,914) tree.c:453: return &rsp->node[0];
    (<0.0>,918) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,919) tree.c:1925: bool ret = false;
    (<0.0>,920) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,921) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,922) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,930)
    (<0.0>,931)
    (<0.0>,932)
    (<0.0>,933) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,936) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,939) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,942) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,943) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,946) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,948) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,951) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,953) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,954) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,956) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,959) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,964) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,966) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,967) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,970) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,972) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,975) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,977) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,980) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,981) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,984) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,987) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,989) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,992) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,993) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,995) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,998) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,999) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1001) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1004) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1005) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1007) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1010) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1012) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1014) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1015) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1017) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1019) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1022) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1024) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1027) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1028) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1031) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1034) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1036) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1039) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1040) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1042) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1045) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1046) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1048) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1051) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1052) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1054) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1057) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1059) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1061) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1062) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1064) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1066) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1069) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1070) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1071) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1080)
    (<0.0>,1081)
    (<0.0>,1082)
    (<0.0>,1083) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1086) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1089) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1092) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1093) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1096) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1097) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1102)
    (<0.0>,1103)
    (<0.0>,1104) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1107)
    (<0.0>,1108) tree.c:453: return &rsp->node[0];
    (<0.0>,1112) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1115) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1117) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1118) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1120) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1123) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1125) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1127) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1129) tree.c:1261: }
    (<0.0>,1131) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1132) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1134) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1137) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1139) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1142) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1143) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1146) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1149) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1153) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1155) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1157) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1160) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1162) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1165) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1166) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1169) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1172) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1176) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1178) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1180) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1183) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1185) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1189) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1192) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1195) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1196) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1198) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1201) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1202) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1203) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1205) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1208) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1210) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1212) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1214) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1217) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1220) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1221) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1223) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1226) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1227) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1228) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1230) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1233) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1235) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1237) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1239) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1242) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1245) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1246) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1248) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1251) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1252) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1253) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1255) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1258) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1260) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1262) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1264) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1267) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1268) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1277)
    (<0.0>,1278)
    (<0.0>,1279)
    (<0.0>,1280) tree.c:1289: bool ret = false;
    (<0.0>,1281) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1283) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1286)
    (<0.0>,1287) tree.c:453: return &rsp->node[0];
    (<0.0>,1291) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1292) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1294) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1295) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1300)
    (<0.0>,1301)
    (<0.0>,1302) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1305)
    (<0.0>,1306) tree.c:453: return &rsp->node[0];
    (<0.0>,1310) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1313) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1315) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1316) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1318) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1321) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1323) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1325) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1327) tree.c:1261: }
    (<0.0>,1329) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1330) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1331) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1332) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1338)
    (<0.0>,1339)
    (<0.0>,1340)
    (<0.0>,1341) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1345) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1347) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1350) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1353) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1355) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1356) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1358) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1361) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1363) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1364) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1366) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1369) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1370) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1373) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1375) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1376) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1381)
    (<0.0>,1382)
    (<0.0>,1383) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1386)
    (<0.0>,1387) tree.c:453: return &rsp->node[0];
    (<0.0>,1391) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1394) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1396) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1397) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1399) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1402) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1404) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1406) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1408) tree.c:1261: }
    (<0.0>,1410) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1411) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1413) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1416) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1417) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1419) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1422) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1426) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1427) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1429) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1432) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1435) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1437) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1439) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1442) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1443) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1445) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1448) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1453) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1455) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1457) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1460) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1461) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1463) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1466) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1471) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1473) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1475) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1478) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1480) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1483) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1486) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1488) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1491) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1493) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1494) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1496) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1497) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1499) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1502) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1503) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1504) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1510)
    (<0.0>,1511)
    (<0.0>,1512)
    (<0.0>,1513) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1517) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1519) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1520) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1521) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1527)
    (<0.0>,1528)
    (<0.0>,1529)
    (<0.0>,1530) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1532) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1535) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1536) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1542)
    (<0.0>,1543)
    (<0.0>,1544) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1547)
    (<0.0>,1548) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1550) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1551) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1553) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1559) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1565)
    (<0.0>,1566) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1569)
    (<0.0>,1570) tree.c:453: return &rsp->node[0];
    (<0.0>,1574) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1575) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1577) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1581) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1582) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1584) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1587) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1588) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1589) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1593) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1595) tree.c:494: }
    (<0.0>,1599) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1601) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1604) tree.c:1909: return true;
    (<0.0>,1606) tree.c:1910: }
    (<0.0>,1609) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1612) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1613) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1617) tree.c:1372: if (c_out != NULL)
    (<0.0>,1620) tree.c:1374: return ret;
    (<0.0>,1624) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1625) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1628) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1629) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1635) tree.c:1482: return ret;
    (<0.0>,1637) tree.c:1482: return ret;
    (<0.0>,1639) tree.c:1483: }
    (<0.0>,1641) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1643) tree.c:1527: }
    (<0.0>,1647) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1648) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1649) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1650) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1656)
    (<0.0>,1657)
    (<0.0>,1658)
    (<0.0>,1659) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1661) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1664) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1665) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1671)
    (<0.0>,1672)
    (<0.0>,1673) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1676)
    (<0.0>,1677) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1679) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1680) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1682) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1688) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1694)
    (<0.0>,1695) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1698)
    (<0.0>,1699) tree.c:453: return &rsp->node[0];
    (<0.0>,1703) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1704) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1706) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1710) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1711) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1713) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1716) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1717) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1718) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1722) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1724) tree.c:494: }
    (<0.0>,1728) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1730) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1733) tree.c:1909: return true;
    (<0.0>,1735) tree.c:1910: }
    (<0.0>,1739) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1740) tree.c:1937: return ret;
    (<0.0>,1744) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,1745) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1748)
    (<0.0>,1749) tree.c:453: return &rsp->node[0];
    (<0.0>,1754) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1758)
    (<0.0>,1759)
    (<0.0>,1760) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1761) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1764) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,1767) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1769) fake_sched.h:43: return __running_cpu;
    (<0.0>,1773) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1775) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1779) fake_sched.h:43: return __running_cpu;
    (<0.0>,1783) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1790) tree.c:2563: if (needwake)
    (<0.0>,1793) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,1796)
    (<0.0>,1797) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1798) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1800) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1807) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,1810)
    (<0.0>,1811) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1813) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1816) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1823) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,1826) tree_plugin.h:2720: }
    (<0.0>,1830) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1833) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1834) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1835) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1839) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1840) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1841) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1843) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1847) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,1856) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1858) fake_sched.h:43: return __running_cpu;
    (<0.0>,1861) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1863) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1865) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1866) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1868) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1875) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1876) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1878) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1884) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1885) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1886) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1887) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1888) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1892)
    (<0.0>,1893)
    (<0.0>,1894) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1895) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1902)
    (<0.0>,1903)
    (<0.0>,1904) tree.c:1584: local_irq_save(flags);
    (<0.0>,1907) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1909) fake_sched.h:43: return __running_cpu;
    (<0.0>,1913) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1915) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1919) fake_sched.h:43: return __running_cpu;
    (<0.0>,1923) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1928) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1930) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1931) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1932) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1934) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1935) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1937) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1940) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1942) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1943) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1945) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1948) tree.c:1589: local_irq_restore(flags);
    (<0.0>,1951) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1953) fake_sched.h:43: return __running_cpu;
    (<0.0>,1957) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1959) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1963) fake_sched.h:43: return __running_cpu;
    (<0.0>,1967) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1974) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1976) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1981) tree.c:2558: local_irq_save(flags);
    (<0.0>,1984) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1986) fake_sched.h:43: return __running_cpu;
    (<0.0>,1990) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1992) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1996) fake_sched.h:43: return __running_cpu;
    (<0.0>,2000) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2005) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2006) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2012)
    (<0.0>,2013)
    (<0.0>,2014) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,2017)
    (<0.0>,2018) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2020) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2021) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2023) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2029) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,2035)
    (<0.0>,2036) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2039)
    (<0.0>,2040) tree.c:453: return &rsp->node[0];
    (<0.0>,2044) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2045) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2047) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2051) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2052) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2054) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2057) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2058) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2059) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2063) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2066) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2069) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2072) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2073) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2076) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2078) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2081) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2084) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2087) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2088) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2090) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2093) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2097) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2099) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2101) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2104) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2107) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2110) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2111) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2113) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2116) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2120) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2122) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2124) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2127) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,2129) tree.c:494: }
    (<0.0>,2133) tree.c:2566: local_irq_restore(flags);
    (<0.0>,2136) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2138) fake_sched.h:43: return __running_cpu;
    (<0.0>,2142) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2144) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2148) fake_sched.h:43: return __running_cpu;
    (<0.0>,2152) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2158) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2161)
    (<0.0>,2162) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2164) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2167) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2174) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2177) tree_plugin.h:2720: }
    (<0.0>,2181) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2184) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2185) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2186) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2190) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2191) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2192) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2194) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2202) fake_sched.h:43: return __running_cpu;
    (<0.0>,2206) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,2215) tree.c:624: local_irq_save(flags);
    (<0.0>,2218) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2220) fake_sched.h:43: return __running_cpu;
    (<0.0>,2224) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2226) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2230) fake_sched.h:43: return __running_cpu;
    (<0.0>,2234) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2240) fake_sched.h:43: return __running_cpu;
    (<0.0>,2244) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2245) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2247) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2248) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2249) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2251) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2253) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2254) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2256) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2261) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2262) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2264) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2268) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2269) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2270) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2271) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2273) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2281) tree_plugin.h:3141: }
    (<0.0>,2283) tree.c:634: local_irq_restore(flags);
    (<0.0>,2286) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2288) fake_sched.h:43: return __running_cpu;
    (<0.0>,2292) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2294) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2298) fake_sched.h:43: return __running_cpu;
    (<0.0>,2302) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2311) fake_sched.h:43: return __running_cpu;
    (<0.0>,2315) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,2318) tree.c:580: local_irq_save(flags);
    (<0.0>,2321) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2323) fake_sched.h:43: return __running_cpu;
    (<0.0>,2327) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2329) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2333) fake_sched.h:43: return __running_cpu;
    (<0.0>,2337) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2350) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2352) fake_sched.h:43: return __running_cpu;
    (<0.0>,2356) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2357) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2359) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2360) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2361) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2367) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2368) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2373) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2374) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2375) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2376) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,2380) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2382) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2383) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2384) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2403)
    (<0.0>,2405) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2407) fake_sched.h:43: return __running_cpu;
    (<0.0>,2411) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2414) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,2418) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2419) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2420) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2424) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2425) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2426) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2428) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2433) fake_sched.h:43: return __running_cpu;
    (<0.0>,2436) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2438) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2440) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2441) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2444) tree_plugin.h:2720: }
    (<0.0>,2447) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2450) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2451) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2452) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2456) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2457) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2458) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2460) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2465) fake_sched.h:43: return __running_cpu;
    (<0.0>,2468) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2470) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2472) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2473) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2476) tree_plugin.h:2720: }
    (<0.0>,2479) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2482) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2483) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2484) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2488) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2489) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2490) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2492) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2499) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2502) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2503) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2504) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2506) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2507) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2509) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2512) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2518) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2519) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2522) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2527) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2528) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2529) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2543) tree_plugin.h:3141: }
    (<0.0>,2545) tree.c:583: local_irq_restore(flags);
    (<0.0>,2548) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2550) fake_sched.h:43: return __running_cpu;
    (<0.0>,2554) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2556) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2560) fake_sched.h:43: return __running_cpu;
    (<0.0>,2564) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2570) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2573) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2578) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2580) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2589) fake_sched.h:43: return __running_cpu;
    (<0.0>,2593)
    (<0.0>,2594) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2597) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2602) tree.c:704: local_irq_save(flags);
    (<0.0>,2605) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2607) fake_sched.h:43: return __running_cpu;
    (<0.0>,2611) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2613) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2617) fake_sched.h:43: return __running_cpu;
    (<0.0>,2621) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2634) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2636) fake_sched.h:43: return __running_cpu;
    (<0.0>,2640) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2641) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2643) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2644) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2645) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2650) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2651) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2655) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2656) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2657) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2658) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,2662) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2664) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2665) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2666) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2680)
    (<0.0>,2681) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2683) fake_sched.h:43: return __running_cpu;
    (<0.0>,2687) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2691) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2694) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2695) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2696) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2698) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2699) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2701) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2704) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2711) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2712) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2715) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2720) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2721) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2722) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2727) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,2736) tree_plugin.h:3145: }
    (<0.0>,2738) tree.c:707: local_irq_restore(flags);
    (<0.0>,2741) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2743) fake_sched.h:43: return __running_cpu;
    (<0.0>,2747) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2749) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2753) fake_sched.h:43: return __running_cpu;
    (<0.0>,2757) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2764) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,2776)
    (<0.0>,2777) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2780)
    (<0.0>,2781) tree.c:453: return &rsp->node[0];
    (<0.0>,2785) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2789) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2793) fake_sync.h:92: local_irq_disable();
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2804) fake_sched.h:43: return __running_cpu;
    (<0.0>,2808) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2813) fake_sched.h:43: return __running_cpu;
    (<0.0>,2817) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,2820) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2821) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2827) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2829) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2832) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2834) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2835) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2838)
    (<0.0>,2839) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2841) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2842) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2844) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2852) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2853) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2856)
    (<0.0>,2857) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2859) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2860) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2870) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2871) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2874) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,2879)
    (<0.0>,2880) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2881) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2882) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2883) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2885) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2889) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2890) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2891) update.c:344: if (till_stall_check < 3) {
    (<0.0>,2894) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,2898) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,2903) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,2904) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2905) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2907) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2909) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2910) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2911) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2914) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2916) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2920) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2922) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2924) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2926) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2930) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,2934)
    (<0.0>,2935) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2936) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2941) fake_sched.h:43: return __running_cpu;
    (<0.0>,2945) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,2947) fake_sched.h:43: return __running_cpu;
    (<0.0>,2951) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2957) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,2961)
    (<0.0>,2962) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2964) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2970) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2973) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2975) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2976) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2978) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2983) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2987) fake_sync.h:92: local_irq_disable();
    (<0.0>,2990) fake_sched.h:43: return __running_cpu;
    (<0.0>,2994) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2998) fake_sched.h:43: return __running_cpu;
    (<0.0>,3002) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3007) fake_sched.h:43: return __running_cpu;
    (<0.0>,3011) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3014) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3015) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3022) fake_sched.h:43: return __running_cpu;
    (<0.0>,3025) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3027) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3029) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3030) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,3036)
    (<0.0>,3037) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3039) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3044) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3045) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3047) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3051) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3052) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3053) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3055) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3057) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3058) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3060) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3061) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3063) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3064) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3066) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3067) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3069) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3070) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3072) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3077) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3078) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3080) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3081) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3083) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3087) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3088) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3089) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3090) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3092) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3093) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3095) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3096) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3097) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3099) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3102) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3103) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3104) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3110)
    (<0.0>,3111)
    (<0.0>,3112)
    (<0.0>,3113) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3115) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3116) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3118) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3121) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3122) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3123) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3132)
    (<0.0>,3133)
    (<0.0>,3134)
    (<0.0>,3135) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3138) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3141) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3144) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3145) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3148) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3149) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3154)
    (<0.0>,3155)
    (<0.0>,3156) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3159)
    (<0.0>,3160) tree.c:453: return &rsp->node[0];
    (<0.0>,3164) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3167) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3169) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3170) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3172) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3175) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3177) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3179) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3181) tree.c:1261: }
    (<0.0>,3183) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3184) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3186) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3189) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3191) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3194) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3195) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3198) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3201) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3205) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3207) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3209) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3212) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3214) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3217) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3218) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3221) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3224) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3227) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3229) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3232) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3233) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3238) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3240) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3244) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3247) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3250) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3251) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3253) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3256) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3257) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3258) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3260) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3263) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3265) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3267) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3269) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3272) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3275) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3276) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3278) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3281) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3282) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3283) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3285) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3288) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3290) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3292) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3294) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3297) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3298) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3307)
    (<0.0>,3308)
    (<0.0>,3309)
    (<0.0>,3310) tree.c:1289: bool ret = false;
    (<0.0>,3311) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3313) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3316)
    (<0.0>,3317) tree.c:453: return &rsp->node[0];
    (<0.0>,3321) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3322) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3324) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3325) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3330)
    (<0.0>,3331)
    (<0.0>,3332) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3335)
    (<0.0>,3336) tree.c:453: return &rsp->node[0];
    (<0.0>,3340) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3343) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3345) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3346) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3348) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3351) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3353) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3355) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3357) tree.c:1261: }
    (<0.0>,3359) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3360) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3361) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3362) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3368)
    (<0.0>,3369)
    (<0.0>,3370)
    (<0.0>,3371) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,3375) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3377) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3380) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3383) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3385) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3386) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3388) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3391) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3393) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3396) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3398) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3399) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3400) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3401) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3407)
    (<0.0>,3408)
    (<0.0>,3409)
    (<0.0>,3410) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,3415) tree.c:1372: if (c_out != NULL)
    (<0.0>,3418) tree.c:1374: return ret;
    (<0.0>,3422) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3423) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3426) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3427) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3433) tree.c:1482: return ret;
    (<0.0>,3435) tree.c:1482: return ret;
    (<0.0>,3437) tree.c:1483: }
    (<0.0>,3440) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3442) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3444) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3445) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3447) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3450) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3452) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3453) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3455) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3458) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3460) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3461) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3463) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3464) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3466) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3471) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3474) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3475) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,3478) tree_plugin.h:1950: }
    (<0.0>,3481) tree.c:1575: return ret;
    (<0.0>,3485) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,3488) tree_plugin.h:1487: }
    (<0.0>,3492) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,3496)
    (<0.0>,3497) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3498) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3503) fake_sched.h:43: return __running_cpu;
    (<0.0>,3507) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,3509) fake_sched.h:43: return __running_cpu;
    (<0.0>,3513) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3526) fake_sched.h:43: return __running_cpu;
    (<0.0>,3531) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,3537) fake_sched.h:43: return __running_cpu;
    (<0.0>,3542) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,3548) fake_sched.h:43: return __running_cpu;
    (<0.0>,3552) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,3564) fake_sched.h:43: return __running_cpu;
    (<0.0>,3568) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,3571) tree.c:580: local_irq_save(flags);
    (<0.0>,3574) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3576) fake_sched.h:43: return __running_cpu;
    (<0.0>,3580) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3582) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3586) fake_sched.h:43: return __running_cpu;
    (<0.0>,3590) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3603) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3605) fake_sched.h:43: return __running_cpu;
    (<0.0>,3609) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3610) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3612) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3613) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3614) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3620) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3621) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3626) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3627) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3628) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3629) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3633) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3635) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3636) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3637) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3656)
    (<0.0>,3658) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3660) fake_sched.h:43: return __running_cpu;
    (<0.0>,3664) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3667) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,3671) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3672) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3673) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3677) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3678) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3679) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3681) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3686) fake_sched.h:43: return __running_cpu;
    (<0.0>,3689) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3691) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3693) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3694) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3697) tree_plugin.h:2720: }
    (<0.0>,3700) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3703) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3704) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3705) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3709) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3710) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3711) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3713) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3718) fake_sched.h:43: return __running_cpu;
    (<0.0>,3721) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3723) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3725) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3726) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3729) tree_plugin.h:2720: }
    (<0.0>,3732) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3735) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3736) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3737) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3741) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3742) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3743) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3745) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3752) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3755) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3756) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3757) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3759) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3760) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3762) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3765) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3771) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3772) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3775) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3780) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3781) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3782) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3796) tree_plugin.h:3141: }
    (<0.0>,3798) tree.c:583: local_irq_restore(flags);
    (<0.0>,3801) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3803) fake_sched.h:43: return __running_cpu;
    (<0.0>,3807) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3809) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3813) fake_sched.h:43: return __running_cpu;
    (<0.0>,3817) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3823) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3826) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3831) fake_sched.h:43: return __running_cpu;
    (<0.0>,3835)
    (<0.0>,3836) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3839) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3844) tree.c:704: local_irq_save(flags);
    (<0.0>,3847) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3849) fake_sched.h:43: return __running_cpu;
    (<0.0>,3853) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3855) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3859) fake_sched.h:43: return __running_cpu;
    (<0.0>,3863) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3876) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3878) fake_sched.h:43: return __running_cpu;
    (<0.0>,3882) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3883) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3885) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3886) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3887) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3892) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3893) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3897) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3898) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3899) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3900) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3904) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3906) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3907) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3908) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3922)
    (<0.0>,3923) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3925) fake_sched.h:43: return __running_cpu;
    (<0.0>,3929) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3933) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3936) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3937) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3938) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3940) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3941) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3943) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3946) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3953) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3954) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3957) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3962) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3963) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3964) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3969) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,3978) tree_plugin.h:3145: }
    (<0.0>,3980) tree.c:707: local_irq_restore(flags);
    (<0.0>,3983) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3985) fake_sched.h:43: return __running_cpu;
    (<0.0>,3989) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3991) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3995) fake_sched.h:43: return __running_cpu;
    (<0.0>,3999) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4014) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4016) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4018) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4019) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4021) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4026) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,4030)
    (<0.0>,4031) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4033) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4037) tree.c:1674: return 1;
    (<0.0>,4039) tree.c:1675: }
    (<0.0>,4044) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,4045) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4046) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4047) tree.c:1819: if (j > HZ) {
    (<0.0>,4050) tree.c:1823: ret = 0;
    (<0.0>,4052) tree.c:1825: if (!ret)
    (<0.0>,4055) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4056) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4058) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4060) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4064) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4066) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4070) fake_sched.h:43: return __running_cpu;
    (<0.0>,4074) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4078) fake_sched.h:43: return __running_cpu;
    (<0.0>,4082) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4087) fake_sched.h:43: return __running_cpu;
    (<0.0>,4091) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4101) tree.c:749: local_irq_save(flags);
    (<0.0>,4104) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4106) fake_sched.h:43: return __running_cpu;
    (<0.0>,4110) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4112) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4117) fake_sched.h:43: return __running_cpu;
    (<0.0>,4121) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4122) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4124) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4125) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4126) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4128) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4130) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4131) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4133) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4138) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4139) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4141) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4145) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4146) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4147) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4148) tree.c:754: if (oldval)
    (<0.0>,4156) tree_plugin.h:3145: }
    (<0.0>,4158) tree.c:759: local_irq_restore(flags);
    (<0.0>,4161) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4163) fake_sched.h:43: return __running_cpu;
    (<0.0>,4167) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4169) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4177) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,4182) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,4187) fake_sched.h:43: return __running_cpu;
    (<0.0>,4192) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,4200) fake_sched.h:43: return __running_cpu;
    (<0.0>,4205) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,4219) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4220) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4221) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4225) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4226) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4227) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4229) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4233) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4235) fake_sched.h:43: return __running_cpu;
    (<0.0>,4238) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4240) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4247)
    (<0.0>,4248)
    (<0.0>,4249) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4251) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4252) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4253) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4255) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4257) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4258) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4259) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4269)
    (<0.0>,4270)
    (<0.0>,4271) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,4276) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,4279) tree_plugin.h:3185: return 0;
    (<0.0>,4282) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,4285) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4287) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4290) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4292) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4295) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4297) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4300) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4302) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4305) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4307) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4309) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4310) tree.c:3140: return 1;
    (<0.0>,4312) tree.c:3176: }
    (<0.0>,4316) tree.c:3189: return 1;
    (<0.0>,4318) tree.c:3191: }
    (<0.0>,4325) fake_sched.h:43: return __running_cpu;
    (<0.0>,4329) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,4333) tree.c:2437: if (user)
    (<0.0>,4341) fake_sched.h:43: return __running_cpu;
    (<0.0>,4345) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4347) fake_sched.h:43: return __running_cpu;
    (<0.0>,4351) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4357) fake_sched.h:43: return __running_cpu;
    (<0.0>,4361) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,4371) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,4374) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4375) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4376) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4380) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4381) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4382) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4384) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4388) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,4397) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4399) fake_sched.h:43: return __running_cpu;
    (<0.0>,4402) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4404) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4406) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4407) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4409) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4416) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4417) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4419) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4425) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4426) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4427) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4428) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4429) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4433)
    (<0.0>,4434)
    (<0.0>,4435) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4436) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4443)
    (<0.0>,4444)
    (<0.0>,4445) tree.c:1584: local_irq_save(flags);
    (<0.0>,4448) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4450) fake_sched.h:43: return __running_cpu;
    (<0.0>,4454) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4456) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4460) fake_sched.h:43: return __running_cpu;
    (<0.0>,4464) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4469) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4471) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4472) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4473) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4475) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4476) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4478) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4481) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4483) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4484) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4486) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4489) tree.c:1589: local_irq_restore(flags);
    (<0.0>,4492) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4494) fake_sched.h:43: return __running_cpu;
    (<0.0>,4498) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4500) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4504) fake_sched.h:43: return __running_cpu;
    (<0.0>,4508) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4515) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4517) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4520) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,4522) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,4525) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4527) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4528) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4529) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4538)
    (<0.0>,4539)
    (<0.0>,4540)
    (<0.0>,4541) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4543) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4544) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4545) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,4547) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,4551)
    (<0.0>,4552)
    (<0.0>,4553) fake_sync.h:76: local_irq_save(flags);
    (<0.0>,4556) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4558) fake_sched.h:43: return __running_cpu;
    (<0.0>,4562) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4564) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4568) fake_sched.h:43: return __running_cpu;
    (<0.0>,4572) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4578) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,4579) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,4585) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4587) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4592) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4594) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4595) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4597) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4600) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4602) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4603) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4605) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4608) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4610) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4611) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4612) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4614) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4615) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4619) tree.c:2054: rdp->qs_pending = 0;
    (<0.0>,4621) tree.c:2054: rdp->qs_pending = 0;
    (<0.0>,4622) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4623) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4624) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4633)
    (<0.0>,4634)
    (<0.0>,4635)
    (<0.0>,4636) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4639) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4642) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4645) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4646) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4649) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4650) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4655)
    (<0.0>,4656)
    (<0.0>,4657) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4660)
    (<0.0>,4661) tree.c:453: return &rsp->node[0];
    (<0.0>,4665) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4668) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4670) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4671) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4673) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4676) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4678) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4680) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4682) tree.c:1261: }
    (<0.0>,4684) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4685) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4687) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4690) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4692) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4695) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4696) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4699) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4702) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4706) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4708) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4710) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4713) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4715) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4718) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4719) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4722) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4725) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4728) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4730) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4733) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4734) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4739) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4741) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4745) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4748) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4751) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4752) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4754) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4757) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4758) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4759) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4761) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4764) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4766) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4768) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4770) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4773) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4776) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4777) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4779) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4782) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4783) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4784) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4786) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4789) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4791) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4793) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4795) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4798) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4799) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4808)
    (<0.0>,4809)
    (<0.0>,4810)
    (<0.0>,4811) tree.c:1289: bool ret = false;
    (<0.0>,4812) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4814) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4817)
    (<0.0>,4818) tree.c:453: return &rsp->node[0];
    (<0.0>,4822) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4823) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4825) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4826) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4831)
    (<0.0>,4832)
    (<0.0>,4833) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4836)
    (<0.0>,4837) tree.c:453: return &rsp->node[0];
    (<0.0>,4841) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4844) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4846) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4847) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4849) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4852) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4854) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4856) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4858) tree.c:1261: }
    (<0.0>,4860) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4861) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4862) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4863) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4869)
    (<0.0>,4870)
    (<0.0>,4871)
    (<0.0>,4872) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4876) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4878) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4881) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4884) tree.c:1304: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,4885) tree.c:1304: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,4886) tree.c:1304: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,4892)
    (<0.0>,4893)
    (<0.0>,4894)
    (<0.0>,4895) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4900) tree.c:1372: if (c_out != NULL)
    (<0.0>,4903) tree.c:1374: return ret;
    (<0.0>,4907) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4908) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4911) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4912) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4918) tree.c:1482: return ret;
    (<0.0>,4920) tree.c:1482: return ret;
    (<0.0>,4922) tree.c:1483: }
    (<0.0>,4925) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4926) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,4927) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,4928) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,4929) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,4939)
    (<0.0>,4940)
    (<0.0>,4941)
    (<0.0>,4942) tree.c:1974: for (;;) {
    (<0.0>,4944) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,4946) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,4947) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,4951) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,4953) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,4955) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,4957) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,4960) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,4962) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,4965) tree.c:1990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,4967) tree.c:1990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,4971)
    (<0.0>,4972)
    (<0.0>,4973) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,4974) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,4977) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,4980) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4982) fake_sched.h:43: return __running_cpu;
    (<0.0>,4986) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4988) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4992) fake_sched.h:43: return __running_cpu;
    (<0.0>,4996) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5005) tree.c:2063: if (needwake)
    (<0.0>,5012) tree.c:2558: local_irq_save(flags);
    (<0.0>,5015) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5017) fake_sched.h:43: return __running_cpu;
    (<0.0>,5021) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5023) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5027) fake_sched.h:43: return __running_cpu;
    (<0.0>,5031) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5036) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,5037) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,5043)
    (<0.0>,5044)
    (<0.0>,5045) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,5048)
    (<0.0>,5049) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5051) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5052) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5054) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5060) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,5062) tree.c:494: }
    (<0.0>,5066) tree.c:2566: local_irq_restore(flags);
    (<0.0>,5069) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5071) fake_sched.h:43: return __running_cpu;
    (<0.0>,5075) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5077) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5081) fake_sched.h:43: return __running_cpu;
    (<0.0>,5085) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5091) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,5094)
    (<0.0>,5095) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5097) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5100) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5107) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5110) tree_plugin.h:2720: }
    (<0.0>,5114) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5117) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5118) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5119) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5123) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5124) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5125) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5127) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5131) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,5140) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5142) fake_sched.h:43: return __running_cpu;
    (<0.0>,5145) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5147) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5149) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5150) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5152) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5159) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5160) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5162) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5168) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5169) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5170) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5171) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,5172) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,5176)
    (<0.0>,5177)
    (<0.0>,5178) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,5179) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,5186)
    (<0.0>,5187)
    (<0.0>,5188) tree.c:1584: local_irq_save(flags);
    (<0.0>,5191) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5193) fake_sched.h:43: return __running_cpu;
    (<0.0>,5197) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5199) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5203) fake_sched.h:43: return __running_cpu;
    (<0.0>,5207) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5212) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,5214) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,5215) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,5216) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,5218) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,5219) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,5221) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,5224) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,5226) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,5227) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,5229) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,5232) tree.c:1589: local_irq_restore(flags);
    (<0.0>,5235) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5237) fake_sched.h:43: return __running_cpu;
    (<0.0>,5241) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5243) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5247) fake_sched.h:43: return __running_cpu;
    (<0.0>,5251) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5258) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,5260) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,5265) tree.c:2558: local_irq_save(flags);
    (<0.0>,5268) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5270) fake_sched.h:43: return __running_cpu;
    (<0.0>,5274) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5276) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5280) fake_sched.h:43: return __running_cpu;
    (<0.0>,5284) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5289) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,5290) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,5296)
    (<0.0>,5297)
    (<0.0>,5298) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,5301)
    (<0.0>,5302) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5304) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5305) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5307) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5313) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,5319)
    (<0.0>,5320) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5323)
    (<0.0>,5324) tree.c:453: return &rsp->node[0];
    (<0.0>,5328) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5329) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,5331) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,5335) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,5336) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,5338) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,5341) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,5342) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,5343) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,5347) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5350) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5353) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5356) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5357) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5360) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5362) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5365) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5368) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5371) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5372) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5374) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5377) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5381) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5383) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5385) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5388) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5391) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5394) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5395) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5397) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5400) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5404) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5406) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5408) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5411) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,5413) tree.c:494: }
    (<0.0>,5417) tree.c:2566: local_irq_restore(flags);
    (<0.0>,5420) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5422) fake_sched.h:43: return __running_cpu;
    (<0.0>,5426) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5428) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5432) fake_sched.h:43: return __running_cpu;
    (<0.0>,5436) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5442) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,5445)
    (<0.0>,5446) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5448) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5451) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5458) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5461) tree_plugin.h:2720: }
    (<0.0>,5465) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5468) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5469) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5470) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5474) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5475) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5476) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5478) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5486) fake_sched.h:43: return __running_cpu;
    (<0.0>,5490) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,5499) tree.c:624: local_irq_save(flags);
    (<0.0>,5502) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5504) fake_sched.h:43: return __running_cpu;
    (<0.0>,5508) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5510) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5514) fake_sched.h:43: return __running_cpu;
    (<0.0>,5518) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5524) fake_sched.h:43: return __running_cpu;
    (<0.0>,5528) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5529) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5531) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5532) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5533) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5535) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5537) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5538) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5540) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5545) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5546) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5548) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5552) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5553) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5554) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5555) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,5557) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,5565) tree_plugin.h:3141: }
    (<0.0>,5567) tree.c:634: local_irq_restore(flags);
    (<0.0>,5570) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5572) fake_sched.h:43: return __running_cpu;
    (<0.0>,5576) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5578) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5582) fake_sched.h:43: return __running_cpu;
    (<0.0>,5586) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5595) fake_sched.h:43: return __running_cpu;
    (<0.0>,5599) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,5602) tree.c:580: local_irq_save(flags);
    (<0.0>,5605) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5607) fake_sched.h:43: return __running_cpu;
    (<0.0>,5611) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5613) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5617) fake_sched.h:43: return __running_cpu;
    (<0.0>,5621) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5634) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5636) fake_sched.h:43: return __running_cpu;
    (<0.0>,5640) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5641) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5643) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5644) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5645) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5651) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5652) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5657) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5658) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5659) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5660) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5664) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5666) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5667) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5668) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5687)
    (<0.0>,5689) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5691) fake_sched.h:43: return __running_cpu;
    (<0.0>,5695) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5698) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,5702) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5703) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5704) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5708) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5709) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5710) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5712) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5717) fake_sched.h:43: return __running_cpu;
    (<0.0>,5720) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5722) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5724) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5725) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5728) tree_plugin.h:2720: }
    (<0.0>,5731) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5734) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5735) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5736) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5740) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5741) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5742) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5744) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5749) fake_sched.h:43: return __running_cpu;
    (<0.0>,5752) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5754) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5756) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5757) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5760) tree_plugin.h:2720: }
    (<0.0>,5763) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5766) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5767) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5768) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5772) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5773) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5774) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5776) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5783) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5786) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5787) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5788) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5790) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5791) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5793) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5796) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5802) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5803) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5806) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5811) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5812) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5813) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5827) tree_plugin.h:3141: }
    (<0.0>,5829) tree.c:583: local_irq_restore(flags);
    (<0.0>,5832) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5834) fake_sched.h:43: return __running_cpu;
    (<0.0>,5838) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5840) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5844) fake_sched.h:43: return __running_cpu;
    (<0.0>,5848) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5854) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5857) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5862) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5864) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5866) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5870) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5872) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5879) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5881) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5883) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5887) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5889) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5896) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5898) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5900) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5904) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5906) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5913) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5915) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5917) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5921) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5923) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5930) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5932) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5934) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5938) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
  (<0>,5327) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5328) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5330) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5333) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5335) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5337) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5338) tree.c:3164: return 1;
  (<0>,5340) tree.c:3176: }
  (<0>,5344) tree.c:3189: return 1;
  (<0>,5346) tree.c:3191: }
  (<0>,5353) fake_sched.h:43: return __running_cpu;
  (<0>,5357) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
  (<0>,5361) tree.c:2437: if (user)
  (<0>,5369) fake_sched.h:43: return __running_cpu;
  (<0>,5373) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
  (<0>,5375) fake_sched.h:43: return __running_cpu;
  (<0>,5379) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5385) fake_sched.h:43: return __running_cpu;
  (<0>,5389) fake_sched.h:182: if (need_softirq[get_cpu()]) {
  (<0>,5399) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,5402) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5403) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5404) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5408) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5409) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5410) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5412) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5416) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,5425) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5427) fake_sched.h:43: return __running_cpu;
  (<0>,5430) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5432) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5434) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5435) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5437) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5444) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5445) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5447) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5453) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5454) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5455) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5456) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5457) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5461)
  (<0>,5462)
  (<0>,5463) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5464) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5471)
  (<0>,5472)
  (<0>,5473) tree.c:1584: local_irq_save(flags);
  (<0>,5476) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5478) fake_sched.h:43: return __running_cpu;
  (<0>,5482) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5484) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5488) fake_sched.h:43: return __running_cpu;
  (<0>,5492) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5497) tree.c:1585: rnp = rdp->mynode;
  (<0>,5499) tree.c:1585: rnp = rdp->mynode;
  (<0>,5500) tree.c:1585: rnp = rdp->mynode;
  (<0>,5501) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5503) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5504) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5506) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5509) tree.c:1588: !raw_spin_trylock(&rnp->lock)) { /* irqs already off, so later. */
  (<0>,5514) fake_sync.h:122: preempt_disable();
  (<0>,5516) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
  (<0>,5517) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
  (<0>,5520) fake_sync.h:127: return 1;
  (<0>,5522) fake_sync.h:128: }
  (<0>,5528) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5529) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5530) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5536)
  (<0>,5537)
  (<0>,5538)
  (<0>,5539) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5541) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5542) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5544) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5547) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5548) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5549) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5558)
  (<0>,5559)
  (<0>,5560)
  (<0>,5561) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5564) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5567) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5570) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5571) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5574) tree.c:1434: return false;
  (<0>,5576) tree.c:1483: }
  (<0>,5579) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5581) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5583) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5584) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5586) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5589) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5591) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5592) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5594) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5597) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,5599) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,5600) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5602) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5603) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5605) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5610) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5613) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5614) tree.c:1573: zero_cpu_stall_ticks(rdp);
  (<0>,5617) tree_plugin.h:1950: }
  (<0>,5620) tree.c:1575: return ret;
  (<0>,5624) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5625) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,5627) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,5631)
  (<0>,5632)
  (<0>,5633) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,5634) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,5637) fake_sync.h:86: local_irq_restore(flags);
  (<0>,5640) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5642) fake_sched.h:43: return __running_cpu;
  (<0>,5646) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5648) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5652) fake_sched.h:43: return __running_cpu;
  (<0>,5656) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5663) tree.c:1595: if (needwake)
  (<0>,5667) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5669) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5672) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,5674) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,5679) tree.c:2558: local_irq_save(flags);
  (<0>,5682) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5684) fake_sched.h:43: return __running_cpu;
  (<0>,5688) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5690) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5694) fake_sched.h:43: return __running_cpu;
  (<0>,5698) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5703) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5704) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5710)
  (<0>,5711)
  (<0>,5712) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,5715)
  (<0>,5716) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5718) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5719) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5721) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5727) tree.c:481: return 0;  /* No, a grace period is already in progress. */
  (<0>,5729) tree.c:494: }
  (<0>,5733) tree.c:2566: local_irq_restore(flags);
  (<0>,5736) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5738) fake_sched.h:43: return __running_cpu;
  (<0>,5742) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5744) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5748) fake_sched.h:43: return __running_cpu;
  (<0>,5752) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5758) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,5761)
  (<0>,5762) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5764) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5767) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5774) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,5777) tree_plugin.h:2720: }
  (<0>,5781) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5784) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5785) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5786) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5790) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5791) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5792) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5794) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5798) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,5807) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5809) fake_sched.h:43: return __running_cpu;
  (<0>,5812) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5814) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5816) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5817) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5819) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5826) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5827) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5829) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5835) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5836) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5837) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5838) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5839) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5843)
  (<0>,5844)
  (<0>,5845) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5846) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5853)
  (<0>,5854)
  (<0>,5855) tree.c:1584: local_irq_save(flags);
  (<0>,5858) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5860) fake_sched.h:43: return __running_cpu;
  (<0>,5864) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5866) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5870) fake_sched.h:43: return __running_cpu;
  (<0>,5874) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5879) tree.c:1585: rnp = rdp->mynode;
  (<0>,5881) tree.c:1585: rnp = rdp->mynode;
  (<0>,5882) tree.c:1585: rnp = rdp->mynode;
  (<0>,5883) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5885) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5886) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5888) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5891) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5893) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5894) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5896) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5899) tree.c:1589: local_irq_restore(flags);
  (<0>,5902) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5904) fake_sched.h:43: return __running_cpu;
  (<0>,5908) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5910) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5914) fake_sched.h:43: return __running_cpu;
  (<0>,5918) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5925) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5927) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5932) tree.c:2558: local_irq_save(flags);
  (<0>,5935) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5937) fake_sched.h:43: return __running_cpu;
  (<0>,5941) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5943) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5947) fake_sched.h:43: return __running_cpu;
  (<0>,5951) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5956) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5957) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5963)
  (<0>,5964)
  (<0>,5965) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,5968)
  (<0>,5969) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5971) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5972) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5974) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5980) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5986)
  (<0>,5987) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5990)
  (<0>,5991) tree.c:453: return &rsp->node[0];
  (<0>,5995) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5996) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5998) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6002) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6003) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6005) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6008) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6009) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6010) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6014) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6017) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6020) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6023) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6024) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6027) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6029) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6032) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6035) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6038) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6039) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6041) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6044) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6048) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6050) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6052) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6055) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6058) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6061) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6062) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6064) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6067) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6071) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6073) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6075) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6078) tree.c:493: return 0; /* No grace period needed. */
  (<0>,6080) tree.c:494: }
  (<0>,6084) tree.c:2566: local_irq_restore(flags);
  (<0>,6087) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6089) fake_sched.h:43: return __running_cpu;
  (<0>,6093) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6095) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6099) fake_sched.h:43: return __running_cpu;
  (<0>,6103) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6109) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,6112)
  (<0>,6113) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6115) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6118) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6125) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,6128) tree_plugin.h:2720: }
  (<0>,6132) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6135) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6136) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6137) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6141) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6142) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6143) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6145) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6153) fake_sched.h:43: return __running_cpu;
  (<0>,6157) fake_sched.h:184: need_softirq[get_cpu()] = 0;
  (<0>,6166) tree.c:624: local_irq_save(flags);
  (<0>,6169) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6171) fake_sched.h:43: return __running_cpu;
  (<0>,6175) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6177) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6181) fake_sched.h:43: return __running_cpu;
  (<0>,6185) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6191) fake_sched.h:43: return __running_cpu;
  (<0>,6195) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6196) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6198) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6199) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6200) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6202) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6204) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6205) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6207) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6212) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6213) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6215) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6219) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6220) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6221) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6222) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,6224) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,6232) tree_plugin.h:3141: }
  (<0>,6234) tree.c:634: local_irq_restore(flags);
  (<0>,6237) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6239) fake_sched.h:43: return __running_cpu;
  (<0>,6243) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6245) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6249) fake_sched.h:43: return __running_cpu;
  (<0>,6253) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6267) fake_sched.h:43: return __running_cpu;
  (<0>,6272) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
  (<0>,6278) fake_sched.h:43: return __running_cpu;
  (<0>,6283) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
  (<0>,6289) fake_sched.h:43: return __running_cpu;
  (<0>,6293) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
  (<0>,6305) fake_sched.h:43: return __running_cpu;
  (<0>,6309) fake_sched.h:96: rcu_idle_enter();
  (<0>,6312) tree.c:580: local_irq_save(flags);
  (<0>,6315) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6317) fake_sched.h:43: return __running_cpu;
  (<0>,6321) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6323) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6327) fake_sched.h:43: return __running_cpu;
  (<0>,6331) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6344) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6346) fake_sched.h:43: return __running_cpu;
  (<0>,6350) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6351) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6353) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6354) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6355) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6361) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6362) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6367) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6368) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6369) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6370) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,6374) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,6376) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,6377) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,6378) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,6397)
  (<0>,6399) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6401) fake_sched.h:43: return __running_cpu;
  (<0>,6405) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6408) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,6412) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6413) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6414) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6418) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6419) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6420) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6422) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6427) fake_sched.h:43: return __running_cpu;
  (<0>,6430) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6432) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6434) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6435) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,6438) tree_plugin.h:2720: }
  (<0>,6441) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6444) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6445) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6446) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6450) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6451) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6452) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6454) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6459) fake_sched.h:43: return __running_cpu;
  (<0>,6462) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6464) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6466) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6467) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,6470) tree_plugin.h:2720: }
  (<0>,6473) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6476) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6477) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6478) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6482) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6483) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6484) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6486) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6493) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6496) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6497) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6498) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6500) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6501) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6503) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6506) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6512) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6513) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6516) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6521) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6522) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6523) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6537) tree_plugin.h:3141: }
  (<0>,6539) tree.c:583: local_irq_restore(flags);
  (<0>,6542) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6544) fake_sched.h:43: return __running_cpu;
  (<0>,6548) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6550) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6554) fake_sched.h:43: return __running_cpu;
  (<0>,6558) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6564) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6567) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6572) fake_sched.h:43: return __running_cpu;
  (<0>,6576)
  (<0>,6577) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,6580) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,6585) tree.c:704: local_irq_save(flags);
  (<0>,6588) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6590) fake_sched.h:43: return __running_cpu;
  (<0>,6594) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6596) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6600) fake_sched.h:43: return __running_cpu;
  (<0>,6604) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6617) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6619) fake_sched.h:43: return __running_cpu;
  (<0>,6623) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6624) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6626) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6627) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6628) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6633) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6634) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6638) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6639) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6640) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6641) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,6645) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6647) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6648) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,6649) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,6663)
  (<0>,6664) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6666) fake_sched.h:43: return __running_cpu;
  (<0>,6670) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6674) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6677) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6678) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6679) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6681) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6682) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6684) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6687) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6694) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6695) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6698) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6703) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6704) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6705) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6710) tree.c:656: if (!user && !is_idle_task(current)) {
  (<0>,6719) tree_plugin.h:3145: }
  (<0>,6721) tree.c:707: local_irq_restore(flags);
  (<0>,6724) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6726) fake_sched.h:43: return __running_cpu;
  (<0>,6730) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6732) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6736) fake_sched.h:43: return __running_cpu;
  (<0>,6740) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6751) fake_sched.h:43: return __running_cpu;
  (<0>,6755) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
  (<0>,6759) fake_sched.h:43: return __running_cpu;
  (<0>,6763) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6768) fake_sched.h:43: return __running_cpu;
  (<0>,6772) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
  (<0>,6782) tree.c:749: local_irq_save(flags);
  (<0>,6785) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6787) fake_sched.h:43: return __running_cpu;
  (<0>,6791) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6793) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6798) fake_sched.h:43: return __running_cpu;
  (<0>,6802) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6803) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6805) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6806) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6807) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6809) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6811) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6812) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6814) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6819) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6820) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6822) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6826) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6827) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6828) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6829) tree.c:754: if (oldval)
  (<0>,6837) tree_plugin.h:3145: }
  (<0>,6839) tree.c:759: local_irq_restore(flags);
  (<0>,6842) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6844) fake_sched.h:43: return __running_cpu;
  (<0>,6848) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6850) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6858) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,6863) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,6868) fake_sched.h:43: return __running_cpu;
  (<0>,6873) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,6881) fake_sched.h:43: return __running_cpu;
  (<0>,6886) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
  (<0>,6900) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6901) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6902) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6906) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6907) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6908) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6910) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6914) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6916) fake_sched.h:43: return __running_cpu;
  (<0>,6919) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6921) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6928)
  (<0>,6929)
  (<0>,6930) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6932) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6933) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6934) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,6936) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,6938) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,6939) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,6940) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,6950)
  (<0>,6951)
  (<0>,6952) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
  (<0>,6957) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
  (<0>,6960) tree_plugin.h:3185: return 0;
  (<0>,6963) tree.c:3135: if (rcu_scheduler_fully_active &&
  (<0>,6966) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,6968) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,6971) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,6973) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,6976) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,6978) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,6981) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,6983) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,6986) tree.c:3139: rdp->n_rp_report_qs++;
  (<0>,6988) tree.c:3139: rdp->n_rp_report_qs++;
  (<0>,6990) tree.c:3139: rdp->n_rp_report_qs++;
  (<0>,6991) tree.c:3140: return 1;
  (<0>,6993) tree.c:3176: }
  (<0>,6997) tree.c:3189: return 1;
  (<0>,6999) tree.c:3191: }
  (<0>,7006) fake_sched.h:43: return __running_cpu;
  (<0>,7010) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
  (<0>,7014) tree.c:2437: if (user)
  (<0>,7022) fake_sched.h:43: return __running_cpu;
  (<0>,7026) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
  (<0>,7028) fake_sched.h:43: return __running_cpu;
  (<0>,7032) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7038) fake_sched.h:43: return __running_cpu;
  (<0>,7042) fake_sched.h:182: if (need_softirq[get_cpu()]) {
  (<0>,7052) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,7055) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7056) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7057) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7061) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7062) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7063) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7065) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7069) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,7078) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7080) fake_sched.h:43: return __running_cpu;
  (<0>,7083) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7085) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7087) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7088) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7090) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7097) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7098) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7100) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7106) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7107) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7108) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7109) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7110) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7114)
  (<0>,7115)
  (<0>,7116) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7117) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7124)
  (<0>,7125)
  (<0>,7126) tree.c:1584: local_irq_save(flags);
  (<0>,7129) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7131) fake_sched.h:43: return __running_cpu;
  (<0>,7135) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7137) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7141) fake_sched.h:43: return __running_cpu;
  (<0>,7145) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7150) tree.c:1585: rnp = rdp->mynode;
  (<0>,7152) tree.c:1585: rnp = rdp->mynode;
  (<0>,7153) tree.c:1585: rnp = rdp->mynode;
  (<0>,7154) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7156) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7157) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7159) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7162) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7164) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7165) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7167) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7170) tree.c:1589: local_irq_restore(flags);
  (<0>,7173) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7175) fake_sched.h:43: return __running_cpu;
  (<0>,7179) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7181) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7185) fake_sched.h:43: return __running_cpu;
  (<0>,7189) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7196) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7198) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7201) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,7203) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,7206) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,7208) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,7209) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,7210) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,7219)
  (<0>,7220)
  (<0>,7221)
  (<0>,7222) tree.c:2034: rnp = rdp->mynode;
  (<0>,7224) tree.c:2034: rnp = rdp->mynode;
  (<0>,7225) tree.c:2034: rnp = rdp->mynode;
  (<0>,7226) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,7228) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,7232)
  (<0>,7233)
  (<0>,7234) fake_sync.h:76: local_irq_save(flags);
  (<0>,7237) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7239) fake_sched.h:43: return __running_cpu;
  (<0>,7243) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7245) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7249) fake_sched.h:43: return __running_cpu;
  (<0>,7253) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7259) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,7260) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,7266) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7268) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7273) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7275) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7276) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7278) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7281) tree.c:2038: rnp->completed == rnp->gpnum) {
  (<0>,7283) tree.c:2038: rnp->completed == rnp->gpnum) {
  (<0>,7284) tree.c:2038: rnp->completed == rnp->gpnum) {
  (<0>,7286) tree.c:2038: rnp->completed == rnp->gpnum) {
  (<0>,7289) tree.c:2050: mask = rdp->grpmask;
  (<0>,7291) tree.c:2050: mask = rdp->grpmask;
  (<0>,7292) tree.c:2050: mask = rdp->grpmask;
  (<0>,7293) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
  (<0>,7295) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
  (<0>,7296) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
  (<0>,7300) tree.c:2054: rdp->qs_pending = 0;
  (<0>,7302) tree.c:2054: rdp->qs_pending = 0;
  (<0>,7303) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7304) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7305) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7314)
  (<0>,7315)
  (<0>,7316)
  (<0>,7317) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7320) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7323) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7326) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7327) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7330) tree.c:1434: return false;
  (<0>,7332) tree.c:1483: }
  (<0>,7335) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7336) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,7337) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,7338) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,7339) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,7349)
  (<0>,7350)
  (<0>,7351)
  (<0>,7352) tree.c:1974: for (;;) {
  (<0>,7354) tree.c:1975: if (!(rnp->qsmask & mask)) {
  (<0>,7356) tree.c:1975: if (!(rnp->qsmask & mask)) {
  (<0>,7357) tree.c:1975: if (!(rnp->qsmask & mask)) {
  (<0>,7361) tree.c:1981: rnp->qsmask &= ~mask;
  (<0>,7363) tree.c:1981: rnp->qsmask &= ~mask;
  (<0>,7365) tree.c:1981: rnp->qsmask &= ~mask;
  (<0>,7367) tree.c:1981: rnp->qsmask &= ~mask;
  (<0>,7370) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,7372) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,7375) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,7378) tree_plugin.h:958: return 0;
  (<0>,7382) tree.c:1994: mask = rnp->grpmask;
  (<0>,7384) tree.c:1994: mask = rnp->grpmask;
  (<0>,7385) tree.c:1994: mask = rnp->grpmask;
  (<0>,7386) tree.c:1995: if (rnp->parent == NULL) {
  (<0>,7388) tree.c:1995: if (rnp->parent == NULL) {
  (<0>,7392) tree.c:2014: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
  (<0>,7393) tree.c:2014: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
  (<0>,7400)
  (<0>,7401)
  (<0>,7402) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,7405)
  (<0>,7406) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7408) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7409) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7411) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7420) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,7421) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,7424)
  (<0>,7425) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7427) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7428) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7430) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7437) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,7438) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,7439) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,7440) tree.c:1951: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
  (<0>,7443)
  (<0>,7444) tree.c:453: return &rsp->node[0];
  (<0>,7449) tree.c:1951: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
  (<0>,7453)
  (<0>,7454)
  (<0>,7455) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,7456) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,7459) fake_sync.h:86: local_irq_restore(flags);
  (<0>,7462) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7464) fake_sched.h:43: return __running_cpu;
  (<0>,7468) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7470) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7474) fake_sched.h:43: return __running_cpu;
  (<0>,7478) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7485) tree.c:1952: rcu_gp_kthread_wake(rsp);
  (<0>,7488)
  (<0>,7489) tree.c:1406: if (current == rsp->gp_kthread ||
  (<0>,7490) tree.c:1406: if (current == rsp->gp_kthread ||
  (<0>,7492) tree.c:1406: if (current == rsp->gp_kthread ||
  (<0>,7495) tree.c:1407: !ACCESS_ONCE(rsp->gp_flags) ||
  (<0>,7497) tree.c:1407: !ACCESS_ONCE(rsp->gp_flags) ||
  (<0>,7505) tree.c:2063: if (needwake)
  (<0>,7512) tree.c:2558: local_irq_save(flags);
  (<0>,7515) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7517) fake_sched.h:43: return __running_cpu;
  (<0>,7521) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7523) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7527) fake_sched.h:43: return __running_cpu;
  (<0>,7531) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7536) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7537) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7543)
  (<0>,7544)
  (<0>,7545) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,7548)
  (<0>,7549) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7551) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7552) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7554) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7560) tree.c:481: return 0;  /* No, a grace period is already in progress. */
  (<0>,7562) tree.c:494: }
  (<0>,7566) tree.c:2566: local_irq_restore(flags);
  (<0>,7569) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7571) fake_sched.h:43: return __running_cpu;
  (<0>,7575) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7577) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7581) fake_sched.h:43: return __running_cpu;
  (<0>,7585) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7591) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7594)
  (<0>,7595) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7597) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7600) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7607) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,7610) tree_plugin.h:2720: }
  (<0>,7614) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7617) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7618) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7619) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7623) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7624) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7625) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7627) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7631) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,7640) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7642) fake_sched.h:43: return __running_cpu;
  (<0>,7645) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7647) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7649) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7650) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7652) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7659) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7660) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7662) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7668) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7669) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7670) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7671) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7672) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7676)
  (<0>,7677)
  (<0>,7678) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7679) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7686)
  (<0>,7687)
  (<0>,7688) tree.c:1584: local_irq_save(flags);
  (<0>,7691) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7693) fake_sched.h:43: return __running_cpu;
  (<0>,7697) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7699) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7703) fake_sched.h:43: return __running_cpu;
  (<0>,7707) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7712) tree.c:1585: rnp = rdp->mynode;
  (<0>,7714) tree.c:1585: rnp = rdp->mynode;
  (<0>,7715) tree.c:1585: rnp = rdp->mynode;
  (<0>,7716) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7718) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7719) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7721) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7724) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7726) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7727) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7729) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7732) tree.c:1589: local_irq_restore(flags);
  (<0>,7735) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7737) fake_sched.h:43: return __running_cpu;
  (<0>,7741) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7743) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7747) fake_sched.h:43: return __running_cpu;
  (<0>,7751) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7758) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7760) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7765) tree.c:2558: local_irq_save(flags);
  (<0>,7768) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7770) fake_sched.h:43: return __running_cpu;
  (<0>,7774) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7776) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7780) fake_sched.h:43: return __running_cpu;
  (<0>,7784) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7789) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7790) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7796)
  (<0>,7797)
  (<0>,7798) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,7801)
  (<0>,7802) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7804) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7805) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7807) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7813) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,7819)
  (<0>,7820) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7823)
  (<0>,7824) tree.c:453: return &rsp->node[0];
  (<0>,7828) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7829) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7831) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7835) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7836) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7838) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7841) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7842) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7843) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7847) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7850) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7853) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7856) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7857) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7860) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7862) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7865) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7868) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7871) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7872) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7874) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7877) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7881) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7883) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7885) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7888) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7891) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7894) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7895) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7897) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7900) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7904) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7906) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7908) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7911) tree.c:493: return 0; /* No grace period needed. */
  (<0>,7913) tree.c:494: }
  (<0>,7917) tree.c:2566: local_irq_restore(flags);
  (<0>,7920) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7922) fake_sched.h:43: return __running_cpu;
  (<0>,7926) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7928) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7932) fake_sched.h:43: return __running_cpu;
  (<0>,7936) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7942) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7945)
  (<0>,7946) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7948) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7951) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7958) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,7961) tree_plugin.h:2720: }
  (<0>,7965) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7968) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7969) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7970) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7974) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7975) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7976) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7978) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7986) fake_sched.h:43: return __running_cpu;
  (<0>,7990) fake_sched.h:184: need_softirq[get_cpu()] = 0;
  (<0>,7999) tree.c:624: local_irq_save(flags);
  (<0>,8002) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8004) fake_sched.h:43: return __running_cpu;
  (<0>,8008) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8010) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8014) fake_sched.h:43: return __running_cpu;
  (<0>,8018) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8024) fake_sched.h:43: return __running_cpu;
  (<0>,8028) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,8029) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,8031) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,8032) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,8033) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,8035) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,8037) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,8038) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8040) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8045) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8046) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8048) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8052) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8053) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8054) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8055) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,8057) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,8065) tree_plugin.h:3141: }
  (<0>,8067) tree.c:634: local_irq_restore(flags);
  (<0>,8070) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8072) fake_sched.h:43: return __running_cpu;
  (<0>,8076) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8078) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8082) fake_sched.h:43: return __running_cpu;
  (<0>,8086) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5940) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5943) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5946)
    (<0.0>,5955) fake_sched.h:43: return __running_cpu;
    (<0.0>,5959)
    (<0.0>,5960) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5963) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5968) tree.c:704: local_irq_save(flags);
    (<0.0>,5971)
    (<0.0>,5973) fake_sched.h:43: return __running_cpu;
    (<0.0>,5977) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5979) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5983) fake_sched.h:43: return __running_cpu;
    (<0.0>,5987) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6000)
    (<0.0>,6002) fake_sched.h:43: return __running_cpu;
    (<0.0>,6006) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6007) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6009) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6010) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6011) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6016) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6017) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6021) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6022) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6023) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6024) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,6028) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6030) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6031) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6032) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6046)
    (<0.0>,6047)
    (<0.0>,6049) fake_sched.h:43: return __running_cpu;
    (<0.0>,6053) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6057) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6060) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6061) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6062) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6064) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6065) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6067) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6070) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6077) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6078) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6081) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6086) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6087) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6088) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6093) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,6102)
    (<0.0>,6104) tree.c:707: local_irq_restore(flags);
    (<0.0>,6107)
    (<0.0>,6109) fake_sched.h:43: return __running_cpu;
    (<0.0>,6113) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6115) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6119) fake_sched.h:43: return __running_cpu;
    (<0.0>,6123) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6130) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,6131) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,6132) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,6133) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,6135) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,6138) tree.c:1840: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,6141)
    (<0.0>,6146) tree.c:1872: rcu_gp_cleanup(rsp);
    (<0.0>,6154)
    (<0.0>,6155) tree.c:1720: bool needgp = false;
    (<0.0>,6156) tree.c:1721: int nocb = 0;
    (<0.0>,6157) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6160)
    (<0.0>,6161) tree.c:453: return &rsp->node[0];
    (<0.0>,6165) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6166) tree.c:1725: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,6170)
    (<0.0>,6173) fake_sched.h:43: return __running_cpu;
    (<0.0>,6177) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6181) fake_sched.h:43: return __running_cpu;
    (<0.0>,6185) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6190) fake_sched.h:43: return __running_cpu;
    (<0.0>,6194) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,6197) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6198) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6204) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,6205) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,6207) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,6209) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,6210) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,6211) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,6213) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,6216) tree.c:1739: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,6220)
    (<0.0>,6221) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6222) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6227) fake_sched.h:43: return __running_cpu;
    (<0.0>,6231) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6233) fake_sched.h:43: return __running_cpu;
    (<0.0>,6237) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6243) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6246) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6248) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6249) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6251) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6256) tree.c:1751: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,6260)
    (<0.0>,6263) fake_sched.h:43: return __running_cpu;
    (<0.0>,6267) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6271) fake_sched.h:43: return __running_cpu;
    (<0.0>,6275) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6280) fake_sched.h:43: return __running_cpu;
    (<0.0>,6284) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,6287) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6288) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6294) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,6296) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,6297) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,6299) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,6301) fake_sched.h:43: return __running_cpu;
    (<0.0>,6304) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6306) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6308) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6309) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,6310) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,6312) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,6315) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6316) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6317) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6323)
    (<0.0>,6324)
    (<0.0>,6325)
    (<0.0>,6326) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,6328) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,6329) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,6331) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,6334) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,6335) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,6336) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,6344)
    (<0.0>,6345)
    (<0.0>,6346)
    (<0.0>,6347) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6350) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6353) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6356) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6357) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6360) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6362) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6365) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6367) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6368) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6370) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6373) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6377) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6379) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6382) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6383) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6386) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6388) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6390) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6392) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6395) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6397) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6398) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6400) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6403) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6408) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6410) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6411) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6414) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6417) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6418) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6420) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6423) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6425) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6427) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6429) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6430) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6433) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,6435) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,6438) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6440) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6443) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6444) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6447) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6451) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6452) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6453) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6462)
    (<0.0>,6463)
    (<0.0>,6464)
    (<0.0>,6465) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6468) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6471) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6474) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6475) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6478) tree.c:1434: return false;
    (<0.0>,6480) tree.c:1483: }
    (<0.0>,6482) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6484) tree.c:1527: }
    (<0.0>,6487) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,6488) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6490) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6491) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6493) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6497) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6499) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6500) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6502) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6505) tree.c:1575: return ret;
    (<0.0>,6509) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6513) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6515) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6516) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6523)
    (<0.0>,6524)
    (<0.0>,6525) tree.c:1385: int c = rnp->completed;
    (<0.0>,6527) tree.c:1385: int c = rnp->completed;
    (<0.0>,6529) tree.c:1385: int c = rnp->completed;
    (<0.0>,6531) fake_sched.h:43: return __running_cpu;
    (<0.0>,6534) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6536) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6538) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6539) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,6540) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,6544)
    (<0.0>,6545)
    (<0.0>,6547) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6550) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6553) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6554) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6558) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6561) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6562) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6563) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6564) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6565) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6567) tree.c:1393: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,6575)
    (<0.0>,6576)
    (<0.0>,6577)
    (<0.0>,6578)
    (<0.0>,6582) tree.c:1394: return needmore;
    (<0.0>,6584) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6586) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6587) tree.c:1759: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,6591)
    (<0.0>,6592) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6593) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6598) fake_sched.h:43: return __running_cpu;
    (<0.0>,6602) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6604) fake_sched.h:43: return __running_cpu;
    (<0.0>,6608) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6621) fake_sched.h:43: return __running_cpu;
    (<0.0>,6626) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,6633) fake_sched.h:43: return __running_cpu;
    (<0.0>,6637) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,6649) fake_sched.h:43: return __running_cpu;
    (<0.0>,6653)
    (<0.0>,6656) tree.c:580: local_irq_save(flags);
    (<0.0>,6659)
    (<0.0>,6661) fake_sched.h:43: return __running_cpu;
    (<0.0>,6665) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6667) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6671) fake_sched.h:43: return __running_cpu;
    (<0.0>,6675) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6688)
    (<0.0>,6690) fake_sched.h:43: return __running_cpu;
    (<0.0>,6694) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6695) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6697) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6698) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6699) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6705) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6706) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6711) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6712) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6713) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6714) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,6718) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,6720) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,6721) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6722) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6741)
    (<0.0>,6743)
    (<0.0>,6745) fake_sched.h:43: return __running_cpu;
    (<0.0>,6749) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6752) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,6756) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6757) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6758) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6762) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6763) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6764) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6766) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6771) fake_sched.h:43: return __running_cpu;
    (<0.0>,6774) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6776) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6778) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6779) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6782)
    (<0.0>,6785) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6788) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6789) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6790) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6794) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6795) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6796) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6798) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6803) fake_sched.h:43: return __running_cpu;
    (<0.0>,6806) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6808) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6810) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6811) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6814)
    (<0.0>,6817) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6820) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6821) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6822) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6826) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6827) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6828) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6830) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6837) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6840) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6841) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6842) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6844) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6845) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6847) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6850) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6856) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6857) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6860) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6865) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6866) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6867) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6881)
    (<0.0>,6883) tree.c:583: local_irq_restore(flags);
    (<0.0>,6886)
    (<0.0>,6888) fake_sched.h:43: return __running_cpu;
    (<0.0>,6892) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6894) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6898) fake_sched.h:43: return __running_cpu;
    (<0.0>,6902) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6908) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6911) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6916) fake_sched.h:43: return __running_cpu;
    (<0.0>,6920)
    (<0.0>,6921) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6924) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6929) tree.c:704: local_irq_save(flags);
    (<0.0>,6932)
    (<0.0>,6934) fake_sched.h:43: return __running_cpu;
    (<0.0>,6938) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6940) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6944) fake_sched.h:43: return __running_cpu;
    (<0.0>,6948) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6961)
    (<0.0>,6963) fake_sched.h:43: return __running_cpu;
    (<0.0>,6967) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6968) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6970) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6971) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6972) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6977) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6978) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6982) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6983) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6984) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6985) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,6989) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6991) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6992) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6993) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,7007)
    (<0.0>,7008)
    (<0.0>,7010) fake_sched.h:43: return __running_cpu;
    (<0.0>,7014) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,7018) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7021) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7022) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7023) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7025) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7026) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7028) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7031) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7038) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7039) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7042) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7047) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7048) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7049) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7054) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,7063)
    (<0.0>,7065) tree.c:707: local_irq_restore(flags);
    (<0.0>,7068)
    (<0.0>,7070) fake_sched.h:43: return __running_cpu;
    (<0.0>,7074) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7076) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7080) fake_sched.h:43: return __running_cpu;
    (<0.0>,7084) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7099) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7101) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7103) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7104) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7106) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7111) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,7114)
    (<0.0>,7115) tree.c:453: return &rsp->node[0];
    (<0.0>,7119) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,7120) tree.c:1763: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,7124)
    (<0.0>,7127) fake_sched.h:43: return __running_cpu;
    (<0.0>,7131) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,7135) fake_sched.h:43: return __running_cpu;
    (<0.0>,7139) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7144) fake_sched.h:43: return __running_cpu;
    (<0.0>,7148) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,7151) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,7152) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,7158) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,7159) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,7163)
    (<0.0>,7164)
    (<0.0>,7166) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,7168) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,7169) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,7171) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,7174) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,7176) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,7178) fake_sched.h:43: return __running_cpu;
    (<0.0>,7181) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,7183) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,7185) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,7186) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,7187) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,7188) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,7196)
    (<0.0>,7197)
    (<0.0>,7198)
    (<0.0>,7199) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7202) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7205) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7208) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7209) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7212) tree.c:1502: return false;
    (<0.0>,7214) tree.c:1527: }
    (<0.0>,7217) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,7221) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,7222) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7225) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7226) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7232)
    (<0.0>,7233)
    (<0.0>,7234) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7237)
    (<0.0>,7238) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7240) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7241) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7243) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7249) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7255)
    (<0.0>,7256) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7259)
    (<0.0>,7260) tree.c:453: return &rsp->node[0];
    (<0.0>,7264) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7265) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7267) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7271) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7272) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7274) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7277) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7278) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7279) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7283) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7285) tree.c:494: }
    (<0.0>,7289) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7291) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7295) tree.c:1780: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,7299)
    (<0.0>,7300) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,7301) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,7306) fake_sched.h:43: return __running_cpu;
    (<0.0>,7310) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,7312) fake_sched.h:43: return __running_cpu;
    (<0.0>,7316) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7327) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,7329) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,7333) fake_sched.h:43: return __running_cpu;
    (<0.0>,7337) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,7341) fake_sched.h:43: return __running_cpu;
    (<0.0>,7345) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7350) fake_sched.h:43: return __running_cpu;
    (<0.0>,7354) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,7364) tree.c:749: local_irq_save(flags);
    (<0.0>,7367)
    (<0.0>,7369) fake_sched.h:43: return __running_cpu;
    (<0.0>,7373) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7375) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7380) fake_sched.h:43: return __running_cpu;
    (<0.0>,7384) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,7385) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,7387) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,7388) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,7389) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,7391) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,7393) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,7394) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7396) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7401) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7402) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7404) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7408) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7409) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7410) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7411) tree.c:754: if (oldval)
    (<0.0>,7419)
    (<0.0>,7421) tree.c:759: local_irq_restore(flags);
    (<0.0>,7424)
    (<0.0>,7426) fake_sched.h:43: return __running_cpu;
    (<0.0>,7430) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7432) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7440)
    (<0.0>,7445) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,7450) fake_sched.h:43: return __running_cpu;
    (<0.0>,7455) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,7463) fake_sched.h:43: return __running_cpu;
    (<0.0>,7468) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,7482) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7483) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7484) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7488) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7489) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7490) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7492) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7496) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7498) fake_sched.h:43: return __running_cpu;
    (<0.0>,7501) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7503) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7510)
    (<0.0>,7511)
    (<0.0>,7512) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7514) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7515) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7516) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7518) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7520) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7521) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,7522) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,7532)
    (<0.0>,7533)
    (<0.0>,7534) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,7539) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,7542)
    (<0.0>,7545) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,7548) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7550) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7553) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7555) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7559) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,7562)
    (<0.0>,7563) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7565) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7568) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7571) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7574) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7581) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,7583) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,7585) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,7586) tree.c:3146: return 1;
    (<0.0>,7588) tree.c:3176: }
    (<0.0>,7592) tree.c:3189: return 1;
    (<0.0>,7594) tree.c:3191: }
    (<0.0>,7601) fake_sched.h:43: return __running_cpu;
    (<0.0>,7605) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,7609) tree.c:2437: if (user)
    (<0.0>,7617) fake_sched.h:43: return __running_cpu;
    (<0.0>,7621) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,7623) fake_sched.h:43: return __running_cpu;
    (<0.0>,7627) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7633) fake_sched.h:43: return __running_cpu;
    (<0.0>,7637) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,7647)
    (<0.0>,7650) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7651) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7652) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7656) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7657) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7658) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7660) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7664) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,7673)
    (<0.0>,7675) fake_sched.h:43: return __running_cpu;
    (<0.0>,7678) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7680) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7682) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7683) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7685) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7692) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7693) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7695) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7701) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7702) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7703) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7704) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7705) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7709)
    (<0.0>,7710)
    (<0.0>,7711) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7712) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7719)
    (<0.0>,7720)
    (<0.0>,7721) tree.c:1584: local_irq_save(flags);
    (<0.0>,7724)
    (<0.0>,7726) fake_sched.h:43: return __running_cpu;
    (<0.0>,7730) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7732) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7736) fake_sched.h:43: return __running_cpu;
    (<0.0>,7740) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7745) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7747) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7748) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7749) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7751) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7752) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7754) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7757) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7759) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7760) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7762) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7765) tree.c:1589: local_irq_restore(flags);
    (<0.0>,7768)
    (<0.0>,7770) fake_sched.h:43: return __running_cpu;
    (<0.0>,7774) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7776) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7780) fake_sched.h:43: return __running_cpu;
    (<0.0>,7784) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7791) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7793) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7798) tree.c:2558: local_irq_save(flags);
    (<0.0>,7801)
    (<0.0>,7803) fake_sched.h:43: return __running_cpu;
    (<0.0>,7807) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7809) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7813) fake_sched.h:43: return __running_cpu;
    (<0.0>,7817) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7822) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7823) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7829)
    (<0.0>,7830)
    (<0.0>,7831) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7834)
    (<0.0>,7835) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7837) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7838) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7840) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7846) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7852)
    (<0.0>,7853) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7856)
    (<0.0>,7857) tree.c:453: return &rsp->node[0];
    (<0.0>,7861) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7862) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7864) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7868) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7869) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7871) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7874) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7875) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7876) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7880) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7882) tree.c:494: }
    (<0.0>,7886) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,7889)
    (<0.0>,7890) tree.c:453: return &rsp->node[0];
    (<0.0>,7897)
    (<0.0>,7899) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7900) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7904) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,7910)
    (<0.0>,7912) fake_sched.h:43: return __running_cpu;
    (<0.0>,7915) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7917) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7919) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7920) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7923)
    (<0.0>,7924) tree.c:453: return &rsp->node[0];
    (<0.0>,7928) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7929) tree.c:1925: bool ret = false;
    (<0.0>,7930) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7931) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7932) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7940)
    (<0.0>,7941)
    (<0.0>,7942)
    (<0.0>,7943) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7946) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7949) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7952) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7953) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7956) tree.c:1502: return false;
    (<0.0>,7958) tree.c:1527: }
    (<0.0>,7961) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7965) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7966) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7967) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7968) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7974)
    (<0.0>,7975)
    (<0.0>,7976)
    (<0.0>,7977) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7979) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7982) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7983) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7989)
    (<0.0>,7990)
    (<0.0>,7991) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7994)
    (<0.0>,7995) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7997) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7998) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8000) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8006) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,8012)
    (<0.0>,8013) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8016)
    (<0.0>,8017) tree.c:453: return &rsp->node[0];
    (<0.0>,8021) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8022) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8024) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8028) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8029) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8031) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8034) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8035) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8036) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8040) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,8042) tree.c:494: }
    (<0.0>,8046) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,8048) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,8051) tree.c:1909: return true;
    (<0.0>,8053) tree.c:1910: }
    (<0.0>,8057) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,8058) tree.c:1937: return ret;
    (<0.0>,8062) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,8063) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,8066)
    (<0.0>,8067) tree.c:453: return &rsp->node[0];
    (<0.0>,8072) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,8076)
    (<0.0>,8077)
    (<0.0>,8078) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,8079) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,8082) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,8085)
    (<0.0>,8087) fake_sched.h:43: return __running_cpu;
    (<0.0>,8091) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8093) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8097) fake_sched.h:43: return __running_cpu;
    (<0.0>,8101) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8108) tree.c:2563: if (needwake)
    (<0.0>,8111) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,8114)
    (<0.0>,8115) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,8116) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,8118) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,8125) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,8128)
    (<0.0>,8129) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8131) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8134) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8137) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,8140) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,8147) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,8148) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,8152)
    (<0.0>,8153)
    (<0.0>,8154) tree.c:2601: if (unlikely(!ACCESS_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,8163) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,8165) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,8174) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,8175) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,8192)
    (<0.0>,8193)
    (<0.0>,8194) tree.c:2313: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,8197)
    (<0.0>,8198) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8200) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8203) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8206) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,8209) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,8216) tree.c:2325: local_irq_save(flags);
    (<0.0>,8219)
    (<0.0>,8221) fake_sched.h:43: return __running_cpu;
    (<0.0>,8225) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8227) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8231) fake_sched.h:43: return __running_cpu;
    (<0.0>,8235) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8240) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8241) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8242) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8243) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8244) tree.c:2327: bl = rdp->blimit;
    (<0.0>,8246) tree.c:2327: bl = rdp->blimit;
    (<0.0>,8247) tree.c:2327: bl = rdp->blimit;
    (<0.0>,8250) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,8252) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,8253) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,8254) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8257) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8258) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8259) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8261) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8262) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,8265) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,8266) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,8267) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8270) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8271) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8272) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8274) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8277) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8279) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8282) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8283) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8286) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8289) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8291) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8293) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8296) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8299) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8301) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8303) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8306) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8308) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8311) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8312) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8315) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8318) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8320) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8322) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8325) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8328) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8330) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8332) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8335) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8337) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8340) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8341) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8344) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8347) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8349) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8351) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8354) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8357) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8359) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8361) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8364) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8366) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8369) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8370) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8373) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8376) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8378) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8380) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8383) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8386) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8388) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8390) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8393) tree.c:2336: local_irq_restore(flags);
    (<0.0>,8396)
    (<0.0>,8398) fake_sched.h:43: return __running_cpu;
    (<0.0>,8402) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8404) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8408) fake_sched.h:43: return __running_cpu;
    (<0.0>,8412) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8417) tree.c:2339: count = count_lazy = 0;
    (<0.0>,8418) tree.c:2339: count = count_lazy = 0;
    (<0.0>,8420) tree.c:2340: while (list) {
    (<0.0>,8423) tree.c:2341: next = list->next;
    (<0.0>,8425) tree.c:2341: next = list->next;
    (<0.0>,8426) tree.c:2341: next = list->next;
    (<0.0>,8429) tree.c:2343: debug_rcu_head_unqueue(list);
    (<0.0>,8432)
    (<0.0>,8434) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8436) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8437) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8443)
    (<0.0>,8444)
    (<0.0>,8445) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8447) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8449) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8452) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,8455) rcu.h:118: head->func(head);
    (<0.0>,8457) rcu.h:118: head->func(head);
    (<0.0>,8458) rcu.h:118: head->func(head);
    (<0.0>,8464)
    (<0.0>,8465) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8466) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8467) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8471) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8472) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8473) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8474) update.c:216: complete(&rcu->completion);
    (<0.0>,8478)
    (<0.0>,8479) fake_sync.h:248: x->done++;
    (<0.0>,8481) fake_sync.h:248: x->done++;
    (<0.0>,8483) fake_sync.h:248: x->done++;
    (<0.0>,8488) rcu.h:120: return false;
    (<0.0>,8490) rcu.h:122: }
    (<0.0>,8493) tree.c:2346: list = next;
    (<0.0>,8494) tree.c:2346: list = next;
    (<0.0>,8495) tree.c:2348: if (++count >= bl &&
    (<0.0>,8497) tree.c:2348: if (++count >= bl &&
    (<0.0>,8498) tree.c:2348: if (++count >= bl &&
    (<0.0>,8502) tree.c:2340: while (list) {
    (<0.0>,8505) tree.c:2354: local_irq_save(flags);
    (<0.0>,8508)
    (<0.0>,8510) fake_sched.h:43: return __running_cpu;
    (<0.0>,8514) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8516) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8520) fake_sched.h:43: return __running_cpu;
    (<0.0>,8524) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8531) tree.c:2360: if (list != NULL) {
    (<0.0>,8535) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8536) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8538) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8540) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8541) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8543) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8544) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8546) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8548) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8549) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8550) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8552) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8554) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8555) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,8557) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,8560) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8562) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8565) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8567) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8570) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8572) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8573) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8575) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8576) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8581) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8583) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8586) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8588) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8595) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8596) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8598) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8601) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8603) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8609) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8610) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8611) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8612) tree.c:2386: local_irq_restore(flags);
    (<0.0>,8615)
    (<0.0>,8617) fake_sched.h:43: return __running_cpu;
    (<0.0>,8621) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8623) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8627) fake_sched.h:43: return __running_cpu;
    (<0.0>,8631) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8636) tree.c:2389: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,8639)
    (<0.0>,8640) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8642) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8645) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8656) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8659)
    (<0.0>,8663) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8666) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8667) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8668) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8672) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8673) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8674) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8676) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8680) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,8689)
    (<0.0>,8691) fake_sched.h:43: return __running_cpu;
    (<0.0>,8694) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8696) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8698) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8699) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8701) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8708) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8709) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8711) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8717) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8718) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8719) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8720) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,8721) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,8725)
    (<0.0>,8726)
    (<0.0>,8727) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,8728) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,8735)
    (<0.0>,8736)
    (<0.0>,8737) tree.c:1584: local_irq_save(flags);
    (<0.0>,8740)
    (<0.0>,8742) fake_sched.h:43: return __running_cpu;
    (<0.0>,8746) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8748) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8752) fake_sched.h:43: return __running_cpu;
    (<0.0>,8756) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8761) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8763) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8764) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8765) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8767) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8768) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8770) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8773) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8775) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8776) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8778) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8781) tree.c:1589: local_irq_restore(flags);
    (<0.0>,8784)
    (<0.0>,8786) fake_sched.h:43: return __running_cpu;
    (<0.0>,8790) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8792) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8796) fake_sched.h:43: return __running_cpu;
    (<0.0>,8800) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8807) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,8809) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,8814) tree.c:2558: local_irq_save(flags);
    (<0.0>,8817)
    (<0.0>,8819) fake_sched.h:43: return __running_cpu;
    (<0.0>,8823) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8825) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8829) fake_sched.h:43: return __running_cpu;
    (<0.0>,8833) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8838) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8839) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8845)
    (<0.0>,8846)
    (<0.0>,8847) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,8850)
    (<0.0>,8851) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8853) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8854) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8856) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8862) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,8868)
    (<0.0>,8869) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8872)
    (<0.0>,8873) tree.c:453: return &rsp->node[0];
    (<0.0>,8877) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8878) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8880) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8884) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8885) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8887) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8890) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8891) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8892) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8896) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8899) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8902) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8905) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8906) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8909) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8911) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8914) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8917) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8920) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8921) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8923) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8926) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8930) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8932) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8934) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8937) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8940) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8943) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8944) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8946) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8949) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8953) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8955) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8957) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8960) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,8962) tree.c:494: }
    (<0.0>,8966) tree.c:2566: local_irq_restore(flags);
    (<0.0>,8969)
    (<0.0>,8971) fake_sched.h:43: return __running_cpu;
    (<0.0>,8975) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8977) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8981) fake_sched.h:43: return __running_cpu;
    (<0.0>,8985) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8991) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,8994)
    (<0.0>,8995) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8997) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9000) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9007) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9010)
    (<0.0>,9014) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9017) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9018) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9019) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9023) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9024) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9025) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9027) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9035) fake_sched.h:43: return __running_cpu;
    (<0.0>,9039) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,9048) tree.c:624: local_irq_save(flags);
    (<0.0>,9051)
    (<0.0>,9053) fake_sched.h:43: return __running_cpu;
    (<0.0>,9057) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9059) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9063) fake_sched.h:43: return __running_cpu;
    (<0.0>,9067) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9073) fake_sched.h:43: return __running_cpu;
    (<0.0>,9077) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9078) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,9080) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,9081) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,9082) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,9084) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,9086) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,9087) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9089) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9094) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9095) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9097) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9101) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9102) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9103) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9104) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,9106) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,9114)
    (<0.0>,9116) tree.c:634: local_irq_restore(flags);
    (<0.0>,9119)
    (<0.0>,9121) fake_sched.h:43: return __running_cpu;
    (<0.0>,9125) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9127) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9131) fake_sched.h:43: return __running_cpu;
    (<0.0>,9135) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9144) fake_sched.h:43: return __running_cpu;
    (<0.0>,9148)
    (<0.0>,9151) tree.c:580: local_irq_save(flags);
    (<0.0>,9154)
    (<0.0>,9156) fake_sched.h:43: return __running_cpu;
    (<0.0>,9160) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9162) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9166) fake_sched.h:43: return __running_cpu;
    (<0.0>,9170) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9183)
    (<0.0>,9185) fake_sched.h:43: return __running_cpu;
    (<0.0>,9189) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9190) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9192) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9193) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9194) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9200) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9201) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9206) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9207) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9208) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9209) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,9213) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9215) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9216) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9217) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9236)
    (<0.0>,9238)
    (<0.0>,9240) fake_sched.h:43: return __running_cpu;
    (<0.0>,9244) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9247) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,9251) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9252) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9253) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9257) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9258) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9259) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9261) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9266) fake_sched.h:43: return __running_cpu;
    (<0.0>,9269) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9271) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9273) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9274) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9277)
    (<0.0>,9280) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9283) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9284) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9285) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9289) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9290) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9291) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9293) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9298) fake_sched.h:43: return __running_cpu;
    (<0.0>,9301) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9303) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9305) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9306) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9309)
    (<0.0>,9312) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9315) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9316) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9317) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9321) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9322) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9323) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9325) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9332) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9335) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9336) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9337) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9339) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9340) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9342) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9345) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9351) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9352) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9355) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9360) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9361) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9362) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9376)
    (<0.0>,9378) tree.c:583: local_irq_restore(flags);
    (<0.0>,9381)
    (<0.0>,9383) fake_sched.h:43: return __running_cpu;
    (<0.0>,9387) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9389) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9393) fake_sched.h:43: return __running_cpu;
    (<0.0>,9397) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9403) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9406) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9411) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,9413) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,9422) fake_sched.h:43: return __running_cpu;
    (<0.0>,9426)
    (<0.0>,9427) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9430) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9435) tree.c:704: local_irq_save(flags);
    (<0.0>,9438)
    (<0.0>,9440) fake_sched.h:43: return __running_cpu;
    (<0.0>,9444) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9446) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9450) fake_sched.h:43: return __running_cpu;
    (<0.0>,9454) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9467)
    (<0.0>,9469) fake_sched.h:43: return __running_cpu;
    (<0.0>,9473) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9474) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9476) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9477) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9478) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9483) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9484) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9488) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9489) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9490) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9491) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,9495) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9497) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9498) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9499) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9513)
    (<0.0>,9514)
    (<0.0>,9516) fake_sched.h:43: return __running_cpu;
    (<0.0>,9520) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9524) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9527) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9528) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9529) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9531) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9532) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9534) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9537) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9544) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9545) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9548) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9553) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9554) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9555) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9560) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,9569)
    (<0.0>,9571) tree.c:707: local_irq_restore(flags);
    (<0.0>,9574)
    (<0.0>,9576) fake_sched.h:43: return __running_cpu;
    (<0.0>,9580) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9582) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9586) fake_sched.h:43: return __running_cpu;
    (<0.0>,9590) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9597) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,9609)
    (<0.0>,9610) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9613)
    (<0.0>,9614) tree.c:453: return &rsp->node[0];
    (<0.0>,9618) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9622) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,9626)
    (<0.0>,9629) fake_sched.h:43: return __running_cpu;
    (<0.0>,9633) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9637) fake_sched.h:43: return __running_cpu;
    (<0.0>,9641) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9646) fake_sched.h:43: return __running_cpu;
    (<0.0>,9650) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9653) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9654) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9660) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,9662) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,9665) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,9667) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,9668) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9671)
    (<0.0>,9672) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9674) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9675) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9677) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9685) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9686) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9689)
    (<0.0>,9690) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9692) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9693) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9695) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9702) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9703) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9704) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9707) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,9712)
    (<0.0>,9713) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,9714) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,9715) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9716) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9718) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9722) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,9723) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,9724) update.c:344: if (till_stall_check < 3) {
    (<0.0>,9727) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,9731) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,9736) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,9737) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9738) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9740) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9742) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9743) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9744) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9747) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9749) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9753) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9755) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9757) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9759) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9763) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,9767)
    (<0.0>,9768) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9769) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9774) fake_sched.h:43: return __running_cpu;
    (<0.0>,9778) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9780) fake_sched.h:43: return __running_cpu;
    (<0.0>,9784) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9790) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,9794)
    (<0.0>,9795) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,9797) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,9803) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9806) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9808) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9809) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9811) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9816) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,9820)
    (<0.0>,9823) fake_sched.h:43: return __running_cpu;
    (<0.0>,9827) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9831) fake_sched.h:43: return __running_cpu;
    (<0.0>,9835) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9840) fake_sched.h:43: return __running_cpu;
    (<0.0>,9844) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9847) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9848) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9855) fake_sched.h:43: return __running_cpu;
    (<0.0>,9858) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9860) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9862) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9863) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,9869)
    (<0.0>,9870) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9872) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9877) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9878) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9880) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9884) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9885) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9886) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9888) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9890) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9891) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9893) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9894) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9896) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9897) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9899) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9900) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9902) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9903) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9905) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9910) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9911) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9913) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9914) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9916) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9920) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9921) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9922) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9923) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9925) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9926) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9928) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9929) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9930) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9932) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9935) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9936) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9937) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9943)
    (<0.0>,9944)
    (<0.0>,9945)
    (<0.0>,9946) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9948) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9949) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9951) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9954) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9955) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9956) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9965)
    (<0.0>,9966)
    (<0.0>,9967)
    (<0.0>,9968) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9971) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9974) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9977) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9978) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9981) tree.c:1434: return false;
    (<0.0>,9983) tree.c:1483: }
    (<0.0>,9986) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9988) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9990) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9991) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9993) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9996) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9998) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9999) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,10001) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,10004) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,10006) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,10007) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10009) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10010) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10012) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10017) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10020) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10021) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,10024)
    (<0.0>,10027) tree.c:1575: return ret;
    (<0.0>,10031) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,10034)
    (<0.0>,10038) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,10042)
    (<0.0>,10043) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,10044) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,10049) fake_sched.h:43: return __running_cpu;
    (<0.0>,10053) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,10055) fake_sched.h:43: return __running_cpu;
    (<0.0>,10059) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10072) fake_sched.h:43: return __running_cpu;
    (<0.0>,10077) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,10083) fake_sched.h:43: return __running_cpu;
    (<0.0>,10088) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,10094) fake_sched.h:43: return __running_cpu;
    (<0.0>,10098) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,10110) fake_sched.h:43: return __running_cpu;
    (<0.0>,10114)
    (<0.0>,10117) tree.c:580: local_irq_save(flags);
    (<0.0>,10120)
    (<0.0>,10122) fake_sched.h:43: return __running_cpu;
    (<0.0>,10126) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10128) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10132) fake_sched.h:43: return __running_cpu;
    (<0.0>,10136) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10149)
    (<0.0>,10151) fake_sched.h:43: return __running_cpu;
    (<0.0>,10155) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10156) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10158) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10159) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10160) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10166) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10167) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10172) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10173) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10174) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10175) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,10179) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,10181) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,10182) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,10183) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,10202)
    (<0.0>,10204)
    (<0.0>,10206) fake_sched.h:43: return __running_cpu;
    (<0.0>,10210) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10213) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,10217) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10218) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10219) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10223) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10224) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10225) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10227) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10232) fake_sched.h:43: return __running_cpu;
    (<0.0>,10235) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10237) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10239) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10240) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10243)
    (<0.0>,10246) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10249) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10250) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10251) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10255) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10256) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10257) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10259) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10264) fake_sched.h:43: return __running_cpu;
    (<0.0>,10267) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10269) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10271) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10272) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10275)
    (<0.0>,10278) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10281) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10282) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10283) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10287) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10288) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10289) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10291) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10298) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10301) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10302) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10303) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10305) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10306) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10308) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10311) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10317) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10318) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10321) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10326) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10327) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10328) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10342)
    (<0.0>,10344) tree.c:583: local_irq_restore(flags);
    (<0.0>,10347)
    (<0.0>,10349) fake_sched.h:43: return __running_cpu;
    (<0.0>,10353) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10355) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10359) fake_sched.h:43: return __running_cpu;
    (<0.0>,10363) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10369) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10372) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10377) fake_sched.h:43: return __running_cpu;
    (<0.0>,10381)
    (<0.0>,10382) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10385) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10390) tree.c:704: local_irq_save(flags);
    (<0.0>,10393)
    (<0.0>,10395) fake_sched.h:43: return __running_cpu;
    (<0.0>,10399) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10401) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10405) fake_sched.h:43: return __running_cpu;
    (<0.0>,10409) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10422)
    (<0.0>,10424) fake_sched.h:43: return __running_cpu;
    (<0.0>,10428) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10429) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10431) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10432) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10433) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10438) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10439) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10443) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10444) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10445) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10446) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,10450) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10452) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10453) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10454) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10468)
    (<0.0>,10469)
    (<0.0>,10471) fake_sched.h:43: return __running_cpu;
    (<0.0>,10475) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10479) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10482) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10483) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10484) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10486) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10487) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10489) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10492) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10499) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10500) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10503) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10508) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10509) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10510) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10515) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,10524)
    (<0.0>,10526) tree.c:707: local_irq_restore(flags);
    (<0.0>,10529)
    (<0.0>,10531) fake_sched.h:43: return __running_cpu;
    (<0.0>,10535) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10537) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10541) fake_sched.h:43: return __running_cpu;
    (<0.0>,10545) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10560) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10562) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10564) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10565) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10567) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10572) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,10576)
    (<0.0>,10577) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,10579) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,10583) tree.c:1674: return 1;
    (<0.0>,10585) tree.c:1675: }
    (<0.0>,10590) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,10591) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,10592) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,10593) tree.c:1819: if (j > HZ) {
    (<0.0>,10596) tree.c:1823: ret = 0;
    (<0.0>,10598) tree.c:1825: if (!ret)
    (<0.0>,10601) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10602) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10604) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10606) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10610) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,10612) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,10616) fake_sched.h:43: return __running_cpu;
    (<0.0>,10620) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,10624) fake_sched.h:43: return __running_cpu;
    (<0.0>,10628) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10633) fake_sched.h:43: return __running_cpu;
    (<0.0>,10637) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,10647) tree.c:749: local_irq_save(flags);
    (<0.0>,10650)
    (<0.0>,10652) fake_sched.h:43: return __running_cpu;
    (<0.0>,10656) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10658) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10663) fake_sched.h:43: return __running_cpu;
    (<0.0>,10667) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10668) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10670) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10671) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10672) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10674) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10676) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10677) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10679) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10684) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10685) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10687) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10691) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10692) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10693) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10694) tree.c:754: if (oldval)
    (<0.0>,10702)
    (<0.0>,10704) tree.c:759: local_irq_restore(flags);
    (<0.0>,10707)
    (<0.0>,10709) fake_sched.h:43: return __running_cpu;
    (<0.0>,10713) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10715) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10723)
    (<0.0>,10728) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,10733) fake_sched.h:43: return __running_cpu;
    (<0.0>,10738) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,10746) fake_sched.h:43: return __running_cpu;
    (<0.0>,10751) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,10765) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10766) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10767) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10771) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10772) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10773) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10775) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10779) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10781) fake_sched.h:43: return __running_cpu;
    (<0.0>,10784) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10786) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10793)
    (<0.0>,10794)
    (<0.0>,10795) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10797) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10798) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10799) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10801) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10803) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10804) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10805) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10815)
    (<0.0>,10816)
    (<0.0>,10817) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,10822) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,10825)
    (<0.0>,10828) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,10831) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10833) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10836) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10838) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10841) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10843) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10846) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10848) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10851) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10853) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10855) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10856) tree.c:3140: return 1;
    (<0.0>,10858) tree.c:3176: }
    (<0.0>,10862) tree.c:3189: return 1;
    (<0.0>,10864) tree.c:3191: }
    (<0.0>,10871) fake_sched.h:43: return __running_cpu;
    (<0.0>,10875) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,10879) tree.c:2437: if (user)
    (<0.0>,10887) fake_sched.h:43: return __running_cpu;
    (<0.0>,10891) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,10893) fake_sched.h:43: return __running_cpu;
    (<0.0>,10897) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10903) fake_sched.h:43: return __running_cpu;
    (<0.0>,10907) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,10917)
    (<0.0>,10920) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10921) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10922) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10926) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10927) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10928) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10930) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10934) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,10943)
    (<0.0>,10945) fake_sched.h:43: return __running_cpu;
    (<0.0>,10948) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10950) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10952) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10953) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10955) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10962) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10963) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10965) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10971) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10972) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10973) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10974) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10975) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10979)
    (<0.0>,10980)
    (<0.0>,10981) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,10982) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,10989)
    (<0.0>,10990)
    (<0.0>,10991) tree.c:1584: local_irq_save(flags);
    (<0.0>,10994)
    (<0.0>,10996) fake_sched.h:43: return __running_cpu;
    (<0.0>,11000) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11002) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11006) fake_sched.h:43: return __running_cpu;
    (<0.0>,11010) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11015) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11017) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11018) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11019) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11021) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11022) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11024) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11027) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11029) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11030) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11032) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11035) tree.c:1589: local_irq_restore(flags);
    (<0.0>,11038)
    (<0.0>,11040) fake_sched.h:43: return __running_cpu;
    (<0.0>,11044) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11046) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11050) fake_sched.h:43: return __running_cpu;
    (<0.0>,11054) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11061) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11063) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11066) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,11068) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,11071) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,11073) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,11074) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,11075) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,11084)
    (<0.0>,11085)
    (<0.0>,11086)
    (<0.0>,11087) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,11089) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,11090) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,11091) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,11093) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,11097)
    (<0.0>,11098)
    (<0.0>,11099) fake_sync.h:76: local_irq_save(flags);
    (<0.0>,11102)
    (<0.0>,11104) fake_sched.h:43: return __running_cpu;
    (<0.0>,11108) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11110) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11114) fake_sched.h:43: return __running_cpu;
    (<0.0>,11118) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11124) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,11125) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,11131) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11133) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11138) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11140) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11141) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11143) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11146) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,11148) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,11149) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,11151) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,11154) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,11156) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,11157) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,11158) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,11160) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,11161) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,11165) tree.c:2054: rdp->qs_pending = 0;
    (<0.0>,11167) tree.c:2054: rdp->qs_pending = 0;
    (<0.0>,11168) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,11169) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,11170) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,11179)
    (<0.0>,11180)
    (<0.0>,11181)
    (<0.0>,11182) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11185) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11188) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11191) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11192) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11195) tree.c:1434: return false;
    (<0.0>,11197) tree.c:1483: }
    (<0.0>,11200) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,11201) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,11202) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,11203) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,11204) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,11214)
    (<0.0>,11215)
    (<0.0>,11216)
    (<0.0>,11217)
    (<0.0>,11219) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,11221) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,11222) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,11226) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,11228) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,11230) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,11232) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,11235) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,11237) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,11240) tree.c:1990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,11242) tree.c:1990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,11246)
    (<0.0>,11247)
    (<0.0>,11248) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,11249) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,11252) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,11255)
    (<0.0>,11257) fake_sched.h:43: return __running_cpu;
    (<0.0>,11261) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11263) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11267) fake_sched.h:43: return __running_cpu;
    (<0.0>,11271) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11280) tree.c:2063: if (needwake)
    (<0.0>,11287) tree.c:2558: local_irq_save(flags);
    (<0.0>,11290)
    (<0.0>,11292) fake_sched.h:43: return __running_cpu;
    (<0.0>,11296) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11298) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11302) fake_sched.h:43: return __running_cpu;
    (<0.0>,11306) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11311) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11312) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11318)
    (<0.0>,11319)
    (<0.0>,11320) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,11323)
    (<0.0>,11324) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11326) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11327) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11329) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11335) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,11337) tree.c:494: }
    (<0.0>,11341) tree.c:2566: local_irq_restore(flags);
    (<0.0>,11344)
    (<0.0>,11346) fake_sched.h:43: return __running_cpu;
    (<0.0>,11350) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11352) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11356) fake_sched.h:43: return __running_cpu;
    (<0.0>,11360) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11366) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11369)
    (<0.0>,11370) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11372) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11375) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11382) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11385)
    (<0.0>,11389) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11392) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11393) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11394) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11398) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11399) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11400) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11402) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11406) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,11415)
    (<0.0>,11417) fake_sched.h:43: return __running_cpu;
    (<0.0>,11420) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11422) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11424) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11425) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11427) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11434) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11435) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11437) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11443) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11444) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11445) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11446) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11447) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11451)
    (<0.0>,11452)
    (<0.0>,11453) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,11454) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,11461)
    (<0.0>,11462)
    (<0.0>,11463) tree.c:1584: local_irq_save(flags);
    (<0.0>,11466)
    (<0.0>,11468) fake_sched.h:43: return __running_cpu;
    (<0.0>,11472) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11474) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11478) fake_sched.h:43: return __running_cpu;
    (<0.0>,11482) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11487) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11489) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11490) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11491) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11493) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11494) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11496) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11499) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11501) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11502) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11504) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11507) tree.c:1589: local_irq_restore(flags);
    (<0.0>,11510)
    (<0.0>,11512) fake_sched.h:43: return __running_cpu;
    (<0.0>,11516) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11518) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11522) fake_sched.h:43: return __running_cpu;
    (<0.0>,11526) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11533) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11535) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11540) tree.c:2558: local_irq_save(flags);
    (<0.0>,11543)
    (<0.0>,11545) fake_sched.h:43: return __running_cpu;
    (<0.0>,11549) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11551) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11555) fake_sched.h:43: return __running_cpu;
    (<0.0>,11559) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11564) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11565) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11571)
    (<0.0>,11572)
    (<0.0>,11573) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,11576)
    (<0.0>,11577) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11579) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11580) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11582) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11588) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,11594)
    (<0.0>,11595) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11598)
    (<0.0>,11599) tree.c:453: return &rsp->node[0];
    (<0.0>,11603) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11604) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11606) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11610) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11611) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11613) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11616) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11617) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11618) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11622) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11625) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11628) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11631) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11632) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11635) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11637) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11640) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11643) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11646) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11647) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11649) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11652) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11656) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11658) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11660) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11663) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11666) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11669) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11670) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11672) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11675) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11679) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11681) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11683) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11686) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,11688) tree.c:494: }
    (<0.0>,11692) tree.c:2566: local_irq_restore(flags);
    (<0.0>,11695)
    (<0.0>,11697) fake_sched.h:43: return __running_cpu;
    (<0.0>,11701) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11703) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11707) fake_sched.h:43: return __running_cpu;
    (<0.0>,11711) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11717) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11720)
    (<0.0>,11721) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11723) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11726) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11733) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11736)
    (<0.0>,11740) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11743) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11744) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11745) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11749) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11750) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11751) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11753) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11761) fake_sched.h:43: return __running_cpu;
    (<0.0>,11765) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,11774) tree.c:624: local_irq_save(flags);
    (<0.0>,11777)
    (<0.0>,11779) fake_sched.h:43: return __running_cpu;
    (<0.0>,11783) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11785) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11789) fake_sched.h:43: return __running_cpu;
    (<0.0>,11793) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11799) fake_sched.h:43: return __running_cpu;
    (<0.0>,11803) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11804) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11806) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11807) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11808) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11810) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11812) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11813) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11815) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11820) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11821) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11823) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11827) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11828) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11829) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11830) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,11832) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,11840)
    (<0.0>,11842) tree.c:634: local_irq_restore(flags);
    (<0.0>,11845)
    (<0.0>,11847) fake_sched.h:43: return __running_cpu;
    (<0.0>,11851) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11853) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11857) fake_sched.h:43: return __running_cpu;
    (<0.0>,11861) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11870) fake_sched.h:43: return __running_cpu;
    (<0.0>,11874)
    (<0.0>,11877) tree.c:580: local_irq_save(flags);
    (<0.0>,11880)
    (<0.0>,11882) fake_sched.h:43: return __running_cpu;
    (<0.0>,11886) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11888) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11892) fake_sched.h:43: return __running_cpu;
    (<0.0>,11896) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11909)
    (<0.0>,11911) fake_sched.h:43: return __running_cpu;
    (<0.0>,11915) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11916) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11918) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11919) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11920) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11926) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11927) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11932) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11933) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11934) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11935) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,11939) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11941) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11942) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11943) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11962)
    (<0.0>,11964)
    (<0.0>,11966) fake_sched.h:43: return __running_cpu;
    (<0.0>,11970) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11973) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,11977) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11978) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11979) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11983) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11984) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11985) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11987) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11992) fake_sched.h:43: return __running_cpu;
    (<0.0>,11995) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11997) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11999) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,12000) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12003)
    (<0.0>,12006) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12009) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12010) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12011) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12015) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12016) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12017) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12019) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12024) fake_sched.h:43: return __running_cpu;
    (<0.0>,12027) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,12029) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,12031) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,12032) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12035)
    (<0.0>,12038) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12041) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12042) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12043) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12047) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12048) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12049) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12051) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12058) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12061) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12062) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12063) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12065) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12066) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12068) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12071) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12077) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12078) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12081) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12086) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12087) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12088) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12102)
    (<0.0>,12104) tree.c:583: local_irq_restore(flags);
    (<0.0>,12107)
    (<0.0>,12109) fake_sched.h:43: return __running_cpu;
    (<0.0>,12113) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12115) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12119) fake_sched.h:43: return __running_cpu;
    (<0.0>,12123) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12129) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12132) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12137) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12139) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12141) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12145) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12147) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12154) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12156) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12158) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12162) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12164) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12171) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12173) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12175) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12179) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12181) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12188) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12190) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12192) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12196) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12198) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12205) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12207) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12209) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12213) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12215) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
      (<0.1>,725) fake_sync.h:241: while (!x->done)
      (<0.1>,732) fake_sched.h:43: return __running_cpu;
      (<0.1>,736)
      (<0.1>,737) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,740) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,745) tree.c:704: local_irq_save(flags);
      (<0.1>,748)
      (<0.1>,750) fake_sched.h:43: return __running_cpu;
      (<0.1>,754) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,756) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,760) fake_sched.h:43: return __running_cpu;
      (<0.1>,764) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,777)
      (<0.1>,779) fake_sched.h:43: return __running_cpu;
      (<0.1>,783) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,784) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,786) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,787) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,788) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,793) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,794) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,798) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,799) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,800) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,801) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,805) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,807) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,808) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,809) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,823)
      (<0.1>,824)
      (<0.1>,826) fake_sched.h:43: return __running_cpu;
      (<0.1>,830) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,834) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,837) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,838) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,839) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,841) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,842) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,844) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,847) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,854) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,855) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,858) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,864) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,865) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,870) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,879)
      (<0.1>,881) tree.c:707: local_irq_restore(flags);
      (<0.1>,884)
      (<0.1>,886) fake_sched.h:43: return __running_cpu;
      (<0.1>,890) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,892) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,896) fake_sched.h:43: return __running_cpu;
      (<0.1>,900) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,911)
      (<0.1>,917) litmus.c:91: y = 1;
      (<0.1>,919) fake_sched.h:43: return __running_cpu;
      (<0.1>,923)
      (<0.1>,926) tree.c:580: local_irq_save(flags);
      (<0.1>,929)
      (<0.1>,931) fake_sched.h:43: return __running_cpu;
      (<0.1>,935) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,937) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,941) fake_sched.h:43: return __running_cpu;
      (<0.1>,945) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,958)
      (<0.1>,960) fake_sched.h:43: return __running_cpu;
      (<0.1>,964) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,965) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,967) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,968) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,969) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,975) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,976) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,981) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,982) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,983) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,984) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,988) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,990) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,991) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,992) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1011)
      (<0.1>,1013)
      (<0.1>,1015) fake_sched.h:43: return __running_cpu;
      (<0.1>,1019) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1022) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,1026) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1027) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1028) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1033) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1034) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1041) fake_sched.h:43: return __running_cpu;
      (<0.1>,1044) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1046) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1048) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1049) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1052)
      (<0.1>,1055) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1058) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1059) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1060) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1065) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1066) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1073) fake_sched.h:43: return __running_cpu;
      (<0.1>,1076) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1078) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1080) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1081) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1084)
      (<0.1>,1087) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1090) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1091) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1092) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1097) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1098) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1107) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1110) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1111) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1112) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1117) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1120) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1126) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1127) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1130) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1135) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1136) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1137) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1151)
      (<0.1>,1153) tree.c:583: local_irq_restore(flags);
      (<0.1>,1156)
      (<0.1>,1158) fake_sched.h:43: return __running_cpu;
      (<0.1>,1162) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1164) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1168) fake_sched.h:43: return __running_cpu;
      (<0.1>,1172) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1178) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1181) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8094) litmus.c:69: r_y = y;
  (<0>,8095) litmus.c:69: r_y = y;
  (<0>,8106) fake_sched.h:43: return __running_cpu;
  (<0>,8110)
  (<0>,8113) tree.c:580: local_irq_save(flags);
  (<0>,8116)
  (<0>,8118) fake_sched.h:43: return __running_cpu;
  (<0>,8122) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8124) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8128) fake_sched.h:43: return __running_cpu;
  (<0>,8132) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8145)
  (<0>,8147) fake_sched.h:43: return __running_cpu;
  (<0>,8151) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,8152) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,8154) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,8155) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,8156) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8162) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8163) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8168) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8169) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8170) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8171) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,8175) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,8177) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,8178) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,8179) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,8198)
  (<0>,8200)
  (<0>,8202) fake_sched.h:43: return __running_cpu;
  (<0>,8206) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,8209) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,8213) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8214) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8215) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8219) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8220) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8221) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8223) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8228) fake_sched.h:43: return __running_cpu;
  (<0>,8231) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8233) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8235) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8236) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,8239)
  (<0>,8242) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8245) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8246) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8247) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8251) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8252) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8253) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8255) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8260) fake_sched.h:43: return __running_cpu;
  (<0>,8263) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8265) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8267) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8268) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,8271)
  (<0>,8274) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8277) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8278) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8279) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8283) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8284) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8285) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8287) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8294) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8297) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8298) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8299) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8301) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8302) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8304) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8307) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8313) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8314) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8317) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8322) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8323) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8324) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8338)
  (<0>,8340) tree.c:583: local_irq_restore(flags);
  (<0>,8343)
  (<0>,8345) fake_sched.h:43: return __running_cpu;
  (<0>,8349) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8351) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8355) fake_sched.h:43: return __running_cpu;
  (<0>,8359) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8365) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8368) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8373) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,8377) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8380) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8384) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8385) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8386) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
             Error: Assertion violation at (<0>,8389): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpca97lqis/tmp1jva0dt_.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DFORCE_FAILURE_1 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpca97lqis/tmpoqlgczb3.ll /tmp/tmpca97lqis/tmp1jva0dt_.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=4 --preemption-bounding=PB --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpca97lqis/tmpoqlgczb3.ll
Total wall-clock time: 5.51 s
Trace count: 100000 (also 0 sleepset blocked, 0 schedulings and 0 branches were rejected due to the bound)
No errors were detected.
Total wall-clock time: 0.0 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 59 (also 1 sleepset blocked, 0 schedulings and 37 branches were rejected due to the bound) 

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,73) tree_plugin.h:931: pr_info("Hierarchical RCU implementation.\n");
  (<0>,77) tree_plugin.h:91: if (rcu_fanout_leaf != CONFIG_RCU_FANOUT_LEAF)
  (<0>,89) tree.c:3718: ulong d;
  (<0>,90) tree.c:3722: int rcu_capacity[MAX_RCU_LVLS + 1];
  (<0>,91) tree.c:3732: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,94) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,95) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,97) tree.c:3734: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,100) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,101) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,103) tree.c:3738: if (rcu_fanout_leaf == CONFIG_RCU_FANOUT_LEAF &&
  (<0>,115)
  (<0>,116) tree.c:3628: static void __init rcu_init_one(struct rcu_state *rsp,
  (<0>,117) tree.c:3629: struct rcu_data __percpu *rda)
  (<0>,118) tree.c:3643: int i;
  (<0>,121) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,123) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,124) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,127) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,130) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,133) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,136) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,140) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,142) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,143) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,146) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,149) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,152) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,158)
  (<0>,159) tree.c:3610: static void __init rcu_init_levelspread(struct rcu_state *rsp)
  (<0>,160) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,162) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,164) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,169) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,172) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,173) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,174) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,175) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,183) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,186) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,187) tree.c:3620: cprv = ccur;
  (<0>,188) tree.c:3620: cprv = ccur;
  (<0>,190) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,192) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,194) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,198) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,199) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,201) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,202) tree.c:3661: fl_mask <<= 1;
  (<0>,206) tree.c:3661: fl_mask <<= 1;
  (<0>,207) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,209) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,211) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,219) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,221) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,223) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,224) tree.c:3667: rnp = rsp->level[i];
  (<0>,226) tree.c:3667: rnp = rsp->level[i];
  (<0>,229) tree.c:3667: rnp = rsp->level[i];
  (<0>,230) tree.c:3667: rnp = rsp->level[i];
  (<0>,231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,233) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,236) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,242) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,246)
  (<0>,247) fake_sync.h:68: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,248) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,254) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,258)
  (<0>,259) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,260) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,268) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,269) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,271) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,274) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,275) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,277) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,278) tree.c:3677: rnp->qsmask = 0;
  (<0>,280) tree.c:3677: rnp->qsmask = 0;
  (<0>,281) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,283) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,285) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,287) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,295) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,297) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,298) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,300) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,303) tree.c:3683: if (i == 0) {
  (<0>,306) tree.c:3684: rnp->grpnum = 0;
  (<0>,308) tree.c:3684: rnp->grpnum = 0;
  (<0>,309) tree.c:3685: rnp->grpmask = 0;
  (<0>,311) tree.c:3685: rnp->grpmask = 0;
  (<0>,312) tree.c:3686: rnp->parent = NULL;
  (<0>,314) tree.c:3686: rnp->parent = NULL;
  (<0>,316) tree.c:3693: rnp->level = i;
  (<0>,318) tree.c:3693: rnp->level = i;
  (<0>,320) tree.c:3693: rnp->level = i;
  (<0>,321) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,325)
  (<0>,326) fake_defs.h:273: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,327) fake_defs.h:275: list->next = list;
  (<0>,329) fake_defs.h:275: list->next = list;
  (<0>,330) fake_defs.h:276: list->prev = list;
  (<0>,331) fake_defs.h:276: list->prev = list;
  (<0>,333) fake_defs.h:276: list->prev = list;
  (<0>,335) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,338) tree_plugin.h:2694: }
  (<0>,341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,348) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,351) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,354) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,358) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,360) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,362) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,365) tree.c:3699: rsp->rda = rda;
  (<0>,366) tree.c:3699: rsp->rda = rda;
  (<0>,368) tree.c:3699: rsp->rda = rda;
  (<0>,371) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,374) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,377) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,378) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,379) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,381) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,382) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,389)
  (<0>,390) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,391) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,393) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,394) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,397) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,399) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,400) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,403) fake_defs.h:536: if (offset & mask)
  (<0>,404) fake_defs.h:536: if (offset & mask)
  (<0>,408) fake_defs.h:537: return cpu;
  (<0>,409) fake_defs.h:537: return cpu;
  (<0>,411) fake_defs.h:540: }
  (<0>,413) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,414) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,418) tree.c:3703: while (i > rnp->grphi)
  (<0>,419) tree.c:3703: while (i > rnp->grphi)
  (<0>,421) tree.c:3703: while (i > rnp->grphi)
  (<0>,424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,425) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,429) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,432) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,433) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,434) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,447)
  (<0>,448) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,449) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,451) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,453) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,455) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,456) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,459)
  (<0>,460) tree.c:451: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,464) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,465) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,467) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,471)
  (<0>,472) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,473) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,476) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,478) fake_sched.h:43: return __running_cpu;
  (<0>,482) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,484) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,488) fake_sched.h:43: return __running_cpu;
  (<0>,492) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,498) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,499) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,504) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,506) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,508) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,512) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,514) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,515) tree.c:3412: init_callback_list(rdp);
  (<0>,519)
  (<0>,520) tree.c:1223: static void init_callback_list(struct rcu_data *rdp)
  (<0>,523) tree_plugin.h:2732: return false;
  (<0>,526) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,528) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,529) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,531) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,534) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,538) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,541) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,543) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,545) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,547) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,550) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,554) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,557) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,559) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,561) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,563) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,566) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,570) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,573) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,575) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,577) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,579) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,582) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,586) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,589) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,591) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,593) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,595) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,599) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,601) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,602) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,604) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,608) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,610) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,611) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,613) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,621) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,623) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,629) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,630) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,631) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,634) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,642) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,645) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,652) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,653) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,654) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,655) tree.c:3418: rdp->cpu = cpu;
  (<0>,656) tree.c:3418: rdp->cpu = cpu;
  (<0>,658) tree.c:3418: rdp->cpu = cpu;
  (<0>,659) tree.c:3419: rdp->rsp = rsp;
  (<0>,660) tree.c:3419: rdp->rsp = rsp;
  (<0>,662) tree.c:3419: rdp->rsp = rsp;
  (<0>,663) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,666) tree_plugin.h:2711: }
  (<0>,668) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,670) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,674)
  (<0>,675) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,676) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,677) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,680) fake_sync.h:86: local_irq_restore(flags);
  (<0>,683) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,685) fake_sched.h:43: return __running_cpu;
  (<0>,689) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,691) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,695) fake_sched.h:43: return __running_cpu;
  (<0>,699) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,708) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,709) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,716)
  (<0>,717)
  (<0>,718) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,720) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,721) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,724) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,726) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,727) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,730) fake_defs.h:536: if (offset & mask)
  (<0>,731) fake_defs.h:536: if (offset & mask)
  (<0>,735) fake_defs.h:537: return cpu;
  (<0>,736) fake_defs.h:537: return cpu;
  (<0>,738) fake_defs.h:540: }
  (<0>,740) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,741) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,745) tree.c:3703: while (i > rnp->grphi)
  (<0>,746) tree.c:3703: while (i > rnp->grphi)
  (<0>,748) tree.c:3703: while (i > rnp->grphi)
  (<0>,751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,752) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,756) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,759) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,760) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,761) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,774)
  (<0>,775)
  (<0>,776) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,778) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,780) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,782) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,783) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,786)
  (<0>,787) tree.c:453: return &rsp->node[0];
  (<0>,791) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,792) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,794) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,798)
  (<0>,799)
  (<0>,800) fake_sync.h:76: local_irq_save(flags);
  (<0>,803) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,805) fake_sched.h:43: return __running_cpu;
  (<0>,809) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,811) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,815) fake_sched.h:43: return __running_cpu;
  (<0>,819) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,825) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,826) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,831) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,833) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,835) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,839) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,841) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,842) tree.c:3412: init_callback_list(rdp);
  (<0>,846)
  (<0>,847) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,850) tree_plugin.h:2732: return false;
  (<0>,853) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,855) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,856) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,858) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,861) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,865) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,868) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,870) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,872) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,874) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,877) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,881) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,884) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,886) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,888) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,890) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,893) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,897) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,900) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,902) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,904) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,906) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,909) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,913) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,916) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,918) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,920) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,922) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,926) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,928) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,929) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,931) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,935) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,937) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,938) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,940) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,948) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,950) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,956) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,957) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,958) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,961) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,969) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,972) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,979) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,980) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,981) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,982) tree.c:3418: rdp->cpu = cpu;
  (<0>,983) tree.c:3418: rdp->cpu = cpu;
  (<0>,985) tree.c:3418: rdp->cpu = cpu;
  (<0>,986) tree.c:3419: rdp->rsp = rsp;
  (<0>,987) tree.c:3419: rdp->rsp = rsp;
  (<0>,989) tree.c:3419: rdp->rsp = rsp;
  (<0>,990) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,993) tree_plugin.h:2711: }
  (<0>,995) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,997) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1001)
  (<0>,1002)
  (<0>,1003) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1004) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1007) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1010) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1012) fake_sched.h:43: return __running_cpu;
  (<0>,1016) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1018) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1022) fake_sched.h:43: return __running_cpu;
  (<0>,1026) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1035) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1036) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1043)
  (<0>,1044)
  (<0>,1045) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1047) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1048) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1051) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1053) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1054) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1057) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,1059) fake_defs.h:540: }
  (<0>,1061) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1062) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1065) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1070)
  (<0>,1071) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1072) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1073) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1074) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1076) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1081)
  (<0>,1082) fake_defs.h:279: static inline void __list_add(struct list_head *new,
  (<0>,1083) fake_defs.h:280: struct list_head *prev,
  (<0>,1084) fake_defs.h:281: struct list_head *next)
  (<0>,1085) fake_defs.h:283: next->prev = new;
  (<0>,1087) fake_defs.h:283: next->prev = new;
  (<0>,1088) fake_defs.h:284: new->next = next;
  (<0>,1089) fake_defs.h:284: new->next = next;
  (<0>,1091) fake_defs.h:284: new->next = next;
  (<0>,1092) fake_defs.h:285: new->prev = prev;
  (<0>,1093) fake_defs.h:285: new->prev = prev;
  (<0>,1095) fake_defs.h:285: new->prev = prev;
  (<0>,1096) fake_defs.h:286: prev->next = new;
  (<0>,1097) fake_defs.h:286: prev->next = new;
  (<0>,1099) fake_defs.h:286: prev->next = new;
  (<0>,1110)
  (<0>,1111)
  (<0>,1112) tree.c:3642: int cpustride = 1;
  (<0>,1113) tree.c:3650: if (rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1116) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1118) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1119) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1122) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1125) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1126) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1128) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1133) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1135) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1137) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1141) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1143) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1144) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1147) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,1153)
  (<0>,1154) tree.c:3616: cprv = nr_cpu_ids;
  (<0>,1155) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1157) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1159) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1162) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1164) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1168) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1169) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1170) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1173) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1176) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1182) tree.c:3620: cprv = ccur;
  (<0>,1183) tree.c:3620: cprv = ccur;
  (<0>,1185) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1187) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1189) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1193) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1194) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1196) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1197) tree.c:3661: fl_mask <<= 1;
  (<0>,1201) tree.c:3661: fl_mask <<= 1;
  (<0>,1202) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1204) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1206) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1209) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1211) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1218) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1219) tree.c:3667: rnp = rsp->level[i];
  (<0>,1221) tree.c:3667: rnp = rsp->level[i];
  (<0>,1224) tree.c:3667: rnp = rsp->level[i];
  (<0>,1225) tree.c:3667: rnp = rsp->level[i];
  (<0>,1226) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1228) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1229) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1237) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,1241)
  (<0>,1242) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1243) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1249) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1253)
  (<0>,1254) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1255) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1261) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1263) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1264) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1267) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1269) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1270) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1273) tree.c:3677: rnp->qsmask = 0;
  (<0>,1275) tree.c:3677: rnp->qsmask = 0;
  (<0>,1276) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1278) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1279) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1280) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1282) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1285) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1287) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1293) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1295) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1298) tree.c:3683: if (i == 0) {
  (<0>,1301) tree.c:3684: rnp->grpnum = 0;
  (<0>,1303) tree.c:3684: rnp->grpnum = 0;
  (<0>,1304) tree.c:3685: rnp->grpmask = 0;
  (<0>,1306) tree.c:3685: rnp->grpmask = 0;
  (<0>,1307) tree.c:3686: rnp->parent = NULL;
  (<0>,1309) tree.c:3686: rnp->parent = NULL;
  (<0>,1311) tree.c:3693: rnp->level = i;
  (<0>,1313) tree.c:3693: rnp->level = i;
  (<0>,1315) tree.c:3693: rnp->level = i;
  (<0>,1316) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1320)
  (<0>,1321) fake_defs.h:275: list->next = list;
  (<0>,1322) fake_defs.h:275: list->next = list;
  (<0>,1324) fake_defs.h:275: list->next = list;
  (<0>,1325) fake_defs.h:276: list->prev = list;
  (<0>,1326) fake_defs.h:276: list->prev = list;
  (<0>,1328) fake_defs.h:276: list->prev = list;
  (<0>,1330) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,1333) tree_plugin.h:2694: }
  (<0>,1336) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1338) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1339) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1353) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1355) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1357) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1360) tree.c:3699: rsp->rda = rda;
  (<0>,1361) tree.c:3699: rsp->rda = rda;
  (<0>,1363) tree.c:3699: rsp->rda = rda;
  (<0>,1366) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1369) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1372) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1373) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1374) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1376) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1377) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1384)
  (<0>,1385)
  (<0>,1386) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1388) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1389) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1392) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1394) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1395) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1398) fake_defs.h:536: if (offset & mask)
  (<0>,1399) fake_defs.h:536: if (offset & mask)
  (<0>,1403) fake_defs.h:537: return cpu;
  (<0>,1404) fake_defs.h:537: return cpu;
  (<0>,1406) fake_defs.h:540: }
  (<0>,1408) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1409) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1413) tree.c:3703: while (i > rnp->grphi)
  (<0>,1414) tree.c:3703: while (i > rnp->grphi)
  (<0>,1416) tree.c:3703: while (i > rnp->grphi)
  (<0>,1419) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1420) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1422) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1428) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1429) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1442)
  (<0>,1443)
  (<0>,1444) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1446) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1448) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1450) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1451) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1454)
  (<0>,1455) tree.c:453: return &rsp->node[0];
  (<0>,1459) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1460) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1462) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1466)
  (<0>,1467)
  (<0>,1468) fake_sync.h:76: local_irq_save(flags);
  (<0>,1471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1473) fake_sched.h:43: return __running_cpu;
  (<0>,1477) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1479) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1483) fake_sched.h:43: return __running_cpu;
  (<0>,1487) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1493) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1494) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1498) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1499) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1501) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1507) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1509) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1510) tree.c:3412: init_callback_list(rdp);
  (<0>,1514)
  (<0>,1515) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1518) tree_plugin.h:2732: return false;
  (<0>,1521) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1523) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1529) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1531) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1533) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1538) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1540) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1542) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1545) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1547) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1549) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1554) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1556) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1558) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1561) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1563) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1565) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1570) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1572) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1574) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1577) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1579) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1581) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1586) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1588) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1590) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1594) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1596) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1597) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1599) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1600) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1603) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1606) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1608) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1610) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1616) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1618) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1624) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1626) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1627) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1629) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1638) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1640) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1647) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1649) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1650) tree.c:3418: rdp->cpu = cpu;
  (<0>,1651) tree.c:3418: rdp->cpu = cpu;
  (<0>,1653) tree.c:3418: rdp->cpu = cpu;
  (<0>,1654) tree.c:3419: rdp->rsp = rsp;
  (<0>,1655) tree.c:3419: rdp->rsp = rsp;
  (<0>,1657) tree.c:3419: rdp->rsp = rsp;
  (<0>,1658) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1661) tree_plugin.h:2711: }
  (<0>,1663) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1665) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1669)
  (<0>,1670)
  (<0>,1671) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1672) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1675) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1678) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1680) fake_sched.h:43: return __running_cpu;
  (<0>,1684) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1686) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1690) fake_sched.h:43: return __running_cpu;
  (<0>,1694) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1703) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1704) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1711)
  (<0>,1712)
  (<0>,1713) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1715) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1716) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1719) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1721) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1722) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1725) fake_defs.h:536: if (offset & mask)
  (<0>,1726) fake_defs.h:536: if (offset & mask)
  (<0>,1730) fake_defs.h:537: return cpu;
  (<0>,1731) fake_defs.h:537: return cpu;
  (<0>,1733) fake_defs.h:540: }
  (<0>,1735) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1736) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1740) tree.c:3703: while (i > rnp->grphi)
  (<0>,1741) tree.c:3703: while (i > rnp->grphi)
  (<0>,1743) tree.c:3703: while (i > rnp->grphi)
  (<0>,1746) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1747) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1749) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1755) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1756) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1769)
  (<0>,1770)
  (<0>,1771) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1773) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1775) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1777) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1778) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1781)
  (<0>,1782) tree.c:453: return &rsp->node[0];
  (<0>,1786) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1787) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1789) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1793)
  (<0>,1794)
  (<0>,1795) fake_sync.h:76: local_irq_save(flags);
  (<0>,1798) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1800) fake_sched.h:43: return __running_cpu;
  (<0>,1804) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1806) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1810) fake_sched.h:43: return __running_cpu;
  (<0>,1814) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1820) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1821) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1825) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1826) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1828) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1834) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1836) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1837) tree.c:3412: init_callback_list(rdp);
  (<0>,1841)
  (<0>,1842) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1845) tree_plugin.h:2732: return false;
  (<0>,1848) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1850) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1851) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1853) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1856) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1858) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1860) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1865) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1867) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1869) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1872) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1874) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1876) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1881) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1883) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1885) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1888) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1890) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1892) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1897) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1899) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1901) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1904) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1906) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1908) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1913) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1915) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1917) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1921) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1923) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1924) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1926) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1927) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1930) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1933) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1935) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1937) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1943) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1945) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1951) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1953) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1954) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1956) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1965) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1967) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1974) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1976) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1977) tree.c:3418: rdp->cpu = cpu;
  (<0>,1978) tree.c:3418: rdp->cpu = cpu;
  (<0>,1980) tree.c:3418: rdp->cpu = cpu;
  (<0>,1981) tree.c:3419: rdp->rsp = rsp;
  (<0>,1982) tree.c:3419: rdp->rsp = rsp;
  (<0>,1984) tree.c:3419: rdp->rsp = rsp;
  (<0>,1985) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1988) tree_plugin.h:2711: }
  (<0>,1990) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1992) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1996)
  (<0>,1997)
  (<0>,1998) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1999) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,2002) fake_sync.h:86: local_irq_restore(flags);
  (<0>,2005) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2007) fake_sched.h:43: return __running_cpu;
  (<0>,2011) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2013) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2017) fake_sched.h:43: return __running_cpu;
  (<0>,2021) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2030) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2031) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2038)
  (<0>,2039)
  (<0>,2040) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2042) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2043) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2046) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2048) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2049) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2052) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,2054) fake_defs.h:540: }
  (<0>,2056) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2057) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2060) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,2065)
  (<0>,2066)
  (<0>,2067) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2068) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2069) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2071) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2076)
  (<0>,2077)
  (<0>,2078)
  (<0>,2079) fake_defs.h:283: next->prev = new;
  (<0>,2080) fake_defs.h:283: next->prev = new;
  (<0>,2082) fake_defs.h:283: next->prev = new;
  (<0>,2083) fake_defs.h:284: new->next = next;
  (<0>,2084) fake_defs.h:284: new->next = next;
  (<0>,2086) fake_defs.h:284: new->next = next;
  (<0>,2087) fake_defs.h:285: new->prev = prev;
  (<0>,2088) fake_defs.h:285: new->prev = prev;
  (<0>,2090) fake_defs.h:285: new->prev = prev;
  (<0>,2091) fake_defs.h:286: prev->next = new;
  (<0>,2092) fake_defs.h:286: prev->next = new;
  (<0>,2094) fake_defs.h:286: prev->next = new;
  (<0>,2106) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2108) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2109) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2116)
  (<0>,2117)
  (<0>,2118) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2120) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2121) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2124) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2126) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2127) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2130) fake_defs.h:536: if (offset & mask)
  (<0>,2131) fake_defs.h:536: if (offset & mask)
  (<0>,2135) fake_defs.h:537: return cpu;
  (<0>,2136) fake_defs.h:537: return cpu;
  (<0>,2138) fake_defs.h:540: }
  (<0>,2140) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2141) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2144) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2163)
  (<0>,2164) tree.c:3493: static int rcu_cpu_notify(struct notifier_block *self,
  (<0>,2165) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2166) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2168) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2169) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2170) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2172) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2174) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2175) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2177) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2178) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2181) tree.c:3502: switch (action) {
  (<0>,2183) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2192)
  (<0>,2193) tree.c:3482: static void rcu_prepare_cpu(int cpu)
  (<0>,2194) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2195) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2199) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2200) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2201) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2203) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2207) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2208) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2216)
  (<0>,2217) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2218) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2220) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2222) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2224) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2225) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2228)
  (<0>,2229) tree.c:453: return &rsp->node[0];
  (<0>,2233) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2234) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2238)
  (<0>,2239) fake_sync.h:172: void mutex_lock(struct mutex *l)
  (<0>,2241) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2245) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2247) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2251)
  (<0>,2252)
  (<0>,2253) fake_sync.h:76: local_irq_save(flags);
  (<0>,2256) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2258) fake_sched.h:43: return __running_cpu;
  (<0>,2262) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2264) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2268) fake_sched.h:43: return __running_cpu;
  (<0>,2272) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2278) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2279) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2283) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2285) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2286) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2288) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2289) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2291) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2292) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2294) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2295) tree.c:3446: rdp->blimit = blimit;
  (<0>,2296) tree.c:3446: rdp->blimit = blimit;
  (<0>,2298) tree.c:3446: rdp->blimit = blimit;
  (<0>,2299) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2303)
  (<0>,2304) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2307) tree_plugin.h:2732: return false;
  (<0>,2310) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2312) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2313) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2315) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2318) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2320) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2322) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2325) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2327) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2329) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2331) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2334) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2336) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2338) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2341) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2343) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2345) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2347) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2350) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2352) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2354) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2357) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2359) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2361) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2363) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2366) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2368) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2370) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2373) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2375) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2377) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2379) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2383) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2385) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2387) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2388) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2390) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2393) tree_plugin.h:3164: }
  (<0>,2395) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2397) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2400) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2403) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2405) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2408) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2409) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2413)
  (<0>,2414) fake_sync.h:113: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2415) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2420) tree.c:3455: rnp = rdp->mynode;
  (<0>,2422) tree.c:3455: rnp = rdp->mynode;
  (<0>,2423) tree.c:3455: rnp = rdp->mynode;
  (<0>,2424) tree.c:3456: mask = rdp->grpmask;
  (<0>,2426) tree.c:3456: mask = rdp->grpmask;
  (<0>,2427) tree.c:3456: mask = rdp->grpmask;
  (<0>,2429) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2433) fake_sync.h:108: preempt_disable();
  (<0>,2435) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2436) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2440) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2441) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2443) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2445) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2446) tree.c:3461: mask = rnp->grpmask;
  (<0>,2448) tree.c:3461: mask = rnp->grpmask;
  (<0>,2449) tree.c:3461: mask = rnp->grpmask;
  (<0>,2450) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2451) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2453) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2456) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2458) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2459) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2461) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2462) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2464) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2465) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2467) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2468) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2470) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2471) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2473) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2477) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2481)
  (<0>,2482) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2483) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2488) tree.c:3475: rnp = rnp->parent;
  (<0>,2490) tree.c:3475: rnp = rnp->parent;
  (<0>,2491) tree.c:3475: rnp = rnp->parent;
  (<0>,2493) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2497) tree.c:3477: local_irq_restore(flags);
  (<0>,2500) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2502) fake_sched.h:43: return __running_cpu;
  (<0>,2506) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2508) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2512) fake_sched.h:43: return __running_cpu;
  (<0>,2516) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2521) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2525)
  (<0>,2526) fake_sync.h:178: void mutex_unlock(struct mutex *l)
  (<0>,2528) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2534) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2537) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2538) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2539) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2543) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2544) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2545) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2547) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2551) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2552) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2560)
  (<0>,2561)
  (<0>,2562) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2564) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2566) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2568) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2569) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2572)
  (<0>,2573) tree.c:453: return &rsp->node[0];
  (<0>,2577) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2578) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2582)
  (<0>,2583) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2585) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2589) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2591) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2595)
  (<0>,2596)
  (<0>,2597) fake_sync.h:76: local_irq_save(flags);
  (<0>,2600) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2602) fake_sched.h:43: return __running_cpu;
  (<0>,2606) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2608) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2612) fake_sched.h:43: return __running_cpu;
  (<0>,2616) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2622) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2623) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2627) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2629) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2630) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2632) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2633) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2635) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2636) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2638) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2639) tree.c:3446: rdp->blimit = blimit;
  (<0>,2640) tree.c:3446: rdp->blimit = blimit;
  (<0>,2642) tree.c:3446: rdp->blimit = blimit;
  (<0>,2643) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2647)
  (<0>,2648) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2651) tree_plugin.h:2732: return false;
  (<0>,2654) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2656) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2657) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2659) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2662) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2664) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2666) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2669) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2671) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2673) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2675) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2678) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2680) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2682) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2685) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2687) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2689) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2691) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2694) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2696) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2698) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2701) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2703) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2705) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2707) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2710) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2712) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2714) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2717) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2719) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2721) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2723) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2727) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2729) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2731) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2732) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2734) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2737) tree_plugin.h:3164: }
  (<0>,2739) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2741) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2744) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2747) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2749) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2752) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2753) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2757)
  (<0>,2758) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2759) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2764) tree.c:3455: rnp = rdp->mynode;
  (<0>,2766) tree.c:3455: rnp = rdp->mynode;
  (<0>,2767) tree.c:3455: rnp = rdp->mynode;
  (<0>,2768) tree.c:3456: mask = rdp->grpmask;
  (<0>,2770) tree.c:3456: mask = rdp->grpmask;
  (<0>,2771) tree.c:3456: mask = rdp->grpmask;
  (<0>,2773) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2777) fake_sync.h:108: preempt_disable();
  (<0>,2779) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2780) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2784) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2785) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2787) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2789) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2790) tree.c:3461: mask = rnp->grpmask;
  (<0>,2792) tree.c:3461: mask = rnp->grpmask;
  (<0>,2793) tree.c:3461: mask = rnp->grpmask;
  (<0>,2794) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2795) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2797) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2800) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2802) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2803) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2805) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2806) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2808) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2809) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2811) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2812) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2814) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2815) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2817) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2821) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2825)
  (<0>,2826) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2827) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2832) tree.c:3475: rnp = rnp->parent;
  (<0>,2834) tree.c:3475: rnp = rnp->parent;
  (<0>,2835) tree.c:3475: rnp = rnp->parent;
  (<0>,2837) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2841) tree.c:3477: local_irq_restore(flags);
  (<0>,2844) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2846) fake_sched.h:43: return __running_cpu;
  (<0>,2850) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2852) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2856) fake_sched.h:43: return __running_cpu;
  (<0>,2860) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2865) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2869)
  (<0>,2870) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2872) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2878) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2881) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2882) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2883) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2887) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2888) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2889) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2891) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2896) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,2900) tree_plugin.h:1499: }
  (<0>,2902) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2906) tree_plugin.h:2724: }
  (<0>,2913) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2914) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2921)
  (<0>,2922)
  (<0>,2923) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2925) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2926) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2929) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2931) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2932) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2935) fake_defs.h:536: if (offset & mask)
  (<0>,2936) fake_defs.h:536: if (offset & mask)
  (<0>,2940) fake_defs.h:537: return cpu;
  (<0>,2941) fake_defs.h:537: return cpu;
  (<0>,2943) fake_defs.h:540: }
  (<0>,2945) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2946) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2949) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2968)
  (<0>,2969)
  (<0>,2970)
  (<0>,2971) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2973) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2974) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2975) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2977) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2979) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2980) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2982) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2983) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2986) tree.c:3502: switch (action) {
  (<0>,2988) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2997)
  (<0>,2998) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2999) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3000) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3004) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3005) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3006) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3008) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3012) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3013) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3021)
  (<0>,3022)
  (<0>,3023) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3025) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3027) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3029) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3030) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3033)
  (<0>,3034) tree.c:453: return &rsp->node[0];
  (<0>,3038) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3039) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3043)
  (<0>,3044) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3046) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3050) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3052) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3056)
  (<0>,3057)
  (<0>,3058) fake_sync.h:76: local_irq_save(flags);
  (<0>,3061) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3063) fake_sched.h:43: return __running_cpu;
  (<0>,3067) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3069) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3073) fake_sched.h:43: return __running_cpu;
  (<0>,3077) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3083) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3084) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3088) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3090) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3091) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3093) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3094) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3096) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3097) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3099) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3100) tree.c:3446: rdp->blimit = blimit;
  (<0>,3101) tree.c:3446: rdp->blimit = blimit;
  (<0>,3103) tree.c:3446: rdp->blimit = blimit;
  (<0>,3104) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3108)
  (<0>,3109) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3112) tree_plugin.h:2732: return false;
  (<0>,3115) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3117) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3118) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3120) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3123) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3125) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3127) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3130) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3132) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3134) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3136) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3139) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3141) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3143) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3146) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3148) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3150) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3152) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3155) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3157) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3159) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3162) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3164) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3166) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3168) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3171) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3173) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3175) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3178) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3180) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3182) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3184) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3188) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3190) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3192) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3193) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3195) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3198) tree_plugin.h:3164: }
  (<0>,3200) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3202) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3205) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3208) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3210) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3213) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3214) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3218)
  (<0>,3219) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3220) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3225) tree.c:3455: rnp = rdp->mynode;
  (<0>,3227) tree.c:3455: rnp = rdp->mynode;
  (<0>,3228) tree.c:3455: rnp = rdp->mynode;
  (<0>,3229) tree.c:3456: mask = rdp->grpmask;
  (<0>,3231) tree.c:3456: mask = rdp->grpmask;
  (<0>,3232) tree.c:3456: mask = rdp->grpmask;
  (<0>,3234) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3238) fake_sync.h:108: preempt_disable();
  (<0>,3240) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3241) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3245) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3246) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3248) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3250) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3251) tree.c:3461: mask = rnp->grpmask;
  (<0>,3253) tree.c:3461: mask = rnp->grpmask;
  (<0>,3254) tree.c:3461: mask = rnp->grpmask;
  (<0>,3255) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3256) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3258) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3261) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3263) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3264) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3266) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3267) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3269) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3270) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3272) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3273) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3275) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3276) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3278) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3282) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3286)
  (<0>,3287) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3288) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3293) tree.c:3475: rnp = rnp->parent;
  (<0>,3295) tree.c:3475: rnp = rnp->parent;
  (<0>,3296) tree.c:3475: rnp = rnp->parent;
  (<0>,3298) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3302) tree.c:3477: local_irq_restore(flags);
  (<0>,3305) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3307) fake_sched.h:43: return __running_cpu;
  (<0>,3311) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3313) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3317) fake_sched.h:43: return __running_cpu;
  (<0>,3321) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3326) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3330)
  (<0>,3331) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3333) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3339) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3342) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3343) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3344) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3348) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3349) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3350) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3352) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3356) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3357) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3365)
  (<0>,3366)
  (<0>,3367) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3369) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3371) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3373) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3374) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3377)
  (<0>,3378) tree.c:453: return &rsp->node[0];
  (<0>,3382) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3383) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3387)
  (<0>,3388) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3390) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3394) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3396) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3400)
  (<0>,3401)
  (<0>,3402) fake_sync.h:76: local_irq_save(flags);
  (<0>,3405) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3407) fake_sched.h:43: return __running_cpu;
  (<0>,3411) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3413) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3417) fake_sched.h:43: return __running_cpu;
  (<0>,3421) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3427) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3428) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3432) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3434) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3435) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3437) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3438) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3440) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3441) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3443) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3444) tree.c:3446: rdp->blimit = blimit;
  (<0>,3445) tree.c:3446: rdp->blimit = blimit;
  (<0>,3447) tree.c:3446: rdp->blimit = blimit;
  (<0>,3448) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3452)
  (<0>,3453) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3456) tree_plugin.h:2732: return false;
  (<0>,3459) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3461) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3462) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3464) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3467) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3469) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3471) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3474) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3476) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3478) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3480) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3483) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3485) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3487) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3490) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3492) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3494) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3496) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3499) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3501) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3503) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3506) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3508) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3510) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3512) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3515) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3517) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3519) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3522) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3528) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3532) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3534) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3536) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3537) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3539) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3542) tree_plugin.h:3164: }
  (<0>,3544) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3546) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3549) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3552) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3554) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3557) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3558) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3562)
  (<0>,3563) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3564) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3569) tree.c:3455: rnp = rdp->mynode;
  (<0>,3571) tree.c:3455: rnp = rdp->mynode;
  (<0>,3572) tree.c:3455: rnp = rdp->mynode;
  (<0>,3573) tree.c:3456: mask = rdp->grpmask;
  (<0>,3575) tree.c:3456: mask = rdp->grpmask;
  (<0>,3576) tree.c:3456: mask = rdp->grpmask;
  (<0>,3578) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3582) fake_sync.h:108: preempt_disable();
  (<0>,3584) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3585) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3589) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3590) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3592) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3594) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3595) tree.c:3461: mask = rnp->grpmask;
  (<0>,3597) tree.c:3461: mask = rnp->grpmask;
  (<0>,3598) tree.c:3461: mask = rnp->grpmask;
  (<0>,3599) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3600) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3602) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3605) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3607) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3608) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3610) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3611) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3613) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3614) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3616) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3617) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3619) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3620) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3622) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3626) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3630)
  (<0>,3631) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3632) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3637) tree.c:3475: rnp = rnp->parent;
  (<0>,3639) tree.c:3475: rnp = rnp->parent;
  (<0>,3640) tree.c:3475: rnp = rnp->parent;
  (<0>,3642) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3646) tree.c:3477: local_irq_restore(flags);
  (<0>,3649) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3651) fake_sched.h:43: return __running_cpu;
  (<0>,3655) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3657) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3661) fake_sched.h:43: return __running_cpu;
  (<0>,3665) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3670) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3674)
  (<0>,3675) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3677) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3683) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3686) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3687) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3688) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3692) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3693) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3694) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3696) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3701) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,3705) tree_plugin.h:1499: }
  (<0>,3707) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,3711) tree_plugin.h:2724: }
  (<0>,3718) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3719) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3726)
  (<0>,3727)
  (<0>,3728) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3730) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3731) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3734) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3736) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3737) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3740) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,3742) fake_defs.h:540: }
  (<0>,3744) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3745) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3751) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3753) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3756) litmus.c:123: set_cpu(i);
  (<0>,3759)
  (<0>,3760) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,3761) fake_sched.h:56: __running_cpu = cpu;
  (<0>,3765) tree.c:578: unsigned long flags;
  (<0>,3768) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3770) fake_sched.h:43: return __running_cpu;
  (<0>,3774) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3776) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3780) fake_sched.h:43: return __running_cpu;
  (<0>,3784) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3797) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3799) fake_sched.h:43: return __running_cpu;
  (<0>,3803) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3804) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3806) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3807) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3808) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3814) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3815) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3820) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3821) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3822) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3823) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,3827) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3829) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3830) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3831) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3850)
  (<0>,3852) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3854) fake_sched.h:43: return __running_cpu;
  (<0>,3858) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3861) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,3865) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3866) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3867) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3871) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3872) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3873) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3875) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3880) fake_sched.h:43: return __running_cpu;
  (<0>,3883) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3885) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3887) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3888) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3891) tree_plugin.h:2720: }
  (<0>,3894) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3897) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3898) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3899) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3903) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3904) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3905) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3907) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3912) fake_sched.h:43: return __running_cpu;
  (<0>,3915) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3917) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3919) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3920) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3923) tree_plugin.h:2720: }
  (<0>,3926) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3929) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3930) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3931) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3935) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3936) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3937) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3939) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3946) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3949) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3950) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3951) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3953) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3954) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3956) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3959) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3965) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3966) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3969) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3974) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3975) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3976) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3990) tree_plugin.h:3141: }
  (<0>,3992) tree.c:583: local_irq_restore(flags);
  (<0>,3995) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3997) fake_sched.h:43: return __running_cpu;
  (<0>,4001) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4003) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4007) fake_sched.h:43: return __running_cpu;
  (<0>,4011) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4018) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4020) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4022) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4025) litmus.c:123: set_cpu(i);
  (<0>,4028)
  (<0>,4029) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4030) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4034) tree.c:580: local_irq_save(flags);
  (<0>,4037) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4039) fake_sched.h:43: return __running_cpu;
  (<0>,4043) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4045) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4049) fake_sched.h:43: return __running_cpu;
  (<0>,4053) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4066) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4068) fake_sched.h:43: return __running_cpu;
  (<0>,4072) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4073) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4075) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4076) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4077) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4083) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4084) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4089) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4090) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4091) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4092) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4096) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4098) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4099) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4100) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4119)
  (<0>,4121) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4123) fake_sched.h:43: return __running_cpu;
  (<0>,4127) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4130) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,4134) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4135) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4136) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4140) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4141) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4142) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4144) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4149) fake_sched.h:43: return __running_cpu;
  (<0>,4152) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4154) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4156) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4157) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4160) tree_plugin.h:2720: }
  (<0>,4163) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4166) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4167) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4168) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4172) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4173) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4181) fake_sched.h:43: return __running_cpu;
  (<0>,4184) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4186) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4188) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4189) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4192) tree_plugin.h:2720: }
  (<0>,4195) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4198) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4199) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4200) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4204) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4205) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4206) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4208) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4215) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4218) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4219) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4220) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4222) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4223) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4225) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4228) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4234) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4235) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4238) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4243) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4244) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4245) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4259) tree_plugin.h:3141: }
  (<0>,4261) tree.c:583: local_irq_restore(flags);
  (<0>,4264) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4266) fake_sched.h:43: return __running_cpu;
  (<0>,4270) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4272) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4276) fake_sched.h:43: return __running_cpu;
  (<0>,4280) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4287) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4289) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4291) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4303) tree.c:3561: unsigned long flags;
  (<0>,4304) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4305) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4306) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4310) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4314) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4320) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4322) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4331)
  (<0>,4332) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4333) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4334) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4335) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4336) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4339)
  (<0>,4340)
  (<0>,4347)
  (<0>,4348)
  (<0>,4355)
  (<0>,4356)
  (<0>,4363)
  (<0>,4364)
  (<0>,4371)
  (<0>,4372)
  (<0>,4379)
  (<0>,4380)
  (<0>,4387)
  (<0>,4388)
  (<0>,4395)
  (<0>,4396)
  (<0>,4403)
  (<0>,4404)
  (<0>,4411)
  (<0>,4412) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4421) fake_defs.h:730: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,4427) fake_defs.h:732: task = malloc(sizeof(*task));
  (<0>,4428) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4430) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4432) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4433) fake_defs.h:734: task->tid = t;
  (<0>,4434) fake_defs.h:734: task->tid = t;
  (<0>,4436) fake_defs.h:734: task->tid = t;
  (<0>,4437) fake_defs.h:735: return task;
  (<0>,4438) fake_defs.h:735: return task;
  (<0>,4440) fake_defs.h:738: }
  (<0>,4442) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4443) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4446)
  (<0>,4447) tree.c:453: return &rsp->node[0];
  (<0>,4451) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4452) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4454) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4458)
  (<0>,4459)
  (<0>,4460) fake_sync.h:76: local_irq_save(flags);
  (<0>,4463) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4465) fake_sched.h:43: return __running_cpu;
  (<0>,4469) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4475) fake_sched.h:43: return __running_cpu;
  (<0>,4479) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4485) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4486) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4490) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4491) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4493) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4494) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4496) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4500)
  (<0>,4501)
  (<0>,4502) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4503) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4506) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4509) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4511) fake_sched.h:43: return __running_cpu;
  (<0>,4515) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4517) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4521) fake_sched.h:43: return __running_cpu;
  (<0>,4525) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4533) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4537) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4538) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4542) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4543) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4544) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4546) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4550) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4552) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4554) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4563)
  (<0>,4564)
  (<0>,4565)
  (<0>,4566)
  (<0>,4567) fake_defs.h:727: struct task_struct *task = NULL;
  (<0>,4568) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4571)
  (<0>,4572)
  (<0>,4579)
  (<0>,4580)
  (<0>,4587)
  (<0>,4588)
  (<0>,4595)
  (<0>,4596)
  (<0>,4603)
  (<0>,4604) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4617) fake_defs.h:737: return NULL;
  (<0>,4619) fake_defs.h:738: }
  (<0>,4621) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4622) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4625)
  (<0>,4626) tree.c:453: return &rsp->node[0];
  (<0>,4630) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4631) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4633) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4637)
  (<0>,4638)
  (<0>,4639) fake_sync.h:76: local_irq_save(flags);
  (<0>,4642) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4644) fake_sched.h:43: return __running_cpu;
  (<0>,4648) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4650) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4654) fake_sched.h:43: return __running_cpu;
  (<0>,4658) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4664) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4665) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4669) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4670) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4672) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4673) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4675) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4679)
  (<0>,4680)
  (<0>,4681) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4682) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4685) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4688) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4690) fake_sched.h:43: return __running_cpu;
  (<0>,4694) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4696) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4700) fake_sched.h:43: return __running_cpu;
  (<0>,4704) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4712) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4715) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4716) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4717) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4721) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4722) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4723) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4725) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4739)
  (<0>,4740) litmus.c:51: void *thread_reader(void *arg)
  (<0>,4743)
  (<0>,4744) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4745) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4748) fake_sched.h:43: return __running_cpu;
  (<0>,4752)
  (<0>,4753) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,4756) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,4761) tree.c:702: unsigned long flags;
  (<0>,4764) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4766) fake_sched.h:43: return __running_cpu;
  (<0>,4770) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4772) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4780) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4793) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4795) fake_sched.h:43: return __running_cpu;
  (<0>,4799) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4800) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4802) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4803) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4804) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4809) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4810) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4814) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4815) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4816) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4817) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,4821) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4823) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4824) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4825) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4839)
  (<0>,4840) tree.c:644: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,4842) fake_sched.h:43: return __running_cpu;
  (<0>,4846) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4850) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4853) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4854) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4855) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4857) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4858) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4860) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4870) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4871) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4874) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4879) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4880) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4881) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4886) tree.c:656: if (!user && !is_idle_task(current)) {
  (<0>,4895) tree_plugin.h:3145: }
  (<0>,4897) tree.c:707: local_irq_restore(flags);
  (<0>,4900) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4902) fake_sched.h:43: return __running_cpu;
  (<0>,4906) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4908) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4912) fake_sched.h:43: return __running_cpu;
  (<0>,4916) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4932) litmus.c:57: r_x = x;
  (<0>,4933) litmus.c:57: r_x = x;
  (<0>,4937) fake_sched.h:43: return __running_cpu;
  (<0>,4941) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4949) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4954) fake_sched.h:43: return __running_cpu;
  (<0>,4958) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
  (<0>,4968) tree.c:745: unsigned long flags;
  (<0>,4971) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4973) fake_sched.h:43: return __running_cpu;
  (<0>,4977) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4984) fake_sched.h:43: return __running_cpu;
  (<0>,4988) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4989) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4991) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4992) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4993) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4995) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4997) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4998) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5000) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5005) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5006) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5008) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5012) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5013) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5014) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5015) tree.c:754: if (oldval)
  (<0>,5023) tree_plugin.h:3145: }
  (<0>,5025) tree.c:759: local_irq_restore(flags);
  (<0>,5028) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5030) fake_sched.h:43: return __running_cpu;
  (<0>,5034) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5036) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5044) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,5049) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,5054) fake_sched.h:43: return __running_cpu;
  (<0>,5059) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,5067) fake_sched.h:43: return __running_cpu;
  (<0>,5072) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
  (<0>,5078) fake_sched.h:43: return __running_cpu;
  (<0>,5083) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
  (<0>,5096) tree.c:3185: struct rcu_state *rsp;
  (<0>,5097) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5098) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5102) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5103) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5104) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5106) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5110) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5112) fake_sched.h:43: return __running_cpu;
  (<0>,5115) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5117) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5124)
  (<0>,5125) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5126) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5128) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5129) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5130) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5132) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5134) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5135) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5136) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5146)
  (<0>,5147) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5148) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5153) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
  (<0>,5156) tree_plugin.h:3185: return 0;
  (<0>,5159) tree.c:3135: if (rcu_scheduler_fully_active &&
  (<0>,5162) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5164) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5167) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5169) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5173) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,5176)
  (<0>,5177) tree.c:442: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,5179) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5182) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5189) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5190) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5196)
  (<0>,5197) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5198) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5201)
  (<0>,5202) tree.c:176: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,5204) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5205) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5207) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5213) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5219)
  (<0>,5220) tree.c:461: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,5223)
  (<0>,5224) tree.c:453: return &rsp->node[0];
  (<0>,5228) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5229) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5231) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5235) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5236) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5238) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5241) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5242) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5243) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5247) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5250) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5253) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5256) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5257) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5260) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5262) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5265) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5268) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5271) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5272) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5274) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5277) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5281) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5283) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5285) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5288) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5291) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5294) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5295) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5297) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5300) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5304) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5306) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5308) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5311) tree.c:493: return 0; /* No grace period needed. */
  (<0>,5313) tree.c:494: }
  (<0>,5317) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5319) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5320) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5322) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5325) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,1)
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:704: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,56) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,58) fake_sched.h:43: return __running_cpu;
      (<0.1>,62) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,63) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,67) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,72) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,73) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,77) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,78) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,79) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,80) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,84) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,86) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,87) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,88) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,102)
      (<0.1>,103) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,105) fake_sched.h:43: return __running_cpu;
      (<0.1>,109) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,113) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,116) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,117) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,118) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,120) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,123) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,126) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,133) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,137) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,143) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,144) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,149) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,158) tree_plugin.h:3145: }
      (<0.1>,160) tree.c:707: local_irq_restore(flags);
      (<0.1>,163) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,165) fake_sched.h:43: return __running_cpu;
      (<0.1>,169) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,171) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,175) fake_sched.h:43: return __running_cpu;
      (<0.1>,179) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,186) litmus.c:87: x = 1;
      (<0.1>,196) tree.c:2787: ret = num_online_cpus() <= 1;
      (<0.1>,198) tree.c:2789: return ret;
      (<0.1>,202) tree.c:2841: if (rcu_expedited)
      (<0.1>,208) update.c:223: init_rcu_head_on_stack(&rcu.head);
      (<0.1>,212) rcupdate.h:401: }
      (<0.1>,217)
      (<0.1>,218) fake_sync.h:232: x->done = 0;
      (<0.1>,220) fake_sync.h:232: x->done = 0;
      (<0.1>,224) update.c:226: crf(&rcu.head, wakeme_after_rcu);
      (<0.1>,229)
      (<0.1>,230)
      (<0.1>,231) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,232) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,249)
      (<0.1>,250)
      (<0.1>,251)
      (<0.1>,252)
      (<0.1>,254)
      (<0.1>,255) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,262) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,263) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,269) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,270) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,271) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,272) tree.c:2690: if (debug_rcu_head_queue(head)) {
      (<0.1>,275) rcu.h:92: return 0;
      (<0.1>,279) tree.c:2696: head->func = func;
      (<0.1>,280) tree.c:2696: head->func = func;
      (<0.1>,282) tree.c:2696: head->func = func;
      (<0.1>,283) tree.c:2697: head->next = NULL;
      (<0.1>,285) tree.c:2697: head->next = NULL;
      (<0.1>,286) tree.c:2705: local_irq_save(flags);
      (<0.1>,289) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,291) fake_sched.h:43: return __running_cpu;
      (<0.1>,295) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,297) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,301) fake_sched.h:43: return __running_cpu;
      (<0.1>,305) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,311) fake_sched.h:43: return __running_cpu;
      (<0.1>,314) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,316) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,318) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,319) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,322) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,330) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,333) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,335) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,337) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,339) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,340) tree.c:2721: if (lazy)
      (<0.1>,347) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,348) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,351) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,352) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,353) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,355) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,358) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,359) tree.c:2729: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,366) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,367) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,368) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,369) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,377)
      (<0.1>,378)
      (<0.1>,379)
      (<0.1>,380) tree.c:2628: if (!rcu_is_watching() && cpu_online(smp_processor_id()))
      (<0.1>,386) fake_sched.h:43: return __running_cpu;
      (<0.1>,392) tree.c:815: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,397) tree.c:829: ret = __rcu_is_watching();
      (<0.1>,399) tree.c:831: return ret;
      (<0.1>,403) tree.c:2632: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,406) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,408) fake_sched.h:43: return __running_cpu;
      (<0.1>,412) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,422) tree.c:2737: local_irq_restore(flags);
      (<0.1>,425) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,427) fake_sched.h:43: return __running_cpu;
      (<0.1>,431) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,433) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,437) fake_sched.h:43: return __running_cpu;
      (<0.1>,441) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,452) fake_sync.h:238: might_sleep();
      (<0.1>,456) fake_sched.h:43: return __running_cpu;
      (<0.1>,460) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,463) tree.c:580: local_irq_save(flags);
      (<0.1>,466) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,468) fake_sched.h:43: return __running_cpu;
      (<0.1>,472) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,474) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,478) fake_sched.h:43: return __running_cpu;
      (<0.1>,482) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,495) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,497) fake_sched.h:43: return __running_cpu;
      (<0.1>,501) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,502) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,504) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,505) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,506) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,512) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,513) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,518) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,519) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,520) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,521) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,525) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,527) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,528) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,529) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,548)
      (<0.1>,550) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,552) fake_sched.h:43: return __running_cpu;
      (<0.1>,556) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,559) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,563) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,564) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,565) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,569) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,570) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,571) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,573) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,578) fake_sched.h:43: return __running_cpu;
      (<0.1>,581) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,583) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,585) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,586) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,589) tree_plugin.h:2720: }
      (<0.1>,592) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,595) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,596) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,597) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,601) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,602) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,603) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,605) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,610) fake_sched.h:43: return __running_cpu;
      (<0.1>,613) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,615) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,617) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,618) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,621) tree_plugin.h:2720: }
      (<0.1>,624) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,627) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,628) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,629) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,633) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,635) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,637) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,644) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,647) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,648) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,649) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,651) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,652) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,654) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,657) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,663) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,664) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,667) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,672) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,673) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,674) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,688) tree_plugin.h:3141: }
      (<0.1>,690) tree.c:583: local_irq_restore(flags);
      (<0.1>,693) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,695) fake_sched.h:43: return __running_cpu;
      (<0.1>,699) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,701) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,705) fake_sched.h:43: return __running_cpu;
      (<0.1>,709) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,715) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,718) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,723) fake_sync.h:241: while (!x->done)
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:704: local_irq_save(flags);
    (<0.0>,35) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,64) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,66) fake_sched.h:43: return __running_cpu;
    (<0.0>,70) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,71) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,75) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,80) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,81) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,85) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,86) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,87) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,88) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,92) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,94) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,95) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,96) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,110)
    (<0.0>,111) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,113) fake_sched.h:43: return __running_cpu;
    (<0.0>,117) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,124) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,125) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,126) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,128) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,129) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,131) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,141) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,145) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,150) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,151) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,152) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,157) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,166) tree_plugin.h:3145: }
    (<0.0>,168) tree.c:707: local_irq_restore(flags);
    (<0.0>,171) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,173) fake_sched.h:43: return __running_cpu;
    (<0.0>,177) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,179) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,183) fake_sched.h:43: return __running_cpu;
    (<0.0>,187) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,194) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,206)
    (<0.0>,207) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,209) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,213)
    (<0.0>,214) tree.c:453: return &rsp->node[0];
    (<0.0>,218) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,223) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,225) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) fake_sched.h:43: return __running_cpu;
    (<0.0>,233) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,237) fake_sched.h:43: return __running_cpu;
    (<0.0>,241) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,246) fake_sched.h:43: return __running_cpu;
    (<0.0>,250) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,260) tree.c:749: local_irq_save(flags);
    (<0.0>,263) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,271) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,276) fake_sched.h:43: return __running_cpu;
    (<0.0>,280) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,281) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,283) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,284) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,285) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,287) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,289) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,290) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,292) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,297) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,298) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,300) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,304) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,305) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,306) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,307) tree.c:754: if (oldval)
    (<0.0>,315) tree_plugin.h:3145: }
    (<0.0>,317) tree.c:759: local_irq_restore(flags);
    (<0.0>,320) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,322) fake_sched.h:43: return __running_cpu;
    (<0.0>,326) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,328) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,336) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,341) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,346) fake_sched.h:43: return __running_cpu;
    (<0.0>,351) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,359) fake_sched.h:43: return __running_cpu;
    (<0.0>,364) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,370) fake_sched.h:43: return __running_cpu;
    (<0.0>,375) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,388) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,389) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,390) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,394) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,395) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,396) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,398) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,402) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,404) fake_sched.h:43: return __running_cpu;
    (<0.0>,407) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,409) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,416)
    (<0.0>,417)
    (<0.0>,418) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,420) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,421) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,422) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,424) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,426) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,427) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,428) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,438)
    (<0.0>,439)
    (<0.0>,440) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,445) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,448) tree_plugin.h:3185: return 0;
    (<0.0>,451) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,454) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,456) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,459) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,461) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,465) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,468)
    (<0.0>,469) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,471) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,474) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,481) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,482) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,488)
    (<0.0>,489)
    (<0.0>,490) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,493)
    (<0.0>,494) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,496) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,497) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,499) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,505) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,511)
    (<0.0>,512) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,515)
    (<0.0>,516) tree.c:453: return &rsp->node[0];
    (<0.0>,520) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,521) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,523) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,527) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,528) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,530) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,533) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,534) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,535) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,539) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,542) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,545) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,548) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,549) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,552) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,554) tree.c:494: }
    (<0.0>,558) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,560) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,562) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,563) tree.c:3152: return 1;
    (<0.0>,565) tree.c:3176: }
    (<0.0>,569) tree.c:3189: return 1;
    (<0.0>,571) tree.c:3191: }
    (<0.0>,578) fake_sched.h:43: return __running_cpu;
    (<0.0>,582) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,586) tree.c:2437: if (user)
    (<0.0>,594) fake_sched.h:43: return __running_cpu;
    (<0.0>,598) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,600) fake_sched.h:43: return __running_cpu;
    (<0.0>,604) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,610) fake_sched.h:43: return __running_cpu;
    (<0.0>,614) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,624) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,627) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,628) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,629) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,633) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,634) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,635) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,637) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,641) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,650) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,652) fake_sched.h:43: return __running_cpu;
    (<0.0>,655) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,657) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,659) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,660) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,662) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,669) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,670) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,672) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,678) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,679) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,680) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,681) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,682) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,686)
    (<0.0>,687)
    (<0.0>,688) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,689) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,696)
    (<0.0>,697)
    (<0.0>,698) tree.c:1584: local_irq_save(flags);
    (<0.0>,701) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,703) fake_sched.h:43: return __running_cpu;
    (<0.0>,707) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,709) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,713) fake_sched.h:43: return __running_cpu;
    (<0.0>,717) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,722) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,724) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,725) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,726) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,728) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,729) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,731) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,734) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,736) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,737) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,739) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,742) tree.c:1589: local_irq_restore(flags);
    (<0.0>,745) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,747) fake_sched.h:43: return __running_cpu;
    (<0.0>,751) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,753) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,757) fake_sched.h:43: return __running_cpu;
    (<0.0>,761) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,768) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,770) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,775) tree.c:2558: local_irq_save(flags);
    (<0.0>,778) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,780) fake_sched.h:43: return __running_cpu;
    (<0.0>,784) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,786) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,790) fake_sched.h:43: return __running_cpu;
    (<0.0>,794) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,799) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,800) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,806)
    (<0.0>,807)
    (<0.0>,808) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,811)
    (<0.0>,812) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,814) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,815) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,817) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,823) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,829)
    (<0.0>,830) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,833)
    (<0.0>,834) tree.c:453: return &rsp->node[0];
    (<0.0>,838) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,839) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,841) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,845) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,846) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,848) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,851) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,852) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,853) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,857) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,860) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,863) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,866) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,867) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,870) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,872) tree.c:494: }
    (<0.0>,876) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,879)
    (<0.0>,880) tree.c:453: return &rsp->node[0];
    (<0.0>,887) fake_sync.h:108: preempt_disable();
    (<0.0>,889) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,890) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,894) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,900) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,902) fake_sched.h:43: return __running_cpu;
    (<0.0>,905) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,907) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,909) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,910) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,913)
    (<0.0>,914) tree.c:453: return &rsp->node[0];
    (<0.0>,918) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,919) tree.c:1925: bool ret = false;
    (<0.0>,920) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,921) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,922) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,930)
    (<0.0>,931)
    (<0.0>,932)
    (<0.0>,933) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,936) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,939) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,942) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,943) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,946) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,948) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,951) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,953) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,954) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,956) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,959) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,964) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,966) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,967) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,970) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,972) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,975) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,977) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,980) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,981) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,984) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,987) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,989) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,992) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,993) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,995) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,998) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,999) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1001) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1004) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1005) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1007) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1010) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1012) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1014) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1015) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1017) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1019) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1022) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1024) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1027) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1028) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1031) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1034) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1036) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1039) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1040) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1042) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1045) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1046) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1048) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1051) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1052) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1054) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1057) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1059) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1061) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1062) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1064) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1066) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1069) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1070) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1071) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1080)
    (<0.0>,1081)
    (<0.0>,1082)
    (<0.0>,1083) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1086) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1089) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1092) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1093) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1096) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1097) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1102)
    (<0.0>,1103)
    (<0.0>,1104) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1107)
    (<0.0>,1108) tree.c:453: return &rsp->node[0];
    (<0.0>,1112) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1115) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1117) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1118) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1120) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1123) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1125) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1127) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1129) tree.c:1261: }
    (<0.0>,1131) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1132) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1134) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1137) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1139) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1142) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1143) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1146) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1149) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1153) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1155) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1157) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1160) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1162) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1165) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1166) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1169) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1172) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1176) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1178) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1180) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1183) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1185) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1189) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1192) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1195) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1196) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1198) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1201) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1202) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1203) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1205) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1208) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1210) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1212) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1214) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1217) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1220) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1221) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1223) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1226) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1227) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1228) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1230) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1233) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1235) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1237) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1239) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1242) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1245) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1246) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1248) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1251) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1252) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1253) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1255) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1258) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1260) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1262) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1264) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1267) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1268) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1277)
    (<0.0>,1278)
    (<0.0>,1279)
    (<0.0>,1280) tree.c:1289: bool ret = false;
    (<0.0>,1281) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1283) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1286)
    (<0.0>,1287) tree.c:453: return &rsp->node[0];
    (<0.0>,1291) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1292) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1294) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1295) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1300)
    (<0.0>,1301)
    (<0.0>,1302) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1305)
    (<0.0>,1306) tree.c:453: return &rsp->node[0];
    (<0.0>,1310) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1313) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1315) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1316) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1318) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1321) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1323) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1325) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1327) tree.c:1261: }
    (<0.0>,1329) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1330) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1331) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1332) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1338)
    (<0.0>,1339)
    (<0.0>,1340)
    (<0.0>,1341) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1345) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1347) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1350) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1353) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1355) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1356) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1358) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1361) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1363) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1364) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1366) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1369) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1370) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1373) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1375) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1376) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1381)
    (<0.0>,1382)
    (<0.0>,1383) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1386)
    (<0.0>,1387) tree.c:453: return &rsp->node[0];
    (<0.0>,1391) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1394) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1396) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1397) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1399) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1402) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1404) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1406) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1408) tree.c:1261: }
    (<0.0>,1410) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1411) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1413) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1416) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1417) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1419) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1422) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1426) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1427) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1429) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1432) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1435) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1437) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1439) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1442) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1443) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1445) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1448) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1453) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1455) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1457) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1460) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1461) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1463) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1466) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1471) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1473) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1475) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1478) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1480) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1483) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1486) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1488) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1491) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1493) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1494) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1496) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1497) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1499) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1502) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1503) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1504) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1510)
    (<0.0>,1511)
    (<0.0>,1512)
    (<0.0>,1513) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1517) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1519) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1520) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1521) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1527)
    (<0.0>,1528)
    (<0.0>,1529)
    (<0.0>,1530) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1532) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1535) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1536) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1542)
    (<0.0>,1543)
    (<0.0>,1544) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1547)
    (<0.0>,1548) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1550) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1551) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1553) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1559) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1565)
    (<0.0>,1566) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1569)
    (<0.0>,1570) tree.c:453: return &rsp->node[0];
    (<0.0>,1574) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1575) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1577) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1581) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1582) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1584) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1587) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1588) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1589) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1593) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1595) tree.c:494: }
    (<0.0>,1599) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1601) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1604) tree.c:1909: return true;
    (<0.0>,1606) tree.c:1910: }
    (<0.0>,1609) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1612) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1613) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1617) tree.c:1372: if (c_out != NULL)
    (<0.0>,1620) tree.c:1374: return ret;
    (<0.0>,1624) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1625) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1628) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1629) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1635) tree.c:1482: return ret;
    (<0.0>,1637) tree.c:1482: return ret;
    (<0.0>,1639) tree.c:1483: }
    (<0.0>,1641) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1643) tree.c:1527: }
    (<0.0>,1647) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1648) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1649) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1650) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1656)
    (<0.0>,1657)
    (<0.0>,1658)
    (<0.0>,1659) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1661) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1664) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1665) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1671)
    (<0.0>,1672)
    (<0.0>,1673) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1676)
    (<0.0>,1677) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1679) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1680) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1682) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1688) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1694)
    (<0.0>,1695) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1698)
    (<0.0>,1699) tree.c:453: return &rsp->node[0];
    (<0.0>,1703) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1704) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1706) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1710) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1711) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1713) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1716) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1717) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1718) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1722) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1724) tree.c:494: }
    (<0.0>,1728) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1730) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1733) tree.c:1909: return true;
    (<0.0>,1735) tree.c:1910: }
    (<0.0>,1739) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1740) tree.c:1937: return ret;
    (<0.0>,1744) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,1745) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1748)
    (<0.0>,1749) tree.c:453: return &rsp->node[0];
    (<0.0>,1754) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1758)
    (<0.0>,1759)
    (<0.0>,1760) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1761) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1764) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,1767) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1769) fake_sched.h:43: return __running_cpu;
    (<0.0>,1773) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1775) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1779) fake_sched.h:43: return __running_cpu;
    (<0.0>,1783) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1790) tree.c:2563: if (needwake)
    (<0.0>,1793) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,1796)
    (<0.0>,1797) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1798) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1800) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1807) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,1810)
    (<0.0>,1811) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1813) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1816) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1823) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,1826) tree_plugin.h:2720: }
    (<0.0>,1830) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1833) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1834) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1835) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1839) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1840) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1841) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1843) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1847) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,1856) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1858) fake_sched.h:43: return __running_cpu;
    (<0.0>,1861) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1863) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1865) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1866) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1868) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1875) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1876) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1878) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1884) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1885) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1886) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1887) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1888) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1892)
    (<0.0>,1893)
    (<0.0>,1894) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1895) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1902)
    (<0.0>,1903)
    (<0.0>,1904) tree.c:1584: local_irq_save(flags);
    (<0.0>,1907) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1909) fake_sched.h:43: return __running_cpu;
    (<0.0>,1913) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1915) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1919) fake_sched.h:43: return __running_cpu;
    (<0.0>,1923) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1928) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1930) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1931) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1932) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1934) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1935) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1937) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1940) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1942) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1943) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1945) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1948) tree.c:1589: local_irq_restore(flags);
    (<0.0>,1951) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1953) fake_sched.h:43: return __running_cpu;
    (<0.0>,1957) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1959) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1963) fake_sched.h:43: return __running_cpu;
    (<0.0>,1967) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1974) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1976) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1981) tree.c:2558: local_irq_save(flags);
    (<0.0>,1984) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1986) fake_sched.h:43: return __running_cpu;
    (<0.0>,1990) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1992) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1996) fake_sched.h:43: return __running_cpu;
    (<0.0>,2000) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2005) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2006) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2012)
    (<0.0>,2013)
    (<0.0>,2014) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,2017)
    (<0.0>,2018) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2020) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2021) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2023) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2029) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,2035)
    (<0.0>,2036) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2039)
    (<0.0>,2040) tree.c:453: return &rsp->node[0];
    (<0.0>,2044) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2045) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2047) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2051) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2052) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2054) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2057) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2058) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2059) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2063) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2066) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2069) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2072) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2073) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2076) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2078) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2081) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2084) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2087) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2088) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2090) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2093) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2097) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2099) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2101) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2104) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2107) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2110) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2111) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2113) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2116) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2120) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2122) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2124) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2127) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,2129) tree.c:494: }
    (<0.0>,2133) tree.c:2566: local_irq_restore(flags);
    (<0.0>,2136) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2138) fake_sched.h:43: return __running_cpu;
    (<0.0>,2142) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2144) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2148) fake_sched.h:43: return __running_cpu;
    (<0.0>,2152) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2158) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2161)
    (<0.0>,2162) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2164) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2167) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2174) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2177) tree_plugin.h:2720: }
    (<0.0>,2181) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2184) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2185) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2186) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2190) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2191) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2192) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2194) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2202) fake_sched.h:43: return __running_cpu;
    (<0.0>,2206) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,2215) tree.c:624: local_irq_save(flags);
    (<0.0>,2218) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2220) fake_sched.h:43: return __running_cpu;
    (<0.0>,2224) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2226) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2230) fake_sched.h:43: return __running_cpu;
    (<0.0>,2234) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2240) fake_sched.h:43: return __running_cpu;
    (<0.0>,2244) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2245) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2247) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2248) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2249) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2251) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2253) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2254) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2256) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2261) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2262) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2264) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2268) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2269) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2270) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2271) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2273) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2281) tree_plugin.h:3141: }
    (<0.0>,2283) tree.c:634: local_irq_restore(flags);
    (<0.0>,2286) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2288) fake_sched.h:43: return __running_cpu;
    (<0.0>,2292) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2294) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2298) fake_sched.h:43: return __running_cpu;
    (<0.0>,2302) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2311) fake_sched.h:43: return __running_cpu;
    (<0.0>,2315) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,2318) tree.c:580: local_irq_save(flags);
    (<0.0>,2321) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2323) fake_sched.h:43: return __running_cpu;
    (<0.0>,2327) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2329) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2333) fake_sched.h:43: return __running_cpu;
    (<0.0>,2337) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2350) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2352) fake_sched.h:43: return __running_cpu;
    (<0.0>,2356) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2357) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2359) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2360) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2361) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2367) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2368) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2373) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2374) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2375) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2376) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,2380) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2382) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2383) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2384) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2403)
    (<0.0>,2405) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2407) fake_sched.h:43: return __running_cpu;
    (<0.0>,2411) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2414) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,2418) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2419) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2420) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2424) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2425) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2426) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2428) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2433) fake_sched.h:43: return __running_cpu;
    (<0.0>,2436) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2438) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2440) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2441) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2444) tree_plugin.h:2720: }
    (<0.0>,2447) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2450) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2451) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2452) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2456) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2457) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2458) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2460) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2465) fake_sched.h:43: return __running_cpu;
    (<0.0>,2468) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2470) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2472) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2473) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2476) tree_plugin.h:2720: }
    (<0.0>,2479) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2482) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2483) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2484) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2488) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2489) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2490) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2492) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2499) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2502) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2503) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2504) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2506) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2507) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2509) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2512) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2518) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2519) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2522) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2527) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2528) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2529) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2543) tree_plugin.h:3141: }
    (<0.0>,2545) tree.c:583: local_irq_restore(flags);
    (<0.0>,2548) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2550) fake_sched.h:43: return __running_cpu;
    (<0.0>,2554) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2556) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2560) fake_sched.h:43: return __running_cpu;
    (<0.0>,2564) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2570) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2573) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2578) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2580) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2589) fake_sched.h:43: return __running_cpu;
    (<0.0>,2593)
    (<0.0>,2594) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2597) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2602) tree.c:704: local_irq_save(flags);
    (<0.0>,2605) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2607) fake_sched.h:43: return __running_cpu;
    (<0.0>,2611) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2613) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2617) fake_sched.h:43: return __running_cpu;
    (<0.0>,2621) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2634) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2636) fake_sched.h:43: return __running_cpu;
    (<0.0>,2640) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2641) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2643) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2644) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2645) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2650) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2651) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2655) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2656) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2657) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2658) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,2662) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2664) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2665) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2666) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2680)
    (<0.0>,2681) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2683) fake_sched.h:43: return __running_cpu;
    (<0.0>,2687) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2691) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2694) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2695) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2696) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2698) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2699) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2701) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2704) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2711) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2712) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2715) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2720) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2721) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2722) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2727) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,2736) tree_plugin.h:3145: }
    (<0.0>,2738) tree.c:707: local_irq_restore(flags);
    (<0.0>,2741) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2743) fake_sched.h:43: return __running_cpu;
    (<0.0>,2747) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2749) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2753) fake_sched.h:43: return __running_cpu;
    (<0.0>,2757) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2764) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,2776)
    (<0.0>,2777) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2780)
    (<0.0>,2781) tree.c:453: return &rsp->node[0];
    (<0.0>,2785) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2789) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2793) fake_sync.h:92: local_irq_disable();
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2804) fake_sched.h:43: return __running_cpu;
    (<0.0>,2808) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2813) fake_sched.h:43: return __running_cpu;
    (<0.0>,2817) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,2820) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2821) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2827) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2829) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2832) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2834) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2835) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2838)
    (<0.0>,2839) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2841) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2842) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2844) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2852) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2853) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2856)
    (<0.0>,2857) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2859) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2860) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2870) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2871) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2874) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,2879)
    (<0.0>,2880) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2881) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2882) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2883) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2885) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2889) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2890) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2891) update.c:344: if (till_stall_check < 3) {
    (<0.0>,2894) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,2898) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,2903) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,2904) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2905) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2907) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2909) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2910) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2911) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2914) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2916) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2920) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2922) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2924) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2926) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2930) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,2934)
    (<0.0>,2935) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2936) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2941) fake_sched.h:43: return __running_cpu;
    (<0.0>,2945) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,2947) fake_sched.h:43: return __running_cpu;
    (<0.0>,2951) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2957) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,2961)
    (<0.0>,2962) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2964) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2970) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2973) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2975) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2976) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2978) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2983) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2987) fake_sync.h:92: local_irq_disable();
    (<0.0>,2990) fake_sched.h:43: return __running_cpu;
    (<0.0>,2994) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2998) fake_sched.h:43: return __running_cpu;
    (<0.0>,3002) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3007) fake_sched.h:43: return __running_cpu;
    (<0.0>,3011) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3014) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3015) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3022) fake_sched.h:43: return __running_cpu;
    (<0.0>,3025) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3027) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3029) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3030) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,3036)
    (<0.0>,3037) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3039) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3044) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3045) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3047) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3051) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3052) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3053) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3055) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3057) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3058) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3060) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3061) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3063) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3064) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3066) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3067) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3069) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3070) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3072) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3077) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3078) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3080) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3081) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3083) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3087) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3088) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3089) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3090) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3092) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3093) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3095) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3096) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3097) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3099) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3102) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3103) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3104) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3110)
    (<0.0>,3111)
    (<0.0>,3112)
    (<0.0>,3113) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3115) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3116) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3118) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3121) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3122) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3123) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3132)
    (<0.0>,3133)
    (<0.0>,3134)
    (<0.0>,3135) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3138) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3141) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3144) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3145) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3148) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3149) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3154)
    (<0.0>,3155)
    (<0.0>,3156) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3159)
    (<0.0>,3160) tree.c:453: return &rsp->node[0];
    (<0.0>,3164) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3167) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3169) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3170) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3172) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3175) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3177) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3179) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3181) tree.c:1261: }
    (<0.0>,3183) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3184) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3186) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3189) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3191) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3194) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3195) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3198) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3201) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3205) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3207) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3209) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3212) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3214) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3217) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3218) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3221) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3224) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3227) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3229) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3232) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3233) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3238) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3240) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3244) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3247) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3250) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3251) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3253) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3256) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3257) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3258) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3260) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3263) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3265) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3267) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3269) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3272) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3275) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3276) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3278) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3281) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3282) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3283) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3285) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3288) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3290) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3292) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3294) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3297) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3298) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3307)
    (<0.0>,3308)
    (<0.0>,3309)
    (<0.0>,3310) tree.c:1289: bool ret = false;
    (<0.0>,3311) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3313) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3316)
    (<0.0>,3317) tree.c:453: return &rsp->node[0];
    (<0.0>,3321) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3322) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3324) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3325) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3330)
    (<0.0>,3331)
    (<0.0>,3332) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3335)
    (<0.0>,3336) tree.c:453: return &rsp->node[0];
    (<0.0>,3340) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3343) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3345) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3346) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3348) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3351) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3353) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3355) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3357) tree.c:1261: }
    (<0.0>,3359) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3360) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3361) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3362) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3368)
    (<0.0>,3369)
    (<0.0>,3370)
    (<0.0>,3371) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,3375) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3377) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3380) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3383) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3385) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3386) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3388) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3391) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3393) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3396) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3398) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3399) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3400) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3401) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3407)
    (<0.0>,3408)
    (<0.0>,3409)
    (<0.0>,3410) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,3415) tree.c:1372: if (c_out != NULL)
    (<0.0>,3418) tree.c:1374: return ret;
    (<0.0>,3422) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3423) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3426) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3427) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3433) tree.c:1482: return ret;
    (<0.0>,3435) tree.c:1482: return ret;
    (<0.0>,3437) tree.c:1483: }
    (<0.0>,3440) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3442) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3444) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3445) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3447) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3450) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3452) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3453) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3455) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3458) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3460) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3461) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3463) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3464) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3466) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3471) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3474) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3475) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,3477) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,3479) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,3481) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,3483) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,3484) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,3487) tree_plugin.h:1950: }
    (<0.0>,3490) tree.c:1575: return ret;
    (<0.0>,3494) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,3497) tree_plugin.h:1487: }
    (<0.0>,3501) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,3505)
    (<0.0>,3506) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3507) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3512) fake_sched.h:43: return __running_cpu;
    (<0.0>,3516) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,3518) fake_sched.h:43: return __running_cpu;
    (<0.0>,3522) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3535) fake_sched.h:43: return __running_cpu;
    (<0.0>,3540) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,3546) fake_sched.h:43: return __running_cpu;
    (<0.0>,3551) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,3557) fake_sched.h:43: return __running_cpu;
    (<0.0>,3561) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,3573) fake_sched.h:43: return __running_cpu;
    (<0.0>,3577) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,3580) tree.c:580: local_irq_save(flags);
    (<0.0>,3583) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3585) fake_sched.h:43: return __running_cpu;
    (<0.0>,3589) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3591) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3595) fake_sched.h:43: return __running_cpu;
    (<0.0>,3599) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3612) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3614) fake_sched.h:43: return __running_cpu;
    (<0.0>,3618) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3619) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3621) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3622) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3623) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3629) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3630) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3635) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3636) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3637) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3638) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3642) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3644) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3645) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3646) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3665)
    (<0.0>,3667) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3669) fake_sched.h:43: return __running_cpu;
    (<0.0>,3673) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3676) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,3680) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3681) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3682) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3686) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3687) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3688) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3690) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3695) fake_sched.h:43: return __running_cpu;
    (<0.0>,3698) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3700) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3702) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3703) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3706) tree_plugin.h:2720: }
    (<0.0>,3709) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3712) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3713) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3714) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3718) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3719) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3720) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3722) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3727) fake_sched.h:43: return __running_cpu;
    (<0.0>,3730) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3732) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3734) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3735) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3738) tree_plugin.h:2720: }
    (<0.0>,3741) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3744) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3745) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3746) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3750) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3751) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3752) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3754) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3761) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3764) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3765) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3766) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3768) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3769) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3771) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3774) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3780) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3781) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3784) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3789) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3790) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3791) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3805) tree_plugin.h:3141: }
    (<0.0>,3807) tree.c:583: local_irq_restore(flags);
    (<0.0>,3810) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3812) fake_sched.h:43: return __running_cpu;
    (<0.0>,3816) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3818) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3822) fake_sched.h:43: return __running_cpu;
    (<0.0>,3826) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3832) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3835) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3840) fake_sched.h:43: return __running_cpu;
    (<0.0>,3844)
    (<0.0>,3845) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3848) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3853) tree.c:704: local_irq_save(flags);
    (<0.0>,3856) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3858) fake_sched.h:43: return __running_cpu;
    (<0.0>,3862) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3864) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3868) fake_sched.h:43: return __running_cpu;
    (<0.0>,3872) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3885) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3887) fake_sched.h:43: return __running_cpu;
    (<0.0>,3891) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3892) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3894) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3895) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3896) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3901) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3902) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3906) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3907) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3908) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3909) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3913) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3915) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3916) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3917) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3931)
    (<0.0>,3932) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3934) fake_sched.h:43: return __running_cpu;
    (<0.0>,3938) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3942) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3945) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3946) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3947) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3949) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3950) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3952) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3955) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3962) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3963) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3966) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3971) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3972) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3973) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3978) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,3987) tree_plugin.h:3145: }
    (<0.0>,3989) tree.c:707: local_irq_restore(flags);
    (<0.0>,3992) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3994) fake_sched.h:43: return __running_cpu;
    (<0.0>,3998) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4000) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4004) fake_sched.h:43: return __running_cpu;
    (<0.0>,4008) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4023) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4025) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4027) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4028) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4030) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4035) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,4039)
    (<0.0>,4040) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4042) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4046) tree.c:1674: return 1;
    (<0.0>,4048) tree.c:1675: }
    (<0.0>,4053) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,4054) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4055) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4056) tree.c:1819: if (j > HZ) {
    (<0.0>,4059) tree.c:1823: ret = 0;
    (<0.0>,4061) tree.c:1825: if (!ret)
    (<0.0>,4064) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4065) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4067) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4069) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4073) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4075) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4079) fake_sched.h:43: return __running_cpu;
    (<0.0>,4083) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4087) fake_sched.h:43: return __running_cpu;
    (<0.0>,4091) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4096) fake_sched.h:43: return __running_cpu;
    (<0.0>,4100) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4110) tree.c:749: local_irq_save(flags);
    (<0.0>,4113) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4115) fake_sched.h:43: return __running_cpu;
    (<0.0>,4119) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4121) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4126) fake_sched.h:43: return __running_cpu;
    (<0.0>,4130) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4131) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4133) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4134) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4135) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4137) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4139) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4140) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4142) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4147) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4148) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4150) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4154) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4155) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4156) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4157) tree.c:754: if (oldval)
    (<0.0>,4165) tree_plugin.h:3145: }
    (<0.0>,4167) tree.c:759: local_irq_restore(flags);
    (<0.0>,4170) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4172) fake_sched.h:43: return __running_cpu;
    (<0.0>,4176) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4178) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4186) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,4191) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,4196) fake_sched.h:43: return __running_cpu;
    (<0.0>,4201) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,4209) fake_sched.h:43: return __running_cpu;
    (<0.0>,4214) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,4228) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4229) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4230) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4234) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4235) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4236) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4238) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4242) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4244) fake_sched.h:43: return __running_cpu;
    (<0.0>,4247) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4249) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4256)
    (<0.0>,4257)
    (<0.0>,4258) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4260) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4261) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4262) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4264) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4266) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4267) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4268) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4278)
    (<0.0>,4279)
    (<0.0>,4280) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,4285) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,4288) tree_plugin.h:3185: return 0;
    (<0.0>,4291) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,4294) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4296) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4299) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4301) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4304) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4306) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4309) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4311) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4314) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4316) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4318) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4319) tree.c:3140: return 1;
    (<0.0>,4321) tree.c:3176: }
    (<0.0>,4325) tree.c:3189: return 1;
    (<0.0>,4327) tree.c:3191: }
    (<0.0>,4334) fake_sched.h:43: return __running_cpu;
    (<0.0>,4338) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,4342) tree.c:2437: if (user)
    (<0.0>,4350) fake_sched.h:43: return __running_cpu;
    (<0.0>,4354) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4356) fake_sched.h:43: return __running_cpu;
    (<0.0>,4360) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4366) fake_sched.h:43: return __running_cpu;
    (<0.0>,4370) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,4380) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,4383) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4384) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4385) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4389) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4390) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4391) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4393) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4397) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,4406) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4408) fake_sched.h:43: return __running_cpu;
    (<0.0>,4411) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4413) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4415) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4416) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4418) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4425) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4426) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4428) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4434) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4435) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4436) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4437) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4438) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4442)
    (<0.0>,4443)
    (<0.0>,4444) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4445) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4452)
    (<0.0>,4453)
    (<0.0>,4454) tree.c:1584: local_irq_save(flags);
    (<0.0>,4457) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4459) fake_sched.h:43: return __running_cpu;
    (<0.0>,4463) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4465) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4469) fake_sched.h:43: return __running_cpu;
    (<0.0>,4473) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4478) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4480) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4481) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4482) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4484) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4485) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4487) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4490) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4492) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4493) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4495) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4498) tree.c:1589: local_irq_restore(flags);
    (<0.0>,4501) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4503) fake_sched.h:43: return __running_cpu;
    (<0.0>,4507) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4509) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4513) fake_sched.h:43: return __running_cpu;
    (<0.0>,4517) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4524) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4526) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4529) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,4531) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,4534) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4536) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4537) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4538) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4547)
    (<0.0>,4548)
    (<0.0>,4549)
    (<0.0>,4550) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4552) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4553) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4554) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,4556) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,4560)
    (<0.0>,4561)
    (<0.0>,4562) fake_sync.h:76: local_irq_save(flags);
    (<0.0>,4565) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4567) fake_sched.h:43: return __running_cpu;
    (<0.0>,4571) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4573) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4577) fake_sched.h:43: return __running_cpu;
    (<0.0>,4581) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4587) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,4588) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,4594) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4596) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4601) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4603) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4604) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4606) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4609) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4611) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4612) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4614) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4617) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4619) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4620) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4621) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4623) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4624) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4628) tree.c:2052: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,4630) tree.c:2052: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,4634)
    (<0.0>,4635)
    (<0.0>,4636) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,4637) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,4640) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,4643) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4645) fake_sched.h:43: return __running_cpu;
    (<0.0>,4649) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4651) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4655) fake_sched.h:43: return __running_cpu;
    (<0.0>,4659) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4670) tree.c:2558: local_irq_save(flags);
    (<0.0>,4673) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4675) fake_sched.h:43: return __running_cpu;
    (<0.0>,4679) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4681) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4685) fake_sched.h:43: return __running_cpu;
    (<0.0>,4689) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4694) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4695) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4701)
    (<0.0>,4702)
    (<0.0>,4703) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,4706)
    (<0.0>,4707) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4709) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4710) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4712) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4718) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,4720) tree.c:494: }
    (<0.0>,4724) tree.c:2566: local_irq_restore(flags);
    (<0.0>,4727) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4729) fake_sched.h:43: return __running_cpu;
    (<0.0>,4733) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4735) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4739) fake_sched.h:43: return __running_cpu;
    (<0.0>,4743) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4749) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,4752)
    (<0.0>,4753) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4755) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4758) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4765) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,4768) tree_plugin.h:2720: }
    (<0.0>,4772) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4775) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4776) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4777) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4781) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4782) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4783) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4785) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4789) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,4798) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4800) fake_sched.h:43: return __running_cpu;
    (<0.0>,4803) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4805) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4807) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4808) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4810) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4817) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4818) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4820) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4826) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4827) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4828) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4829) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4830) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4834)
    (<0.0>,4835)
    (<0.0>,4836) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4837) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4844)
    (<0.0>,4845)
    (<0.0>,4846) tree.c:1584: local_irq_save(flags);
    (<0.0>,4849) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4851) fake_sched.h:43: return __running_cpu;
    (<0.0>,4855) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4857) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4861) fake_sched.h:43: return __running_cpu;
    (<0.0>,4865) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4870) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4872) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4873) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4874) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4876) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4877) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4879) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4882) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4884) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4885) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4887) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4890) tree.c:1589: local_irq_restore(flags);
    (<0.0>,4893) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4895) fake_sched.h:43: return __running_cpu;
    (<0.0>,4899) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4901) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4905) fake_sched.h:43: return __running_cpu;
    (<0.0>,4909) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4916) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4918) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4923) tree.c:2558: local_irq_save(flags);
    (<0.0>,4926) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4928) fake_sched.h:43: return __running_cpu;
    (<0.0>,4932) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4934) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4938) fake_sched.h:43: return __running_cpu;
    (<0.0>,4942) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4947) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4948) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4954)
    (<0.0>,4955)
    (<0.0>,4956) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,4959)
    (<0.0>,4960) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4962) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4963) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4965) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4971) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,4977)
    (<0.0>,4978) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,4981)
    (<0.0>,4982) tree.c:453: return &rsp->node[0];
    (<0.0>,4986) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,4987) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4989) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4993) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4994) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,4996) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,4999) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,5000) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,5001) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,5005) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5008) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5011) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5014) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5015) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5018) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5020) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5023) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5026) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5029) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5030) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5032) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5035) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5039) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5041) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5043) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5046) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5049) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5052) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5053) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5055) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5058) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5062) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5064) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5066) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5069) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,5071) tree.c:494: }
    (<0.0>,5075) tree.c:2566: local_irq_restore(flags);
    (<0.0>,5078) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5080) fake_sched.h:43: return __running_cpu;
    (<0.0>,5084) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5086) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5090) fake_sched.h:43: return __running_cpu;
    (<0.0>,5094) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5100) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,5103)
    (<0.0>,5104) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5106) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5109) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5116) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5119) tree_plugin.h:2720: }
    (<0.0>,5123) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5126) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5127) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5128) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5132) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5133) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5134) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5136) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5144) fake_sched.h:43: return __running_cpu;
    (<0.0>,5148) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,5157) tree.c:624: local_irq_save(flags);
    (<0.0>,5160) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5162) fake_sched.h:43: return __running_cpu;
    (<0.0>,5166) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5168) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5172) fake_sched.h:43: return __running_cpu;
    (<0.0>,5176) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5182) fake_sched.h:43: return __running_cpu;
    (<0.0>,5186) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5187) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5189) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5190) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5191) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5193) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5195) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5196) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5198) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5203) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5204) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5206) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5210) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5211) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5212) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5213) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,5215) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,5223) tree_plugin.h:3141: }
    (<0.0>,5225) tree.c:634: local_irq_restore(flags);
    (<0.0>,5228) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5230) fake_sched.h:43: return __running_cpu;
    (<0.0>,5234) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5236) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5240) fake_sched.h:43: return __running_cpu;
    (<0.0>,5244) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5253) fake_sched.h:43: return __running_cpu;
    (<0.0>,5257) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,5260) tree.c:580: local_irq_save(flags);
    (<0.0>,5263) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5265) fake_sched.h:43: return __running_cpu;
    (<0.0>,5269) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5271) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5275) fake_sched.h:43: return __running_cpu;
    (<0.0>,5279) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5292) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5294) fake_sched.h:43: return __running_cpu;
    (<0.0>,5298) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5299) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5301) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5302) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5303) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5309) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5310) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5315) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5316) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5317) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5318) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5322) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5324) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5325) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5326) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5345)
    (<0.0>,5347) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5349) fake_sched.h:43: return __running_cpu;
    (<0.0>,5353) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5356) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,5360) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5361) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5362) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5366) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5367) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5368) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5370) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5375) fake_sched.h:43: return __running_cpu;
    (<0.0>,5378) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5380) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5382) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5383) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5386) tree_plugin.h:2720: }
    (<0.0>,5389) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5392) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5393) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5394) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5398) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5399) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5400) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5402) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5407) fake_sched.h:43: return __running_cpu;
    (<0.0>,5410) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5412) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5414) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5415) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5418) tree_plugin.h:2720: }
    (<0.0>,5421) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5424) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5425) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5426) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5430) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5431) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5432) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5434) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5441) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5444) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5445) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5446) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5448) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5449) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5451) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5454) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5460) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5461) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5464) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5469) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5470) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5471) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5485) tree_plugin.h:3141: }
    (<0.0>,5487) tree.c:583: local_irq_restore(flags);
    (<0.0>,5490) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5492) fake_sched.h:43: return __running_cpu;
    (<0.0>,5496) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5498) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5502) fake_sched.h:43: return __running_cpu;
    (<0.0>,5506) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5512) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5515) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5520) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5522) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5524) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5528) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5530) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5537) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5539) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5541) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5545) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5547) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5554) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5556) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5558) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5562) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5564) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5571) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5573) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5575) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5579) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5581) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5588) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5590) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5592) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5596) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
  (<0>,5327) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5328) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5330) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5333) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5335) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5337) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5338) tree.c:3164: return 1;
  (<0>,5340) tree.c:3176: }
  (<0>,5344) tree.c:3189: return 1;
  (<0>,5346) tree.c:3191: }
  (<0>,5353) fake_sched.h:43: return __running_cpu;
  (<0>,5357) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
  (<0>,5361) tree.c:2437: if (user)
  (<0>,5369) fake_sched.h:43: return __running_cpu;
  (<0>,5373) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
  (<0>,5375) fake_sched.h:43: return __running_cpu;
  (<0>,5379) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5385) fake_sched.h:43: return __running_cpu;
  (<0>,5389) fake_sched.h:182: if (need_softirq[get_cpu()]) {
  (<0>,5399) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,5402) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5403) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5404) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5408) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5409) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5410) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5412) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5416) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,5425) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5427) fake_sched.h:43: return __running_cpu;
  (<0>,5430) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5432) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5434) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5435) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5437) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5444) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5445) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5447) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5453) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5454) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5455) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5456) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5457) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5461)
  (<0>,5462)
  (<0>,5463) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5464) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5471)
  (<0>,5472)
  (<0>,5473) tree.c:1584: local_irq_save(flags);
  (<0>,5476) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5478) fake_sched.h:43: return __running_cpu;
  (<0>,5482) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5484) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5488) fake_sched.h:43: return __running_cpu;
  (<0>,5492) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5497) tree.c:1585: rnp = rdp->mynode;
  (<0>,5499) tree.c:1585: rnp = rdp->mynode;
  (<0>,5500) tree.c:1585: rnp = rdp->mynode;
  (<0>,5501) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5503) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5504) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5506) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5509) tree.c:1588: !raw_spin_trylock(&rnp->lock)) { /* irqs already off, so later. */
  (<0>,5514) fake_sync.h:122: preempt_disable();
  (<0>,5516) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
  (<0>,5517) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
  (<0>,5520) fake_sync.h:127: return 1;
  (<0>,5522) fake_sync.h:128: }
  (<0>,5528) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5529) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5530) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5536)
  (<0>,5537)
  (<0>,5538)
  (<0>,5539) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5541) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5542) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5544) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5547) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5548) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5549) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5558)
  (<0>,5559)
  (<0>,5560)
  (<0>,5561) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5564) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5567) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5570) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5571) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5574) tree.c:1434: return false;
  (<0>,5576) tree.c:1483: }
  (<0>,5579) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5581) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5583) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5584) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5586) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5589) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5591) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5592) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5594) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5597) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,5599) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,5600) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5602) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5603) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5605) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5610) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5613) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5614) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
  (<0>,5616) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
  (<0>,5618) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
  (<0>,5620) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
  (<0>,5622) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
  (<0>,5623) tree.c:1573: zero_cpu_stall_ticks(rdp);
  (<0>,5626) tree_plugin.h:1950: }
  (<0>,5629) tree.c:1575: return ret;
  (<0>,5633) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5634) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,5636) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,5640)
  (<0>,5641)
  (<0>,5642) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,5643) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,5646) fake_sync.h:86: local_irq_restore(flags);
  (<0>,5649) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5651) fake_sched.h:43: return __running_cpu;
  (<0>,5655) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5657) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5661) fake_sched.h:43: return __running_cpu;
  (<0>,5665) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5672) tree.c:1595: if (needwake)
  (<0>,5676) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5678) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5681) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,5683) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,5688) tree.c:2558: local_irq_save(flags);
  (<0>,5691) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5693) fake_sched.h:43: return __running_cpu;
  (<0>,5697) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5699) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5703) fake_sched.h:43: return __running_cpu;
  (<0>,5707) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5712) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5713) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5719)
  (<0>,5720)
  (<0>,5721) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,5724)
  (<0>,5725) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5727) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5728) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5730) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5736) tree.c:481: return 0;  /* No, a grace period is already in progress. */
  (<0>,5738) tree.c:494: }
  (<0>,5742) tree.c:2566: local_irq_restore(flags);
  (<0>,5745) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5747) fake_sched.h:43: return __running_cpu;
  (<0>,5751) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5753) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5757) fake_sched.h:43: return __running_cpu;
  (<0>,5761) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5767) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,5770)
  (<0>,5771) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5773) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5776) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5783) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,5786) tree_plugin.h:2720: }
  (<0>,5790) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5793) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5794) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5795) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5799) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5800) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5801) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5803) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5807) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,5816) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5818) fake_sched.h:43: return __running_cpu;
  (<0>,5821) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5823) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5825) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5826) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5828) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5835) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5836) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5838) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5844) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5845) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5846) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5847) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5848) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5852)
  (<0>,5853)
  (<0>,5854) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5855) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5862)
  (<0>,5863)
  (<0>,5864) tree.c:1584: local_irq_save(flags);
  (<0>,5867) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5869) fake_sched.h:43: return __running_cpu;
  (<0>,5873) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5875) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5879) fake_sched.h:43: return __running_cpu;
  (<0>,5883) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5888) tree.c:1585: rnp = rdp->mynode;
  (<0>,5890) tree.c:1585: rnp = rdp->mynode;
  (<0>,5891) tree.c:1585: rnp = rdp->mynode;
  (<0>,5892) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5894) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5895) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5897) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5900) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5902) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5903) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5905) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5908) tree.c:1589: local_irq_restore(flags);
  (<0>,5911) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5913) fake_sched.h:43: return __running_cpu;
  (<0>,5917) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5919) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5923) fake_sched.h:43: return __running_cpu;
  (<0>,5927) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5934) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5936) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5941) tree.c:2558: local_irq_save(flags);
  (<0>,5944) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5946) fake_sched.h:43: return __running_cpu;
  (<0>,5950) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5952) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5956) fake_sched.h:43: return __running_cpu;
  (<0>,5960) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5965) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5966) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5972)
  (<0>,5973)
  (<0>,5974) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,5977)
  (<0>,5978) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5980) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5981) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5983) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5989) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5995)
  (<0>,5996) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5999)
  (<0>,6000) tree.c:453: return &rsp->node[0];
  (<0>,6004) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6005) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6007) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6011) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6012) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6014) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6017) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6018) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6019) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6023) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6026) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6029) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6032) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6033) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6036) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6038) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6041) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6044) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6047) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6048) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6050) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6053) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6057) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6059) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6061) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6064) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6067) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6070) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6071) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6073) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6076) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6080) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6082) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6084) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6087) tree.c:493: return 0; /* No grace period needed. */
  (<0>,6089) tree.c:494: }
  (<0>,6093) tree.c:2566: local_irq_restore(flags);
  (<0>,6096) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6098) fake_sched.h:43: return __running_cpu;
  (<0>,6102) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6104) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6108) fake_sched.h:43: return __running_cpu;
  (<0>,6112) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6118) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,6121)
  (<0>,6122) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6124) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6127) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6134) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,6137) tree_plugin.h:2720: }
  (<0>,6141) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6144) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6145) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6146) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6150) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6151) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6152) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6154) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6162) fake_sched.h:43: return __running_cpu;
  (<0>,6166) fake_sched.h:184: need_softirq[get_cpu()] = 0;
  (<0>,6175) tree.c:624: local_irq_save(flags);
  (<0>,6178) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6180) fake_sched.h:43: return __running_cpu;
  (<0>,6184) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6186) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6190) fake_sched.h:43: return __running_cpu;
  (<0>,6194) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6200) fake_sched.h:43: return __running_cpu;
  (<0>,6204) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6205) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6207) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6208) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6209) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6211) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6213) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6214) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6216) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6221) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6222) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6224) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6228) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6229) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6230) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6231) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,6233) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,6241) tree_plugin.h:3141: }
  (<0>,6243) tree.c:634: local_irq_restore(flags);
  (<0>,6246) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6248) fake_sched.h:43: return __running_cpu;
  (<0>,6252) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6254) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6258) fake_sched.h:43: return __running_cpu;
  (<0>,6262) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5598) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5601) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5604)
    (<0.0>,5613) fake_sched.h:43: return __running_cpu;
    (<0.0>,5617)
    (<0.0>,5618) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5621) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5626) tree.c:704: local_irq_save(flags);
    (<0.0>,5629)
    (<0.0>,5631) fake_sched.h:43: return __running_cpu;
    (<0.0>,5635) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5637) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5641) fake_sched.h:43: return __running_cpu;
    (<0.0>,5645) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5658)
    (<0.0>,5660) fake_sched.h:43: return __running_cpu;
    (<0.0>,5664) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5665) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5667) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5668) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5669) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5674) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5675) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5679) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5680) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5681) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5682) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,5686) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5688) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5689) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5690) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5704)
    (<0.0>,5705)
    (<0.0>,5707) fake_sched.h:43: return __running_cpu;
    (<0.0>,5711) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5715) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5718) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5719) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5720) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5722) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5723) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5725) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5728) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5735) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5736) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5739) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5744) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5745) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5746) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5751) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,5760)
    (<0.0>,5762) tree.c:707: local_irq_restore(flags);
    (<0.0>,5765)
    (<0.0>,5767) fake_sched.h:43: return __running_cpu;
    (<0.0>,5771) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5773) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5777) fake_sched.h:43: return __running_cpu;
    (<0.0>,5781) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5788) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5789) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5790) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5791) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,5793) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,5796) tree.c:1840: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,5799)
    (<0.0>,5804) tree.c:1872: rcu_gp_cleanup(rsp);
    (<0.0>,5812)
    (<0.0>,5813) tree.c:1720: bool needgp = false;
    (<0.0>,5814) tree.c:1721: int nocb = 0;
    (<0.0>,5815) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5818)
    (<0.0>,5819) tree.c:453: return &rsp->node[0];
    (<0.0>,5823) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5824) tree.c:1725: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,5828)
    (<0.0>,5831) fake_sched.h:43: return __running_cpu;
    (<0.0>,5835) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5839) fake_sched.h:43: return __running_cpu;
    (<0.0>,5843) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5848) fake_sched.h:43: return __running_cpu;
    (<0.0>,5852) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5855) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5856) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5862) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5863) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5865) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5867) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5868) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5869) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5871) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5874) tree.c:1739: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,5878)
    (<0.0>,5879) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5880) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5885) fake_sched.h:43: return __running_cpu;
    (<0.0>,5889) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,5891) fake_sched.h:43: return __running_cpu;
    (<0.0>,5895) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5901) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5904) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5906) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5907) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5909) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5914) tree.c:1751: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,5918)
    (<0.0>,5921) fake_sched.h:43: return __running_cpu;
    (<0.0>,5925) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5929) fake_sched.h:43: return __running_cpu;
    (<0.0>,5933) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5938) fake_sched.h:43: return __running_cpu;
    (<0.0>,5942) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5945) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5946) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5952) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5954) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5955) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5957) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5959) fake_sched.h:43: return __running_cpu;
    (<0.0>,5962) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5964) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5966) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5967) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5968) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5970) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5973) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5974) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5975) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5981)
    (<0.0>,5982)
    (<0.0>,5983)
    (<0.0>,5984) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5986) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5987) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5989) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5992) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5993) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5994) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,6002)
    (<0.0>,6003)
    (<0.0>,6004)
    (<0.0>,6005) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6008) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6011) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6014) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6015) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6018) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6020) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6023) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6025) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6026) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6028) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6031) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6035) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6037) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6040) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6041) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6044) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6046) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6048) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6050) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6053) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6055) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6056) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6058) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6061) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6066) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6068) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6069) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6072) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6075) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6076) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6078) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6081) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6083) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6085) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6087) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6088) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6091) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,6093) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,6096) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6098) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6101) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6102) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6105) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6109) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6110) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6111) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6120)
    (<0.0>,6121)
    (<0.0>,6122)
    (<0.0>,6123) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6126) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6129) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6132) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6133) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6136) tree.c:1434: return false;
    (<0.0>,6138) tree.c:1483: }
    (<0.0>,6140) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6142) tree.c:1527: }
    (<0.0>,6145) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,6146) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6148) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6149) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6151) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6155) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6157) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6158) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6160) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6163) tree.c:1575: return ret;
    (<0.0>,6167) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6171) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6173) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6174) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6181)
    (<0.0>,6182)
    (<0.0>,6183) tree.c:1385: int c = rnp->completed;
    (<0.0>,6185) tree.c:1385: int c = rnp->completed;
    (<0.0>,6187) tree.c:1385: int c = rnp->completed;
    (<0.0>,6189) fake_sched.h:43: return __running_cpu;
    (<0.0>,6192) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6194) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6196) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6197) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,6198) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,6202)
    (<0.0>,6203)
    (<0.0>,6205) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6208) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6211) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6212) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6216) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6219) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6220) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6221) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6222) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6223) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6225) tree.c:1393: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,6233)
    (<0.0>,6234)
    (<0.0>,6235)
    (<0.0>,6236)
    (<0.0>,6240) tree.c:1394: return needmore;
    (<0.0>,6242) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6244) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6245) tree.c:1759: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,6249)
    (<0.0>,6250) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6251) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6256) fake_sched.h:43: return __running_cpu;
    (<0.0>,6260) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6262) fake_sched.h:43: return __running_cpu;
    (<0.0>,6266) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6279) fake_sched.h:43: return __running_cpu;
    (<0.0>,6284) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,6291) fake_sched.h:43: return __running_cpu;
    (<0.0>,6295) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,6307) fake_sched.h:43: return __running_cpu;
    (<0.0>,6311)
    (<0.0>,6314) tree.c:580: local_irq_save(flags);
    (<0.0>,6317)
    (<0.0>,6319) fake_sched.h:43: return __running_cpu;
    (<0.0>,6323) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6325) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6329) fake_sched.h:43: return __running_cpu;
    (<0.0>,6333) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6346)
    (<0.0>,6348) fake_sched.h:43: return __running_cpu;
    (<0.0>,6352) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6353) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6355) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6356) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6357) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6363) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6364) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6369) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6370) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6371) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6372) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,6376) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,6378) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,6379) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6380) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6399)
    (<0.0>,6401)
    (<0.0>,6403) fake_sched.h:43: return __running_cpu;
    (<0.0>,6407) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6410) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,6414) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6415) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6416) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6420) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6421) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6422) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6424) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6429) fake_sched.h:43: return __running_cpu;
    (<0.0>,6432) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6434) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6436) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6437) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6440)
    (<0.0>,6443) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6446) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6447) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6448) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6452) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6453) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6454) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6456) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6461) fake_sched.h:43: return __running_cpu;
    (<0.0>,6464) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6466) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6468) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6469) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6472)
    (<0.0>,6475) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6478) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6479) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6480) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6484) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6485) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6486) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6488) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6495) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6498) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6499) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6500) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6502) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6503) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6505) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6508) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6514) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6515) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6518) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6523) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6524) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6525) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6539)
    (<0.0>,6541) tree.c:583: local_irq_restore(flags);
    (<0.0>,6544)
    (<0.0>,6546) fake_sched.h:43: return __running_cpu;
    (<0.0>,6550) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6552) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6556) fake_sched.h:43: return __running_cpu;
    (<0.0>,6560) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6566) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6569) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6574) fake_sched.h:43: return __running_cpu;
    (<0.0>,6578)
    (<0.0>,6579) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6582) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6587) tree.c:704: local_irq_save(flags);
    (<0.0>,6590)
    (<0.0>,6592) fake_sched.h:43: return __running_cpu;
    (<0.0>,6596) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6598) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6602) fake_sched.h:43: return __running_cpu;
    (<0.0>,6606) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6619)
    (<0.0>,6621) fake_sched.h:43: return __running_cpu;
    (<0.0>,6625) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6626) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6628) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6629) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6630) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6635) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6636) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6640) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6641) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6642) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6643) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,6647) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6649) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6650) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6651) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6665)
    (<0.0>,6666)
    (<0.0>,6668) fake_sched.h:43: return __running_cpu;
    (<0.0>,6672) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6676) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6679) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6680) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6681) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6683) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6684) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6686) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6689) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6696) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6697) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6700) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6705) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6706) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6707) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6712) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,6721)
    (<0.0>,6723) tree.c:707: local_irq_restore(flags);
    (<0.0>,6726)
    (<0.0>,6728) fake_sched.h:43: return __running_cpu;
    (<0.0>,6732) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6734) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6738) fake_sched.h:43: return __running_cpu;
    (<0.0>,6742) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6757) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6759) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6761) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6762) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6764) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6769) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,6772)
    (<0.0>,6773) tree.c:453: return &rsp->node[0];
    (<0.0>,6777) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,6778) tree.c:1763: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,6782)
    (<0.0>,6785) fake_sched.h:43: return __running_cpu;
    (<0.0>,6789) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6793) fake_sched.h:43: return __running_cpu;
    (<0.0>,6797) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6802) fake_sched.h:43: return __running_cpu;
    (<0.0>,6806) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,6809) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6810) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6816) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,6817) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,6821)
    (<0.0>,6822)
    (<0.0>,6824) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6826) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6827) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6829) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6832) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,6834) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,6836) fake_sched.h:43: return __running_cpu;
    (<0.0>,6839) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6841) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6843) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6844) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6845) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6846) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6854)
    (<0.0>,6855)
    (<0.0>,6856)
    (<0.0>,6857) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6860) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6863) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6866) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6867) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6870) tree.c:1502: return false;
    (<0.0>,6872) tree.c:1527: }
    (<0.0>,6875) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6879) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6880) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6883) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6884) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6890)
    (<0.0>,6891)
    (<0.0>,6892) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,6895)
    (<0.0>,6896) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6898) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6899) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6901) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6907) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,6913)
    (<0.0>,6914) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6917)
    (<0.0>,6918) tree.c:453: return &rsp->node[0];
    (<0.0>,6922) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6923) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6925) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6929) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6930) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6932) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6935) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6936) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,6937) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,6941) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,6943) tree.c:494: }
    (<0.0>,6947) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,6949) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,6953) tree.c:1780: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,6957)
    (<0.0>,6958) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6959) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6964) fake_sched.h:43: return __running_cpu;
    (<0.0>,6968) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6970) fake_sched.h:43: return __running_cpu;
    (<0.0>,6974) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6985) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,6987) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,6991) fake_sched.h:43: return __running_cpu;
    (<0.0>,6995) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6999) fake_sched.h:43: return __running_cpu;
    (<0.0>,7003) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7008) fake_sched.h:43: return __running_cpu;
    (<0.0>,7012) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,7022) tree.c:749: local_irq_save(flags);
    (<0.0>,7025)
    (<0.0>,7027) fake_sched.h:43: return __running_cpu;
    (<0.0>,7031) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7033) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7038) fake_sched.h:43: return __running_cpu;
    (<0.0>,7042) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,7043) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,7045) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,7046) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,7047) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,7049) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,7051) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,7052) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7054) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7059) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7060) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7062) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7066) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7067) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7068) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7069) tree.c:754: if (oldval)
    (<0.0>,7077)
    (<0.0>,7079) tree.c:759: local_irq_restore(flags);
    (<0.0>,7082)
    (<0.0>,7084) fake_sched.h:43: return __running_cpu;
    (<0.0>,7088) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7090) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7098)
    (<0.0>,7103) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,7108) fake_sched.h:43: return __running_cpu;
    (<0.0>,7113) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,7121) fake_sched.h:43: return __running_cpu;
    (<0.0>,7126) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,7140) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7141) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7142) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7146) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7147) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7148) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7150) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7154) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7156) fake_sched.h:43: return __running_cpu;
    (<0.0>,7159) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7161) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7168)
    (<0.0>,7169)
    (<0.0>,7170) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7172) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7173) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7174) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7176) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7178) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7179) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,7180) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,7190)
    (<0.0>,7191)
    (<0.0>,7192) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,7197) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,7200)
    (<0.0>,7203) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,7206) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7208) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7211) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7213) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7216) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7218) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7221) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7223) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7226) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,7228) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,7230) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,7231) tree.c:3140: return 1;
    (<0.0>,7233) tree.c:3176: }
    (<0.0>,7237) tree.c:3189: return 1;
    (<0.0>,7239) tree.c:3191: }
    (<0.0>,7246) fake_sched.h:43: return __running_cpu;
    (<0.0>,7250) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,7254) tree.c:2437: if (user)
    (<0.0>,7262) fake_sched.h:43: return __running_cpu;
    (<0.0>,7266) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,7268) fake_sched.h:43: return __running_cpu;
    (<0.0>,7272) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7278) fake_sched.h:43: return __running_cpu;
    (<0.0>,7282) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,7292)
    (<0.0>,7295) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7296) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7297) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7301) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7302) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7303) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7305) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7309) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,7318)
    (<0.0>,7320) fake_sched.h:43: return __running_cpu;
    (<0.0>,7323) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7325) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7327) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7328) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7330) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7337) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7338) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7340) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7346) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7347) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7348) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7349) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7350) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7354)
    (<0.0>,7355)
    (<0.0>,7356) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7357) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7364)
    (<0.0>,7365)
    (<0.0>,7366) tree.c:1584: local_irq_save(flags);
    (<0.0>,7369)
    (<0.0>,7371) fake_sched.h:43: return __running_cpu;
    (<0.0>,7375) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7377) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7381) fake_sched.h:43: return __running_cpu;
    (<0.0>,7385) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7390) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7392) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7393) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7394) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7396) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7397) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7399) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7402) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7404) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7405) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7407) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7410) tree.c:1589: local_irq_restore(flags);
    (<0.0>,7413)
    (<0.0>,7415) fake_sched.h:43: return __running_cpu;
    (<0.0>,7419) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7421) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7425) fake_sched.h:43: return __running_cpu;
    (<0.0>,7429) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7436) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7438) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7441) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,7443) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,7446) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,7448) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,7449) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,7450) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,7459)
    (<0.0>,7460)
    (<0.0>,7461)
    (<0.0>,7462) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,7464) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,7465) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,7466) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,7468) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,7472)
    (<0.0>,7473)
    (<0.0>,7474) fake_sync.h:76: local_irq_save(flags);
    (<0.0>,7477)
    (<0.0>,7479) fake_sched.h:43: return __running_cpu;
    (<0.0>,7483) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7485) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7489) fake_sched.h:43: return __running_cpu;
    (<0.0>,7493) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7499) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,7500) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,7506) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7508) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7513) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7515) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7516) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7518) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7521) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,7523) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,7524) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,7526) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,7529) tree.c:2046: rdp->passed_quiesce = 0;	/* need qs for new gp. */
    (<0.0>,7531) tree.c:2046: rdp->passed_quiesce = 0;	/* need qs for new gp. */
    (<0.0>,7532) tree.c:2047: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,7534) tree.c:2047: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,7538)
    (<0.0>,7539)
    (<0.0>,7540) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7541) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7544) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,7547)
    (<0.0>,7549) fake_sched.h:43: return __running_cpu;
    (<0.0>,7553) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7555) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7559) fake_sched.h:43: return __running_cpu;
    (<0.0>,7563) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7574) tree.c:2558: local_irq_save(flags);
    (<0.0>,7577)
    (<0.0>,7579) fake_sched.h:43: return __running_cpu;
    (<0.0>,7583) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7585) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7589) fake_sched.h:43: return __running_cpu;
    (<0.0>,7593) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7598) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7599) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7605)
    (<0.0>,7606)
    (<0.0>,7607) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7610)
    (<0.0>,7611) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7613) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7614) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7616) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7622) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7628)
    (<0.0>,7629) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7632)
    (<0.0>,7633) tree.c:453: return &rsp->node[0];
    (<0.0>,7637) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7638) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7640) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7644) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7645) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7647) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7650) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7651) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7652) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7656) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7658) tree.c:494: }
    (<0.0>,7662) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,7665)
    (<0.0>,7666) tree.c:453: return &rsp->node[0];
    (<0.0>,7673)
    (<0.0>,7675) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7676) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7680) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,7686)
    (<0.0>,7688) fake_sched.h:43: return __running_cpu;
    (<0.0>,7691) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7693) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7695) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7696) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7699)
    (<0.0>,7700) tree.c:453: return &rsp->node[0];
    (<0.0>,7704) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7705) tree.c:1925: bool ret = false;
    (<0.0>,7706) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7707) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7708) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7716)
    (<0.0>,7717)
    (<0.0>,7718)
    (<0.0>,7719) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7722) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7725) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7728) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7729) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7732) tree.c:1502: return false;
    (<0.0>,7734) tree.c:1527: }
    (<0.0>,7737) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7741) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7742) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7743) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7744) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7750)
    (<0.0>,7751)
    (<0.0>,7752)
    (<0.0>,7753) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7755) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7758) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7759) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7765)
    (<0.0>,7766)
    (<0.0>,7767) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7770)
    (<0.0>,7771) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7773) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7774) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7776) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7782) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7788)
    (<0.0>,7789) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7792)
    (<0.0>,7793) tree.c:453: return &rsp->node[0];
    (<0.0>,7797) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7798) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7800) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7804) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7805) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7807) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7810) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7811) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7812) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7816) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7818) tree.c:494: }
    (<0.0>,7822) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7824) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7827) tree.c:1909: return true;
    (<0.0>,7829) tree.c:1910: }
    (<0.0>,7833) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7834) tree.c:1937: return ret;
    (<0.0>,7838) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,7839) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,7842)
    (<0.0>,7843) tree.c:453: return &rsp->node[0];
    (<0.0>,7848) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,7852)
    (<0.0>,7853)
    (<0.0>,7854) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7855) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7858) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,7861)
    (<0.0>,7863) fake_sched.h:43: return __running_cpu;
    (<0.0>,7867) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7869) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7873) fake_sched.h:43: return __running_cpu;
    (<0.0>,7877) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7884) tree.c:2563: if (needwake)
    (<0.0>,7887) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,7890)
    (<0.0>,7891) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7892) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7894) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7901) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7904)
    (<0.0>,7905) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7907) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7910) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7913) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7916) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7923) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,7924) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,7928)
    (<0.0>,7929)
    (<0.0>,7930) tree.c:2601: if (unlikely(!ACCESS_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,7939) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,7941) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,7950) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,7951) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,7968)
    (<0.0>,7969)
    (<0.0>,7970) tree.c:2313: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,7973)
    (<0.0>,7974) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7976) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7979) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7982) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7985) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7992) tree.c:2325: local_irq_save(flags);
    (<0.0>,7995)
    (<0.0>,7997) fake_sched.h:43: return __running_cpu;
    (<0.0>,8001) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8003) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8007) fake_sched.h:43: return __running_cpu;
    (<0.0>,8011) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8016) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8017) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8018) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8019) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8020) tree.c:2327: bl = rdp->blimit;
    (<0.0>,8022) tree.c:2327: bl = rdp->blimit;
    (<0.0>,8023) tree.c:2327: bl = rdp->blimit;
    (<0.0>,8026) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,8028) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,8029) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,8030) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8033) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8034) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8035) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8037) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8038) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,8041) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,8042) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,8043) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8046) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8047) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8048) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8050) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8053) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8055) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8058) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8059) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8062) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8065) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8067) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8069) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8072) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8075) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8077) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8079) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8082) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8084) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8087) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8088) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8091) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8094) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8096) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8098) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8101) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8104) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8106) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8108) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8111) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8113) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8116) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8117) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8120) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8123) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8125) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8127) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8130) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8133) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8135) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8137) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8140) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8142) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8145) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8146) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8149) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8152) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8154) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8156) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8159) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8162) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8164) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8166) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8169) tree.c:2336: local_irq_restore(flags);
    (<0.0>,8172)
    (<0.0>,8174) fake_sched.h:43: return __running_cpu;
    (<0.0>,8178) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8180) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8184) fake_sched.h:43: return __running_cpu;
    (<0.0>,8188) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8193) tree.c:2339: count = count_lazy = 0;
    (<0.0>,8194) tree.c:2339: count = count_lazy = 0;
    (<0.0>,8196) tree.c:2340: while (list) {
    (<0.0>,8199) tree.c:2341: next = list->next;
    (<0.0>,8201) tree.c:2341: next = list->next;
    (<0.0>,8202) tree.c:2341: next = list->next;
    (<0.0>,8205) tree.c:2343: debug_rcu_head_unqueue(list);
    (<0.0>,8208)
    (<0.0>,8210) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8212) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8213) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8219)
    (<0.0>,8220)
    (<0.0>,8221) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8223) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8225) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8228) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,8231) rcu.h:118: head->func(head);
    (<0.0>,8233) rcu.h:118: head->func(head);
    (<0.0>,8234) rcu.h:118: head->func(head);
    (<0.0>,8240)
    (<0.0>,8241) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8242) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8243) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8247) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8248) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8249) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8250) update.c:216: complete(&rcu->completion);
    (<0.0>,8254)
    (<0.0>,8255) fake_sync.h:248: x->done++;
    (<0.0>,8257) fake_sync.h:248: x->done++;
    (<0.0>,8259) fake_sync.h:248: x->done++;
    (<0.0>,8264) rcu.h:120: return false;
    (<0.0>,8266) rcu.h:122: }
    (<0.0>,8269) tree.c:2346: list = next;
    (<0.0>,8270) tree.c:2346: list = next;
    (<0.0>,8271) tree.c:2348: if (++count >= bl &&
    (<0.0>,8273) tree.c:2348: if (++count >= bl &&
    (<0.0>,8274) tree.c:2348: if (++count >= bl &&
    (<0.0>,8278) tree.c:2340: while (list) {
    (<0.0>,8281) tree.c:2354: local_irq_save(flags);
    (<0.0>,8284)
    (<0.0>,8286) fake_sched.h:43: return __running_cpu;
    (<0.0>,8290) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8292) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8296) fake_sched.h:43: return __running_cpu;
    (<0.0>,8300) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8307) tree.c:2360: if (list != NULL) {
    (<0.0>,8311) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8312) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8314) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8316) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8317) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8319) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8320) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8322) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8324) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8325) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8326) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8328) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8330) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8331) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,8333) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,8336) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8338) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8341) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8343) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8346) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8348) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8349) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8351) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8352) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8357) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8359) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8362) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8364) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8371) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8372) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8374) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8377) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8379) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8385) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8386) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8387) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8388) tree.c:2386: local_irq_restore(flags);
    (<0.0>,8391)
    (<0.0>,8393) fake_sched.h:43: return __running_cpu;
    (<0.0>,8397) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8399) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8403) fake_sched.h:43: return __running_cpu;
    (<0.0>,8407) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8412) tree.c:2389: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,8415)
    (<0.0>,8416) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8418) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8421) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8432) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8435)
    (<0.0>,8439) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8442) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8443) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8444) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8448) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8449) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8450) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8452) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8456) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,8465)
    (<0.0>,8467) fake_sched.h:43: return __running_cpu;
    (<0.0>,8470) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8472) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8474) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8475) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8477) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8484) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8485) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8487) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8493) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8494) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8495) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8496) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,8497) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,8501)
    (<0.0>,8502)
    (<0.0>,8503) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,8504) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,8511)
    (<0.0>,8512)
    (<0.0>,8513) tree.c:1584: local_irq_save(flags);
    (<0.0>,8516)
    (<0.0>,8518) fake_sched.h:43: return __running_cpu;
    (<0.0>,8522) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8524) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8528) fake_sched.h:43: return __running_cpu;
    (<0.0>,8532) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8537) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8539) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8540) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8541) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8543) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8544) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8546) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8549) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8551) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8552) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8554) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8557) tree.c:1589: local_irq_restore(flags);
    (<0.0>,8560)
    (<0.0>,8562) fake_sched.h:43: return __running_cpu;
    (<0.0>,8566) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8568) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8572) fake_sched.h:43: return __running_cpu;
    (<0.0>,8576) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8583) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,8585) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,8590) tree.c:2558: local_irq_save(flags);
    (<0.0>,8593)
    (<0.0>,8595) fake_sched.h:43: return __running_cpu;
    (<0.0>,8599) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8601) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8605) fake_sched.h:43: return __running_cpu;
    (<0.0>,8609) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8614) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8615) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8621)
    (<0.0>,8622)
    (<0.0>,8623) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,8626)
    (<0.0>,8627) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8629) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8630) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8632) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8638) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,8644)
    (<0.0>,8645) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8648)
    (<0.0>,8649) tree.c:453: return &rsp->node[0];
    (<0.0>,8653) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8654) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8656) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8660) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8661) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8663) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8666) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8667) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8668) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8672) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8675) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8678) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8681) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8682) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8685) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8687) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8690) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8693) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8696) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8697) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8699) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8702) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8706) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8708) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8710) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8713) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8716) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8719) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8720) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8722) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8725) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8729) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8731) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8733) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8736) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,8738) tree.c:494: }
    (<0.0>,8742) tree.c:2566: local_irq_restore(flags);
    (<0.0>,8745)
    (<0.0>,8747) fake_sched.h:43: return __running_cpu;
    (<0.0>,8751) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8753) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8757) fake_sched.h:43: return __running_cpu;
    (<0.0>,8761) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8767) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,8770)
    (<0.0>,8771) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8773) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8776) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8783) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8786)
    (<0.0>,8790) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8793) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8794) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8795) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8799) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8800) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8801) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8803) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8811) fake_sched.h:43: return __running_cpu;
    (<0.0>,8815) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,8824) tree.c:624: local_irq_save(flags);
    (<0.0>,8827)
    (<0.0>,8829) fake_sched.h:43: return __running_cpu;
    (<0.0>,8833) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8835) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8839) fake_sched.h:43: return __running_cpu;
    (<0.0>,8843) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8849) fake_sched.h:43: return __running_cpu;
    (<0.0>,8853) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8854) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8856) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8857) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8858) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8860) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8862) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8863) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8865) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8870) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8871) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8873) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8877) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8878) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8879) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8880) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,8882) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,8890)
    (<0.0>,8892) tree.c:634: local_irq_restore(flags);
    (<0.0>,8895)
    (<0.0>,8897) fake_sched.h:43: return __running_cpu;
    (<0.0>,8901) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8903) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8907) fake_sched.h:43: return __running_cpu;
    (<0.0>,8911) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8920) fake_sched.h:43: return __running_cpu;
    (<0.0>,8924)
    (<0.0>,8927) tree.c:580: local_irq_save(flags);
    (<0.0>,8930)
    (<0.0>,8932) fake_sched.h:43: return __running_cpu;
    (<0.0>,8936) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8938) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8942) fake_sched.h:43: return __running_cpu;
    (<0.0>,8946) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8959)
    (<0.0>,8961) fake_sched.h:43: return __running_cpu;
    (<0.0>,8965) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8966) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8968) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8969) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8970) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8976) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8977) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8982) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8983) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8984) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8985) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,8989) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,8991) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,8992) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8993) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9012)
    (<0.0>,9014)
    (<0.0>,9016) fake_sched.h:43: return __running_cpu;
    (<0.0>,9020) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9023) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,9027) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9028) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9029) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9033) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9034) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9035) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9037) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9042) fake_sched.h:43: return __running_cpu;
    (<0.0>,9045) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9047) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9049) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9050) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9053)
    (<0.0>,9056) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9059) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9060) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9061) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9065) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9066) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9067) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9069) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9074) fake_sched.h:43: return __running_cpu;
    (<0.0>,9077) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9079) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9081) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9082) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9085)
    (<0.0>,9088) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9091) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9092) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9093) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9097) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9098) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9099) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9101) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9108) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9111) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9112) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9113) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9115) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9116) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9118) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9121) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9127) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9128) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9131) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9136) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9137) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9138) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9152)
    (<0.0>,9154) tree.c:583: local_irq_restore(flags);
    (<0.0>,9157)
    (<0.0>,9159) fake_sched.h:43: return __running_cpu;
    (<0.0>,9163) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9165) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9169) fake_sched.h:43: return __running_cpu;
    (<0.0>,9173) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9179) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9182) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9187) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,9189) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,9198) fake_sched.h:43: return __running_cpu;
    (<0.0>,9202)
    (<0.0>,9203) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9206) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9211) tree.c:704: local_irq_save(flags);
    (<0.0>,9214)
    (<0.0>,9216) fake_sched.h:43: return __running_cpu;
    (<0.0>,9220) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9222) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9226) fake_sched.h:43: return __running_cpu;
    (<0.0>,9230) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9243)
    (<0.0>,9245) fake_sched.h:43: return __running_cpu;
    (<0.0>,9249) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9250) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9252) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9253) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9254) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9259) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9260) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9264) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9265) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9266) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9267) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,9271) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9273) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9274) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9275) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9289)
    (<0.0>,9290)
    (<0.0>,9292) fake_sched.h:43: return __running_cpu;
    (<0.0>,9296) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9300) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9303) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9304) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9305) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9307) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9308) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9310) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9313) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9320) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9321) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9324) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9329) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9330) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9331) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9336) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,9345)
    (<0.0>,9347) tree.c:707: local_irq_restore(flags);
    (<0.0>,9350)
    (<0.0>,9352) fake_sched.h:43: return __running_cpu;
    (<0.0>,9356) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9358) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9362) fake_sched.h:43: return __running_cpu;
    (<0.0>,9366) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9373) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,9385)
    (<0.0>,9386) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9389)
    (<0.0>,9390) tree.c:453: return &rsp->node[0];
    (<0.0>,9394) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9398) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,9402)
    (<0.0>,9405) fake_sched.h:43: return __running_cpu;
    (<0.0>,9409) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9413) fake_sched.h:43: return __running_cpu;
    (<0.0>,9417) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9422) fake_sched.h:43: return __running_cpu;
    (<0.0>,9426) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9429) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9430) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9436) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,9438) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,9441) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,9443) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,9444) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9447)
    (<0.0>,9448) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9450) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9451) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9453) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9461) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9462) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9465)
    (<0.0>,9466) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9468) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9469) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9471) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9478) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9479) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9480) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9483) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,9488)
    (<0.0>,9489) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,9490) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,9491) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9492) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9494) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9498) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,9499) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,9500) update.c:344: if (till_stall_check < 3) {
    (<0.0>,9503) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,9507) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,9512) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,9513) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9514) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9516) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9518) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9519) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9520) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9523) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9525) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9529) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9531) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9533) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9535) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9539) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,9543)
    (<0.0>,9544) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9545) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9550) fake_sched.h:43: return __running_cpu;
    (<0.0>,9554) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9556) fake_sched.h:43: return __running_cpu;
    (<0.0>,9560) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9566) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,9570)
    (<0.0>,9571) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,9573) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,9579) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9582) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9584) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9585) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9587) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9592) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,9596)
    (<0.0>,9599) fake_sched.h:43: return __running_cpu;
    (<0.0>,9603) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9607) fake_sched.h:43: return __running_cpu;
    (<0.0>,9611) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9616) fake_sched.h:43: return __running_cpu;
    (<0.0>,9620) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9623) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9624) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9631) fake_sched.h:43: return __running_cpu;
    (<0.0>,9634) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9636) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9638) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9639) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,9645)
    (<0.0>,9646) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9648) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9653) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9654) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9656) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9660) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9661) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9662) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9664) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9666) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9667) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9669) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9670) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9672) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9673) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9675) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9676) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9678) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9679) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9681) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9686) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9687) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9689) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9690) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9692) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9696) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9697) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9698) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9699) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9701) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9702) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9704) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9705) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9706) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9708) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9711) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9712) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9713) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9719)
    (<0.0>,9720)
    (<0.0>,9721)
    (<0.0>,9722) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9724) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9725) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9727) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9730) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9731) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9732) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9741)
    (<0.0>,9742)
    (<0.0>,9743)
    (<0.0>,9744) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9747) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9750) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9753) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9754) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9757) tree.c:1434: return false;
    (<0.0>,9759) tree.c:1483: }
    (<0.0>,9762) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9764) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9766) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9767) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9769) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9772) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9774) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9775) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9777) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9780) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,9782) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,9783) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9785) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9786) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9788) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9793) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9796) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9797) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,9799) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,9801) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,9803) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,9805) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,9806) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,9809)
    (<0.0>,9812) tree.c:1575: return ret;
    (<0.0>,9816) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,9819)
    (<0.0>,9823) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,9827)
    (<0.0>,9828) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9829) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9834) fake_sched.h:43: return __running_cpu;
    (<0.0>,9838) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9840) fake_sched.h:43: return __running_cpu;
    (<0.0>,9844) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9857) fake_sched.h:43: return __running_cpu;
    (<0.0>,9862) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,9868) fake_sched.h:43: return __running_cpu;
    (<0.0>,9873) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,9879) fake_sched.h:43: return __running_cpu;
    (<0.0>,9883) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,9895) fake_sched.h:43: return __running_cpu;
    (<0.0>,9899)
    (<0.0>,9902) tree.c:580: local_irq_save(flags);
    (<0.0>,9905)
    (<0.0>,9907) fake_sched.h:43: return __running_cpu;
    (<0.0>,9911) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9913) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9917) fake_sched.h:43: return __running_cpu;
    (<0.0>,9921) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9934)
    (<0.0>,9936) fake_sched.h:43: return __running_cpu;
    (<0.0>,9940) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9941) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9943) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9944) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9945) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9951) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9952) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9957) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9958) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9959) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9960) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,9964) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9966) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9967) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9968) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9987)
    (<0.0>,9989)
    (<0.0>,9991) fake_sched.h:43: return __running_cpu;
    (<0.0>,9995) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9998) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,10002) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10003) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10004) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10008) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10009) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10010) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10012) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10017) fake_sched.h:43: return __running_cpu;
    (<0.0>,10020) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10022) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10024) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10025) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10028)
    (<0.0>,10031) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10034) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10035) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10036) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10040) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10041) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10042) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10044) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10049) fake_sched.h:43: return __running_cpu;
    (<0.0>,10052) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10054) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10056) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10057) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10060)
    (<0.0>,10063) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10066) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10067) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10068) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10072) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10073) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10074) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10076) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10083) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10086) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10087) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10088) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10090) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10091) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10093) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10096) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10102) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10103) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10106) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10111) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10112) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10113) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10127)
    (<0.0>,10129) tree.c:583: local_irq_restore(flags);
    (<0.0>,10132)
    (<0.0>,10134) fake_sched.h:43: return __running_cpu;
    (<0.0>,10138) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10140) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10144) fake_sched.h:43: return __running_cpu;
    (<0.0>,10148) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10154) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10157) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10162) fake_sched.h:43: return __running_cpu;
    (<0.0>,10166)
    (<0.0>,10167) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10170) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10175) tree.c:704: local_irq_save(flags);
    (<0.0>,10178)
    (<0.0>,10180) fake_sched.h:43: return __running_cpu;
    (<0.0>,10184) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10186) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10190) fake_sched.h:43: return __running_cpu;
    (<0.0>,10194) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10207)
    (<0.0>,10209) fake_sched.h:43: return __running_cpu;
    (<0.0>,10213) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10214) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10216) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10217) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10218) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10223) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10224) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10228) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10229) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10230) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10231) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,10235) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10237) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10238) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10239) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10253)
    (<0.0>,10254)
    (<0.0>,10256) fake_sched.h:43: return __running_cpu;
    (<0.0>,10260) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10264) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10267) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10268) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10269) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10271) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10272) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10274) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10277) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10284) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10285) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10288) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10293) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10294) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10295) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10300) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,10309)
    (<0.0>,10311) tree.c:707: local_irq_restore(flags);
    (<0.0>,10314)
    (<0.0>,10316) fake_sched.h:43: return __running_cpu;
    (<0.0>,10320) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10322) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10326) fake_sched.h:43: return __running_cpu;
    (<0.0>,10330) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10345) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10347) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10349) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10350) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10352) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10357) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,10361)
    (<0.0>,10362) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,10364) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,10368) tree.c:1674: return 1;
    (<0.0>,10370) tree.c:1675: }
    (<0.0>,10375) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,10376) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,10377) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,10378) tree.c:1819: if (j > HZ) {
    (<0.0>,10381) tree.c:1823: ret = 0;
    (<0.0>,10383) tree.c:1825: if (!ret)
    (<0.0>,10386) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10387) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10389) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10391) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10395) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,10397) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,10401) fake_sched.h:43: return __running_cpu;
    (<0.0>,10405) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,10409) fake_sched.h:43: return __running_cpu;
    (<0.0>,10413) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10418) fake_sched.h:43: return __running_cpu;
    (<0.0>,10422) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,10432) tree.c:749: local_irq_save(flags);
    (<0.0>,10435)
    (<0.0>,10437) fake_sched.h:43: return __running_cpu;
    (<0.0>,10441) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10443) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10448) fake_sched.h:43: return __running_cpu;
    (<0.0>,10452) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10453) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10455) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10456) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10457) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10459) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10461) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10462) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10464) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10469) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10470) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10472) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10476) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10477) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10478) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10479) tree.c:754: if (oldval)
    (<0.0>,10487)
    (<0.0>,10489) tree.c:759: local_irq_restore(flags);
    (<0.0>,10492)
    (<0.0>,10494) fake_sched.h:43: return __running_cpu;
    (<0.0>,10498) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10500) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10508)
    (<0.0>,10513) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,10518) fake_sched.h:43: return __running_cpu;
    (<0.0>,10523) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,10531) fake_sched.h:43: return __running_cpu;
    (<0.0>,10536) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,10550) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10551) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10552) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10556) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10557) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10558) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10560) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10564) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10566) fake_sched.h:43: return __running_cpu;
    (<0.0>,10569) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10571) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10578)
    (<0.0>,10579)
    (<0.0>,10580) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10582) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10583) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10584) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10586) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10588) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10589) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10590) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10600)
    (<0.0>,10601)
    (<0.0>,10602) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,10607) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,10610)
    (<0.0>,10613) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,10616) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10618) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10621) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10623) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10626) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10628) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10631) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10633) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10636) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10638) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10640) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10641) tree.c:3140: return 1;
    (<0.0>,10643) tree.c:3176: }
    (<0.0>,10647) tree.c:3189: return 1;
    (<0.0>,10649) tree.c:3191: }
    (<0.0>,10656) fake_sched.h:43: return __running_cpu;
    (<0.0>,10660) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,10664) tree.c:2437: if (user)
    (<0.0>,10672) fake_sched.h:43: return __running_cpu;
    (<0.0>,10676) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,10678) fake_sched.h:43: return __running_cpu;
    (<0.0>,10682) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10688) fake_sched.h:43: return __running_cpu;
    (<0.0>,10692) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,10702)
    (<0.0>,10705) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10706) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10707) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10711) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10712) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10713) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10715) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10719) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,10728)
    (<0.0>,10730) fake_sched.h:43: return __running_cpu;
    (<0.0>,10733) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10735) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10737) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10738) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10740) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10747) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10748) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10750) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10756) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10757) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10758) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10759) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10760) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10764)
    (<0.0>,10765)
    (<0.0>,10766) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,10767) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,10774)
    (<0.0>,10775)
    (<0.0>,10776) tree.c:1584: local_irq_save(flags);
    (<0.0>,10779)
    (<0.0>,10781) fake_sched.h:43: return __running_cpu;
    (<0.0>,10785) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10787) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10791) fake_sched.h:43: return __running_cpu;
    (<0.0>,10795) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10800) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,10802) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,10803) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,10804) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,10806) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,10807) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,10809) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,10812) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,10814) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,10815) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,10817) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,10820) tree.c:1589: local_irq_restore(flags);
    (<0.0>,10823)
    (<0.0>,10825) fake_sched.h:43: return __running_cpu;
    (<0.0>,10829) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10831) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10835) fake_sched.h:43: return __running_cpu;
    (<0.0>,10839) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10846) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,10848) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,10851) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,10853) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,10856) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10858) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10859) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10860) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10869)
    (<0.0>,10870)
    (<0.0>,10871)
    (<0.0>,10872) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,10874) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,10875) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,10876) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,10878) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,10882)
    (<0.0>,10883)
    (<0.0>,10884) fake_sync.h:76: local_irq_save(flags);
    (<0.0>,10887)
    (<0.0>,10889) fake_sched.h:43: return __running_cpu;
    (<0.0>,10893) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10895) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10899) fake_sched.h:43: return __running_cpu;
    (<0.0>,10903) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10909) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,10910) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,10916) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10918) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10923) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10925) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10926) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10928) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10931) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,10933) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,10934) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,10936) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,10939) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,10941) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,10942) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,10943) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,10945) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,10946) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,10950) tree.c:2052: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,10952) tree.c:2052: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,10956)
    (<0.0>,10957)
    (<0.0>,10958) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,10959) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,10962) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,10965)
    (<0.0>,10967) fake_sched.h:43: return __running_cpu;
    (<0.0>,10971) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10973) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10977) fake_sched.h:43: return __running_cpu;
    (<0.0>,10981) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10992) tree.c:2558: local_irq_save(flags);
    (<0.0>,10995)
    (<0.0>,10997) fake_sched.h:43: return __running_cpu;
    (<0.0>,11001) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11003) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11007) fake_sched.h:43: return __running_cpu;
    (<0.0>,11011) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11016) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11017) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11023)
    (<0.0>,11024)
    (<0.0>,11025) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,11028)
    (<0.0>,11029) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11031) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11032) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11034) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11040) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,11042) tree.c:494: }
    (<0.0>,11046) tree.c:2566: local_irq_restore(flags);
    (<0.0>,11049)
    (<0.0>,11051) fake_sched.h:43: return __running_cpu;
    (<0.0>,11055) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11057) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11061) fake_sched.h:43: return __running_cpu;
    (<0.0>,11065) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11071) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11074)
    (<0.0>,11075) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11077) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11080) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11087) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11090)
    (<0.0>,11094) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11097) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11098) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11099) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11103) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11104) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11105) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11107) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11111) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,11120)
    (<0.0>,11122) fake_sched.h:43: return __running_cpu;
    (<0.0>,11125) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11127) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11129) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11130) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11132) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11139) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11140) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11142) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11148) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11149) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11150) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11151) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11152) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11156)
    (<0.0>,11157)
    (<0.0>,11158) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,11159) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,11166)
    (<0.0>,11167)
    (<0.0>,11168) tree.c:1584: local_irq_save(flags);
    (<0.0>,11171)
    (<0.0>,11173) fake_sched.h:43: return __running_cpu;
    (<0.0>,11177) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11179) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11183) fake_sched.h:43: return __running_cpu;
    (<0.0>,11187) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11192) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11194) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11195) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11196) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11198) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11199) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11201) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11204) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11206) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11207) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11209) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11212) tree.c:1589: local_irq_restore(flags);
    (<0.0>,11215)
    (<0.0>,11217) fake_sched.h:43: return __running_cpu;
    (<0.0>,11221) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11223) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11227) fake_sched.h:43: return __running_cpu;
    (<0.0>,11231) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11238) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11240) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11245) tree.c:2558: local_irq_save(flags);
    (<0.0>,11248)
    (<0.0>,11250) fake_sched.h:43: return __running_cpu;
    (<0.0>,11254) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11256) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11260) fake_sched.h:43: return __running_cpu;
    (<0.0>,11264) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11269) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11270) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11276)
    (<0.0>,11277)
    (<0.0>,11278) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,11281)
    (<0.0>,11282) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11284) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11285) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11287) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11293) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,11299)
    (<0.0>,11300) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11303)
    (<0.0>,11304) tree.c:453: return &rsp->node[0];
    (<0.0>,11308) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11309) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11311) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11315) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11316) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11318) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11321) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11322) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11323) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11327) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11330) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11333) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11336) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11337) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11340) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11342) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11345) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11348) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11351) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11352) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11354) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11357) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11361) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11363) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11365) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11368) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11371) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11374) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11375) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11377) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11380) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11384) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11386) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11388) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11391) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,11393) tree.c:494: }
    (<0.0>,11397) tree.c:2566: local_irq_restore(flags);
    (<0.0>,11400)
    (<0.0>,11402) fake_sched.h:43: return __running_cpu;
    (<0.0>,11406) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11408) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11412) fake_sched.h:43: return __running_cpu;
    (<0.0>,11416) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11422) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11425)
    (<0.0>,11426) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11428) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11431) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11438) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11441)
    (<0.0>,11445) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11448) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11449) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11450) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11454) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11455) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11456) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11458) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11466) fake_sched.h:43: return __running_cpu;
    (<0.0>,11470) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,11479) tree.c:624: local_irq_save(flags);
    (<0.0>,11482)
    (<0.0>,11484) fake_sched.h:43: return __running_cpu;
    (<0.0>,11488) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11490) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11494) fake_sched.h:43: return __running_cpu;
    (<0.0>,11498) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11504) fake_sched.h:43: return __running_cpu;
    (<0.0>,11508) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11509) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11511) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11512) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11513) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11515) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11517) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11518) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11520) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11525) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11526) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11528) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11532) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11533) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11534) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11535) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,11537) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,11545)
    (<0.0>,11547) tree.c:634: local_irq_restore(flags);
    (<0.0>,11550)
    (<0.0>,11552) fake_sched.h:43: return __running_cpu;
    (<0.0>,11556) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11558) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11562) fake_sched.h:43: return __running_cpu;
    (<0.0>,11566) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11575) fake_sched.h:43: return __running_cpu;
    (<0.0>,11579)
    (<0.0>,11582) tree.c:580: local_irq_save(flags);
    (<0.0>,11585)
    (<0.0>,11587) fake_sched.h:43: return __running_cpu;
    (<0.0>,11591) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11593) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11597) fake_sched.h:43: return __running_cpu;
    (<0.0>,11601) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11614)
    (<0.0>,11616) fake_sched.h:43: return __running_cpu;
    (<0.0>,11620) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11621) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11623) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11624) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11625) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11631) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11632) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11637) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11638) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11639) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11640) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,11644) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11646) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11647) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11648) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11667)
    (<0.0>,11669)
    (<0.0>,11671) fake_sched.h:43: return __running_cpu;
    (<0.0>,11675) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11678) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,11682) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11683) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11684) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11688) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11689) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11690) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11692) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11697) fake_sched.h:43: return __running_cpu;
    (<0.0>,11700) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11702) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11704) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11705) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11708)
    (<0.0>,11711) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11714) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11715) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11716) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11720) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11721) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11722) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11724) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11729) fake_sched.h:43: return __running_cpu;
    (<0.0>,11732) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11734) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11736) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11737) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11740)
    (<0.0>,11743) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11746) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11747) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11748) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11752) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11753) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11754) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11756) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11763) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11766) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11767) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11768) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11770) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11771) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11773) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11776) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11782) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11783) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11786) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11791) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11792) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11793) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11807)
    (<0.0>,11809) tree.c:583: local_irq_restore(flags);
    (<0.0>,11812)
    (<0.0>,11814) fake_sched.h:43: return __running_cpu;
    (<0.0>,11818) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11820) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11824) fake_sched.h:43: return __running_cpu;
    (<0.0>,11828) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11834) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11837) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11842) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11844) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11846) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11850) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11852) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11859) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11861) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11863) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11867) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11869) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11876) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11878) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11880) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11884) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11886) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11893) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11895) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11897) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11901) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11903) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11910) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11912) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11914) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11918) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11920) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
      (<0.1>,725) fake_sync.h:241: while (!x->done)
      (<0.1>,732) fake_sched.h:43: return __running_cpu;
      (<0.1>,736)
      (<0.1>,737) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,740) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,745) tree.c:704: local_irq_save(flags);
      (<0.1>,748)
      (<0.1>,750) fake_sched.h:43: return __running_cpu;
      (<0.1>,754) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,756) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,760) fake_sched.h:43: return __running_cpu;
      (<0.1>,764) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,777)
      (<0.1>,779) fake_sched.h:43: return __running_cpu;
      (<0.1>,783) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,784) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,786) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,787) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,788) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,793) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,794) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,798) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,799) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,800) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,801) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,805) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,807) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,808) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,809) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,823)
      (<0.1>,824)
      (<0.1>,826) fake_sched.h:43: return __running_cpu;
      (<0.1>,830) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,834) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,837) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,838) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,839) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,841) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,842) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,844) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,847) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,854) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,855) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,858) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,864) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,865) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,870) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,879)
      (<0.1>,881) tree.c:707: local_irq_restore(flags);
      (<0.1>,884)
      (<0.1>,886) fake_sched.h:43: return __running_cpu;
      (<0.1>,890) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,892) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,896) fake_sched.h:43: return __running_cpu;
      (<0.1>,900) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,911)
      (<0.1>,917) litmus.c:91: y = 1;
      (<0.1>,919) fake_sched.h:43: return __running_cpu;
      (<0.1>,923)
      (<0.1>,926) tree.c:580: local_irq_save(flags);
      (<0.1>,929)
      (<0.1>,931) fake_sched.h:43: return __running_cpu;
      (<0.1>,935) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,937) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,941) fake_sched.h:43: return __running_cpu;
      (<0.1>,945) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,958)
      (<0.1>,960) fake_sched.h:43: return __running_cpu;
      (<0.1>,964) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,965) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,967) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,968) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,969) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,975) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,976) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,981) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,982) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,983) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,984) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,988) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,990) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,991) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,992) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1011)
      (<0.1>,1013)
      (<0.1>,1015) fake_sched.h:43: return __running_cpu;
      (<0.1>,1019) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1022) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,1026) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1027) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1028) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1033) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1034) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1041) fake_sched.h:43: return __running_cpu;
      (<0.1>,1044) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1046) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1048) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1049) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1052)
      (<0.1>,1055) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1058) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1059) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1060) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1065) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1066) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1073) fake_sched.h:43: return __running_cpu;
      (<0.1>,1076) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1078) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1080) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1081) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1084)
      (<0.1>,1087) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1090) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1091) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1092) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1097) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1098) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1107) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1110) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1111) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1112) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1117) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1120) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1126) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1127) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1130) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1135) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1136) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1137) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1151)
      (<0.1>,1153) tree.c:583: local_irq_restore(flags);
      (<0.1>,1156)
      (<0.1>,1158) fake_sched.h:43: return __running_cpu;
      (<0.1>,1162) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1164) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1168) fake_sched.h:43: return __running_cpu;
      (<0.1>,1172) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1178) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1181) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6270) litmus.c:69: r_y = y;
  (<0>,6271) litmus.c:69: r_y = y;
  (<0>,6282) fake_sched.h:43: return __running_cpu;
  (<0>,6286)
  (<0>,6289) tree.c:580: local_irq_save(flags);
  (<0>,6292)
  (<0>,6294) fake_sched.h:43: return __running_cpu;
  (<0>,6298) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6300) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6304) fake_sched.h:43: return __running_cpu;
  (<0>,6308) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6321)
  (<0>,6323) fake_sched.h:43: return __running_cpu;
  (<0>,6327) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6328) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6330) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6331) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6332) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6338) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6339) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6344) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6345) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6346) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6347) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,6351) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,6353) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,6354) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,6355) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,6374)
  (<0>,6376)
  (<0>,6378) fake_sched.h:43: return __running_cpu;
  (<0>,6382) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6385) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,6389) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6390) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6391) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6395) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6396) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6397) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6399) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6404) fake_sched.h:43: return __running_cpu;
  (<0>,6407) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6409) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6411) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6412) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,6415)
  (<0>,6418) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6421) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6422) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6423) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6427) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6428) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6429) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6431) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6436) fake_sched.h:43: return __running_cpu;
  (<0>,6439) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6441) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6443) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6444) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,6447)
  (<0>,6450) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6453) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6454) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6455) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6459) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6460) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6461) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6463) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6470) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6473) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6474) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6475) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6477) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6478) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6480) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6483) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6489) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6490) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6493) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6498) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6499) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6500) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6514)
  (<0>,6516) tree.c:583: local_irq_restore(flags);
  (<0>,6519)
  (<0>,6521) fake_sched.h:43: return __running_cpu;
  (<0>,6525) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6527) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6531) fake_sched.h:43: return __running_cpu;
  (<0>,6535) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6541) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6544) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6549) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,6553) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,6556) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,6560) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,6561) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,6562) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
             Error: Assertion violation at (<0>,6565): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpv7fwhowm/tmp71_kux98.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DFORCE_FAILURE_5 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpv7fwhowm/tmp9pxt6zd1.ll /tmp/tmpv7fwhowm/tmp71_kux98.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=4 --preemption-bounding=PB --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpv7fwhowm/tmp9pxt6zd1.ll
Total wall-clock time: 2.34 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 497 (also 20 sleepset blocked, 0 schedulings and 198 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpukmhibi4/tmpqfuwv5mo.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DLIVENESS_CHECK_1 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpukmhibi4/tmppr9soatk.ll /tmp/tmpukmhibi4/tmpqfuwv5mo.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpukmhibi4/tmppr9soatk.ll
Total wall-clock time: 10.66 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 497 (also 20 sleepset blocked, 0 schedulings and 198 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmph29bh0mh/tmpx8u7jjg9.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DLIVENESS_CHECK_2 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmph29bh0mh/tmp2k55946k.ll /tmp/tmph29bh0mh/tmpx8u7jjg9.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmph29bh0mh/tmp2k55946k.ll
Total wall-clock time: 10.61 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 635 (also 20 sleepset blocked, 0 schedulings and 249 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp20z9kyz3/tmp9kst9blm.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DLIVENESS_CHECK_3 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp20z9kyz3/tmpdfa1wie8.ll /tmp/tmp20z9kyz3/tmp9kst9blm.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp20z9kyz3/tmpdfa1wie8.ll
Total wall-clock time: 14.04 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 2282 (also 187 sleepset blocked, 0 schedulings and 2247 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp9ydxr63t/tmpbkp_qe3i.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp9ydxr63t/tmp0p018zoy.ll /tmp/tmp9ydxr63t/tmpbkp_qe3i.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp9ydxr63t/tmp0p018zoy.ll
Total wall-clock time: 122.71 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 287 (also 13 sleepset blocked, 0 schedulings and 225 branches were rejected due to the bound) 

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,75) tree_plugin.h:731: pr_info("Hierarchical RCU implementation.\n");
  (<0>,79) tree_plugin.h:76: if (rcu_fanout_exact)
  (<0>,82) tree_plugin.h:87: if (rcu_fanout_leaf != RCU_FANOUT_LEAF)
  (<0>,94) tree.c:4147: ulong d;
  (<0>,95) tree.c:4159: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,98) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,99) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,101) tree.c:4161: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,104) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,105) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,107) tree.c:4165: if (rcu_fanout_leaf == RCU_FANOUT_LEAF &&
  (<0>,120)
  (<0>,121) tree.c:4056: static void __init rcu_init_one(struct rcu_state *rsp)
  (<0>,122) tree.c:4067: int i;
  (<0>,125) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,128) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,130) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,131) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,134) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,137) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,141) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,143) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,145) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,147) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,151) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,153) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,154) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,165)
  (<0>,166) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,167) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,170) tree.c:4041: int ccur;
  (<0>,171) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,173) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,175) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,178) tree.c:4046: ccur = levelcnt[i];
  (<0>,180) tree.c:4046: ccur = levelcnt[i];
  (<0>,182) tree.c:4046: ccur = levelcnt[i];
  (<0>,183) tree.c:4046: ccur = levelcnt[i];
  (<0>,184) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,185) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,188) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,190) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,192) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,194) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,195) tree.c:4048: cprv = ccur;
  (<0>,196) tree.c:4048: cprv = ccur;
  (<0>,198) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,200) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,202) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,207) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,208) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,210) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,211) tree.c:4085: fl_mask <<= 1;
  (<0>,215) tree.c:4085: fl_mask <<= 1;
  (<0>,216) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,218) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,220) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,223) tree.c:4090: cpustride *= levelspread[i];
  (<0>,226) tree.c:4090: cpustride *= levelspread[i];
  (<0>,227) tree.c:4090: cpustride *= levelspread[i];
  (<0>,229) tree.c:4090: cpustride *= levelspread[i];
  (<0>,230) tree.c:4091: rnp = rsp->level[i];
  (<0>,232) tree.c:4091: rnp = rsp->level[i];
  (<0>,235) tree.c:4091: rnp = rsp->level[i];
  (<0>,236) tree.c:4091: rnp = rsp->level[i];
  (<0>,237) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,240) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,243) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,246) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,250)
  (<0>,251) fake_sync.h:75: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,252) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,258) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,262)
  (<0>,263) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,264) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,270) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,273) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,275) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,276) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,278) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,279) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,281) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,282) tree.c:4101: rnp->qsmask = 0;
  (<0>,284) tree.c:4101: rnp->qsmask = 0;
  (<0>,285) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,287) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,288) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,291) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,293) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,294) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,296) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,299) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,301) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,302) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,304) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,307) tree.c:4107: if (i == 0) {
  (<0>,310) tree.c:4108: rnp->grpnum = 0;
  (<0>,312) tree.c:4108: rnp->grpnum = 0;
  (<0>,313) tree.c:4109: rnp->grpmask = 0;
  (<0>,315) tree.c:4109: rnp->grpmask = 0;
  (<0>,316) tree.c:4110: rnp->parent = NULL;
  (<0>,318) tree.c:4110: rnp->parent = NULL;
  (<0>,320) tree.c:4117: rnp->level = i;
  (<0>,322) tree.c:4117: rnp->level = i;
  (<0>,324) tree.c:4117: rnp->level = i;
  (<0>,325) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,329)
  (<0>,330) fake_defs.h:358: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,331) fake_defs.h:360: list->next = list;
  (<0>,333) fake_defs.h:360: list->next = list;
  (<0>,334) fake_defs.h:361: list->prev = list;
  (<0>,335) fake_defs.h:361: list->prev = list;
  (<0>,337) fake_defs.h:361: list->prev = list;
  (<0>,339) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,342) tree_plugin.h:2431: }
  (<0>,352) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,356)
  (<0>,357) fake_sync.h:141: void spin_lock_init(raw_spinlock_t *l)
  (<0>,358) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,363) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,365) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,366) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,368) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,370) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,371) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,374) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,378) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,380) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,382) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,389) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,392) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,395) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,396) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,397) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,399) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,400) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,407)
  (<0>,408) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,409) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,411) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,412) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,415) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,417) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,418) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,421) fake_defs.h:635: if (offset & mask)
  (<0>,422) fake_defs.h:635: if (offset & mask)
  (<0>,426) fake_defs.h:636: return cpu;
  (<0>,427) fake_defs.h:636: return cpu;
  (<0>,429) fake_defs.h:639: }
  (<0>,431) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,432) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,436) tree.c:4132: while (i > rnp->grphi)
  (<0>,437) tree.c:4132: while (i > rnp->grphi)
  (<0>,439) tree.c:4132: while (i > rnp->grphi)
  (<0>,442) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,443) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,445) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,447) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,450) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,451) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,452) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,475)
  (<0>,476) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,477) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,479) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,481) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,483) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,484) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,487)
  (<0>,488) tree.c:623: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,492) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,496) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,497) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,498) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,500) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,504)
  (<0>,505) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,506) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,509) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,511) fake_sched.h:43: return __running_cpu;
  (<0>,515) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,517) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,521) fake_sched.h:43: return __running_cpu;
  (<0>,525) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,531) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,532) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,539) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,540) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,542) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,544) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,548) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,550) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,551) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,554) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,556) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,557) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,559) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,561) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,566) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,567) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,569) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,571) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,575) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,576) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,577) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,578) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,579) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,581) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,584) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,585) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,586) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,591) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,592) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,593) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,595) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,598) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,599) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,600) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,604) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,605) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,606) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,607) tree.c:3776: rdp->cpu = cpu;
  (<0>,608) tree.c:3776: rdp->cpu = cpu;
  (<0>,610) tree.c:3776: rdp->cpu = cpu;
  (<0>,611) tree.c:3777: rdp->rsp = rsp;
  (<0>,612) tree.c:3777: rdp->rsp = rsp;
  (<0>,614) tree.c:3777: rdp->rsp = rsp;
  (<0>,615) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,618) tree_plugin.h:2448: }
  (<0>,623) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,624) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,625) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,627) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,631)
  (<0>,632) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,633) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,634) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,637) fake_sync.h:93: local_irq_restore(flags);
  (<0>,640) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,642) fake_sched.h:43: return __running_cpu;
  (<0>,646) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,648) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,652) fake_sched.h:43: return __running_cpu;
  (<0>,656) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,666) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,667) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,674)
  (<0>,675)
  (<0>,676) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,678) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,679) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,682) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,684) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,685) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,688) fake_defs.h:635: if (offset & mask)
  (<0>,689) fake_defs.h:635: if (offset & mask)
  (<0>,693) fake_defs.h:636: return cpu;
  (<0>,694) fake_defs.h:636: return cpu;
  (<0>,696) fake_defs.h:639: }
  (<0>,698) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,699) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,703) tree.c:4132: while (i > rnp->grphi)
  (<0>,704) tree.c:4132: while (i > rnp->grphi)
  (<0>,706) tree.c:4132: while (i > rnp->grphi)
  (<0>,709) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,710) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,712) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,714) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,717) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,718) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,719) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,742)
  (<0>,743)
  (<0>,744) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,746) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,748) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,750) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,751) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,754)
  (<0>,755) tree.c:625: return &rsp->node[0];
  (<0>,759) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,763) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,764) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,765) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,767) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,771)
  (<0>,772)
  (<0>,773) fake_sync.h:83: local_irq_save(flags);
  (<0>,776) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,778) fake_sched.h:43: return __running_cpu;
  (<0>,782) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,784) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,788) fake_sched.h:43: return __running_cpu;
  (<0>,792) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,798) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,799) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,806) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,807) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,809) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,811) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,815) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,817) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,818) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,821) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,823) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,824) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,826) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,828) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,833) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,834) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,836) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,838) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,842) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,843) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,844) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,845) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,846) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,848) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,851) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,852) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,853) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,858) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,859) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,860) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,862) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,865) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,866) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,867) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,871) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,872) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,873) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,874) tree.c:3776: rdp->cpu = cpu;
  (<0>,875) tree.c:3776: rdp->cpu = cpu;
  (<0>,877) tree.c:3776: rdp->cpu = cpu;
  (<0>,878) tree.c:3777: rdp->rsp = rsp;
  (<0>,879) tree.c:3777: rdp->rsp = rsp;
  (<0>,881) tree.c:3777: rdp->rsp = rsp;
  (<0>,882) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,885) tree_plugin.h:2448: }
  (<0>,890) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,891) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,892) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,894) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,898)
  (<0>,899)
  (<0>,900) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,901) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,904) fake_sync.h:93: local_irq_restore(flags);
  (<0>,907) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,909) fake_sched.h:43: return __running_cpu;
  (<0>,913) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,915) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,919) fake_sched.h:43: return __running_cpu;
  (<0>,923) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,933) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,934) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,941)
  (<0>,942)
  (<0>,943) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,945) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,946) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,949) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,951) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,952) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,955) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,957) fake_defs.h:639: }
  (<0>,959) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,960) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,963) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,968)
  (<0>,969) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,970) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,971) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,972) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,974) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,979)
  (<0>,980) fake_defs.h:364: static inline void __list_add(struct list_head *new,
  (<0>,981) fake_defs.h:365: struct list_head *prev,
  (<0>,982) fake_defs.h:366: struct list_head *next)
  (<0>,983) fake_defs.h:368: next->prev = new;
  (<0>,985) fake_defs.h:368: next->prev = new;
  (<0>,986) fake_defs.h:369: new->next = next;
  (<0>,987) fake_defs.h:369: new->next = next;
  (<0>,989) fake_defs.h:369: new->next = next;
  (<0>,990) fake_defs.h:370: new->prev = prev;
  (<0>,991) fake_defs.h:370: new->prev = prev;
  (<0>,993) fake_defs.h:370: new->prev = prev;
  (<0>,994) fake_defs.h:371: prev->next = new;
  (<0>,995) fake_defs.h:371: prev->next = new;
  (<0>,997) fake_defs.h:371: prev->next = new;
  (<0>,1009)
  (<0>,1010) tree.c:4066: int cpustride = 1;
  (<0>,1011) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1014) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1017) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1019) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1020) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1023) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1026) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1027) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1030) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1032) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1034) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1036) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1037) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1040) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1042) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1043) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1054)
  (<0>,1055)
  (<0>,1056) tree.c:4036: if (rcu_fanout_exact) {
  (<0>,1059) tree.c:4044: cprv = nr_cpu_ids;
  (<0>,1060) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1062) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1064) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1067) tree.c:4046: ccur = levelcnt[i];
  (<0>,1069) tree.c:4046: ccur = levelcnt[i];
  (<0>,1071) tree.c:4046: ccur = levelcnt[i];
  (<0>,1072) tree.c:4046: ccur = levelcnt[i];
  (<0>,1073) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1074) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1077) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1079) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1081) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1083) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1084) tree.c:4048: cprv = ccur;
  (<0>,1085) tree.c:4048: cprv = ccur;
  (<0>,1087) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1089) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1091) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1096) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1097) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1099) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1100) tree.c:4085: fl_mask <<= 1;
  (<0>,1104) tree.c:4085: fl_mask <<= 1;
  (<0>,1105) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1107) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1109) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1112) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1115) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1116) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1118) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1119) tree.c:4091: rnp = rsp->level[i];
  (<0>,1121) tree.c:4091: rnp = rsp->level[i];
  (<0>,1124) tree.c:4091: rnp = rsp->level[i];
  (<0>,1125) tree.c:4091: rnp = rsp->level[i];
  (<0>,1126) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1128) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1129) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1132) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1135) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,1139)
  (<0>,1140) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1141) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1147) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1151)
  (<0>,1152) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1153) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1159) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1161) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1162) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1164) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1165) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1167) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1168) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1170) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1171) tree.c:4101: rnp->qsmask = 0;
  (<0>,1173) tree.c:4101: rnp->qsmask = 0;
  (<0>,1174) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1176) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1177) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1178) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1180) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1182) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1183) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1185) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1188) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1190) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1191) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1193) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1196) tree.c:4107: if (i == 0) {
  (<0>,1199) tree.c:4108: rnp->grpnum = 0;
  (<0>,1201) tree.c:4108: rnp->grpnum = 0;
  (<0>,1202) tree.c:4109: rnp->grpmask = 0;
  (<0>,1204) tree.c:4109: rnp->grpmask = 0;
  (<0>,1205) tree.c:4110: rnp->parent = NULL;
  (<0>,1207) tree.c:4110: rnp->parent = NULL;
  (<0>,1209) tree.c:4117: rnp->level = i;
  (<0>,1211) tree.c:4117: rnp->level = i;
  (<0>,1213) tree.c:4117: rnp->level = i;
  (<0>,1214) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1218)
  (<0>,1219) fake_defs.h:360: list->next = list;
  (<0>,1220) fake_defs.h:360: list->next = list;
  (<0>,1222) fake_defs.h:360: list->next = list;
  (<0>,1223) fake_defs.h:361: list->prev = list;
  (<0>,1224) fake_defs.h:361: list->prev = list;
  (<0>,1226) fake_defs.h:361: list->prev = list;
  (<0>,1228) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,1231) tree_plugin.h:2431: }
  (<0>,1241) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,1245)
  (<0>,1246) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1247) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1252) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1254) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1255) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1257) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1259) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1260) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1263) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1267) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1269) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1271) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1278) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1281) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1284) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1285) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1286) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1288) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1289) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1296)
  (<0>,1297)
  (<0>,1298) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1300) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1301) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1304) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1306) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1307) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1310) fake_defs.h:635: if (offset & mask)
  (<0>,1311) fake_defs.h:635: if (offset & mask)
  (<0>,1315) fake_defs.h:636: return cpu;
  (<0>,1316) fake_defs.h:636: return cpu;
  (<0>,1318) fake_defs.h:639: }
  (<0>,1320) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1321) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1325) tree.c:4132: while (i > rnp->grphi)
  (<0>,1326) tree.c:4132: while (i > rnp->grphi)
  (<0>,1328) tree.c:4132: while (i > rnp->grphi)
  (<0>,1331) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1332) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1334) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1336) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1339) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1340) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1341) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1364)
  (<0>,1365)
  (<0>,1366) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1368) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1370) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1372) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1373) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1376)
  (<0>,1377) tree.c:625: return &rsp->node[0];
  (<0>,1381) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1385) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1386) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1387) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1389) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1393)
  (<0>,1394)
  (<0>,1395) fake_sync.h:83: local_irq_save(flags);
  (<0>,1398) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1400) fake_sched.h:43: return __running_cpu;
  (<0>,1404) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1406) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1410) fake_sched.h:43: return __running_cpu;
  (<0>,1414) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1420) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1421) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1428) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1429) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1431) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1433) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1437) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1439) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1440) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1443) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1445) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1446) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1448) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1450) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1455) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1456) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1458) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1460) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1464) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1465) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1466) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1467) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1468) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1470) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1473) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1474) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1475) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1480) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1481) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1482) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1484) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1487) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1488) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1489) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1493) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1494) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1495) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1496) tree.c:3776: rdp->cpu = cpu;
  (<0>,1497) tree.c:3776: rdp->cpu = cpu;
  (<0>,1499) tree.c:3776: rdp->cpu = cpu;
  (<0>,1500) tree.c:3777: rdp->rsp = rsp;
  (<0>,1501) tree.c:3777: rdp->rsp = rsp;
  (<0>,1503) tree.c:3777: rdp->rsp = rsp;
  (<0>,1504) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1507) tree_plugin.h:2448: }
  (<0>,1512) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1513) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1514) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1516) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1520)
  (<0>,1521)
  (<0>,1522) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1523) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1526) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1529) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1531) fake_sched.h:43: return __running_cpu;
  (<0>,1535) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1541) fake_sched.h:43: return __running_cpu;
  (<0>,1545) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1555) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1556) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1563)
  (<0>,1564)
  (<0>,1565) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1567) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1568) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1571) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1573) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1574) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1577) fake_defs.h:635: if (offset & mask)
  (<0>,1578) fake_defs.h:635: if (offset & mask)
  (<0>,1582) fake_defs.h:636: return cpu;
  (<0>,1583) fake_defs.h:636: return cpu;
  (<0>,1585) fake_defs.h:639: }
  (<0>,1587) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1588) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1592) tree.c:4132: while (i > rnp->grphi)
  (<0>,1593) tree.c:4132: while (i > rnp->grphi)
  (<0>,1595) tree.c:4132: while (i > rnp->grphi)
  (<0>,1598) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1599) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1601) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1603) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1606) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1607) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1608) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1631)
  (<0>,1632)
  (<0>,1633) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1635) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1637) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1639) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1640) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1643)
  (<0>,1644) tree.c:625: return &rsp->node[0];
  (<0>,1648) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1652) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1653) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1654) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1656) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1660)
  (<0>,1661)
  (<0>,1662) fake_sync.h:83: local_irq_save(flags);
  (<0>,1665) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1667) fake_sched.h:43: return __running_cpu;
  (<0>,1671) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1673) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1677) fake_sched.h:43: return __running_cpu;
  (<0>,1681) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1687) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1688) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1695) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1696) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1698) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1700) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1704) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1706) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1707) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1710) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1712) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1713) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1715) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1717) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1722) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1723) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1725) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1727) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1731) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1732) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1733) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1734) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1735) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1737) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1740) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1741) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1742) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1747) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1748) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1749) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1751) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1754) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1755) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1756) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1760) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1761) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1762) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1763) tree.c:3776: rdp->cpu = cpu;
  (<0>,1764) tree.c:3776: rdp->cpu = cpu;
  (<0>,1766) tree.c:3776: rdp->cpu = cpu;
  (<0>,1767) tree.c:3777: rdp->rsp = rsp;
  (<0>,1768) tree.c:3777: rdp->rsp = rsp;
  (<0>,1770) tree.c:3777: rdp->rsp = rsp;
  (<0>,1771) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1774) tree_plugin.h:2448: }
  (<0>,1779) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1780) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1781) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1783) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1787)
  (<0>,1788)
  (<0>,1789) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1790) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1793) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1796) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1798) fake_sched.h:43: return __running_cpu;
  (<0>,1802) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1804) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1808) fake_sched.h:43: return __running_cpu;
  (<0>,1812) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1822) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1823) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1830)
  (<0>,1831)
  (<0>,1832) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1834) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1835) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1838) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1840) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1841) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1844) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,1846) fake_defs.h:639: }
  (<0>,1848) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1849) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1852) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1857)
  (<0>,1858)
  (<0>,1859) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1860) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1861) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1863) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1868)
  (<0>,1869)
  (<0>,1870)
  (<0>,1871) fake_defs.h:368: next->prev = new;
  (<0>,1872) fake_defs.h:368: next->prev = new;
  (<0>,1874) fake_defs.h:368: next->prev = new;
  (<0>,1875) fake_defs.h:369: new->next = next;
  (<0>,1876) fake_defs.h:369: new->next = next;
  (<0>,1878) fake_defs.h:369: new->next = next;
  (<0>,1879) fake_defs.h:370: new->prev = prev;
  (<0>,1880) fake_defs.h:370: new->prev = prev;
  (<0>,1882) fake_defs.h:370: new->prev = prev;
  (<0>,1883) fake_defs.h:371: prev->next = new;
  (<0>,1884) fake_defs.h:371: prev->next = new;
  (<0>,1886) fake_defs.h:371: prev->next = new;
  (<0>,1890) tree.c:4251: if (dump_tree)
  (<0>,1899) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1901) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1902) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1909)
  (<0>,1910)
  (<0>,1911) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1913) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1914) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1917) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1919) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1920) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1923) fake_defs.h:635: if (offset & mask)
  (<0>,1924) fake_defs.h:635: if (offset & mask)
  (<0>,1928) fake_defs.h:636: return cpu;
  (<0>,1929) fake_defs.h:636: return cpu;
  (<0>,1931) fake_defs.h:639: }
  (<0>,1933) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1934) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1937) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,1945)
  (<0>,1946) tree.c:3829: int rcutree_prepare_cpu(unsigned int cpu)
  (<0>,1947) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1948) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1952) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1953) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1954) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1956) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1960) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1961) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1987)
  (<0>,1988) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1989) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1991) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1993) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1995) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1996) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1999)
  (<0>,2000) tree.c:625: return &rsp->node[0];
  (<0>,2004) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2008) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2009) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2010) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2012) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2016)
  (<0>,2017)
  (<0>,2018) fake_sync.h:83: local_irq_save(flags);
  (<0>,2021) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2023) fake_sched.h:43: return __running_cpu;
  (<0>,2027) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2029) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2033) fake_sched.h:43: return __running_cpu;
  (<0>,2037) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2043) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2044) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2051) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2053) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2054) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2056) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2057) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2059) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2060) tree.c:3800: rdp->blimit = blimit;
  (<0>,2061) tree.c:3800: rdp->blimit = blimit;
  (<0>,2063) tree.c:3800: rdp->blimit = blimit;
  (<0>,2064) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2066) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2069) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2072)
  (<0>,2073) tree.c:1551: static void init_callback_list(struct rcu_data *rdp)
  (<0>,2076) tree_plugin.h:2469: return false;
  (<0>,2079) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2083)
  (<0>,2084) tree.c:1539: static void init_default_callback_list(struct rcu_data *rdp)
  (<0>,2086) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2087) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2089) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2092) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2094) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2096) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2099) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2101) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2103) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2105) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2108) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2110) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2112) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2115) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2117) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2119) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2121) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2124) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2126) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2128) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2131) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2133) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2135) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2137) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2140) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2142) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2144) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2147) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2149) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2151) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2153) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2160) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2162) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2164) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2165) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2167) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2170) tree_plugin.h:2902: }
  (<0>,2172) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2173) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2175) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2178) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2179) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2180) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2183) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2185) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2188) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2189) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2192)
  (<0>,2193) tree.h:721: static inline void raw_spin_unlock_rcu_node(struct rcu_node *rnp)
  (<0>,2197)
  (<0>,2198) fake_sync.h:120: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2199) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2205) tree.c:3814: rnp = rdp->mynode;
  (<0>,2207) tree.c:3814: rnp = rdp->mynode;
  (<0>,2208) tree.c:3814: rnp = rdp->mynode;
  (<0>,2209) tree.c:3815: mask = rdp->grpmask;
  (<0>,2211) tree.c:3815: mask = rdp->grpmask;
  (<0>,2212) tree.c:3815: mask = rdp->grpmask;
  (<0>,2213) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2216)
  (<0>,2217) tree.h:715: static inline void raw_spin_lock_rcu_node(struct rcu_node *rnp)
  (<0>,2221) fake_sync.h:115: preempt_disable();
  (<0>,2223) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2224) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2231) tree.c:3817: if (!rdp->beenonline)
  (<0>,2233) tree.c:3817: if (!rdp->beenonline)
  (<0>,2237) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2242) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2243) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2244) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2245) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2247) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2249) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2250) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2252) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2255) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2256) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2257) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2259) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2260) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2265) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2266) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2267) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2268) fake_defs.h:237: switch (size) {
  (<0>,2270) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2272) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2273) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2275) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2278) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2279) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2280) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2282) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2284) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2285) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2287) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2288) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2290) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2291) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2293) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2294) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2296) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2297) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2301) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2302) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2305) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2306) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2308) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2309) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2311) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2317) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2318) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2319) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2321) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2325)
  (<0>,2326)
  (<0>,2327) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2328) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2331) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2334) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2336) fake_sched.h:43: return __running_cpu;
  (<0>,2340) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2342) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2346) fake_sched.h:43: return __running_cpu;
  (<0>,2350) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2360) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2363) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2364) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2365) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2369) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2370) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2371) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2373) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2377) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2378) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2404)
  (<0>,2405)
  (<0>,2406) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2408) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2410) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2412) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2413) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2416)
  (<0>,2417) tree.c:625: return &rsp->node[0];
  (<0>,2421) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2425) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2426) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2427) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2429) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2433)
  (<0>,2434)
  (<0>,2435) fake_sync.h:83: local_irq_save(flags);
  (<0>,2438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2440) fake_sched.h:43: return __running_cpu;
  (<0>,2444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2446) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2450) fake_sched.h:43: return __running_cpu;
  (<0>,2454) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2460) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2461) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2468) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2470) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2471) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2473) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2474) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2476) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2477) tree.c:3800: rdp->blimit = blimit;
  (<0>,2478) tree.c:3800: rdp->blimit = blimit;
  (<0>,2480) tree.c:3800: rdp->blimit = blimit;
  (<0>,2481) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2483) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2486) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2489)
  (<0>,2490) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,2493) tree_plugin.h:2469: return false;
  (<0>,2496) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2500)
  (<0>,2501) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2503) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2504) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2506) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2509) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2511) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2513) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2516) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2518) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2520) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2522) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2525) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2527) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2529) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2532) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2534) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2536) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2538) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2541) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2543) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2545) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2548) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2550) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2552) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2554) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2557) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2559) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2561) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2564) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2566) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2568) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2570) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2577) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2579) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2581) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2582) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2584) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2587) tree_plugin.h:2902: }
  (<0>,2589) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2590) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2592) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2595) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2596) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2597) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2600) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2602) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2605) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2606) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2609)
  (<0>,2610) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2614)
  (<0>,2615) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2616) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2622) tree.c:3814: rnp = rdp->mynode;
  (<0>,2624) tree.c:3814: rnp = rdp->mynode;
  (<0>,2625) tree.c:3814: rnp = rdp->mynode;
  (<0>,2626) tree.c:3815: mask = rdp->grpmask;
  (<0>,2628) tree.c:3815: mask = rdp->grpmask;
  (<0>,2629) tree.c:3815: mask = rdp->grpmask;
  (<0>,2630) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2633)
  (<0>,2634) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2638) fake_sync.h:115: preempt_disable();
  (<0>,2640) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2641) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2648) tree.c:3817: if (!rdp->beenonline)
  (<0>,2650) tree.c:3817: if (!rdp->beenonline)
  (<0>,2654) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2659) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2660) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2661) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2662) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2664) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2666) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2667) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2669) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2672) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2673) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2674) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2676) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2677) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2682) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2683) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2684) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2685) fake_defs.h:237: switch (size) {
  (<0>,2687) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2689) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2690) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2692) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2695) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2696) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2697) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2699) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2701) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2702) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2704) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2705) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2707) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2708) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2710) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2711) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2713) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2714) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2718) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2719) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2722) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2723) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2725) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2726) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2728) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2734) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2735) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2736) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2738) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2742)
  (<0>,2743)
  (<0>,2744) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2745) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2748) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2751) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2753) fake_sched.h:43: return __running_cpu;
  (<0>,2757) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2759) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2763) fake_sched.h:43: return __running_cpu;
  (<0>,2767) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2777) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2780) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2781) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2782) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2786) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2787) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2788) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2790) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2794) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,2797) tree_plugin.h:1243: }
  (<0>,2799) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2802) tree_plugin.h:2461: }
  (<0>,2805) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,2823)
  (<0>,2824) tree.c:3890: void rcu_cpu_starting(unsigned int cpu)
  (<0>,2825) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2826) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2830) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2831) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2832) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2834) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2839) fake_sched.h:43: return __running_cpu;
  (<0>,2842) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2844) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2846) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2847) tree.c:3900: rnp = rdp->mynode;
  (<0>,2849) tree.c:3900: rnp = rdp->mynode;
  (<0>,2850) tree.c:3900: rnp = rdp->mynode;
  (<0>,2851) tree.c:3901: mask = rdp->grpmask;
  (<0>,2853) tree.c:3901: mask = rdp->grpmask;
  (<0>,2854) tree.c:3901: mask = rdp->grpmask;
  (<0>,2858) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2859) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2860) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2862) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2866)
  (<0>,2867)
  (<0>,2868) fake_sync.h:83: local_irq_save(flags);
  (<0>,2871) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2873) fake_sched.h:43: return __running_cpu;
  (<0>,2877) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2879) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2883) fake_sched.h:43: return __running_cpu;
  (<0>,2887) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2893) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2894) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2901) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2902) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2904) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2906) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2907) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2908) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2910) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2912) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2916) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2917) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2918) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2920) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2924)
  (<0>,2925)
  (<0>,2926) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2927) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2930) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2933) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2935) fake_sched.h:43: return __running_cpu;
  (<0>,2939) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2941) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2945) fake_sched.h:43: return __running_cpu;
  (<0>,2949) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2958) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2961) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2962) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2963) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2967) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2968) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2969) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2971) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2976) fake_sched.h:43: return __running_cpu;
  (<0>,2979) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2981) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2983) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2984) tree.c:3900: rnp = rdp->mynode;
  (<0>,2986) tree.c:3900: rnp = rdp->mynode;
  (<0>,2987) tree.c:3900: rnp = rdp->mynode;
  (<0>,2988) tree.c:3901: mask = rdp->grpmask;
  (<0>,2990) tree.c:3901: mask = rdp->grpmask;
  (<0>,2991) tree.c:3901: mask = rdp->grpmask;
  (<0>,2995) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2996) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2997) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2999) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3003)
  (<0>,3004)
  (<0>,3005) fake_sync.h:83: local_irq_save(flags);
  (<0>,3008) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3010) fake_sched.h:43: return __running_cpu;
  (<0>,3014) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3016) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3020) fake_sched.h:43: return __running_cpu;
  (<0>,3024) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3030) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3031) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3038) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3039) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3041) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3043) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3044) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3045) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3047) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3049) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3053) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3054) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3055) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3057) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3061)
  (<0>,3062)
  (<0>,3063) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3064) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3067) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3070) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3072) fake_sched.h:43: return __running_cpu;
  (<0>,3076) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3078) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3082) fake_sched.h:43: return __running_cpu;
  (<0>,3086) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3095) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3098) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3099) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3100) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3104) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3105) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3106) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3108) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3114) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3115) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3122)
  (<0>,3123)
  (<0>,3124) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3126) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3127) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3130) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3132) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3133) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3136) fake_defs.h:635: if (offset & mask)
  (<0>,3137) fake_defs.h:635: if (offset & mask)
  (<0>,3141) fake_defs.h:636: return cpu;
  (<0>,3142) fake_defs.h:636: return cpu;
  (<0>,3144) fake_defs.h:639: }
  (<0>,3146) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3147) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3150) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,3158)
  (<0>,3159) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3160) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3161) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3165) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3166) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3167) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3169) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3173) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3174) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3200)
  (<0>,3201)
  (<0>,3202) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3204) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3206) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3208) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3209) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3212)
  (<0>,3213) tree.c:625: return &rsp->node[0];
  (<0>,3217) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3221) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3222) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3223) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3225) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3229)
  (<0>,3230)
  (<0>,3231) fake_sync.h:83: local_irq_save(flags);
  (<0>,3234) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3236) fake_sched.h:43: return __running_cpu;
  (<0>,3240) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3242) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3246) fake_sched.h:43: return __running_cpu;
  (<0>,3250) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3256) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3257) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3264) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3266) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3267) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3269) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3270) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3272) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3273) tree.c:3800: rdp->blimit = blimit;
  (<0>,3274) tree.c:3800: rdp->blimit = blimit;
  (<0>,3276) tree.c:3800: rdp->blimit = blimit;
  (<0>,3277) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3279) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3282) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3285)
  (<0>,3286) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3289) tree_plugin.h:2469: return false;
  (<0>,3292) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3296)
  (<0>,3297) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3299) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3300) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3302) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3305) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3307) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3309) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3312) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3314) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3316) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3318) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3321) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3323) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3325) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3328) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3330) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3332) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3334) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3337) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3339) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3341) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3344) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3346) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3348) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3350) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3353) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3355) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3357) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3360) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3362) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3364) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3366) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3373) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3375) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3377) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3378) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3380) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3383) tree_plugin.h:2902: }
  (<0>,3385) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3386) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3388) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3391) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3392) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3393) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3396) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3398) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3401) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3402) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3405)
  (<0>,3406) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3410)
  (<0>,3411) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3412) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3418) tree.c:3814: rnp = rdp->mynode;
  (<0>,3420) tree.c:3814: rnp = rdp->mynode;
  (<0>,3421) tree.c:3814: rnp = rdp->mynode;
  (<0>,3422) tree.c:3815: mask = rdp->grpmask;
  (<0>,3424) tree.c:3815: mask = rdp->grpmask;
  (<0>,3425) tree.c:3815: mask = rdp->grpmask;
  (<0>,3426) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3429)
  (<0>,3430) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3434) fake_sync.h:115: preempt_disable();
  (<0>,3436) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3437) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3444) tree.c:3817: if (!rdp->beenonline)
  (<0>,3446) tree.c:3817: if (!rdp->beenonline)
  (<0>,3450) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3455) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3456) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3457) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3458) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3460) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3462) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3463) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3465) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3468) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3469) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3470) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3472) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3473) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3478) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3479) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3480) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3481) fake_defs.h:237: switch (size) {
  (<0>,3483) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3485) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3486) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3488) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3491) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3492) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3493) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3495) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3497) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3498) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3500) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3501) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3503) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3504) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3506) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3507) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3509) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3510) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3514) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3515) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3518) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3519) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3521) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3522) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3524) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3530) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3531) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3532) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3534) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3538)
  (<0>,3539)
  (<0>,3540) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3541) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3544) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3547) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3549) fake_sched.h:43: return __running_cpu;
  (<0>,3553) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3555) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3559) fake_sched.h:43: return __running_cpu;
  (<0>,3563) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3573) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3576) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3577) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3578) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3582) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3583) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3584) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3586) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3590) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3591) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3617)
  (<0>,3618)
  (<0>,3619) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3621) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3623) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3625) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3626) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3629)
  (<0>,3630) tree.c:625: return &rsp->node[0];
  (<0>,3634) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3638) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3639) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3640) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3642) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3646)
  (<0>,3647)
  (<0>,3648) fake_sync.h:83: local_irq_save(flags);
  (<0>,3651) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3653) fake_sched.h:43: return __running_cpu;
  (<0>,3657) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3659) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3663) fake_sched.h:43: return __running_cpu;
  (<0>,3667) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3673) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3674) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3681) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3683) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3684) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3686) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3687) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3689) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3690) tree.c:3800: rdp->blimit = blimit;
  (<0>,3691) tree.c:3800: rdp->blimit = blimit;
  (<0>,3693) tree.c:3800: rdp->blimit = blimit;
  (<0>,3694) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3696) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3699) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3702)
  (<0>,3703) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3706) tree_plugin.h:2469: return false;
  (<0>,3709) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3713)
  (<0>,3714) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3716) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3717) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3719) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3722) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3724) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3726) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3729) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3731) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3733) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3735) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3738) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3740) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3742) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3745) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3747) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3749) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3751) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3754) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3756) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3758) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3761) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3763) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3765) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3767) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3770) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3772) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3774) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3777) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3779) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3781) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3783) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3790) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3792) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3794) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3795) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3797) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3800) tree_plugin.h:2902: }
  (<0>,3802) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3803) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3805) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3808) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3809) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3810) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3813) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3815) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3818) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3819) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3822)
  (<0>,3823) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3827)
  (<0>,3828) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3829) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3835) tree.c:3814: rnp = rdp->mynode;
  (<0>,3837) tree.c:3814: rnp = rdp->mynode;
  (<0>,3838) tree.c:3814: rnp = rdp->mynode;
  (<0>,3839) tree.c:3815: mask = rdp->grpmask;
  (<0>,3841) tree.c:3815: mask = rdp->grpmask;
  (<0>,3842) tree.c:3815: mask = rdp->grpmask;
  (<0>,3843) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3846)
  (<0>,3847) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3851) fake_sync.h:115: preempt_disable();
  (<0>,3853) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3854) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3861) tree.c:3817: if (!rdp->beenonline)
  (<0>,3863) tree.c:3817: if (!rdp->beenonline)
  (<0>,3867) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3872) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3873) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3874) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3875) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3877) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3879) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3880) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3882) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3885) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3886) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3887) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3889) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3890) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3895) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3896) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3897) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3898) fake_defs.h:237: switch (size) {
  (<0>,3900) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3902) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3903) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3905) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3908) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3909) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3910) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3912) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3914) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3915) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3917) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3918) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3920) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3921) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3923) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3924) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3926) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3927) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3931) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3932) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3935) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3936) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3938) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3939) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3941) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3947) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3948) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3949) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3951) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3955)
  (<0>,3956)
  (<0>,3957) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3958) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3961) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3964) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3966) fake_sched.h:43: return __running_cpu;
  (<0>,3970) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3972) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3976) fake_sched.h:43: return __running_cpu;
  (<0>,3980) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3990) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3993) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3994) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3995) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3999) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4000) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4001) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4003) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4007) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,4010) tree_plugin.h:1243: }
  (<0>,4012) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,4015) tree_plugin.h:2461: }
  (<0>,4018) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,4036)
  (<0>,4037) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4038) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4039) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4043) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4044) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4045) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4047) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4052) fake_sched.h:43: return __running_cpu;
  (<0>,4055) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4057) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4059) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4060) tree.c:3900: rnp = rdp->mynode;
  (<0>,4062) tree.c:3900: rnp = rdp->mynode;
  (<0>,4063) tree.c:3900: rnp = rdp->mynode;
  (<0>,4064) tree.c:3901: mask = rdp->grpmask;
  (<0>,4066) tree.c:3901: mask = rdp->grpmask;
  (<0>,4067) tree.c:3901: mask = rdp->grpmask;
  (<0>,4071) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4072) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4073) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4075) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4079)
  (<0>,4080)
  (<0>,4081) fake_sync.h:83: local_irq_save(flags);
  (<0>,4084) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4086) fake_sched.h:43: return __running_cpu;
  (<0>,4090) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4092) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4096) fake_sched.h:43: return __running_cpu;
  (<0>,4100) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4106) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4107) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4114) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4115) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4117) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4119) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4120) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4121) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4123) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4125) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4129) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4130) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4131) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4133) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4137)
  (<0>,4138)
  (<0>,4139) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4140) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4143) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4146) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4148) fake_sched.h:43: return __running_cpu;
  (<0>,4152) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4154) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4158) fake_sched.h:43: return __running_cpu;
  (<0>,4162) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4171) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4175) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4180) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4181) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4182) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4184) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4189) fake_sched.h:43: return __running_cpu;
  (<0>,4192) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4194) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4196) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4197) tree.c:3900: rnp = rdp->mynode;
  (<0>,4199) tree.c:3900: rnp = rdp->mynode;
  (<0>,4200) tree.c:3900: rnp = rdp->mynode;
  (<0>,4201) tree.c:3901: mask = rdp->grpmask;
  (<0>,4203) tree.c:3901: mask = rdp->grpmask;
  (<0>,4204) tree.c:3901: mask = rdp->grpmask;
  (<0>,4208) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4209) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4210) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4212) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4216)
  (<0>,4217)
  (<0>,4218) fake_sync.h:83: local_irq_save(flags);
  (<0>,4221) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4223) fake_sched.h:43: return __running_cpu;
  (<0>,4227) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4229) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4233) fake_sched.h:43: return __running_cpu;
  (<0>,4237) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4243) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4244) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4251) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4252) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4254) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4256) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4257) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4258) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4260) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4262) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4266) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4267) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4268) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4270) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4274)
  (<0>,4275)
  (<0>,4276) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4277) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4280) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4283) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4285) fake_sched.h:43: return __running_cpu;
  (<0>,4289) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4291) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4295) fake_sched.h:43: return __running_cpu;
  (<0>,4299) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4308) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4313) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4317) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4319) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4321) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4327) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4328) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4335)
  (<0>,4336)
  (<0>,4337) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4339) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4340) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4343) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4345) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4346) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4349) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,4351) fake_defs.h:639: }
  (<0>,4353) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4354) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4358) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4360) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4363) litmus.c:123: set_cpu(i);
  (<0>,4366)
  (<0>,4367) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,4368) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4370) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4388)
  (<0>,4389) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4390) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4391) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4395) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4396) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4397) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4399) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4404) fake_sched.h:43: return __running_cpu;
  (<0>,4407) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4409) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4411) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4412) tree.c:3900: rnp = rdp->mynode;
  (<0>,4414) tree.c:3900: rnp = rdp->mynode;
  (<0>,4415) tree.c:3900: rnp = rdp->mynode;
  (<0>,4416) tree.c:3901: mask = rdp->grpmask;
  (<0>,4418) tree.c:3901: mask = rdp->grpmask;
  (<0>,4419) tree.c:3901: mask = rdp->grpmask;
  (<0>,4423) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4424) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4425) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4427) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4431)
  (<0>,4432)
  (<0>,4433) fake_sync.h:83: local_irq_save(flags);
  (<0>,4436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4438) fake_sched.h:43: return __running_cpu;
  (<0>,4442) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4448) fake_sched.h:43: return __running_cpu;
  (<0>,4452) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4458) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4459) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4466) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4467) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4469) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4471) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4472) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4473) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4475) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4477) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4481) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4482) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4483) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4485) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4489)
  (<0>,4490)
  (<0>,4491) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4492) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4495) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4498) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4500) fake_sched.h:43: return __running_cpu;
  (<0>,4504) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4506) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4510) fake_sched.h:43: return __running_cpu;
  (<0>,4514) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4523) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4526) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4527) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4528) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4532) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4533) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4534) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4541) fake_sched.h:43: return __running_cpu;
  (<0>,4544) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4546) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4548) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4549) tree.c:3900: rnp = rdp->mynode;
  (<0>,4551) tree.c:3900: rnp = rdp->mynode;
  (<0>,4552) tree.c:3900: rnp = rdp->mynode;
  (<0>,4553) tree.c:3901: mask = rdp->grpmask;
  (<0>,4555) tree.c:3901: mask = rdp->grpmask;
  (<0>,4556) tree.c:3901: mask = rdp->grpmask;
  (<0>,4560) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4561) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4562) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4564) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4568)
  (<0>,4569)
  (<0>,4570) fake_sync.h:83: local_irq_save(flags);
  (<0>,4573) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4575) fake_sched.h:43: return __running_cpu;
  (<0>,4579) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4581) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4585) fake_sched.h:43: return __running_cpu;
  (<0>,4589) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4595) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4596) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4603) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4604) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4606) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4608) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4609) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4610) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4612) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4614) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4618) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4619) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4620) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4622) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4626)
  (<0>,4627)
  (<0>,4628) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4629) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4632) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4635) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4637) fake_sched.h:43: return __running_cpu;
  (<0>,4641) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4643) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4647) fake_sched.h:43: return __running_cpu;
  (<0>,4651) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4660) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4663) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4664) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4665) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4669) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4670) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4671) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4673) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4680) tree.c:753: unsigned long flags;
  (<0>,4683) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4685) fake_sched.h:43: return __running_cpu;
  (<0>,4689) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4691) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4695) fake_sched.h:43: return __running_cpu;
  (<0>,4699) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4711) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4713) fake_sched.h:43: return __running_cpu;
  (<0>,4717) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4718) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4720) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4721) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4722) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4723) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4724) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4725) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4726) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4730) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4732) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4733) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4734) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4750)
  (<0>,4752) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4754) fake_sched.h:43: return __running_cpu;
  (<0>,4758) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4761) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4762) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4763) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4767) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4768) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4769) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4771) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4779) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4781) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4783) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4784) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4787) tree_plugin.h:2457: }
  (<0>,4790) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4793) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4794) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4795) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4799) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4800) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4801) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4803) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4808) fake_sched.h:43: return __running_cpu;
  (<0>,4811) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4813) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4815) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4816) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4819) tree_plugin.h:2457: }
  (<0>,4822) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4825) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4826) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4827) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4831) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4832) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4833) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4835) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4842) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4845) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4846) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4847) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4849) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4850) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4852) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4853) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4854) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4855) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4869) tree_plugin.h:2879: }
  (<0>,4871) tree.c:758: local_irq_restore(flags);
  (<0>,4874) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4876) fake_sched.h:43: return __running_cpu;
  (<0>,4880) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4882) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4886) fake_sched.h:43: return __running_cpu;
  (<0>,4890) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4897) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4899) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4901) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4904) litmus.c:123: set_cpu(i);
  (<0>,4907)
  (<0>,4908) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4909) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4911) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4929)
  (<0>,4930) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4931) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4932) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4936) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4937) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4938) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4940) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4948) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4950) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4952) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4953) tree.c:3900: rnp = rdp->mynode;
  (<0>,4955) tree.c:3900: rnp = rdp->mynode;
  (<0>,4956) tree.c:3900: rnp = rdp->mynode;
  (<0>,4957) tree.c:3901: mask = rdp->grpmask;
  (<0>,4959) tree.c:3901: mask = rdp->grpmask;
  (<0>,4960) tree.c:3901: mask = rdp->grpmask;
  (<0>,4964) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4965) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4966) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4968) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4972)
  (<0>,4973)
  (<0>,4974) fake_sync.h:83: local_irq_save(flags);
  (<0>,4977) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:43: return __running_cpu;
  (<0>,4983) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4985) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4989) fake_sched.h:43: return __running_cpu;
  (<0>,4993) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4999) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5000) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5007) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5008) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5010) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5012) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5013) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5014) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5016) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5018) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5022) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5023) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5024) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5026) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5030)
  (<0>,5031)
  (<0>,5032) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5033) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5036) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5039) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5041) fake_sched.h:43: return __running_cpu;
  (<0>,5045) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5047) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5051) fake_sched.h:43: return __running_cpu;
  (<0>,5055) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5064) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5067) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5068) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5069) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5073) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5074) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5075) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5077) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5082) fake_sched.h:43: return __running_cpu;
  (<0>,5085) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5087) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5089) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5090) tree.c:3900: rnp = rdp->mynode;
  (<0>,5092) tree.c:3900: rnp = rdp->mynode;
  (<0>,5093) tree.c:3900: rnp = rdp->mynode;
  (<0>,5094) tree.c:3901: mask = rdp->grpmask;
  (<0>,5096) tree.c:3901: mask = rdp->grpmask;
  (<0>,5097) tree.c:3901: mask = rdp->grpmask;
  (<0>,5101) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5102) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5103) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5105) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5109)
  (<0>,5110)
  (<0>,5111) fake_sync.h:83: local_irq_save(flags);
  (<0>,5114) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5116) fake_sched.h:43: return __running_cpu;
  (<0>,5120) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5122) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5126) fake_sched.h:43: return __running_cpu;
  (<0>,5130) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5136) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5137) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5144) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5145) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5147) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5149) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5150) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5151) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5153) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5155) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5159) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5160) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5161) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5163) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5167)
  (<0>,5168)
  (<0>,5169) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5170) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5173) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5176) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5178) fake_sched.h:43: return __running_cpu;
  (<0>,5182) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5184) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5188) fake_sched.h:43: return __running_cpu;
  (<0>,5192) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5201) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5204) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5205) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5206) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5210) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5211) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5212) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5214) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5221) tree.c:755: local_irq_save(flags);
  (<0>,5224) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5226) fake_sched.h:43: return __running_cpu;
  (<0>,5230) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5232) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5236) fake_sched.h:43: return __running_cpu;
  (<0>,5240) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5252) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5254) fake_sched.h:43: return __running_cpu;
  (<0>,5258) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5259) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5261) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5262) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5263) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5264) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5265) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5266) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5267) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,5271) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5273) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5274) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5275) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5291)
  (<0>,5293) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5295) fake_sched.h:43: return __running_cpu;
  (<0>,5299) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5302) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5303) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5304) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5308) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5309) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5310) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5312) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5317) fake_sched.h:43: return __running_cpu;
  (<0>,5320) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5322) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5324) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5325) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5328) tree_plugin.h:2457: }
  (<0>,5331) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5334) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5335) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5336) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5340) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5341) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5342) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5344) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5349) fake_sched.h:43: return __running_cpu;
  (<0>,5352) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5354) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5356) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5357) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5360) tree_plugin.h:2457: }
  (<0>,5363) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5366) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5367) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5368) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5372) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5373) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5374) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5376) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5383) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5386) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5387) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5388) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5390) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5391) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5393) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5394) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5395) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5396) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5410) tree_plugin.h:2879: }
  (<0>,5412) tree.c:758: local_irq_restore(flags);
  (<0>,5415) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5417) fake_sched.h:43: return __running_cpu;
  (<0>,5421) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5423) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5427) fake_sched.h:43: return __running_cpu;
  (<0>,5431) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5438) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5440) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5442) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5462) tree.c:3971: unsigned long flags;
  (<0>,5463) tree.c:3972: int kthread_prio_in = kthread_prio;
  (<0>,5464) tree.c:3973: struct rcu_node *rnp;
  (<0>,5467) tree.c:3983: else if (kthread_prio > 99)
  (<0>,5471) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5472) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5475) tree.c:3989: rcu_scheduler_fully_active = 1;
  (<0>,5476) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5477) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5478) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5482) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5483) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5484) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5486) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5490) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5492) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5494) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5503)
  (<0>,5504) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5505) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5506) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5507) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5508) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5511)
  (<0>,5512)
  (<0>,5519)
  (<0>,5520)
  (<0>,5527)
  (<0>,5528)
  (<0>,5535)
  (<0>,5536)
  (<0>,5543)
  (<0>,5544)
  (<0>,5551)
  (<0>,5552)
  (<0>,5559)
  (<0>,5560)
  (<0>,5567)
  (<0>,5568)
  (<0>,5575)
  (<0>,5576)
  (<0>,5583)
  (<0>,5584) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5593) fake_defs.h:868: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,5599) fake_defs.h:870: task = malloc(sizeof(*task));
  (<0>,5600) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5602) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5604) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5605) fake_defs.h:872: task->tid = t;
  (<0>,5606) fake_defs.h:872: task->tid = t;
  (<0>,5608) fake_defs.h:872: task->tid = t;
  (<0>,5609) fake_defs.h:873: return task;
  (<0>,5610) fake_defs.h:873: return task;
  (<0>,5612) fake_defs.h:876: }
  (<0>,5614) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5615) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5618)
  (<0>,5619) tree.c:625: return &rsp->node[0];
  (<0>,5623) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5627) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5628) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5629) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5631) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5635)
  (<0>,5636)
  (<0>,5637) fake_sync.h:83: local_irq_save(flags);
  (<0>,5640) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5642) fake_sched.h:43: return __running_cpu;
  (<0>,5646) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5648) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5652) fake_sched.h:43: return __running_cpu;
  (<0>,5656) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5662) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5663) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5670) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5671) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5673) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5674) tree.c:3996: if (kthread_prio) {
  (<0>,5680) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5681) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5682) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5684) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5688)
  (<0>,5689)
  (<0>,5690) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5691) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5694) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5697) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5699) fake_sched.h:43: return __running_cpu;
  (<0>,5703) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5705) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5709) fake_sched.h:43: return __running_cpu;
  (<0>,5713) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5724) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5727) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5728) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5729) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5733) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5734) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5735) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5737) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5741) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5743) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5745) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5754)
  (<0>,5755)
  (<0>,5756)
  (<0>,5757)
  (<0>,5758) fake_defs.h:865: struct task_struct *task = NULL;
  (<0>,5759) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5762)
  (<0>,5763)
  (<0>,5770)
  (<0>,5771)
  (<0>,5778)
  (<0>,5779)
  (<0>,5786)
  (<0>,5787)
  (<0>,5794)
  (<0>,5795) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5808) fake_defs.h:875: return NULL;
  (<0>,5810) fake_defs.h:876: }
  (<0>,5812) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5813) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5816)
  (<0>,5817) tree.c:625: return &rsp->node[0];
  (<0>,5821) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5825) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5826) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5827) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5829) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5833)
  (<0>,5834)
  (<0>,5835) fake_sync.h:83: local_irq_save(flags);
  (<0>,5838) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5840) fake_sched.h:43: return __running_cpu;
  (<0>,5844) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5846) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5850) fake_sched.h:43: return __running_cpu;
  (<0>,5854) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5860) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5861) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5868) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5869) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5871) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5872) tree.c:3996: if (kthread_prio) {
  (<0>,5878) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5879) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5880) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5882) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5886)
  (<0>,5887)
  (<0>,5888) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5889) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5892) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5895) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5897) fake_sched.h:43: return __running_cpu;
  (<0>,5901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5903) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5907) fake_sched.h:43: return __running_cpu;
  (<0>,5911) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5922) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5925) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5926) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5927) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5931) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5932) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5933) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5935) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5949)
  (<0>,5950) litmus.c:51: void *thread_reader(void *arg)
  (<0>,5953)
  (<0>,5954) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5955) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5958) fake_sched.h:43: return __running_cpu;
  (<0>,5962)
  (<0>,5963) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,5966) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,5971) tree.c:890: unsigned long flags;
  (<0>,5974) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5976) fake_sched.h:43: return __running_cpu;
  (<0>,5980) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5982) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5986) fake_sched.h:43: return __running_cpu;
  (<0>,5990) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6002) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6004) fake_sched.h:43: return __running_cpu;
  (<0>,6008) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6009) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6011) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6012) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6013) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6014) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6015) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6016) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6017) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,6021) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6023) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6024) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6025) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6036)
  (<0>,6037) tree.c:830: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,6039) fake_sched.h:43: return __running_cpu;
  (<0>,6043) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6047) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6050) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6051) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6052) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6054) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6055) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6057) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6058) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6059) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6060) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6070) tree_plugin.h:2883: }
  (<0>,6072) tree.c:895: local_irq_restore(flags);
  (<0>,6075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6077) fake_sched.h:43: return __running_cpu;
  (<0>,6081) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6083) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6087) fake_sched.h:43: return __running_cpu;
  (<0>,6091) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6106) litmus.c:57: r_x = x;
  (<0>,6107) litmus.c:57: r_x = x;
  (<0>,6111) fake_sched.h:43: return __running_cpu;
  (<0>,6115) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
  (<0>,6119) fake_sched.h:43: return __running_cpu;
  (<0>,6123) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6128) fake_sched.h:43: return __running_cpu;
  (<0>,6132) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
  (<0>,6143) fake_sched.h:43: return __running_cpu;
  (<0>,6147) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6148) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6150) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6151) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6152) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6154) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6156) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6157) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6158) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6159) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6160) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6161) tree.c:942: if (oldval)
  (<0>,6169) tree_plugin.h:2883: }
  (<0>,6175) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,6184) tree_plugin.h:1669: struct rcu_state *rsp;
  (<0>,6185) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6186) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6190) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6191) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6192) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6194) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6199) fake_sched.h:43: return __running_cpu;
  (<0>,6202) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6204) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6207) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6209) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6211) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6214) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6215) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6216) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6220) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6221) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6222) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6224) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6229) fake_sched.h:43: return __running_cpu;
  (<0>,6232) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6234) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6237) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6239) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6241) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6244) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6245) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6246) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6250) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6251) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6252) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6254) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6259) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,6264) fake_sched.h:43: return __running_cpu;
  (<0>,6269) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,6277) fake_sched.h:43: return __running_cpu;
  (<0>,6283) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
  (<0>,6289) fake_sched.h:43: return __running_cpu;
  (<0>,6296) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
  (<0>,6309) tree.c:3557: struct rcu_state *rsp;
  (<0>,6310) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6311) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6315) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6316) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6317) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6319) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6323) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6325) fake_sched.h:43: return __running_cpu;
  (<0>,6328) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6330) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6352)
  (<0>,6353) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6354) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6356) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6357) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6358) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6360) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6362) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6363) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6364) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6399)
  (<0>,6400) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6401) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6404) tree.c:1469: !rcu_gp_in_progress(rsp))
  (<0>,6417)
  (<0>,6418) tree.c:237: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,6423) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6424) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6425) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6426) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6428) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6430) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6431) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6433) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6436) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6437) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6438) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6439) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6444) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6445) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6446) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6447) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6449) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6451) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6452) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6454) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6457) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6458) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6459) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6467) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
  (<0>,6470) tree_plugin.h:2923: return false;
  (<0>,6473) tree.c:3503: if (rcu_scheduler_fully_active &&
  (<0>,6476) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6478) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6481) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6483) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6487) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,6490)
  (<0>,6491) tree.c:614: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,6493) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6496) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6503) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6504) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6515)
  (<0>,6516) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6517) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6530)
  (<0>,6531) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6536) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6537) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6538) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6539) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6541) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6543) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6544) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6546) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6549) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6550) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6551) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6552) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6557) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6560) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6562) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6564) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6565) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6567) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6570) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6571) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6572) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6578) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,6594)
  (<0>,6595) tree.c:633: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,6598)
  (<0>,6599) tree.c:625: return &rsp->node[0];
  (<0>,6603) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6604) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6609) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6610) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6611) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6612) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6614) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6616) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6617) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6619) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6622) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6623) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6624) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6628) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6629) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6631) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6634) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6635) tree.c:639: return READ_ONCE(*fp);
  (<0>,6639) tree.c:639: return READ_ONCE(*fp);
  (<0>,6640) tree.c:639: return READ_ONCE(*fp);
  (<0>,6641) tree.c:639: return READ_ONCE(*fp);
  (<0>,6642) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6644) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6646) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6647) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6649) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6652) tree.c:639: return READ_ONCE(*fp);
  (<0>,6653) tree.c:639: return READ_ONCE(*fp);
  (<0>,6654) tree.c:639: return READ_ONCE(*fp);
  (<0>,6658) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6661) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6664) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6667) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6668) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6671) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6673) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6676) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6679) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6682) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6683) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6685) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6688) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6692) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6694) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6696) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6699) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6702) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6705) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6706) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6708) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6711) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6715) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6717) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6719) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6722) tree.c:665: return false; /* No grace period needed. */
  (<0>,6724) tree.c:666: }
  (<0>,6727) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6732) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6733) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6734) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6735) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6737) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6739) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6740) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6742) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6745) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6746) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6747) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6748) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6750) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6753) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6758) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6759) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6760) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6761) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6763) fake_defs.h:266: __READ_ONCE_SIZE;
      (<0.1>,1)
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:892: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,55) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,57) fake_sched.h:43: return __running_cpu;
      (<0.1>,61) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,62) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,64) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,67) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,68) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,69) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,70) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,74) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,76) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,77) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,78) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,89)
      (<0.1>,90) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,92) fake_sched.h:43: return __running_cpu;
      (<0.1>,96) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,100) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,103) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,104) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,105) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,107) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,110) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,111) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,112) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,113) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,123) tree_plugin.h:2883: }
      (<0.1>,125) tree.c:895: local_irq_restore(flags);
      (<0.1>,128) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,130) fake_sched.h:43: return __running_cpu;
      (<0.1>,134) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,140) fake_sched.h:43: return __running_cpu;
      (<0.1>,144) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,151) litmus.c:87: x = 1;
      (<0.1>,163) tree.c:3255: ret = num_online_cpus() <= 1;
      (<0.1>,165) tree.c:3257: return ret;
      (<0.1>,172) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,175) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,176) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,177) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,178) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,181) update.c:148: rcu_scheduler_active == RCU_SCHEDULER_INIT;
      (<0.1>,198)
      (<0.1>,199)
      (<0.1>,200)
      (<0.1>,201)
      (<0.1>,202) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,204) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,205) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,208) update.c:352: if (checktiny &&
      (<0.1>,211) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,213) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,218) rcupdate.h:476: }
      (<0.1>,220) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,222) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,227)
      (<0.1>,228) fake_sync.h:272: x->done = 0;
      (<0.1>,230) fake_sync.h:272: x->done = 0;
      (<0.1>,234) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,236) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,238) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,239) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,241) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,247)
      (<0.1>,248)
      (<0.1>,249) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,250) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,281)
      (<0.1>,282)
      (<0.1>,283)
      (<0.1>,284)
      (<0.1>,286)
      (<0.1>,287) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,294) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,295) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,301) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,302) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,303) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,304) tree.c:3147: if (debug_rcu_head_queue(head)) {
      (<0.1>,307) rcu.h:92: return 0;
      (<0.1>,311) tree.c:3153: head->func = func;
      (<0.1>,312) tree.c:3153: head->func = func;
      (<0.1>,315) tree.c:3153: head->func = func;
      (<0.1>,316) tree.c:3154: head->next = NULL;
      (<0.1>,318) tree.c:3154: head->next = NULL;
      (<0.1>,319) tree.c:3162: local_irq_save(flags);
      (<0.1>,322) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,324) fake_sched.h:43: return __running_cpu;
      (<0.1>,328) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,330) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,334) fake_sched.h:43: return __running_cpu;
      (<0.1>,338) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,344) fake_sched.h:43: return __running_cpu;
      (<0.1>,347) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,349) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,351) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,352) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,355) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,363) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,367) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,369) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,371) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,372) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,377) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,378) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,379) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,380) fake_defs.h:237: switch (size) {
      (<0.1>,382) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,384) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,385) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,387) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,390) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,391) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,392) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,393) tree.c:3189: if (lazy)
      (<0.1>,400) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,401) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,404) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,405) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,406) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,408) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,411) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,412) tree.c:3197: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,419) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,420) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,421) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,422) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,430)
      (<0.1>,431)
      (<0.1>,432)
      (<0.1>,433) tree.c:3086: if (!rcu_is_watching())
      (<0.1>,440) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,442) fake_sched.h:43: return __running_cpu;
      (<0.1>,448) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,449) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,450) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,455) tree.c:1060: ret = __rcu_is_watching();
      (<0.1>,457) tree.c:1062: return ret;
      (<0.1>,461) tree.c:3090: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,464) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,466) fake_sched.h:43: return __running_cpu;
      (<0.1>,470) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,480) tree.c:3205: local_irq_restore(flags);
      (<0.1>,483) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,485) fake_sched.h:43: return __running_cpu;
      (<0.1>,489) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,491) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,495) fake_sched.h:43: return __running_cpu;
      (<0.1>,499) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,508) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,510) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,512) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,513) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,516) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,518) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,519) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,522) update.c:365: if (checktiny &&
      (<0.1>,525) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,527) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,532) fake_sync.h:278: might_sleep();
      (<0.1>,536) fake_sched.h:43: return __running_cpu;
      (<0.1>,540) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,543) tree.c:755: local_irq_save(flags);
      (<0.1>,546) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,548) fake_sched.h:43: return __running_cpu;
      (<0.1>,552) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,554) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,558) fake_sched.h:43: return __running_cpu;
      (<0.1>,562) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,574) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,576) fake_sched.h:43: return __running_cpu;
      (<0.1>,580) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,581) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,583) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,584) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,585) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,586) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,587) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,588) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,589) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,593) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,595) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,596) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,597) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,613)
      (<0.1>,615) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,617) fake_sched.h:43: return __running_cpu;
      (<0.1>,621) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,624) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,625) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,626) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,630) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,631) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,632) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,639) fake_sched.h:43: return __running_cpu;
      (<0.1>,642) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,644) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,646) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,647) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,650) tree_plugin.h:2457: }
      (<0.1>,653) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,656) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,657) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,658) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,662) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,663) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,664) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,666) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,671) fake_sched.h:43: return __running_cpu;
      (<0.1>,674) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,676) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,678) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,679) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,682) tree_plugin.h:2457: }
      (<0.1>,685) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,688) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,689) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,690) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,694) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,695) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,696) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,698) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,705) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,708) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,709) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,710) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,712) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,713) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,715) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,716) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,717) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,718) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,732) tree_plugin.h:2879: }
      (<0.1>,734) tree.c:758: local_irq_restore(flags);
      (<0.1>,737) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,739) fake_sched.h:43: return __running_cpu;
      (<0.1>,743) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,745) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,749) fake_sched.h:43: return __running_cpu;
      (<0.1>,753) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,759) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,762) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,767) fake_sync.h:281: while (!x->done)
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:892: local_irq_save(flags);
    (<0.0>,35) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,63) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,65) fake_sched.h:43: return __running_cpu;
    (<0.0>,69) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,70) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,72) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,75) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,76) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,77) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,78) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,82) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,84) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,85) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,86) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,97)
    (<0.0>,98) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,100) fake_sched.h:43: return __running_cpu;
    (<0.0>,104) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,111) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,112) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,113) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,115) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,116) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,118) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,119) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,120) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,121) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,131) tree_plugin.h:2883: }
    (<0.0>,133) tree.c:895: local_irq_restore(flags);
    (<0.0>,136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,138) fake_sched.h:43: return __running_cpu;
    (<0.0>,142) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,144) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,148) fake_sched.h:43: return __running_cpu;
    (<0.0>,152) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,159) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,207)
    (<0.0>,208) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,211) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,214)
    (<0.0>,215) tree.c:625: return &rsp->node[0];
    (<0.0>,219) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,227) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,233) fake_sched.h:43: return __running_cpu;
    (<0.0>,237) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,241) fake_sched.h:43: return __running_cpu;
    (<0.0>,245) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,250) fake_sched.h:43: return __running_cpu;
    (<0.0>,254) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,270) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,272) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,273) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,274) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,276) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,278) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,279) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,280) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,281) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,282) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,283) tree.c:942: if (oldval)
    (<0.0>,291) tree_plugin.h:2883: }
    (<0.0>,297) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,306) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,307) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,308) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,312) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,313) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,314) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,316) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,321) fake_sched.h:43: return __running_cpu;
    (<0.0>,324) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,326) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,329) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,331) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,333) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,336) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,337) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,338) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,342) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,343) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,344) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,346) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,351) fake_sched.h:43: return __running_cpu;
    (<0.0>,354) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,356) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,359) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,361) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,363) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,366) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,367) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,368) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,372) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,373) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,374) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,376) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,381) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,386) fake_sched.h:43: return __running_cpu;
    (<0.0>,391) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,399) fake_sched.h:43: return __running_cpu;
    (<0.0>,405) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,411) fake_sched.h:43: return __running_cpu;
    (<0.0>,418) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,431) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,432) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,433) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,437) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,438) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,439) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,441) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,445) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,447) fake_sched.h:43: return __running_cpu;
    (<0.0>,450) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,452) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,474)
    (<0.0>,475)
    (<0.0>,476) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,478) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,479) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,480) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,482) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,484) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,485) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,486) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,521)
    (<0.0>,522)
    (<0.0>,523) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,526) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,539)
    (<0.0>,540) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,545) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,546) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,547) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,548) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,550) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,552) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,553) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,555) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,560) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,561) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,566) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,567) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,569) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,571) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,573) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,574) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,576) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,579) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,580) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,581) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,589) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,592) tree_plugin.h:2923: return false;
    (<0.0>,595) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,598) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,600) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,603) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,605) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,609) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,612)
    (<0.0>,613) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,615) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,618) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,625) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,626) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,637)
    (<0.0>,638)
    (<0.0>,639) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,652)
    (<0.0>,653) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,658) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,659) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,660) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,661) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,663) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,665) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,666) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,668) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,671) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,672) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,673) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,674) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,679) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,680) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,681) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,682) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,684) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,686) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,692) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,693) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,694) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,700) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,716)
    (<0.0>,717) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,720)
    (<0.0>,721) tree.c:625: return &rsp->node[0];
    (<0.0>,725) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,726) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,731) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,732) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,733) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,734) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,736) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,739) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,744) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,745) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,746) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,750) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,751) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,753) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,756) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,757) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,761) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,762) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,763) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,774) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,775) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,776) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,780) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,783) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,786) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,789) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,790) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,793) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,795) tree.c:666: }
    (<0.0>,798) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,800) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,802) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,803) tree.c:3523: return 1;
    (<0.0>,805) tree.c:3548: }
    (<0.0>,809) tree.c:3561: return 1;
    (<0.0>,811) tree.c:3563: }
    (<0.0>,818) fake_sched.h:43: return __running_cpu;
    (<0.0>,822) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,826) tree.c:2891: if (user)
    (<0.0>,834) fake_sched.h:43: return __running_cpu;
    (<0.0>,838) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,840) fake_sched.h:43: return __running_cpu;
    (<0.0>,844) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,850) fake_sched.h:43: return __running_cpu;
    (<0.0>,854) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,864) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,867) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,868) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,869) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,873) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,874) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,875) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,877) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,881) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,893) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,895) fake_sched.h:43: return __running_cpu;
    (<0.0>,898) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,900) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,902) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,903) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,905) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,912) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,913) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,915) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,921) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,922) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,923) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,924) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,925) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,929)
    (<0.0>,930)
    (<0.0>,931) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,932) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,957)
    (<0.0>,958)
    (<0.0>,959) tree.c:1905: local_irq_save(flags);
    (<0.0>,962) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,964) fake_sched.h:43: return __running_cpu;
    (<0.0>,968) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,970) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,974) fake_sched.h:43: return __running_cpu;
    (<0.0>,978) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,983) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,985) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,986) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,987) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,989) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,990) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,995) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,996) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,997) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,998) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1000) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1002) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1003) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1005) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1008) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1009) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1010) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1013) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1015) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1016) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1021) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1022) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1023) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1024) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1026) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1028) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1029) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1031) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1034) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1035) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1036) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1039) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1043) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1044) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1045) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1046) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1048) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1050) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1054) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1057) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1058) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1066) tree.c:1911: local_irq_restore(flags);
    (<0.0>,1069) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1071) fake_sched.h:43: return __running_cpu;
    (<0.0>,1075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1077) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1081) fake_sched.h:43: return __running_cpu;
    (<0.0>,1085) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1092) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1094) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1099) tree.c:3016: local_irq_save(flags);
    (<0.0>,1102) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1104) fake_sched.h:43: return __running_cpu;
    (<0.0>,1108) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1110) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1114) fake_sched.h:43: return __running_cpu;
    (<0.0>,1118) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1123) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1124) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1135)
    (<0.0>,1136)
    (<0.0>,1137) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,1150)
    (<0.0>,1151) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1156) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1157) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1158) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1161) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1163) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1164) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1166) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1169) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1170) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1171) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1172) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1177) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1178) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1179) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1180) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1182) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1184) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1185) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1187) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1190) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1191) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1192) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1198) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,1214)
    (<0.0>,1215) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1218)
    (<0.0>,1219) tree.c:625: return &rsp->node[0];
    (<0.0>,1223) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1224) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1229) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1230) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1231) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1232) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1234) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1236) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1237) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1239) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1242) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1243) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1244) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1248) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1249) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1251) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1254) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1255) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1259) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1260) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1261) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1262) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1264) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1266) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1267) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1269) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1272) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1273) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1274) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1278) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1281) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1284) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1287) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1288) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1291) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,1293) tree.c:666: }
    (<0.0>,1296) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,1299)
    (<0.0>,1300) tree.c:625: return &rsp->node[0];
    (<0.0>,1306)
    (<0.0>,1307) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,1311) fake_sync.h:115: preempt_disable();
    (<0.0>,1313) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1314) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1321) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,1327) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1329) fake_sched.h:43: return __running_cpu;
    (<0.0>,1332) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1334) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1336) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1337) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1340)
    (<0.0>,1341) tree.c:625: return &rsp->node[0];
    (<0.0>,1345) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1346) tree.c:2334: bool ret = false;
    (<0.0>,1347) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1348) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1349) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1357)
    (<0.0>,1358)
    (<0.0>,1359)
    (<0.0>,1360) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1363) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1366) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1369) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1370) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1373) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1375) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1378) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1380) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1381) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1383) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1386) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1391) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1393) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1394) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1397) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1399) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1402) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1404) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1407) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1408) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1411) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1414) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1416) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1419) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1420) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1422) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1425) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1426) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1428) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1431) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1432) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1434) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1437) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1439) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1441) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1442) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1444) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1446) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1449) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1451) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1454) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1455) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1458) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1461) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1463) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1466) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1467) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1469) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1472) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1473) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1475) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1478) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1479) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1481) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1484) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1486) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1488) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1489) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1491) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1493) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1496) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1497) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1498) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1507)
    (<0.0>,1508)
    (<0.0>,1509)
    (<0.0>,1510) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1513) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1516) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1519) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1520) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1523) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1524) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1529)
    (<0.0>,1530)
    (<0.0>,1531) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1534)
    (<0.0>,1535) tree.c:625: return &rsp->node[0];
    (<0.0>,1539) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1542) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1544) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1545) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1547) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1550) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1552) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1554) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1556) tree.c:1585: }
    (<0.0>,1558) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1559) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1561) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1564) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1566) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1569) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1570) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1573) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1576) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1580) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1582) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1584) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1587) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1589) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1592) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1593) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1596) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1599) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1603) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1605) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1607) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1610) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1612) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1616) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1619) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1622) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1623) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1625) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1628) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1629) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1630) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1632) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1635) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1637) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1639) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1641) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1644) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1647) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1648) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1650) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1653) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1654) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1655) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1657) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1660) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1662) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1664) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1666) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1669) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1672) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1673) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1675) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1678) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1679) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1680) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1682) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1685) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1687) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1689) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1691) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1694) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1695) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1714)
    (<0.0>,1715)
    (<0.0>,1716)
    (<0.0>,1717) tree.c:1613: bool ret = false;
    (<0.0>,1718) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1720) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1723)
    (<0.0>,1724) tree.c:625: return &rsp->node[0];
    (<0.0>,1728) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1729) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1731) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1732) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1737)
    (<0.0>,1738)
    (<0.0>,1739) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1742)
    (<0.0>,1743) tree.c:625: return &rsp->node[0];
    (<0.0>,1747) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1750) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1752) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1753) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1755) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1758) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1760) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1762) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1764) tree.c:1585: }
    (<0.0>,1766) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1767) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1768) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1769) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1775)
    (<0.0>,1776)
    (<0.0>,1777)
    (<0.0>,1778) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1782) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1784) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1787) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1790) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1792) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1793) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1795) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1798) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1803) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1804) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1805) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1806) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1808) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1810) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1811) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1813) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1816) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1817) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1818) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1819) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1824) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1825) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1826) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1834) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1837) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1838) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1839) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1842) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1843) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1846) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1848) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1849) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1854)
    (<0.0>,1855)
    (<0.0>,1856) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1859)
    (<0.0>,1860) tree.c:625: return &rsp->node[0];
    (<0.0>,1864) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1867) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1869) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1870) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1872) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1875) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1877) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1879) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1881) tree.c:1585: }
    (<0.0>,1883) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1884) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1886) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1889) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1890) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1892) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1895) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1899) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1900) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1902) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1905) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1908) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1910) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1912) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1915) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1916) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1918) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1921) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1926) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1928) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1930) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1933) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1934) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1936) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1939) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1944) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1946) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1948) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1951) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1953) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1956) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1959) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1961) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1964) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1966) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1967) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1969) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1970) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1972) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1975) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1976) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1977) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1983)
    (<0.0>,1984)
    (<0.0>,1985)
    (<0.0>,1986) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1990) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1992) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1993) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1994) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2005)
    (<0.0>,2006)
    (<0.0>,2007)
    (<0.0>,2008) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2010) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2013) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2014) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2025)
    (<0.0>,2026)
    (<0.0>,2027) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2040)
    (<0.0>,2041) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2046) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2047) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2048) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2053) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2054) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2056) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2059) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2060) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2061) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2062) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2067) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2068) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2069) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2070) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2072) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2074) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2075) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2077) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2080) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2081) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2082) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2088) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2104)
    (<0.0>,2105) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2108)
    (<0.0>,2109) tree.c:625: return &rsp->node[0];
    (<0.0>,2113) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2114) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2119) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2120) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2121) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2122) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2124) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2126) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2127) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2129) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2132) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2133) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2134) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2138) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2139) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2141) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2144) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2145) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2149) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2150) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2151) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2152) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2154) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2156) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2157) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2162) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2163) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2164) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2168) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2170) tree.c:666: }
    (<0.0>,2175) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2180) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2181) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2182) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2183) fake_defs.h:237: switch (size) {
    (<0.0>,2185) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2187) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2188) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2190) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2193) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2194) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2195) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2198) tree.c:2318: return true;
    (<0.0>,2200) tree.c:2319: }
    (<0.0>,2203) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2206) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2207) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2211) tree.c:1689: if (c_out != NULL)
    (<0.0>,2214) tree.c:1691: return ret;
    (<0.0>,2218) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,2219) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2222) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2223) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2229) tree.c:1798: return ret;
    (<0.0>,2231) tree.c:1798: return ret;
    (<0.0>,2233) tree.c:1799: }
    (<0.0>,2235) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,2237) tree.c:1843: }
    (<0.0>,2241) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,2242) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2243) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2244) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2255)
    (<0.0>,2256)
    (<0.0>,2257)
    (<0.0>,2258) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2260) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2263) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2264) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2275)
    (<0.0>,2276)
    (<0.0>,2277) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2290)
    (<0.0>,2291) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2296) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2297) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2298) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2299) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2301) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2303) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2304) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2306) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2309) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2310) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2311) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2312) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2317) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2318) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2320) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2322) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2324) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2325) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2327) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2330) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2331) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2332) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2338) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2354)
    (<0.0>,2355) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2358)
    (<0.0>,2359) tree.c:625: return &rsp->node[0];
    (<0.0>,2363) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2364) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2369) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2370) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2371) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2372) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2374) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2376) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2377) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2379) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2382) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2383) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2384) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2388) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2389) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2391) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2394) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2395) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2399) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2400) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2401) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2407) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2409) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2412) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2413) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2414) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2418) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2420) tree.c:666: }
    (<0.0>,2425) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2430) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2431) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2432) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2433) fake_defs.h:237: switch (size) {
    (<0.0>,2435) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2437) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2438) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2440) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2443) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2444) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2445) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2448) tree.c:2318: return true;
    (<0.0>,2450) tree.c:2319: }
    (<0.0>,2454) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2455) tree.c:2346: return ret;
    (<0.0>,2459) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,2463) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2464) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2465) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2468)
    (<0.0>,2469) tree.c:625: return &rsp->node[0];
    (<0.0>,2474) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2478)
    (<0.0>,2479)
    (<0.0>,2480) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2481) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2484) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,2487) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2489) fake_sched.h:43: return __running_cpu;
    (<0.0>,2493) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2495) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2499) fake_sched.h:43: return __running_cpu;
    (<0.0>,2503) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2511) tree.c:3021: if (needwake)
    (<0.0>,2514) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,2522)
    (<0.0>,2523) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2524) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2526) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2533) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2536)
    (<0.0>,2537) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2539) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2542) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2549) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2552) tree_plugin.h:2457: }
    (<0.0>,2556) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2559) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2560) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2561) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2565) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2566) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2567) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2569) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2573) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,2585) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2587) fake_sched.h:43: return __running_cpu;
    (<0.0>,2590) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2592) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2594) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2595) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2597) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2604) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2605) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2607) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2613) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2614) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2615) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2616) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2617) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2621)
    (<0.0>,2622)
    (<0.0>,2623) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2624) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2649)
    (<0.0>,2650)
    (<0.0>,2651) tree.c:1905: local_irq_save(flags);
    (<0.0>,2654) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2656) fake_sched.h:43: return __running_cpu;
    (<0.0>,2660) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2662) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2666) fake_sched.h:43: return __running_cpu;
    (<0.0>,2670) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2675) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2677) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2678) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2679) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2681) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2682) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2687) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2688) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2689) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2690) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2695) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2697) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2700) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2701) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2702) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2705) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2707) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2708) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2713) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2714) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2715) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2716) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2718) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2720) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2721) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2723) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2726) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2727) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2728) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2731) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2735) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2736) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2737) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2740) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2742) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2743) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2746) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2749) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2750) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2758) tree.c:1911: local_irq_restore(flags);
    (<0.0>,2761) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2763) fake_sched.h:43: return __running_cpu;
    (<0.0>,2767) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2769) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2773) fake_sched.h:43: return __running_cpu;
    (<0.0>,2777) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2784) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2786) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2791) tree.c:3016: local_irq_save(flags);
    (<0.0>,2794) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2802) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2806) fake_sched.h:43: return __running_cpu;
    (<0.0>,2810) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2815) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2816) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2827)
    (<0.0>,2828)
    (<0.0>,2829) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2842)
    (<0.0>,2843) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2849) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2850) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2855) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2858) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2863) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2864) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2870) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2871) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2872) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2877) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2879) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2882) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2883) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2890) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2906)
    (<0.0>,2907) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2910)
    (<0.0>,2911) tree.c:625: return &rsp->node[0];
    (<0.0>,2915) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2916) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2921) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2922) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2923) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2924) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2926) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2929) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2934) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2935) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2936) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2940) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2941) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2943) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2946) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2947) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2951) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2952) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2953) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2954) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2956) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2959) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2964) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2965) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2966) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2970) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2973) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2976) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2979) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2980) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2983) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2985) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2988) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2991) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2994) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2995) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2997) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3000) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3004) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3006) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3008) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3011) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3014) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3017) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3018) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3020) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3023) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3027) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3029) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3031) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3034) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,3036) tree.c:666: }
    (<0.0>,3039) tree.c:3024: local_irq_restore(flags);
    (<0.0>,3042) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3044) fake_sched.h:43: return __running_cpu;
    (<0.0>,3048) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3050) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3054) fake_sched.h:43: return __running_cpu;
    (<0.0>,3058) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3064) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,3067)
    (<0.0>,3068) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3070) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3073) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3080) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3083) tree_plugin.h:2457: }
    (<0.0>,3087) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3090) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3091) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3092) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3096) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3097) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3098) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3100) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3108) fake_sched.h:43: return __running_cpu;
    (<0.0>,3112) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,3122) fake_sched.h:43: return __running_cpu;
    (<0.0>,3126) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3127) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3129) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3130) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3131) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3133) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3135) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3136) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3137) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3138) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3139) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3140) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3142) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3150) tree_plugin.h:2879: }
    (<0.0>,3156) fake_sched.h:43: return __running_cpu;
    (<0.0>,3160) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,3163) tree.c:755: local_irq_save(flags);
    (<0.0>,3166) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3168) fake_sched.h:43: return __running_cpu;
    (<0.0>,3172) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3174) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3178) fake_sched.h:43: return __running_cpu;
    (<0.0>,3182) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3194) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3196) fake_sched.h:43: return __running_cpu;
    (<0.0>,3200) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3201) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3203) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3204) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3205) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3206) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3207) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3208) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3209) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3213) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3215) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3216) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3217) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3233)
    (<0.0>,3235) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3237) fake_sched.h:43: return __running_cpu;
    (<0.0>,3241) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3244) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3245) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3246) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3250) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3251) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3252) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3254) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3259) fake_sched.h:43: return __running_cpu;
    (<0.0>,3262) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3264) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3266) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3267) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3270) tree_plugin.h:2457: }
    (<0.0>,3273) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3276) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3277) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3278) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3282) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3283) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3284) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3286) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3291) fake_sched.h:43: return __running_cpu;
    (<0.0>,3294) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3296) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3298) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3299) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3302) tree_plugin.h:2457: }
    (<0.0>,3305) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3308) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3309) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3310) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3314) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3315) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3316) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3318) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3325) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3328) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3329) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3330) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3332) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3333) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3335) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3336) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3337) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3338) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3352) tree_plugin.h:2879: }
    (<0.0>,3354) tree.c:758: local_irq_restore(flags);
    (<0.0>,3357) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3359) fake_sched.h:43: return __running_cpu;
    (<0.0>,3363) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3365) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3369) fake_sched.h:43: return __running_cpu;
    (<0.0>,3373) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3379) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3382) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3387) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3392) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3393) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3394) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3395) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3397) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3399) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3400) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3405) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3406) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3407) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3414) fake_sched.h:43: return __running_cpu;
    (<0.0>,3418)
    (<0.0>,3419) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3422) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3427) tree.c:892: local_irq_save(flags);
    (<0.0>,3430) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3432) fake_sched.h:43: return __running_cpu;
    (<0.0>,3436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3442) fake_sched.h:43: return __running_cpu;
    (<0.0>,3446) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3458) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3460) fake_sched.h:43: return __running_cpu;
    (<0.0>,3464) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3465) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3467) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3468) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3469) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3470) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3471) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3472) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3473) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3477) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3479) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3480) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3481) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3492)
    (<0.0>,3493) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3495) fake_sched.h:43: return __running_cpu;
    (<0.0>,3499) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3503) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3506) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3507) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3508) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3510) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3511) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3513) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3514) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3515) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3516) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3526) tree_plugin.h:2883: }
    (<0.0>,3528) tree.c:895: local_irq_restore(flags);
    (<0.0>,3531) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3533) fake_sched.h:43: return __running_cpu;
    (<0.0>,3537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3539) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3543) fake_sched.h:43: return __running_cpu;
    (<0.0>,3547) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3554) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3556) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3557) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,3602)
    (<0.0>,3603) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3606)
    (<0.0>,3607) tree.c:625: return &rsp->node[0];
    (<0.0>,3611) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3613) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3614) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3615) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3620) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3621) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3622) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3623) fake_defs.h:237: switch (size) {
    (<0.0>,3625) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3627) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3628) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3630) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3633) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3634) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3635) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3636) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,3639)
    (<0.0>,3640) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,3644) fake_sync.h:99: local_irq_disable();
    (<0.0>,3647) fake_sched.h:43: return __running_cpu;
    (<0.0>,3651) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,3655) fake_sched.h:43: return __running_cpu;
    (<0.0>,3659) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3664) fake_sched.h:43: return __running_cpu;
    (<0.0>,3668) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3671) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3672) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3679) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3684) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3685) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3686) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3691) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3697) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3698) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3699) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3710)
    (<0.0>,3716)
    (<0.0>,3721) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3726) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3727) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3728) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3729) fake_defs.h:237: switch (size) {
    (<0.0>,3731) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3733) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3734) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3736) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3739) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3740) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3741) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3742) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3755)
    (<0.0>,3756) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3761) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3762) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3763) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3774) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3775) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3776) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3777) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3782) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3783) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3784) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3785) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3787) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3789) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3790) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3792) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3795) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3796) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3797) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3805) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3806) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3819)
    (<0.0>,3820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3825) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3826) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3827) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3830) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3833) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3835) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3838) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3839) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3840) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3841) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3846) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3847) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3849) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3854) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3859) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3860) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3868) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3869) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3870) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3873) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,3888)
    (<0.0>,3889) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3890) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3891) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3892) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3894) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3915) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3916) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3917) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3918) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3920) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3922) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3923) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3925) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3928) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3929) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3930) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3931) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3932) update.c:466: if (till_stall_check < 3) {
    (<0.0>,3935) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,3939) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,3944) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,3946) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3947) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3949) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3950) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3955) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3956) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3957) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3958) fake_defs.h:237: switch (size) {
    (<0.0>,3960) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3962) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3963) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3965) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3968) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3969) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3970) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3971) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3972) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3975) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3977) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3978) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3983) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3984) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3985) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3986) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3988) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3990) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3991) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3993) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3996) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3997) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3998) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3999) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4001) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4005) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4007) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4009) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4010) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4012) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4013) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4014) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4018) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4021)
    (<0.0>,4022) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4026)
    (<0.0>,4027) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4028) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4033) fake_sched.h:43: return __running_cpu;
    (<0.0>,4037) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4039) fake_sched.h:43: return __running_cpu;
    (<0.0>,4043) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4050) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4053) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4056) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4057) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4059) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4060) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4062) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4067) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4068) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4072)
    (<0.0>,4073)
    (<0.0>,4074) tree.c:1922: if (delay > 0 &&
    (<0.0>,4078) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4081)
    (<0.0>,4082) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4086) fake_sync.h:99: local_irq_disable();
    (<0.0>,4089) fake_sched.h:43: return __running_cpu;
    (<0.0>,4093) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4097) fake_sched.h:43: return __running_cpu;
    (<0.0>,4101) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4106) fake_sched.h:43: return __running_cpu;
    (<0.0>,4110) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4113) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4114) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4121) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4123) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4124) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4126) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4129) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4131) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4132) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4133) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4135) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4136) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4138) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4139) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4143) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4145) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4151) tree.c:1983: if (!oldmask) /* First online CPU for this rcu_node. */
    (<0.0>,4154) tree.c:1984: rcu_init_new_rnp(rnp);
    (<0.0>,4159)
    (<0.0>,4160) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4161) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4163) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4165) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4166) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4167) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4169) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4170) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4171) tree.c:3752: if (rnp == NULL)
    (<0.0>,4177) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4179) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4182) tree.c:2007: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4185)
    (<0.0>,4186) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4190)
    (<0.0>,4191) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4192) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4197) fake_sched.h:43: return __running_cpu;
    (<0.0>,4201) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4203) fake_sched.h:43: return __running_cpu;
    (<0.0>,4207) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4215) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4217) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4219) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4220) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4222) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4227) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4230) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4232) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4233) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4235) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4240) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4241) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4245)
    (<0.0>,4246)
    (<0.0>,4247) tree.c:1922: if (delay > 0 &&
    (<0.0>,4251) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4254)
    (<0.0>,4255) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4259) fake_sync.h:99: local_irq_disable();
    (<0.0>,4262) fake_sched.h:43: return __running_cpu;
    (<0.0>,4266) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4270) fake_sched.h:43: return __running_cpu;
    (<0.0>,4274) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4279) fake_sched.h:43: return __running_cpu;
    (<0.0>,4283) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4286) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4287) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4295) fake_sched.h:43: return __running_cpu;
    (<0.0>,4298) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4300) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4302) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4303) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,4309)
    (<0.0>,4310) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4312) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4317) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4318) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4320) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4324) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4325) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4326) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4328) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4330) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4331) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4333) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4335) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4337) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4338) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4339) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4344) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4345) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4346) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4347) fake_defs.h:237: switch (size) {
    (<0.0>,4349) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4351) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4352) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4354) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4357) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4358) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4359) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4360) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4362) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4363) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4365) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4370) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4371) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4373) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4374) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4376) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4380) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4381) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4382) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4385) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4386) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4388) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4391) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4392) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4393) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4415)
    (<0.0>,4416)
    (<0.0>,4417)
    (<0.0>,4418) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4420) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4421) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4423) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4426) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4430) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4431) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4432) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4433) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4435) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4436) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4437) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4438) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4441) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4444) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4445) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4453) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4454) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4455) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4464)
    (<0.0>,4465)
    (<0.0>,4466)
    (<0.0>,4467) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4470) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4473) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4476) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4477) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4480) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4481) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4486)
    (<0.0>,4487)
    (<0.0>,4488) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4491)
    (<0.0>,4492) tree.c:625: return &rsp->node[0];
    (<0.0>,4496) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4499) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4501) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4502) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4504) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4507) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4509) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4511) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4513) tree.c:1585: }
    (<0.0>,4515) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4516) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4518) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4521) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4523) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4526) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4527) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4530) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4533) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4537) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4539) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4541) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4544) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4546) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4549) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4550) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4553) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4556) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4559) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4561) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4564) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4565) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4570) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4572) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4576) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4579) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4582) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4583) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4585) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4588) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4589) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4590) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4592) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4595) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4597) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4599) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4601) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4604) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4607) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4608) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4610) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4613) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4614) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4615) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4617) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4620) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4622) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4624) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4626) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4629) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4630) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4649)
    (<0.0>,4650)
    (<0.0>,4651)
    (<0.0>,4652) tree.c:1613: bool ret = false;
    (<0.0>,4653) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4655) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4658)
    (<0.0>,4659) tree.c:625: return &rsp->node[0];
    (<0.0>,4663) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4664) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4666) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4667) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4672)
    (<0.0>,4673)
    (<0.0>,4674) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4677)
    (<0.0>,4678) tree.c:625: return &rsp->node[0];
    (<0.0>,4682) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4685) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4687) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4688) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4690) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4693) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4695) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4697) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4699) tree.c:1585: }
    (<0.0>,4701) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4702) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4703) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4704) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4710)
    (<0.0>,4711)
    (<0.0>,4712)
    (<0.0>,4713) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4717) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4719) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4722) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4725) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4727) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4728) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4730) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4733) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4735) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4738) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4740) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4741) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4742) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4743) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4749)
    (<0.0>,4750)
    (<0.0>,4751)
    (<0.0>,4752) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4757) tree.c:1689: if (c_out != NULL)
    (<0.0>,4760) tree.c:1691: return ret;
    (<0.0>,4764) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4765) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4768) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4769) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4775) tree.c:1798: return ret;
    (<0.0>,4777) tree.c:1798: return ret;
    (<0.0>,4779) tree.c:1799: }
    (<0.0>,4782) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4784) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4786) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4787) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4789) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4792) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4794) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4795) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4797) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4800) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4802) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4803) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4805) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4811) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4812) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4815) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4819) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4821) fake_sched.h:43: return __running_cpu;
    (<0.0>,4825) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4826) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4828) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4829) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4831) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4834) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4835) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,4838)
    (<0.0>,4839) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4841) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4842) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4844) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4854)
    (<0.0>,4859) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4863) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4864) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4865) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4866) fake_defs.h:237: switch (size) {
    (<0.0>,4868) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4869) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4870) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4871) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4874) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4877) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4878) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4881) tree.c:1896: return ret;
    (<0.0>,4885) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,4888) tree_plugin.h:1231: }
    (<0.0>,4892) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4895)
    (<0.0>,4896) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4900)
    (<0.0>,4901) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4902) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4907) fake_sched.h:43: return __running_cpu;
    (<0.0>,4911) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4913) fake_sched.h:43: return __running_cpu;
    (<0.0>,4917) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4932) fake_sched.h:43: return __running_cpu;
    (<0.0>,4938) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,4944) fake_sched.h:43: return __running_cpu;
    (<0.0>,4951) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,4953) fake_sched.h:43: return __running_cpu;
    (<0.0>,4960) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
    (<0.0>,4968) fake_sched.h:43: return __running_cpu;
    (<0.0>,4972) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,4985) fake_sched.h:43: return __running_cpu;
    (<0.0>,4989) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,4992) tree.c:755: local_irq_save(flags);
    (<0.0>,4995) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4997) fake_sched.h:43: return __running_cpu;
    (<0.0>,5001) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5003) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5007) fake_sched.h:43: return __running_cpu;
    (<0.0>,5011) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5023) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5025) fake_sched.h:43: return __running_cpu;
    (<0.0>,5029) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5030) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5032) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5033) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5034) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5035) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5036) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5037) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5038) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5042) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5044) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5045) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5046) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5062)
    (<0.0>,5064) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5066) fake_sched.h:43: return __running_cpu;
    (<0.0>,5070) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5073) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5074) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5075) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5079) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5080) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5081) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5083) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5088) fake_sched.h:43: return __running_cpu;
    (<0.0>,5091) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5093) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5095) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5096) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5099) tree_plugin.h:2457: }
    (<0.0>,5102) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5105) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5106) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5107) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5111) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5112) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5113) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5115) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5120) fake_sched.h:43: return __running_cpu;
    (<0.0>,5123) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5125) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5127) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5128) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5131) tree_plugin.h:2457: }
    (<0.0>,5134) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5137) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5138) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5139) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5143) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5144) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5145) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5147) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5154) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5157) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5158) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5159) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5161) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5162) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5164) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5165) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5166) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5167) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5181) tree_plugin.h:2879: }
    (<0.0>,5183) tree.c:758: local_irq_restore(flags);
    (<0.0>,5186) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5188) fake_sched.h:43: return __running_cpu;
    (<0.0>,5192) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5194) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5198) fake_sched.h:43: return __running_cpu;
    (<0.0>,5202) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5208) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5211) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5216) fake_sched.h:43: return __running_cpu;
    (<0.0>,5220)
    (<0.0>,5221) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5224) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5229) tree.c:892: local_irq_save(flags);
    (<0.0>,5232) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5234) fake_sched.h:43: return __running_cpu;
    (<0.0>,5238) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5240) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5244) fake_sched.h:43: return __running_cpu;
    (<0.0>,5248) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5260) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5262) fake_sched.h:43: return __running_cpu;
    (<0.0>,5266) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5267) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5269) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5270) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5271) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5272) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5273) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5274) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5275) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,5279) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5281) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5282) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5283) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5294)
    (<0.0>,5295) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5297) fake_sched.h:43: return __running_cpu;
    (<0.0>,5301) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5305) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5308) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5309) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5310) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5312) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5313) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5315) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5316) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5317) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5318) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5328) tree_plugin.h:2883: }
    (<0.0>,5330) tree.c:895: local_irq_restore(flags);
    (<0.0>,5333) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5335) fake_sched.h:43: return __running_cpu;
    (<0.0>,5339) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5341) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5345) fake_sched.h:43: return __running_cpu;
    (<0.0>,5349) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5363) fake_sched.h:43: return __running_cpu;
    (<0.0>,5367) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,5376) fake_sched.h:43: return __running_cpu;
    (<0.0>,5383) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,5392) fake_sched.h:43: return __running_cpu;
    (<0.0>,5396) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5398) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5404) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5405) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5406) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5411) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5412) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5413) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5414) fake_defs.h:237: switch (size) {
    (<0.0>,5416) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5418) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5419) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5421) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5424) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5425) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5426) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5428) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5430) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5432) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5433) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5435) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5440) tree.c:2046: return true;
    (<0.0>,5442) tree.c:2047: }
    (<0.0>,5446) tree.c:2214: first_gp_fqs = true;
    (<0.0>,5447) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5448) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5449) tree.c:2216: if (j > HZ) {
    (<0.0>,5452) tree.c:2220: ret = 0;
    (<0.0>,5454) tree.c:2222: if (!ret) {
    (<0.0>,5457) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5458) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5460) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5462) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5464) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5465) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5468) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5469) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5474) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5475) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5476) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5477) fake_defs.h:237: switch (size) {
    (<0.0>,5479) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5481) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5482) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5484) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5487) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5488) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5489) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5493) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5495) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5499) fake_sched.h:43: return __running_cpu;
    (<0.0>,5503) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5507) fake_sched.h:43: return __running_cpu;
    (<0.0>,5511) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5516) fake_sched.h:43: return __running_cpu;
    (<0.0>,5520) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5531) fake_sched.h:43: return __running_cpu;
    (<0.0>,5535) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5536) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5538) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5539) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5540) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5542) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5544) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5545) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5546) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5547) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5548) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5549) tree.c:942: if (oldval)
    (<0.0>,5557) tree_plugin.h:2883: }
    (<0.0>,5563) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,5572) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5573) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5574) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5578) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5579) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5580) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5582) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5587) fake_sched.h:43: return __running_cpu;
    (<0.0>,5590) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5592) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5595) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5597) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5599) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5602) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5603) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5604) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5608) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5609) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5610) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5612) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5617) fake_sched.h:43: return __running_cpu;
    (<0.0>,5620) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5622) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5625) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5627) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5629) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5632) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5633) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5634) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5638) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5639) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5640) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5642) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5647) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,5652) fake_sched.h:43: return __running_cpu;
    (<0.0>,5657) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,5665) fake_sched.h:43: return __running_cpu;
    (<0.0>,5671) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,5685) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5686) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5687) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5691) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5692) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5693) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5695) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5699) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5701) fake_sched.h:43: return __running_cpu;
    (<0.0>,5704) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5706) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5728)
    (<0.0>,5729)
    (<0.0>,5730) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5732) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5733) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5734) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5736) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5738) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5739) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5740) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5775)
    (<0.0>,5776)
    (<0.0>,5777) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,5780) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,5793)
    (<0.0>,5794) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5799) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5800) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5801) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5802) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5804) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5806) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5807) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5809) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5812) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5813) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5814) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5815) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5821) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5822) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5823) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5825) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5830) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5833) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5834) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5835) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5841) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,5866)
    (<0.0>,5867) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,5872) tree.c:1472: j = jiffies;
    (<0.0>,5873) tree.c:1472: j = jiffies;
    (<0.0>,5874) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5879) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5880) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5881) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5882) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5884) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5886) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5887) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5889) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5892) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5893) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5894) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5895) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5897) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5902) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5903) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5904) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5905) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5907) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5909) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5910) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5912) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5915) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5916) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5917) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5918) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5920) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5925) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5926) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5927) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5930) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5932) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5933) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5935) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5938) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5939) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5940) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5941) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5943) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5948) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5949) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5950) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5951) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5953) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5955) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5956) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5961) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5962) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5963) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5964) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5965) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5966) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5970) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5971) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5977) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,5980) tree_plugin.h:2923: return false;
    (<0.0>,5983) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,5986) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5988) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5991) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5995) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5999) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6001) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6004) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,6008) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,6011) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6013) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6015) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6016) tree.c:3511: return 1;
    (<0.0>,6018) tree.c:3548: }
    (<0.0>,6022) tree.c:3561: return 1;
    (<0.0>,6024) tree.c:3563: }
    (<0.0>,6031) fake_sched.h:43: return __running_cpu;
    (<0.0>,6035) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,6039) tree.c:2891: if (user)
    (<0.0>,6047) fake_sched.h:43: return __running_cpu;
    (<0.0>,6051) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6053) fake_sched.h:43: return __running_cpu;
    (<0.0>,6057) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6063) fake_sched.h:43: return __running_cpu;
    (<0.0>,6067) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,6077) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,6080) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6081) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6082) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6086) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6087) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6088) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6090) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6094) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,6106) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6108) fake_sched.h:43: return __running_cpu;
    (<0.0>,6111) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6113) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6115) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6116) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6118) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6125) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6126) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6128) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6134) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6135) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6136) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6137) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6138) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6142)
    (<0.0>,6143)
    (<0.0>,6144) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6145) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6170)
    (<0.0>,6171)
    (<0.0>,6172) tree.c:1905: local_irq_save(flags);
    (<0.0>,6175) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6177) fake_sched.h:43: return __running_cpu;
    (<0.0>,6181) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6183) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6187) fake_sched.h:43: return __running_cpu;
    (<0.0>,6191) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6196) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6198) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6199) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6200) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6202) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6203) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6208) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6209) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6210) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6211) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6213) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6215) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6216) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6218) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6221) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6222) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6223) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6226) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6228) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6229) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6234) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6235) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6236) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6237) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6239) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6241) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6242) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6244) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6247) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6248) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6249) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6252) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6256) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6257) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6258) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6259) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6261) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6262) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6263) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6264) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6267) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6270) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6271) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6279) tree.c:1911: local_irq_restore(flags);
    (<0.0>,6282) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6284) fake_sched.h:43: return __running_cpu;
    (<0.0>,6288) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6290) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6294) fake_sched.h:43: return __running_cpu;
    (<0.0>,6298) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6305) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6307) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6310) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,6314) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,6318) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6320) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6321) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6322) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6340)
    (<0.0>,6341)
    (<0.0>,6342)
    (<0.0>,6343) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6345) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6346) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6350) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6351) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6352) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6354) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6358)
    (<0.0>,6359)
    (<0.0>,6360) fake_sync.h:83: local_irq_save(flags);
    (<0.0>,6363) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6365) fake_sched.h:43: return __running_cpu;
    (<0.0>,6369) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6371) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6375) fake_sched.h:43: return __running_cpu;
    (<0.0>,6379) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6385) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,6386) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,6393) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,6397) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,6401) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6403) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6404) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6406) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6409) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6411) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6412) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6414) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6417) tree.c:2491: rdp->gpwrap) {
    (<0.0>,6419) tree.c:2491: rdp->gpwrap) {
    (<0.0>,6422) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6424) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6425) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6426) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6428) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6429) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6433) tree.c:2508: rdp->core_needs_qs = false;
    (<0.0>,6435) tree.c:2508: rdp->core_needs_qs = false;
    (<0.0>,6436) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6437) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6438) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6447)
    (<0.0>,6448)
    (<0.0>,6449)
    (<0.0>,6450) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6453) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6456) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6459) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6460) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6463) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,6464) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,6469)
    (<0.0>,6470)
    (<0.0>,6471) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6474)
    (<0.0>,6475) tree.c:625: return &rsp->node[0];
    (<0.0>,6479) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6482) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6484) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6485) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6487) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6490) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6492) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6494) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6496) tree.c:1585: }
    (<0.0>,6498) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,6499) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,6501) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,6504) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6506) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6509) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6510) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6513) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6516) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6520) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,6522) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,6524) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,6527) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6529) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6532) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6533) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6536) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6539) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6542) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,6544) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,6547) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,6548) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,6553) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,6555) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,6559) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6562) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6565) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6566) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6568) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6571) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6572) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6573) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6575) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6578) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6580) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6582) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6584) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6587) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6590) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6591) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6593) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6596) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6597) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6598) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6600) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6603) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6605) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6607) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6609) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6612) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,6613) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,6632)
    (<0.0>,6633)
    (<0.0>,6634)
    (<0.0>,6635) tree.c:1613: bool ret = false;
    (<0.0>,6636) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,6638) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,6641)
    (<0.0>,6642) tree.c:625: return &rsp->node[0];
    (<0.0>,6646) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,6647) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,6649) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,6650) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,6655)
    (<0.0>,6656)
    (<0.0>,6657) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6660)
    (<0.0>,6661) tree.c:625: return &rsp->node[0];
    (<0.0>,6665) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6668) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6670) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6671) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6673) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6676) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6678) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6680) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6682) tree.c:1585: }
    (<0.0>,6684) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,6685) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,6686) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,6687) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,6693)
    (<0.0>,6694)
    (<0.0>,6695)
    (<0.0>,6696) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,6700) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,6702) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,6705) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,6708) tree.c:1623: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,6709) tree.c:1623: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,6710) tree.c:1623: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,6716)
    (<0.0>,6717)
    (<0.0>,6718)
    (<0.0>,6719) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,6724) tree.c:1689: if (c_out != NULL)
    (<0.0>,6727) tree.c:1691: return ret;
    (<0.0>,6731) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,6732) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,6735) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,6736) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,6742) tree.c:1798: return ret;
    (<0.0>,6744) tree.c:1798: return ret;
    (<0.0>,6746) tree.c:1799: }
    (<0.0>,6749) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6750) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6751) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6752) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6753) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6755) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6756) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6780)
    (<0.0>,6781)
    (<0.0>,6782)
    (<0.0>,6783)
    (<0.0>,6784)
    (<0.0>,6785) tree.c:2385: unsigned long oldmask = 0;
    (<0.0>,6787) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6789) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6790) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6794) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6796) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6797) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6800) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6805) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6806) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6810) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6811) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6812) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6813) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,6815) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,6817) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,6819) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,6822) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,6824) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,6830) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6831) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6832) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6834) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6838)
    (<0.0>,6839)
    (<0.0>,6840) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,6841) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,6844) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,6847) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6849) fake_sched.h:43: return __running_cpu;
    (<0.0>,6853) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6855) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6859) fake_sched.h:43: return __running_cpu;
    (<0.0>,6863) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6873) tree.c:2518: if (needwake)
    (<0.0>,6880) tree.c:3016: local_irq_save(flags);
    (<0.0>,6883) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6885) fake_sched.h:43: return __running_cpu;
    (<0.0>,6889) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6891) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6895) fake_sched.h:43: return __running_cpu;
    (<0.0>,6899) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6904) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6905) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6916)
    (<0.0>,6917)
    (<0.0>,6918) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,6931)
    (<0.0>,6932) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6937) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6938) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6939) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6940) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6944) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6945) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6947) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6950) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6951) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6952) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6953) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6958) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6959) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6960) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6963) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6965) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6966) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6968) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6971) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6972) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6973) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6979) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,6981) tree.c:666: }
    (<0.0>,6984) tree.c:3024: local_irq_restore(flags);
    (<0.0>,6987) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6989) fake_sched.h:43: return __running_cpu;
    (<0.0>,6993) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6995) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6999) fake_sched.h:43: return __running_cpu;
    (<0.0>,7003) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7009) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7012)
    (<0.0>,7013) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7015) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7018) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7025) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7028) tree_plugin.h:2457: }
    (<0.0>,7032) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7035) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7036) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7037) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7041) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7042) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7043) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7045) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7049) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,7061) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,7063) fake_sched.h:43: return __running_cpu;
    (<0.0>,7066) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,7068) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,7070) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,7071) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7073) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7080) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7081) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7083) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7089) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7090) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7091) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7092) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7093) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7097)
    (<0.0>,7098)
    (<0.0>,7099) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,7100) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,7125)
    (<0.0>,7126)
    (<0.0>,7127) tree.c:1905: local_irq_save(flags);
    (<0.0>,7130) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7132) fake_sched.h:43: return __running_cpu;
    (<0.0>,7136) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7138) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7142) fake_sched.h:43: return __running_cpu;
    (<0.0>,7146) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7151) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,7153) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,7154) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,7155) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7157) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7158) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7163) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7164) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7165) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7166) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7168) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7170) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7171) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7173) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7176) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7177) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7178) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7181) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7183) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7184) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7189) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7190) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7191) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7192) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7194) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7196) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7197) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7199) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7202) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7203) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7204) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7207) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7211) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7212) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7213) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7214) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7216) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7217) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7218) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7219) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7222) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7225) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7226) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7234) tree.c:1911: local_irq_restore(flags);
    (<0.0>,7237) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7239) fake_sched.h:43: return __running_cpu;
    (<0.0>,7243) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7245) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7249) fake_sched.h:43: return __running_cpu;
    (<0.0>,7253) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7260) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,7262) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,7267) tree.c:3016: local_irq_save(flags);
    (<0.0>,7270) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7272) fake_sched.h:43: return __running_cpu;
    (<0.0>,7276) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7278) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7282) fake_sched.h:43: return __running_cpu;
    (<0.0>,7286) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7291) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7292) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7303)
    (<0.0>,7304)
    (<0.0>,7305) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,7318)
    (<0.0>,7319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7324) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7325) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7326) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7327) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7329) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7331) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7332) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7334) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7337) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7338) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7339) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7340) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7345) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7346) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7347) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7348) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7350) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7352) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7353) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7355) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7358) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7359) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7360) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7366) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,7382)
    (<0.0>,7383) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7386)
    (<0.0>,7387) tree.c:625: return &rsp->node[0];
    (<0.0>,7391) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7392) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7397) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7398) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7399) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7400) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7405) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7407) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7410) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7411) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7412) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7416) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7417) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7419) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7422) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7423) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7427) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7428) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7429) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7430) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7432) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7434) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7435) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7437) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7440) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7441) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7442) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7446) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7449) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7452) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7455) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7456) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7459) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7461) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7464) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7467) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7470) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7471) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7473) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7476) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7480) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7482) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7484) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7487) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7490) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7493) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7494) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7496) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7499) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7503) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7505) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7507) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7510) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,7512) tree.c:666: }
    (<0.0>,7515) tree.c:3024: local_irq_restore(flags);
    (<0.0>,7518) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7520) fake_sched.h:43: return __running_cpu;
    (<0.0>,7524) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7526) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7530) fake_sched.h:43: return __running_cpu;
    (<0.0>,7534) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7540) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7543)
    (<0.0>,7544) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7546) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7549) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7556) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7559) tree_plugin.h:2457: }
    (<0.0>,7563) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7566) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7567) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7568) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7572) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7573) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7574) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7576) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7584) fake_sched.h:43: return __running_cpu;
    (<0.0>,7588) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,7598) fake_sched.h:43: return __running_cpu;
    (<0.0>,7602) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7603) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7605) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7606) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7607) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7609) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7611) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7612) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7613) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7614) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7615) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7616) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,7618) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,7626) tree_plugin.h:2879: }
    (<0.0>,7632) fake_sched.h:43: return __running_cpu;
    (<0.0>,7636) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,7639) tree.c:755: local_irq_save(flags);
    (<0.0>,7642) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7644) fake_sched.h:43: return __running_cpu;
    (<0.0>,7648) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7650) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7654) fake_sched.h:43: return __running_cpu;
    (<0.0>,7658) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7670) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7672) fake_sched.h:43: return __running_cpu;
    (<0.0>,7676) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7677) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7679) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7680) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7681) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7682) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7683) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7684) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7685) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,7689) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,7691) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,7692) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,7693) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,7709)
    (<0.0>,7711) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7713) fake_sched.h:43: return __running_cpu;
    (<0.0>,7717) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7720) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7721) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7722) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7726) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7727) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7728) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7730) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7735) fake_sched.h:43: return __running_cpu;
    (<0.0>,7738) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7740) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7742) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7743) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7746) tree_plugin.h:2457: }
    (<0.0>,7749) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7752) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7753) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7754) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7758) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7759) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7760) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7762) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7767) fake_sched.h:43: return __running_cpu;
    (<0.0>,7770) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7772) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7774) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7775) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7778) tree_plugin.h:2457: }
    (<0.0>,7781) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7784) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7785) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7786) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7790) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7791) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7792) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7794) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7801) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7804) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7805) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7806) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7808) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7809) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7811) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7812) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7813) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7814) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7828) tree_plugin.h:2879: }
    (<0.0>,7830) tree.c:758: local_irq_restore(flags);
    (<0.0>,7833) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7835) fake_sched.h:43: return __running_cpu;
    (<0.0>,7839) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7841) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7845) fake_sched.h:43: return __running_cpu;
    (<0.0>,7849) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7855) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,7858) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,7863) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7879)
    (<0.0>,7880)
    (<0.0>,7881) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7884)
    (<0.0>,7885) tree.c:625: return &rsp->node[0];
    (<0.0>,7889) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7890) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7895) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7896) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7897) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7898) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7900) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7902) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7903) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7905) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7908) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7909) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7910) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7912) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7913) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7914) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7915) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7919) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7924) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7925) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7926) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7927) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7929) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7932) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7934) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7937) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7938) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7939) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7942) tree.c:2066: return false;
    (<0.0>,7944) tree.c:2067: }
    (<0.0>,7949) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7965)
    (<0.0>,7966)
    (<0.0>,7967) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7970)
    (<0.0>,7971) tree.c:625: return &rsp->node[0];
    (<0.0>,7975) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7976) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7981) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7982) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7983) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7984) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7986) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7988) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7989) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7991) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7994) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7995) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7996) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7998) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7999) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8000) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,8001) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,8005) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8010) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8011) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8012) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8013) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8015) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8017) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8018) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8020) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8023) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8024) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8025) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8028) tree.c:2066: return false;
    (<0.0>,8030) tree.c:2067: }
    (<0.0>,8035) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8051)
    (<0.0>,8052)
    (<0.0>,8053) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8056)
    (<0.0>,8057) tree.c:625: return &rsp->node[0];
    (<0.0>,8061) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8062) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8067) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8068) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8069) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8070) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8072) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8074) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8075) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8077) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8080) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8081) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8082) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8084) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8085) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8086) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,8087) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,8091) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8096) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8097) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8098) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8099) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8101) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8103) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8104) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8106) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8109) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8110) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8111) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8114) tree.c:2066: return false;
    (<0.0>,8116) tree.c:2067: }
    (<0.0>,8121) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8137)
    (<0.0>,8138)
    (<0.0>,8139) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8142)
    (<0.0>,8143) tree.c:625: return &rsp->node[0];
    (<0.0>,8147) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8148) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8153) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8154) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8155) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8156) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8158) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8160) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8161) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8163) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8166) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8167) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8168) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8170) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8171) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8172) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,8173) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,8177) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8182) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8183) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8184) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8185) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8187) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8189) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8190) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8192) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8195) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8196) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8197) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8200) tree.c:2066: return false;
    (<0.0>,8202) tree.c:2067: }
    (<0.0>,8207) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8223)
    (<0.0>,8224)
    (<0.0>,8225) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8228)
    (<0.0>,8229) tree.c:625: return &rsp->node[0];
    (<0.0>,8233) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8234) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8239) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8240) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8241) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8242) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8244) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8246) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8247) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8249) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8252) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8253) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8254) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8256) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8257) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8258) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,8259) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,8263) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8268) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8269) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8270) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8271) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8273) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6765) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6766) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6768) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6771) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6772) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6773) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6774) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6776) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6779) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6781) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6783) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6784) tree.c:3536: return 1;
  (<0>,6786) tree.c:3548: }
  (<0>,6790) tree.c:3561: return 1;
  (<0>,6792) tree.c:3563: }
  (<0>,6799) fake_sched.h:43: return __running_cpu;
  (<0>,6803) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
  (<0>,6807) tree.c:2891: if (user)
  (<0>,6815) fake_sched.h:43: return __running_cpu;
  (<0>,6819) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
  (<0>,6821) fake_sched.h:43: return __running_cpu;
  (<0>,6825) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6831) fake_sched.h:43: return __running_cpu;
  (<0>,6835) fake_sched.h:183: if (need_softirq[get_cpu()]) {
  (<0>,6845) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,6848) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6849) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6850) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6854) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6855) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6856) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6858) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6862) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,6874) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6876) fake_sched.h:43: return __running_cpu;
  (<0>,6879) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6881) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6883) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6884) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6886) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6893) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6894) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6896) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6902) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6903) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6904) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6905) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6906) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6910)
  (<0>,6911)
  (<0>,6912) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,6913) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,6938)
  (<0>,6939)
  (<0>,6940) tree.c:1905: local_irq_save(flags);
  (<0>,6943) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6945) fake_sched.h:43: return __running_cpu;
  (<0>,6949) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6951) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6955) fake_sched.h:43: return __running_cpu;
  (<0>,6959) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6964) tree.c:1906: rnp = rdp->mynode;
  (<0>,6966) tree.c:1906: rnp = rdp->mynode;
  (<0>,6967) tree.c:1906: rnp = rdp->mynode;
  (<0>,6968) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6970) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6971) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6976) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6977) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6978) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6979) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6981) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6983) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6984) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6986) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6989) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6990) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6991) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6994) tree.c:1910: !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */
  (<0>,6998)
  (<0>,6999) tree.h:752: bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,7004) fake_sync.h:129: preempt_disable();
  (<0>,7006) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
  (<0>,7007) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
  (<0>,7010) fake_sync.h:134: return 1;
  (<0>,7012) fake_sync.h:135: }
  (<0>,7016) tree.h:752: bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,7017) tree.h:754: if (locked)
  (<0>,7023) tree.h:756: return locked;
  (<0>,7027) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7028) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7029) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7051)
  (<0>,7052)
  (<0>,7053)
  (<0>,7054) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7056) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7057) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7059) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7062) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7066) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7067) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7068) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7069) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7071) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7072) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7073) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7074) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7077) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7080) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7081) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7089) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7090) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7091) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7100)
  (<0>,7101)
  (<0>,7102)
  (<0>,7103) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7106) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7109) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7112) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7113) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7116) tree.c:1750: return false;
  (<0>,7118) tree.c:1799: }
  (<0>,7121) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7123) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7125) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7126) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7128) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7131) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7133) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7134) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7136) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7139) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7141) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7142) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7144) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7150) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7151) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7154) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7158) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7160) fake_sched.h:43: return __running_cpu;
  (<0>,7164) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7165) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7167) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7168) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7170) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7173) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7174) tree.c:1893: zero_cpu_stall_ticks(rdp);
  (<0>,7177)
  (<0>,7178) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,7180) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,7181) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,7183) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,7193)
  (<0>,7198) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7202) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7203) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7204) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7205) fake_defs.h:237: switch (size) {
  (<0>,7207) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7208) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7209) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7210) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7213) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7216) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7217) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7220) tree.c:1896: return ret;
  (<0>,7224) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7228) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7229) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7230) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7232) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7236)
  (<0>,7237)
  (<0>,7238) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,7239) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,7242) fake_sync.h:93: local_irq_restore(flags);
  (<0>,7245) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7247) fake_sched.h:43: return __running_cpu;
  (<0>,7251) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7253) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7257) fake_sched.h:43: return __running_cpu;
  (<0>,7261) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7269) tree.c:1916: if (needwake)
  (<0>,7273) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7275) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7278) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,7282) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,7286) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7288) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7290) fake_sched.h:43: return __running_cpu;
  (<0>,7294) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7299) tree.c:3016: local_irq_save(flags);
  (<0>,7302) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7304) fake_sched.h:43: return __running_cpu;
  (<0>,7308) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7310) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7314) fake_sched.h:43: return __running_cpu;
  (<0>,7318) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7323) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7324) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7335)
  (<0>,7336)
  (<0>,7337) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7350)
  (<0>,7351) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7356) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7357) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7358) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7359) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7361) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7363) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7364) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7366) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7369) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7370) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7371) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7372) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7377) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7378) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7379) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7380) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7382) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7384) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7385) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7387) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7390) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7391) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7392) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7398) tree.c:653: return false;  /* No, a grace period is already in progress. */
  (<0>,7400) tree.c:666: }
  (<0>,7403) tree.c:3024: local_irq_restore(flags);
  (<0>,7406) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7408) fake_sched.h:43: return __running_cpu;
  (<0>,7412) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7414) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7418) fake_sched.h:43: return __running_cpu;
  (<0>,7422) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7428) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7431)
  (<0>,7432) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7434) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7437) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7444) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,7447) tree_plugin.h:2457: }
  (<0>,7451) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7454) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7455) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7456) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7460) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7461) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7462) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7464) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7468) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,7480) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7482) fake_sched.h:43: return __running_cpu;
  (<0>,7485) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7487) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7489) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7490) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7492) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7499) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7500) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7502) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7508) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7509) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7510) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7511) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7512) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7516)
  (<0>,7517)
  (<0>,7518) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,7519) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,7544)
  (<0>,7545)
  (<0>,7546) tree.c:1905: local_irq_save(flags);
  (<0>,7549) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7551) fake_sched.h:43: return __running_cpu;
  (<0>,7555) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7557) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7561) fake_sched.h:43: return __running_cpu;
  (<0>,7565) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7570) tree.c:1906: rnp = rdp->mynode;
  (<0>,7572) tree.c:1906: rnp = rdp->mynode;
  (<0>,7573) tree.c:1906: rnp = rdp->mynode;
  (<0>,7574) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7576) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7577) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7582) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7583) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7584) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7585) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7587) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7589) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7590) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7592) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7595) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7596) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7597) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7600) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7602) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7603) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7608) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7609) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7610) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7611) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7613) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7615) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7616) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7618) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7621) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7622) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7623) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7626) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7630) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7631) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7632) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7633) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7635) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7636) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7637) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7638) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7641) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7644) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7645) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7653) tree.c:1911: local_irq_restore(flags);
  (<0>,7656) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7658) fake_sched.h:43: return __running_cpu;
  (<0>,7662) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7664) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7668) fake_sched.h:43: return __running_cpu;
  (<0>,7672) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7679) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7681) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7686) tree.c:3016: local_irq_save(flags);
  (<0>,7689) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7691) fake_sched.h:43: return __running_cpu;
  (<0>,7695) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7697) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7701) fake_sched.h:43: return __running_cpu;
  (<0>,7705) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7710) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7711) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7722)
  (<0>,7723)
  (<0>,7724) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7737)
  (<0>,7738) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7743) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7744) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7745) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7746) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7748) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7750) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7751) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7753) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7756) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7757) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7758) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7759) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7764) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7765) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7766) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7767) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7769) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7771) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7772) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7774) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7777) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7778) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7779) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7785) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,7801)
  (<0>,7802) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7805)
  (<0>,7806) tree.c:625: return &rsp->node[0];
  (<0>,7810) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7811) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7816) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7817) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7818) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7819) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7821) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7823) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7824) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7826) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7829) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7830) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7831) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7835) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7836) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7838) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7841) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7842) tree.c:639: return READ_ONCE(*fp);
  (<0>,7846) tree.c:639: return READ_ONCE(*fp);
  (<0>,7847) tree.c:639: return READ_ONCE(*fp);
  (<0>,7848) tree.c:639: return READ_ONCE(*fp);
  (<0>,7849) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7851) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7853) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7854) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7856) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7859) tree.c:639: return READ_ONCE(*fp);
  (<0>,7860) tree.c:639: return READ_ONCE(*fp);
  (<0>,7861) tree.c:639: return READ_ONCE(*fp);
  (<0>,7865) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7868) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7871) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7874) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7875) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7878) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7880) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7883) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7886) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7889) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7890) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7892) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7895) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7899) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7901) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7903) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7906) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7909) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7912) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7913) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7915) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7918) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7922) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7924) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7926) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7929) tree.c:665: return false; /* No grace period needed. */
  (<0>,7931) tree.c:666: }
  (<0>,7934) tree.c:3024: local_irq_restore(flags);
  (<0>,7937) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7939) fake_sched.h:43: return __running_cpu;
  (<0>,7943) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7945) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7949) fake_sched.h:43: return __running_cpu;
  (<0>,7953) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7959) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7962)
  (<0>,7963) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7965) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7968) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7975) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,7978) tree_plugin.h:2457: }
  (<0>,7982) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7985) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7986) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7987) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7991) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7992) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7993) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7995) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8003) fake_sched.h:43: return __running_cpu;
  (<0>,8007) fake_sched.h:185: need_softirq[get_cpu()] = 0;
  (<0>,8017) fake_sched.h:43: return __running_cpu;
  (<0>,8021) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8022) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8024) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8025) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8026) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8028) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8030) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8031) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8032) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8033) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8034) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8035) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,8037) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,8045) tree_plugin.h:2879: }
  (<0>,8057) fake_sched.h:43: return __running_cpu;
  (<0>,8063) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
  (<0>,8069) fake_sched.h:43: return __running_cpu;
  (<0>,8076) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
  (<0>,8078) fake_sched.h:43: return __running_cpu;
  (<0>,8085) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
  (<0>,8093) fake_sched.h:43: return __running_cpu;
  (<0>,8097) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
  (<0>,8110) fake_sched.h:43: return __running_cpu;
  (<0>,8114) fake_sched.h:96: rcu_idle_enter();
  (<0>,8117) tree.c:755: local_irq_save(flags);
  (<0>,8120) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8122) fake_sched.h:43: return __running_cpu;
  (<0>,8126) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8128) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8132) fake_sched.h:43: return __running_cpu;
  (<0>,8136) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8148) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8150) fake_sched.h:43: return __running_cpu;
  (<0>,8154) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8155) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8157) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8158) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8159) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8160) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8161) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8162) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8163) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,8167) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,8169) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,8170) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,8171) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,8187)
  (<0>,8189) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8191) fake_sched.h:43: return __running_cpu;
  (<0>,8195) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8198) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8199) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8200) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8204) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8205) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8206) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8208) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8213) fake_sched.h:43: return __running_cpu;
  (<0>,8216) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8218) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8220) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8221) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,8224) tree_plugin.h:2457: }
  (<0>,8227) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8230) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8231) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8232) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8236) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8237) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8238) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8240) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8245) fake_sched.h:43: return __running_cpu;
  (<0>,8248) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8250) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8252) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8253) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,8256) tree_plugin.h:2457: }
  (<0>,8259) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8262) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8263) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8264) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8268) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8269) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8270) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8272) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8279) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8282) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8283) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8284) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8286) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8287) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8289) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8290) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8291) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8292) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8306) tree_plugin.h:2879: }
  (<0>,8308) tree.c:758: local_irq_restore(flags);
  (<0>,8311) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8313) fake_sched.h:43: return __running_cpu;
  (<0>,8317) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8319) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8323) fake_sched.h:43: return __running_cpu;
  (<0>,8327) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8333) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8336) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8341) fake_sched.h:43: return __running_cpu;
  (<0>,8345)
  (<0>,8346) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,8349) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,8354) tree.c:892: local_irq_save(flags);
  (<0>,8357) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8359) fake_sched.h:43: return __running_cpu;
  (<0>,8363) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8365) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8369) fake_sched.h:43: return __running_cpu;
  (<0>,8373) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8385) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8387) fake_sched.h:43: return __running_cpu;
  (<0>,8391) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8392) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,8394) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,8395) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,8396) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8397) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8398) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8399) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8400) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,8404) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,8406) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,8407) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,8408) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,8419)
  (<0>,8420) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8422) fake_sched.h:43: return __running_cpu;
  (<0>,8426) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8430) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8433) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8434) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8435) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8437) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8438) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8440) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8441) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8442) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8443) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8453) tree_plugin.h:2883: }
  (<0>,8455) tree.c:895: local_irq_restore(flags);
  (<0>,8458) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8460) fake_sched.h:43: return __running_cpu;
  (<0>,8464) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8466) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8470) fake_sched.h:43: return __running_cpu;
  (<0>,8474) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8485) fake_sched.h:43: return __running_cpu;
  (<0>,8489) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
  (<0>,8493) fake_sched.h:43: return __running_cpu;
  (<0>,8497) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8502) fake_sched.h:43: return __running_cpu;
  (<0>,8506) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
  (<0>,8517) fake_sched.h:43: return __running_cpu;
  (<0>,8521) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8522) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,8524) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,8525) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,8526) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,8528) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,8530) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,8531) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8532) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8533) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8534) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8535) tree.c:942: if (oldval)
  (<0>,8543) tree_plugin.h:2883: }
  (<0>,8549) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,8558) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8559) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8560) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8564) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8565) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8566) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8568) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8573) fake_sched.h:43: return __running_cpu;
  (<0>,8576) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8578) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8581) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8583) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8585) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8588) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8589) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8590) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8594) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8595) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8596) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8598) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8603) fake_sched.h:43: return __running_cpu;
  (<0>,8606) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8608) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8611) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8613) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8615) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8618) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8619) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8620) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8624) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8625) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8626) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8628) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8633) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,8638) fake_sched.h:43: return __running_cpu;
  (<0>,8643) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,8651) fake_sched.h:43: return __running_cpu;
  (<0>,8657) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
  (<0>,8671) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8672) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8673) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8677) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8678) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8679) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8681) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8685) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8687) fake_sched.h:43: return __running_cpu;
  (<0>,8690) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8692) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8714)
  (<0>,8715)
  (<0>,8716) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8718) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8719) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8720) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8722) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8724) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8725) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,8726) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,8761)
  (<0>,8762)
  (<0>,8763) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
  (<0>,8766) tree.c:1469: !rcu_gp_in_progress(rsp))
  (<0>,8779)
  (<0>,8780) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8785) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8786) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8787) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8788) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8790) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8792) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8793) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8795) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8798) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8799) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8800) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8801) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8806) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8807) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8808) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8809) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8811) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8813) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8814) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8816) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8819) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8821) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8827) tree.c:1471: rcu_stall_kick_kthreads(rsp);
  (<0>,8852)
  (<0>,8853) tree.c:1310: if (!rcu_kick_kthreads)
  (<0>,8858) tree.c:1472: j = jiffies;
  (<0>,8859) tree.c:1472: j = jiffies;
  (<0>,8860) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8865) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8866) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8867) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8868) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8870) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8872) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8873) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8875) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8878) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8879) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8880) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8881) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8883) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8888) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8889) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8890) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8891) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8893) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8895) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8896) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8898) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8901) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8902) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8903) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8904) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8906) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8911) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8912) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8913) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8914) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8916) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8918) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8919) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8921) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8924) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8925) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8926) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8927) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8929) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8934) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8935) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8936) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8937) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8939) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8941) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8942) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8944) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8947) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8948) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8949) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8950) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8951) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
  (<0>,8952) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
  (<0>,8956) tree.c:1499: ULONG_CMP_LT(j, js) ||
  (<0>,8957) tree.c:1499: ULONG_CMP_LT(j, js) ||
  (<0>,8963) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
  (<0>,8966) tree_plugin.h:2923: return false;
  (<0>,8969) tree.c:3503: if (rcu_scheduler_fully_active &&
  (<0>,8972) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8974) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8977) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8981) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8985) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,8987) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,8990) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
  (<0>,8994) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
  (<0>,8997) tree.c:3510: rdp->n_rp_report_qs++;
  (<0>,8999) tree.c:3510: rdp->n_rp_report_qs++;
  (<0>,9001) tree.c:3510: rdp->n_rp_report_qs++;
  (<0>,9002) tree.c:3511: return 1;
  (<0>,9004) tree.c:3548: }
  (<0>,9008) tree.c:3561: return 1;
  (<0>,9010) tree.c:3563: }
  (<0>,9017) fake_sched.h:43: return __running_cpu;
  (<0>,9021) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
  (<0>,9025) tree.c:2891: if (user)
  (<0>,9033) fake_sched.h:43: return __running_cpu;
  (<0>,9037) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
  (<0>,9039) fake_sched.h:43: return __running_cpu;
  (<0>,9043) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9049) fake_sched.h:43: return __running_cpu;
  (<0>,9053) fake_sched.h:183: if (need_softirq[get_cpu()]) {
  (<0>,9063) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,9066) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9067) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9068) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9072) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9073) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9074) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9076) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9080) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,9092) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9094) fake_sched.h:43: return __running_cpu;
  (<0>,9097) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9099) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9101) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9102) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9104) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9111) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9112) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9114) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9120) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9121) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9122) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9123) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,9124) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,9128)
  (<0>,9129)
  (<0>,9130) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,9131) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,9156)
  (<0>,9157)
  (<0>,9158) tree.c:1905: local_irq_save(flags);
  (<0>,9161) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9163) fake_sched.h:43: return __running_cpu;
  (<0>,9167) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9169) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9173) fake_sched.h:43: return __running_cpu;
  (<0>,9177) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9182) tree.c:1906: rnp = rdp->mynode;
  (<0>,9184) tree.c:1906: rnp = rdp->mynode;
  (<0>,9185) tree.c:1906: rnp = rdp->mynode;
  (<0>,9186) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9188) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9189) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9194) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9195) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9196) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9197) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9199) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9201) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9202) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9204) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9207) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9208) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9209) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9212) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9214) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9215) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9220) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9221) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9222) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9223) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9225) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9227) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9228) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9230) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9233) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9234) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9235) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9238) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9242) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9243) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9244) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9245) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9247) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9248) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9249) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9250) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9253) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9256) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9257) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9265) tree.c:1911: local_irq_restore(flags);
  (<0>,9268) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9270) fake_sched.h:43: return __running_cpu;
  (<0>,9274) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9276) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9280) fake_sched.h:43: return __running_cpu;
  (<0>,9284) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9291) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,9293) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,9296) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,9300) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,9304) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,9306) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,9307) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,9308) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,9326)
  (<0>,9327)
  (<0>,9328)
  (<0>,9329) tree.c:2486: rnp = rdp->mynode;
  (<0>,9331) tree.c:2486: rnp = rdp->mynode;
  (<0>,9332) tree.c:2486: rnp = rdp->mynode;
  (<0>,9336) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,9337) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,9338) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,9340) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,9344)
  (<0>,9345)
  (<0>,9346) fake_sync.h:83: local_irq_save(flags);
  (<0>,9349) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9351) fake_sched.h:43: return __running_cpu;
  (<0>,9355) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9357) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9361) fake_sched.h:43: return __running_cpu;
  (<0>,9365) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9371) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,9372) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,9379) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
  (<0>,9383) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
  (<0>,9387) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9389) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9390) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9392) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9395) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9397) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9398) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9400) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9403) tree.c:2491: rdp->gpwrap) {
  (<0>,9405) tree.c:2491: rdp->gpwrap) {
  (<0>,9408) tree.c:2504: mask = rdp->grpmask;
  (<0>,9410) tree.c:2504: mask = rdp->grpmask;
  (<0>,9411) tree.c:2504: mask = rdp->grpmask;
  (<0>,9412) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
  (<0>,9414) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
  (<0>,9415) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
  (<0>,9419) tree.c:2508: rdp->core_needs_qs = false;
  (<0>,9421) tree.c:2508: rdp->core_needs_qs = false;
  (<0>,9422) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,9423) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,9424) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,9433)
  (<0>,9434)
  (<0>,9435)
  (<0>,9436) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9439) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9442) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9445) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9446) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9449) tree.c:1750: return false;
  (<0>,9451) tree.c:1799: }
  (<0>,9454) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,9455) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9456) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9457) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9458) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9460) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9461) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9485)
  (<0>,9486)
  (<0>,9487)
  (<0>,9488)
  (<0>,9489)
  (<0>,9490) tree.c:2385: unsigned long oldmask = 0;
  (<0>,9492) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9494) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9495) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9499) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9501) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9502) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9505) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9510) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9511) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9515) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9516) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9517) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9518) tree.c:2400: rnp->qsmask &= ~mask;
  (<0>,9520) tree.c:2400: rnp->qsmask &= ~mask;
  (<0>,9522) tree.c:2400: rnp->qsmask &= ~mask;
  (<0>,9524) tree.c:2400: rnp->qsmask &= ~mask;
  (<0>,9527) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,9529) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,9532) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,9535) tree_plugin.h:749: return 0;
  (<0>,9539) tree.c:2413: mask = rnp->grpmask;
  (<0>,9541) tree.c:2413: mask = rnp->grpmask;
  (<0>,9542) tree.c:2413: mask = rnp->grpmask;
  (<0>,9543) tree.c:2414: if (rnp->parent == NULL) {
  (<0>,9545) tree.c:2414: if (rnp->parent == NULL) {
  (<0>,9549) tree.c:2432: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
  (<0>,9550) tree.c:2432: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
  (<0>,9570)
  (<0>,9571)
  (<0>,9572) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,9585)
  (<0>,9586) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9591) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9592) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9593) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9594) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9596) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9598) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9599) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9601) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9604) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9605) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9606) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9607) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9612) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9613) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9614) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9615) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9617) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9619) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9620) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9622) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9625) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9626) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9627) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9636) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,9637) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,9650)
  (<0>,9651) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9656) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9657) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9658) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9659) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9661) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9663) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9664) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9666) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9669) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9670) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9671) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9672) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9677) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9678) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9679) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9680) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9682) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9684) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9685) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9687) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9690) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9691) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9692) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9699) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,9700) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,9701) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
  (<0>,9703) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9708) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9709) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9710) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9711) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9713) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9715) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9716) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9718) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9721) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9722) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9723) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9727) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9728) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9733) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9734) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9735) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9736) fake_defs.h:237: switch (size) {
  (<0>,9738) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
  (<0>,9740) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
  (<0>,9741) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
  (<0>,9743) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
  (<0>,9746) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9747) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9748) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
  (<0>,9752) tree.c:2363: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
  (<0>,9753) tree.c:2363: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
  (<0>,9754) tree.c:2363: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
  (<0>,9757)
  (<0>,9758) tree.c:625: return &rsp->node[0];
  (<0>,9763) tree.c:2363: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
  (<0>,9767)
  (<0>,9768)
  (<0>,9769) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,9770) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,9773) fake_sync.h:93: local_irq_restore(flags);
  (<0>,9776) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9778) fake_sched.h:43: return __running_cpu;
  (<0>,9782) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9784) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9788) fake_sched.h:43: return __running_cpu;
  (<0>,9792) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9800) tree.c:2364: rcu_gp_kthread_wake(rsp);
  (<0>,9808)
  (<0>,9809) tree.c:1722: if (current == rsp->gp_kthread ||
  (<0>,9810) tree.c:1722: if (current == rsp->gp_kthread ||
  (<0>,9812) tree.c:1722: if (current == rsp->gp_kthread ||
  (<0>,9815) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,9820) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,9821) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,9822) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,9823) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9825) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9827) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9828) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9830) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9833) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,9834) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,9835) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,9838) tree.c:1724: !rsp->gp_kthread)
  (<0>,9840) tree.c:1724: !rsp->gp_kthread)
  (<0>,9849) tree.c:2518: if (needwake)
  (<0>,9856) tree.c:3016: local_irq_save(flags);
  (<0>,9859) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9861) fake_sched.h:43: return __running_cpu;
  (<0>,9865) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9867) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9871) fake_sched.h:43: return __running_cpu;
  (<0>,9875) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9880) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9881) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9892)
  (<0>,9893)
  (<0>,9894) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,9907)
  (<0>,9908) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9913) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9914) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9915) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9916) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9918) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9920) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9921) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9923) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9926) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9927) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9928) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9929) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9934) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9935) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9936) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9937) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9939) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9941) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9942) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9944) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9947) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9948) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9949) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9955) tree.c:653: return false;  /* No, a grace period is already in progress. */
  (<0>,9957) tree.c:666: }
  (<0>,9960) tree.c:3024: local_irq_restore(flags);
  (<0>,9963) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9965) fake_sched.h:43: return __running_cpu;
  (<0>,9969) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9971) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9975) fake_sched.h:43: return __running_cpu;
  (<0>,9979) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9985) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,9988)
  (<0>,9989) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9991) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9994) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10001) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,10004) tree_plugin.h:2457: }
  (<0>,10008) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10011) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10012) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10013) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10017) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10018) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10019) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10021) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10025) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,10037) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,10039) fake_sched.h:43: return __running_cpu;
  (<0>,10042) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,10044) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,10046) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,10047) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10049) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10056) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10057) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10059) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10065) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10066) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10067) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10068) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,10069) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,10073)
  (<0>,10074)
  (<0>,10075) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,10076) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,10101)
  (<0>,10102)
  (<0>,10103) tree.c:1905: local_irq_save(flags);
  (<0>,10106) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10108) fake_sched.h:43: return __running_cpu;
  (<0>,10112) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10114) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10118) fake_sched.h:43: return __running_cpu;
  (<0>,10122) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,10127) tree.c:1906: rnp = rdp->mynode;
  (<0>,10129) tree.c:1906: rnp = rdp->mynode;
  (<0>,10130) tree.c:1906: rnp = rdp->mynode;
  (<0>,10131) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10133) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10134) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10139) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10140) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10141) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10142) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10144) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10146) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10147) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10149) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10152) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10153) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10154) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10157) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10159) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10160) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10165) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10166) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10167) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10168) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10170) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10172) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10173) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10175) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10178) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10179) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10180) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10183) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10187) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10188) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10189) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10190) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10192) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10193) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10194) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10195) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10198) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10201) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10202) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10210) tree.c:1911: local_irq_restore(flags);
  (<0>,10213) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10215) fake_sched.h:43: return __running_cpu;
  (<0>,10219) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10221) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10225) fake_sched.h:43: return __running_cpu;
  (<0>,10229) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,10236) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,10238) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,10243) tree.c:3016: local_irq_save(flags);
  (<0>,10246) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10248) fake_sched.h:43: return __running_cpu;
  (<0>,10252) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10254) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10258) fake_sched.h:43: return __running_cpu;
  (<0>,10262) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,10267) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,10268) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,10279)
  (<0>,10280)
  (<0>,10281) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,10294)
  (<0>,10295) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10300) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10301) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10302) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10303) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10305) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10307) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10308) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10310) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10313) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10314) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10315) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10316) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10321) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10322) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10323) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10324) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10326) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10328) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10329) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10331) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10334) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10335) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10336) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10342) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,10358)
  (<0>,10359) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,10362)
  (<0>,10363) tree.c:625: return &rsp->node[0];
  (<0>,10367) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,10368) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10373) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10374) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10375) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10376) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10378) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10380) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10381) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10383) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10386) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10387) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10388) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10392) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10393) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,10395) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,10398) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,10399) tree.c:639: return READ_ONCE(*fp);
  (<0>,10403) tree.c:639: return READ_ONCE(*fp);
  (<0>,10404) tree.c:639: return READ_ONCE(*fp);
  (<0>,10405) tree.c:639: return READ_ONCE(*fp);
  (<0>,10406) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10408) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10410) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10411) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10413) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10416) tree.c:639: return READ_ONCE(*fp);
  (<0>,10417) tree.c:639: return READ_ONCE(*fp);
  (<0>,10418) tree.c:639: return READ_ONCE(*fp);
  (<0>,10422) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,10425) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,10428) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,10431) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,10432) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,10435) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10437) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10440) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10443) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10446) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10447) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10449) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10452) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10456) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10458) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10460) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10463) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10466) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10469) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10470) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10472) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10475) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10479) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10481) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10483) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10486) tree.c:665: return false; /* No grace period needed. */
  (<0>,10488) tree.c:666: }
  (<0>,10491) tree.c:3024: local_irq_restore(flags);
  (<0>,10494) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10496) fake_sched.h:43: return __running_cpu;
  (<0>,10500) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10502) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10506) fake_sched.h:43: return __running_cpu;
  (<0>,10510) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,10516) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,10519)
  (<0>,10520) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10522) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10525) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10532) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,10535) tree_plugin.h:2457: }
  (<0>,10539) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10542) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10543) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10544) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10548) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10549) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10550) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10552) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10560) fake_sched.h:43: return __running_cpu;
  (<0>,10564) fake_sched.h:185: need_softirq[get_cpu()] = 0;
  (<0>,10574) fake_sched.h:43: return __running_cpu;
  (<0>,10578) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,10579) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,10581) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,10582) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,10583) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,10585) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,10587) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,10588) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10589) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10590) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10591) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10592) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,10594) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,10602) tree_plugin.h:2879: }
    (<0.0>,8275) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8276) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8278) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8281) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8282) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8283) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8286) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8289)
    (<0.0>,8293) tree.c:2064: return true;
    (<0.0>,8295) tree.c:2067: }
    (<0.0>,8300) fake_sched.h:43: return __running_cpu;
    (<0.0>,8304)
    (<0.0>,8305) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8308) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8313) tree.c:892: local_irq_save(flags);
    (<0.0>,8316)
    (<0.0>,8318) fake_sched.h:43: return __running_cpu;
    (<0.0>,8322) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8324) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8328) fake_sched.h:43: return __running_cpu;
    (<0.0>,8332) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8344)
    (<0.0>,8346) fake_sched.h:43: return __running_cpu;
    (<0.0>,8350) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8351) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8353) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8354) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8355) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8356) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8357) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8358) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8359) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,8363) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8365) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8366) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8367) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8378)
    (<0.0>,8379)
    (<0.0>,8381) fake_sched.h:43: return __running_cpu;
    (<0.0>,8385) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8389) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8392) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8393) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8394) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8396) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8397) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8399) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8400) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8401) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8402) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8412)
    (<0.0>,8414) tree.c:895: local_irq_restore(flags);
    (<0.0>,8417)
    (<0.0>,8419) fake_sched.h:43: return __running_cpu;
    (<0.0>,8423) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8425) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8429) fake_sched.h:43: return __running_cpu;
    (<0.0>,8433) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8440) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8441) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8442) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8443) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,8445) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,8446) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8451) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8452) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8453) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8454) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8456) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8458) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8459) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8461) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8464) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8465) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8466) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8469) tree.c:2237: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8472)
    (<0.0>,8477) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,8479) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,8480) tree.c:2280: rcu_gp_cleanup(rsp);
    (<0.0>,8520)
    (<0.0>,8521) tree.c:2109: bool needgp = false;
    (<0.0>,8522) tree.c:2110: int nocb = 0;
    (<0.0>,8523) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8526)
    (<0.0>,8527) tree.c:625: return &rsp->node[0];
    (<0.0>,8531) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8533) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8534) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8535) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8540) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8541) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8542) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8543) fake_defs.h:237: switch (size) {
    (<0.0>,8545) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8547) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8548) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8550) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8553) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8554) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8555) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8556) tree.c:2116: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,8559)
    (<0.0>,8560) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8564)
    (<0.0>,8567) fake_sched.h:43: return __running_cpu;
    (<0.0>,8571) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8575) fake_sched.h:43: return __running_cpu;
    (<0.0>,8579) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8584) fake_sched.h:43: return __running_cpu;
    (<0.0>,8588) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8591) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8592) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8599) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8600) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8602) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8604) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8605) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,8606) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,8608) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,8611) tree.c:2129: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,8614)
    (<0.0>,8615) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8619)
    (<0.0>,8620) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8621) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8626) fake_sched.h:43: return __running_cpu;
    (<0.0>,8630) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,8632) fake_sched.h:43: return __running_cpu;
    (<0.0>,8636) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8643) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8646) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8648) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8649) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8651) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8656) tree.c:2141: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,8659)
    (<0.0>,8660) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8664)
    (<0.0>,8667) fake_sched.h:43: return __running_cpu;
    (<0.0>,8671) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8675) fake_sched.h:43: return __running_cpu;
    (<0.0>,8679) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8684) fake_sched.h:43: return __running_cpu;
    (<0.0>,8688) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8691) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8692) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8699) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8702)
    (<0.0>,8708) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8709) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8712)
    (<0.0>,8717) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8718) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8719) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8720) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8722) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8727) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8728) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8730) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8734) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8735) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8736) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8738) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8740) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8741) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8742) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8747) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8748) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8749) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8750) fake_defs.h:237: switch (size) {
    (<0.0>,8752) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8754) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8755) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8757) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8760) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8761) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8762) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8764) fake_sched.h:43: return __running_cpu;
    (<0.0>,8767) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8769) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8771) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8772) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8773) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8775) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8778) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8779) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8780) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8802)
    (<0.0>,8803)
    (<0.0>,8804)
    (<0.0>,8805) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8807) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8808) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8810) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8813) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8814) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8815) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8823)
    (<0.0>,8824)
    (<0.0>,8825)
    (<0.0>,8826) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8829) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8832) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8835) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8836) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8839) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8841) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8844) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8846) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8847) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8849) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8852) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8856) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8858) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8861) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8862) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8865) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8867) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8869) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8871) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8874) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8876) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8877) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8879) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8882) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8887) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8889) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8890) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8893) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8896) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8897) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8899) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8902) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8904) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8906) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8908) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8909) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8912) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,8914) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,8917) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8919) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8922) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8923) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8926) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8930) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8931) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8932) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8941)
    (<0.0>,8942)
    (<0.0>,8943)
    (<0.0>,8944) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8947) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8950) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8953) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8954) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8957) tree.c:1750: return false;
    (<0.0>,8959) tree.c:1799: }
    (<0.0>,8961) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8963) tree.c:1843: }
    (<0.0>,8966) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8967) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8969) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8970) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8972) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8976) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8978) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8979) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8981) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8984) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8988) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8989) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8990) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8991) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8993) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8994) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8995) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8996) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8999) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,9002) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,9003) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,9011) tree.c:1896: return ret;
    (<0.0>,9015) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,9019) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,9021) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,9022) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,9029)
    (<0.0>,9030)
    (<0.0>,9031) tree.c:1702: int c = rnp->completed;
    (<0.0>,9033) tree.c:1702: int c = rnp->completed;
    (<0.0>,9035) tree.c:1702: int c = rnp->completed;
    (<0.0>,9037) fake_sched.h:43: return __running_cpu;
    (<0.0>,9040) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9042) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9044) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9045) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,9048) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,9051) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,9052) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,9056) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,9059) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,9060) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,9061) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,9062) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,9063) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,9065) tree.c:1709: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,9073)
    (<0.0>,9074)
    (<0.0>,9075)
    (<0.0>,9076)
    (<0.0>,9080) tree.c:1710: return needmore;
    (<0.0>,9082) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,9084) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,9085) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,9088)
    (<0.0>,9090) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,9091) tree.c:2151: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,9094)
    (<0.0>,9095) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,9099)
    (<0.0>,9100) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,9101) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,9106) fake_sched.h:43: return __running_cpu;
    (<0.0>,9110) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9112) fake_sched.h:43: return __running_cpu;
    (<0.0>,9116) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9123) tree.c:2152: rcu_nocb_gp_cleanup(sq);
    (<0.0>,9126)
    (<0.0>,9136) fake_sched.h:43: return __running_cpu;
    (<0.0>,9142) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,9150) fake_sched.h:43: return __running_cpu;
    (<0.0>,9154) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,9167) fake_sched.h:43: return __running_cpu;
    (<0.0>,9171)
    (<0.0>,9174) tree.c:755: local_irq_save(flags);
    (<0.0>,9177)
    (<0.0>,9179) fake_sched.h:43: return __running_cpu;
    (<0.0>,9183) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9185) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9189) fake_sched.h:43: return __running_cpu;
    (<0.0>,9193) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9205)
    (<0.0>,9207) fake_sched.h:43: return __running_cpu;
    (<0.0>,9211) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9212) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,9214) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,9215) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,9216) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9217) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9218) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9219) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9220) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,9224) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,9226) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,9227) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9228) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9244)
    (<0.0>,9246)
    (<0.0>,9248) fake_sched.h:43: return __running_cpu;
    (<0.0>,9252) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9255) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9256) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9257) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9261) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9262) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9263) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9265) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9270) fake_sched.h:43: return __running_cpu;
    (<0.0>,9273) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9275) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9277) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9278) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9281)
    (<0.0>,9284) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9287) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9288) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9289) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9293) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9294) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9295) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9297) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9302) fake_sched.h:43: return __running_cpu;
    (<0.0>,9305) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9307) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9309) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9310) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9313)
    (<0.0>,9316) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9319) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9320) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9321) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9325) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9326) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9327) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9329) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9336) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9339) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9340) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9341) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9343) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9344) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9346) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9347) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9348) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9349) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9363)
    (<0.0>,9365) tree.c:758: local_irq_restore(flags);
    (<0.0>,9368)
    (<0.0>,9370) fake_sched.h:43: return __running_cpu;
    (<0.0>,9374) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9376) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9380) fake_sched.h:43: return __running_cpu;
    (<0.0>,9384) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9390) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9393) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9398) fake_sched.h:43: return __running_cpu;
    (<0.0>,9402)
    (<0.0>,9403) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9406) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9411) tree.c:892: local_irq_save(flags);
    (<0.0>,9414)
    (<0.0>,9416) fake_sched.h:43: return __running_cpu;
    (<0.0>,9420) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9422) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9426) fake_sched.h:43: return __running_cpu;
    (<0.0>,9430) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9442)
    (<0.0>,9444) fake_sched.h:43: return __running_cpu;
    (<0.0>,9448) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9449) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,9451) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,9452) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,9453) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9454) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9455) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9456) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9457) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,9461) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9463) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9464) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9465) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9476)
    (<0.0>,9477)
    (<0.0>,9479) fake_sched.h:43: return __running_cpu;
    (<0.0>,9483) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9487) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9490) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9491) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9492) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9494) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9495) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9497) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9498) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9499) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9500) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9510)
    (<0.0>,9512) tree.c:895: local_irq_restore(flags);
    (<0.0>,9515)
    (<0.0>,9517) fake_sched.h:43: return __running_cpu;
    (<0.0>,9521) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9523) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9527) fake_sched.h:43: return __running_cpu;
    (<0.0>,9531) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9545) fake_sched.h:43: return __running_cpu;
    (<0.0>,9549) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,9558) fake_sched.h:43: return __running_cpu;
    (<0.0>,9565) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,9574) fake_sched.h:43: return __running_cpu;
    (<0.0>,9578) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,9580) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,9586) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9587) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9588) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9593) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9594) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9595) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9596) fake_defs.h:237: switch (size) {
    (<0.0>,9598) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9600) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9601) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9603) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9606) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9607) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9608) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9609) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,9610) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,9614)
    (<0.0>,9615)
    (<0.0>,9616) tree.c:1922: if (delay > 0 &&
    (<0.0>,9621) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9623) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9625) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9626) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9628) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9633) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,9636)
    (<0.0>,9637) tree.c:625: return &rsp->node[0];
    (<0.0>,9641) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,9642) tree.c:2158: raw_spin_lock_irq_rcu_node(rnp); /* Order GP before ->completed update. */
    (<0.0>,9645)
    (<0.0>,9646) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,9650)
    (<0.0>,9653) fake_sched.h:43: return __running_cpu;
    (<0.0>,9657) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9661) fake_sched.h:43: return __running_cpu;
    (<0.0>,9665) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9670) fake_sched.h:43: return __running_cpu;
    (<0.0>,9674) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9677) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,9678) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,9685) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,9686) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,9690)
    (<0.0>,9691)
    (<0.0>,9694) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9696) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9697) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9698) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9703) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9704) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9705) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9706) fake_defs.h:237: switch (size) {
    (<0.0>,9708) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9710) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9711) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9713) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9716) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9717) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9718) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9721) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,9723) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,9725) fake_sched.h:43: return __running_cpu;
    (<0.0>,9728) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9730) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9732) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9733) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9734) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9735) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9743)
    (<0.0>,9744)
    (<0.0>,9745)
    (<0.0>,9746) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9749) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9752) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9755) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9756) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9759) tree.c:1818: return false;
    (<0.0>,9761) tree.c:1843: }
    (<0.0>,9764) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9768) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9769) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9772) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9773) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9784)
    (<0.0>,9785)
    (<0.0>,9786) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,9799)
    (<0.0>,9800) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9805) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9806) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9807) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9808) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9810) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9812) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9813) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9815) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9818) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9819) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9821) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9826) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9827) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9828) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9833) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9834) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9836) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9839) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9840) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9841) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9847) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,9863)
    (<0.0>,9864) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9867)
    (<0.0>,9868) tree.c:625: return &rsp->node[0];
    (<0.0>,9872) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9873) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9878) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9879) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9880) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9881) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9883) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9885) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9886) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9888) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9891) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9892) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9893) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9897) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9898) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9900) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9903) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9904) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9908) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9909) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9910) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9911) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9913) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9915) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9916) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9918) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9921) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9922) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9923) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9927) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,9929) tree.c:666: }
    (<0.0>,9934) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9939) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9940) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9941) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9942) fake_defs.h:237: switch (size) {
    (<0.0>,9944) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9946) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9947) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9949) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9952) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9953) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9954) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9958) tree.c:2174: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,9961)
    (<0.0>,9962) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,9966)
    (<0.0>,9967) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,9968) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,9973) fake_sched.h:43: return __running_cpu;
    (<0.0>,9977) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9979) fake_sched.h:43: return __running_cpu;
    (<0.0>,9983) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9991) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,9993) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,9998) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,10000) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,10004) fake_sched.h:43: return __running_cpu;
    (<0.0>,10008) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,10012) fake_sched.h:43: return __running_cpu;
    (<0.0>,10016) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10021) fake_sched.h:43: return __running_cpu;
    (<0.0>,10025) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,10036) fake_sched.h:43: return __running_cpu;
    (<0.0>,10040) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,10041) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,10043) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,10044) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,10045) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,10047) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,10049) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,10050) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,10051) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,10052) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,10053) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,10054) tree.c:942: if (oldval)
    (<0.0>,10062)
    (<0.0>,10068)
    (<0.0>,10077) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10078) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10079) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10083) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10084) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10085) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10087) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10092) fake_sched.h:43: return __running_cpu;
    (<0.0>,10095) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,10097) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,10100) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,10102) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,10104) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10107) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10108) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10109) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10113) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10114) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10115) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10117) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10122) fake_sched.h:43: return __running_cpu;
    (<0.0>,10125) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,10127) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,10130) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,10132) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,10134) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10137) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10138) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10139) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10143) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10144) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10145) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10147) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10152) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,10157) fake_sched.h:43: return __running_cpu;
    (<0.0>,10162) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,10170) fake_sched.h:43: return __running_cpu;
    (<0.0>,10176) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,10190) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10191) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10192) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10196) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10197) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10198) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10200) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10204) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,10206) fake_sched.h:43: return __running_cpu;
    (<0.0>,10209) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,10211) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,10233)
    (<0.0>,10234)
    (<0.0>,10235) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10237) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10238) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10239) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,10241) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,10243) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,10244) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,10245) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,10280)
    (<0.0>,10281)
    (<0.0>,10282) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,10285) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,10298)
    (<0.0>,10299) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10304) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10305) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10306) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10307) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10309) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10311) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10312) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10314) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10317) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10318) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10320) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10325) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10326) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10327) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10328) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10330) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10332) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10333) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10335) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10338) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10339) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10340) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10348) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,10351)
    (<0.0>,10354) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,10357) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,10359) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,10362) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,10364) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,10368) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,10371)
    (<0.0>,10372) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10374) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10377) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10380) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10383) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10390) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,10392) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,10394) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,10395) tree.c:3517: return 1;
    (<0.0>,10397) tree.c:3548: }
    (<0.0>,10401) tree.c:3561: return 1;
    (<0.0>,10403) tree.c:3563: }
    (<0.0>,10410) fake_sched.h:43: return __running_cpu;
    (<0.0>,10414) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,10418) tree.c:2891: if (user)
    (<0.0>,10426) fake_sched.h:43: return __running_cpu;
    (<0.0>,10430) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,10432) fake_sched.h:43: return __running_cpu;
    (<0.0>,10436) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10442) fake_sched.h:43: return __running_cpu;
    (<0.0>,10446) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,10456)
    (<0.0>,10459) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10460) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10461) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10465) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10466) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10467) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10469) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10473) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,10485)
    (<0.0>,10487) fake_sched.h:43: return __running_cpu;
    (<0.0>,10490) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10492) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10494) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10495) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10497) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10504) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10505) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10507) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10513) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10514) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10515) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10516) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10517) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10521)
    (<0.0>,10522)
    (<0.0>,10523) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,10524) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,10549)
    (<0.0>,10550)
    (<0.0>,10551) tree.c:1905: local_irq_save(flags);
    (<0.0>,10554)
    (<0.0>,10556) fake_sched.h:43: return __running_cpu;
    (<0.0>,10560) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10562) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10566) fake_sched.h:43: return __running_cpu;
    (<0.0>,10570) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10575) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10577) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10578) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10579) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10581) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10582) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10587) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10588) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10589) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10590) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10592) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10594) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10595) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10597) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10600) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10601) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10602) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10605) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10607) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10608) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10613) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10614) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10615) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10616) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10618) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10620) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10621) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10623) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10626) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10627) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10628) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10631) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10635) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10636) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10637) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10638) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10640) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10641) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10642) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10643) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10646) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10649) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10650) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10658) tree.c:1911: local_irq_restore(flags);
    (<0.0>,10661)
    (<0.0>,10663) fake_sched.h:43: return __running_cpu;
    (<0.0>,10667) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10669) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10673) fake_sched.h:43: return __running_cpu;
    (<0.0>,10677) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10684) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,10686) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,10691) tree.c:3016: local_irq_save(flags);
    (<0.0>,10694)
    (<0.0>,10696) fake_sched.h:43: return __running_cpu;
    (<0.0>,10700) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10702) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10706) fake_sched.h:43: return __running_cpu;
    (<0.0>,10710) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10715) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10716) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10727)
    (<0.0>,10728)
    (<0.0>,10729) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,10742)
    (<0.0>,10743) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10748) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10749) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10750) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10751) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10753) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10755) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10756) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10758) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10761) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10762) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10763) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10764) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10769) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10770) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10771) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10772) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10774) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10776) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10777) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10779) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10782) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10783) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10784) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10790) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,10806)
    (<0.0>,10807) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10810)
    (<0.0>,10811) tree.c:625: return &rsp->node[0];
    (<0.0>,10815) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10816) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10821) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10822) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10823) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10824) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10826) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10834) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10835) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10836) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10840) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10841) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10843) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10846) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10847) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10851) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10852) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10853) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10854) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10858) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10859) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10861) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10864) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10865) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10866) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10870) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,10872) tree.c:666: }
    (<0.0>,10875) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,10878)
    (<0.0>,10879) tree.c:625: return &rsp->node[0];
    (<0.0>,10885)
    (<0.0>,10886) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,10890)
    (<0.0>,10892) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,10893) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,10900) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,10906)
    (<0.0>,10908) fake_sched.h:43: return __running_cpu;
    (<0.0>,10911) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10913) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10915) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10916) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10919)
    (<0.0>,10920) tree.c:625: return &rsp->node[0];
    (<0.0>,10924) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10925) tree.c:2334: bool ret = false;
    (<0.0>,10926) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10927) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10928) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10936)
    (<0.0>,10937)
    (<0.0>,10938)
    (<0.0>,10939) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10942) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10945) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10948) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10949) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10952) tree.c:1818: return false;
    (<0.0>,10954) tree.c:1843: }
    (<0.0>,10957) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10961) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10962) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10963) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10964) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10975)
    (<0.0>,10976)
    (<0.0>,10977)
    (<0.0>,10978) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10980) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10983) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10984) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10995)
    (<0.0>,10996)
    (<0.0>,10997) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,11010)
    (<0.0>,11011) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11016) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11017) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11018) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11019) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11021) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11023) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11024) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11026) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11029) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11030) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11031) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11032) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11037) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11038) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11039) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11040) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11042) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11044) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11045) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11047) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11050) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11051) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11052) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11058) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,11074)
    (<0.0>,11075) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11078)
    (<0.0>,11079) tree.c:625: return &rsp->node[0];
    (<0.0>,11083) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11084) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11089) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11090) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11091) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11092) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11094) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11096) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11097) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11099) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11102) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11103) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11104) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11108) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11109) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11111) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11114) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11115) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11119) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11120) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11121) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11122) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11124) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11126) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11127) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11129) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11132) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11133) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11134) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11138) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,11140) tree.c:666: }
    (<0.0>,11145) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11150) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11151) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11152) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11153) fake_defs.h:237: switch (size) {
    (<0.0>,11155) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11157) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11158) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11160) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11163) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11164) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11165) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11168) tree.c:2318: return true;
    (<0.0>,11170) tree.c:2319: }
    (<0.0>,11174) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,11175) tree.c:2346: return ret;
    (<0.0>,11179) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,11183) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,11184) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,11185) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,11188)
    (<0.0>,11189) tree.c:625: return &rsp->node[0];
    (<0.0>,11194) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,11198)
    (<0.0>,11199)
    (<0.0>,11200) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,11201) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,11204) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,11207)
    (<0.0>,11209) fake_sched.h:43: return __running_cpu;
    (<0.0>,11213) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11215) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11219) fake_sched.h:43: return __running_cpu;
    (<0.0>,11223) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11231) tree.c:3021: if (needwake)
    (<0.0>,11234) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,11242)
    (<0.0>,11243) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,11244) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,11246) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,11253) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11256)
    (<0.0>,11257) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11259) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11262) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11265) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11268) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11275) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,11276) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,11285)
    (<0.0>,11286)
    (<0.0>,11289) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11290) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11291) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11292) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11294) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11296) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11297) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11299) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11302) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11303) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11304) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11313) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,11315) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,11324) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,11325) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,11347)
    (<0.0>,11348)
    (<0.0>,11349) tree.c:2767: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,11352)
    (<0.0>,11353) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11355) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11358) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11361) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11364) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11371) tree.c:2779: local_irq_save(flags);
    (<0.0>,11374)
    (<0.0>,11376) fake_sched.h:43: return __running_cpu;
    (<0.0>,11380) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11382) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11386) fake_sched.h:43: return __running_cpu;
    (<0.0>,11390) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11395) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11396) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11397) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11398) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11399) tree.c:2781: bl = rdp->blimit;
    (<0.0>,11401) tree.c:2781: bl = rdp->blimit;
    (<0.0>,11402) tree.c:2781: bl = rdp->blimit;
    (<0.0>,11405) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,11407) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,11408) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,11409) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11412) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11413) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11414) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11416) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11417) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,11420) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,11421) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,11422) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11425) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11426) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11427) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11429) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11432) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11434) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11437) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11438) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11441) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11444) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11446) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11448) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11451) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11454) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11456) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11458) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11461) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11463) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11466) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11467) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11470) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11473) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11475) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11477) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11480) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11483) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11485) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11487) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11490) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11492) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11495) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11496) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11499) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11502) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11504) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11506) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11509) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11512) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11514) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11516) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11519) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11521) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11524) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11525) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11528) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11531) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11533) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11535) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11538) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11541) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11543) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11545) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11548) tree.c:2790: local_irq_restore(flags);
    (<0.0>,11551)
    (<0.0>,11553) fake_sched.h:43: return __running_cpu;
    (<0.0>,11557) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11559) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11563) fake_sched.h:43: return __running_cpu;
    (<0.0>,11567) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11572) tree.c:2793: count = count_lazy = 0;
    (<0.0>,11573) tree.c:2793: count = count_lazy = 0;
    (<0.0>,11575) tree.c:2794: while (list) {
    (<0.0>,11578) tree.c:2795: next = list->next;
    (<0.0>,11580) tree.c:2795: next = list->next;
    (<0.0>,11581) tree.c:2795: next = list->next;
    (<0.0>,11584) tree.c:2797: debug_rcu_head_unqueue(list);
    (<0.0>,11587)
    (<0.0>,11589) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11591) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11592) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11598)
    (<0.0>,11599)
    (<0.0>,11600) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11603) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11605) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11608) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,11611) rcu.h:118: head->func(head);
    (<0.0>,11614) rcu.h:118: head->func(head);
    (<0.0>,11615) rcu.h:118: head->func(head);
    (<0.0>,11621)
    (<0.0>,11622) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11623) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11624) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11628) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11629) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11630) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11631) update.c:341: complete(&rcu->completion);
    (<0.0>,11635)
    (<0.0>,11636) fake_sync.h:288: x->done++;
    (<0.0>,11638) fake_sync.h:288: x->done++;
    (<0.0>,11640) fake_sync.h:288: x->done++;
    (<0.0>,11645) rcu.h:120: return false;
    (<0.0>,11647) rcu.h:122: }
    (<0.0>,11650) tree.c:2800: list = next;
    (<0.0>,11651) tree.c:2800: list = next;
    (<0.0>,11652) tree.c:2802: if (++count >= bl &&
    (<0.0>,11654) tree.c:2802: if (++count >= bl &&
    (<0.0>,11655) tree.c:2802: if (++count >= bl &&
    (<0.0>,11659) tree.c:2794: while (list) {
    (<0.0>,11662) tree.c:2808: local_irq_save(flags);
    (<0.0>,11665)
    (<0.0>,11667) fake_sched.h:43: return __running_cpu;
    (<0.0>,11671) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11673) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11677) fake_sched.h:43: return __running_cpu;
    (<0.0>,11681) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11688) tree.c:2814: if (list != NULL) {
    (<0.0>,11692) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11693) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11695) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11697) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11699) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11701) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11702) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11704) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11705) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11710) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11711) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11712) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11713) fake_defs.h:237: switch (size) {
    (<0.0>,11715) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11717) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11718) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11720) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11723) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11724) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11725) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11726) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11727) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11729) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11731) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11732) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,11734) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,11737) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11739) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11742) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11744) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11747) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11749) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11750) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11752) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11753) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11758) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11760) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11763) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11765) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11772) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11773) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11775) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11778) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11780) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11786) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11787) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11788) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11789) tree.c:2840: local_irq_restore(flags);
    (<0.0>,11792)
    (<0.0>,11794) fake_sched.h:43: return __running_cpu;
    (<0.0>,11798) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11800) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11804) fake_sched.h:43: return __running_cpu;
    (<0.0>,11808) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11813) tree.c:2843: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11816)
    (<0.0>,11817) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11819) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11822) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11833) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11836)
    (<0.0>,11840) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11843) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11844) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11845) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11849) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11850) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11851) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11853) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11857) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,11869)
    (<0.0>,11871) fake_sched.h:43: return __running_cpu;
    (<0.0>,11874) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11876) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11878) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11879) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11881) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11888) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11889) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11891) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11897) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11898) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11899) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11900) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11901) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11905)
    (<0.0>,11906)
    (<0.0>,11907) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,11908) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,11933)
    (<0.0>,11934)
    (<0.0>,11935) tree.c:1905: local_irq_save(flags);
    (<0.0>,11938)
    (<0.0>,11940) fake_sched.h:43: return __running_cpu;
    (<0.0>,11944) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11946) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11950) fake_sched.h:43: return __running_cpu;
    (<0.0>,11954) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11959) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11961) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11962) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11963) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11965) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11966) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11971) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11972) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11973) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11974) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11976) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11978) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11979) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11981) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11984) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11985) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11986) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11989) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11991) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11992) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11997) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11998) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11999) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,12000) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12002) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12004) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12005) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12007) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12010) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,12011) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,12012) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,12015) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,12019) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,12020) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,12021) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,12022) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12024) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12025) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12026) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12027) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12030) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,12033) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,12034) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,12042) tree.c:1911: local_irq_restore(flags);
    (<0.0>,12045)
    (<0.0>,12047) fake_sched.h:43: return __running_cpu;
    (<0.0>,12051) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12053) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12057) fake_sched.h:43: return __running_cpu;
    (<0.0>,12061) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12068) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,12070) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,12075) tree.c:3016: local_irq_save(flags);
    (<0.0>,12078)
    (<0.0>,12080) fake_sched.h:43: return __running_cpu;
    (<0.0>,12084) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12086) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12090) fake_sched.h:43: return __running_cpu;
    (<0.0>,12094) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12099) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,12100) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,12111)
    (<0.0>,12112)
    (<0.0>,12113) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,12126)
    (<0.0>,12127) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12132) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12133) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12134) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12135) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12137) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12139) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12140) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12142) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12145) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12146) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12147) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12148) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12153) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12154) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12155) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12156) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12158) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12160) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12161) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12163) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12166) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12167) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12168) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12174) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,12190)
    (<0.0>,12191) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12194)
    (<0.0>,12195) tree.c:625: return &rsp->node[0];
    (<0.0>,12199) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12200) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12205) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12206) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12207) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12208) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12210) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12212) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12213) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12215) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12218) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12219) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12220) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12224) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12225) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12227) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12230) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12231) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12235) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12236) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12237) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12238) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12240) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12242) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12243) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12245) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12248) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12249) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12250) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12254) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12257) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12260) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12263) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12264) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12267) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12269) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12272) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12275) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12278) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12279) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12281) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12284) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12288) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12290) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12292) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12295) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12298) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12301) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12302) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12304) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12307) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12311) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12313) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12315) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12318) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,12320) tree.c:666: }
    (<0.0>,12323) tree.c:3024: local_irq_restore(flags);
    (<0.0>,12326)
    (<0.0>,12328) fake_sched.h:43: return __running_cpu;
    (<0.0>,12332) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12334) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12338) fake_sched.h:43: return __running_cpu;
    (<0.0>,12342) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12348) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,12351)
    (<0.0>,12352) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12354) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12357) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12364) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12367)
    (<0.0>,12371) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12374) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12375) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12376) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12380) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12381) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12382) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12384) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12392) fake_sched.h:43: return __running_cpu;
    (<0.0>,12396) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,12406) fake_sched.h:43: return __running_cpu;
    (<0.0>,12410) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12411) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,12413) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,12414) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,12415) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,12417) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,12419) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,12420) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12421) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12422) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12423) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12424) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,12426) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,12434)
    (<0.0>,12440) fake_sched.h:43: return __running_cpu;
    (<0.0>,12444)
    (<0.0>,12447) tree.c:755: local_irq_save(flags);
    (<0.0>,12450)
    (<0.0>,12452) fake_sched.h:43: return __running_cpu;
    (<0.0>,12456) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12458) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12462) fake_sched.h:43: return __running_cpu;
    (<0.0>,12466) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12478)
    (<0.0>,12480) fake_sched.h:43: return __running_cpu;
    (<0.0>,12484) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12485) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12487) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12488) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12489) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12490) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12491) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12492) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12493) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,12497) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,12499) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,12500) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12501) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12517)
    (<0.0>,12519)
    (<0.0>,12521) fake_sched.h:43: return __running_cpu;
    (<0.0>,12525) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12528) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12529) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12530) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12534) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12535) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12536) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12538) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12543) fake_sched.h:43: return __running_cpu;
    (<0.0>,12546) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12548) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12550) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12551) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12554)
    (<0.0>,12557) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12560) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12561) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12562) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12566) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12567) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12568) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12570) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12575) fake_sched.h:43: return __running_cpu;
    (<0.0>,12578) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12580) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12582) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12583) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12586)
    (<0.0>,12589) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12592) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12593) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12594) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12598) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12599) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12600) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12602) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12609) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12612) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12613) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12614) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12616) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12617) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12619) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12620) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12621) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12622) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12636)
    (<0.0>,12638) tree.c:758: local_irq_restore(flags);
    (<0.0>,12641)
    (<0.0>,12643) fake_sched.h:43: return __running_cpu;
    (<0.0>,12647) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12649) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12653) fake_sched.h:43: return __running_cpu;
    (<0.0>,12657) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12663) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12666) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12671) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12676) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12677) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12678) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12679) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12681) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12683) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12684) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12686) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12689) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12690) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12691) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12698) fake_sched.h:43: return __running_cpu;
    (<0.0>,12702)
    (<0.0>,12703) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12706) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12711) tree.c:892: local_irq_save(flags);
    (<0.0>,12714)
    (<0.0>,12716) fake_sched.h:43: return __running_cpu;
    (<0.0>,12720) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12722) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12726) fake_sched.h:43: return __running_cpu;
    (<0.0>,12730) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12742)
    (<0.0>,12744) fake_sched.h:43: return __running_cpu;
    (<0.0>,12748) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12749) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12751) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12752) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12753) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12754) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12755) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12756) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12757) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,12761) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,12763) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,12764) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,12765) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,12776)
    (<0.0>,12777)
    (<0.0>,12779) fake_sched.h:43: return __running_cpu;
    (<0.0>,12783) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12787) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12790) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12791) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12792) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12794) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12795) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12797) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12798) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12799) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12800) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12810)
    (<0.0>,12812) tree.c:895: local_irq_restore(flags);
    (<0.0>,12815)
    (<0.0>,12817) fake_sched.h:43: return __running_cpu;
    (<0.0>,12821) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12823) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12827) fake_sched.h:43: return __running_cpu;
    (<0.0>,12831) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12838) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,12840) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,12841) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,12886)
    (<0.0>,12887) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12890)
    (<0.0>,12891) tree.c:625: return &rsp->node[0];
    (<0.0>,12895) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12897) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12898) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12899) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12904) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12905) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12906) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12907) fake_defs.h:237: switch (size) {
    (<0.0>,12909) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12911) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12912) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12914) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12917) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12918) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12919) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12920) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,12923)
    (<0.0>,12924) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12928)
    (<0.0>,12931) fake_sched.h:43: return __running_cpu;
    (<0.0>,12935) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12939) fake_sched.h:43: return __running_cpu;
    (<0.0>,12943) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12948) fake_sched.h:43: return __running_cpu;
    (<0.0>,12952) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12955) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12956) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12963) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12968) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12969) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12970) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12971) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12973) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12975) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12976) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12978) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12981) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12982) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12983) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12994)
    (<0.0>,13000)
    (<0.0>,13005) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,13010) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,13011) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,13012) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,13013) fake_defs.h:237: switch (size) {
    (<0.0>,13015) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,13017) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,13018) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,13020) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,13023) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,13024) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,13025) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,13026) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,13039)
    (<0.0>,13040) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13045) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13046) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13047) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13048) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13050) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13052) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13053) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13055) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13058) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13059) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13060) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13061) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13066) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13067) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13068) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13069) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13071) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13073) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13074) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13076) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13079) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13080) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13081) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13089) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,13090) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,13103)
    (<0.0>,13104) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13109) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13110) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13111) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13112) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13114) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13116) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13117) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13119) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13122) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13123) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13124) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13125) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13130) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13131) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13132) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13133) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13135) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13137) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13138) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13140) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13143) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13144) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13145) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13152) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,13153) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,13154) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,13157) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,13172)
    (<0.0>,13173) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,13174) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,13175) tree.c:1240: rsp->gp_start = j;
    (<0.0>,13176) tree.c:1240: rsp->gp_start = j;
    (<0.0>,13178) tree.c:1240: rsp->gp_start = j;
    (<0.0>,13199) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13200) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13201) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13202) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13204) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13206) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13207) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13209) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13212) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13213) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13214) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13215) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13216) update.c:466: if (till_stall_check < 3) {
    (<0.0>,13219) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,13223) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,13228) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,13230) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13231) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13233) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13234) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13239) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13240) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13241) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13242) fake_defs.h:237: switch (size) {
    (<0.0>,13244) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13246) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13247) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13249) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13252) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13253) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13254) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13255) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13256) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13259) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13261) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13262) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13267) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13268) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13269) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13270) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13272) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13274) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13275) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13277) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13280) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13281) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13282) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13283) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13285) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13289) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13291) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13293) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13294) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13296) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13297) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13298) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13302) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,13305)
    (<0.0>,13306) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13310)
    (<0.0>,13311) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13312) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13317) fake_sched.h:43: return __running_cpu;
    (<0.0>,13321) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,13323) fake_sched.h:43: return __running_cpu;
    (<0.0>,13327) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13334) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13337) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13340) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13341) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13343) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13344) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13346) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13351) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,13352) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,13356)
    (<0.0>,13357)
    (<0.0>,13358) tree.c:1922: if (delay > 0 &&
    (<0.0>,13362) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,13365)
    (<0.0>,13366) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13370)
    (<0.0>,13373) fake_sched.h:43: return __running_cpu;
    (<0.0>,13377) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,13381) fake_sched.h:43: return __running_cpu;
    (<0.0>,13385) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13390) fake_sched.h:43: return __running_cpu;
    (<0.0>,13394) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,13397) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13398) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13405) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13407) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13408) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13410) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13413) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,13415) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,13418) tree.c:1973: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,13421)
    (<0.0>,13422) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13426)
    (<0.0>,13427) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13428) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13433) fake_sched.h:43: return __running_cpu;
    (<0.0>,13437) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,13439) fake_sched.h:43: return __running_cpu;
    (<0.0>,13443) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13451) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13453) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13455) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13456) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13458) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13463) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13466) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13468) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13469) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13471) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13476) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,13477) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,13481)
    (<0.0>,13482)
    (<0.0>,13483) tree.c:1922: if (delay > 0 &&
    (<0.0>,13487) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,13490)
    (<0.0>,13491) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13495)
    (<0.0>,13498) fake_sched.h:43: return __running_cpu;
    (<0.0>,13502) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,13506) fake_sched.h:43: return __running_cpu;
    (<0.0>,13510) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13515) fake_sched.h:43: return __running_cpu;
    (<0.0>,13519) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,13522) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13523) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13531) fake_sched.h:43: return __running_cpu;
    (<0.0>,13534) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13536) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13538) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13539) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,13545)
    (<0.0>,13546) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13548) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13553) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13554) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13556) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13560) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13561) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13562) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13564) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13566) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13567) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13569) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13571) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13573) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13574) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13575) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13580) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13581) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13582) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13583) fake_defs.h:237: switch (size) {
    (<0.0>,13585) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13587) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13588) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13590) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13593) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13594) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13595) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13596) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13598) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13599) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13601) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13606) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13607) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13609) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13610) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13612) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13616) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13617) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13618) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13621) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13622) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13624) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13627) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13628) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13629) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13651)
    (<0.0>,13652)
    (<0.0>,13653)
    (<0.0>,13654) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13656) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13657) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13659) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13662) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13666) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13667) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13668) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13669) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13671) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13672) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13673) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13674) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13677) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13680) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13681) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13689) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13690) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13691) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13700)
    (<0.0>,13701)
    (<0.0>,13702)
    (<0.0>,13703) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13706) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13709) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13712) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13713) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13716) tree.c:1750: return false;
    (<0.0>,13718) tree.c:1799: }
    (<0.0>,13721) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13723) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13725) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13726) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13728) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13731) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13733) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13734) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13736) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13739) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13741) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13742) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13744) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13750) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13751) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13754) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13758) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13760) fake_sched.h:43: return __running_cpu;
    (<0.0>,13764) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13765) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13767) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13768) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13770) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13773) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13774) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,13777)
    (<0.0>,13778) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,13780) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,13781) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,13783) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,13793)
    (<0.0>,13798) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13802) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13803) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13804) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13805) fake_defs.h:237: switch (size) {
    (<0.0>,13807) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13808) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13809) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13810) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13813) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13816) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13817) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13820) tree.c:1896: return ret;
    (<0.0>,13824) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,13827)
    (<0.0>,13831) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,13834)
    (<0.0>,13835) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13839)
    (<0.0>,13840) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13841) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13846) fake_sched.h:43: return __running_cpu;
    (<0.0>,13850) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,13852) fake_sched.h:43: return __running_cpu;
    (<0.0>,13856) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13871) fake_sched.h:43: return __running_cpu;
    (<0.0>,13877) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,13883) fake_sched.h:43: return __running_cpu;
    (<0.0>,13890) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,13892) fake_sched.h:43: return __running_cpu;
    (<0.0>,13899) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
    (<0.0>,13907) fake_sched.h:43: return __running_cpu;
    (<0.0>,13911) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,13924) fake_sched.h:43: return __running_cpu;
    (<0.0>,13928)
    (<0.0>,13931) tree.c:755: local_irq_save(flags);
    (<0.0>,13934)
    (<0.0>,13936) fake_sched.h:43: return __running_cpu;
    (<0.0>,13940) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13942) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13946) fake_sched.h:43: return __running_cpu;
    (<0.0>,13950) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13962)
    (<0.0>,13964) fake_sched.h:43: return __running_cpu;
    (<0.0>,13968) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13969) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13971) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13972) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13973) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13974) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13975) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13976) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13977) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,13981) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,13983) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,13984) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,13985) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,14001)
    (<0.0>,14003)
    (<0.0>,14005) fake_sched.h:43: return __running_cpu;
    (<0.0>,14009) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14012) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14013) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14014) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14018) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14019) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14020) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14022) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14027) fake_sched.h:43: return __running_cpu;
    (<0.0>,14030) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14032) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14034) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14035) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,14038)
    (<0.0>,14041) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14044) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14045) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14046) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14050) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14051) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14052) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14054) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14059) fake_sched.h:43: return __running_cpu;
    (<0.0>,14062) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14064) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14066) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14067) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,14070)
    (<0.0>,14073) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14076) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14077) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14078) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14082) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14083) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14084) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14086) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14093) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14096) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14097) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14098) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14100) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14101) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14103) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14104) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14105) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14106) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14120)
    (<0.0>,14122) tree.c:758: local_irq_restore(flags);
    (<0.0>,14125)
    (<0.0>,14127) fake_sched.h:43: return __running_cpu;
    (<0.0>,14131) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14133) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14137) fake_sched.h:43: return __running_cpu;
    (<0.0>,14141) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14147) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,14150) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,14155) fake_sched.h:43: return __running_cpu;
    (<0.0>,14159)
    (<0.0>,14160) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,14163) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,14168) tree.c:892: local_irq_save(flags);
    (<0.0>,14171)
    (<0.0>,14173) fake_sched.h:43: return __running_cpu;
    (<0.0>,14177) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14179) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14183) fake_sched.h:43: return __running_cpu;
    (<0.0>,14187) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14199)
    (<0.0>,14201) fake_sched.h:43: return __running_cpu;
    (<0.0>,14205) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14206) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14208) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14209) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14210) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14211) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14212) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14213) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14214) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,14218) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,14220) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,14221) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,14222) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,14233)
    (<0.0>,14234)
    (<0.0>,14236) fake_sched.h:43: return __running_cpu;
    (<0.0>,14240) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14244) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14247) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14248) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14249) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14251) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14252) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14254) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14255) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14256) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14257) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14267)
    (<0.0>,14269) tree.c:895: local_irq_restore(flags);
    (<0.0>,14272)
    (<0.0>,14274) fake_sched.h:43: return __running_cpu;
    (<0.0>,14278) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14280) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14284) fake_sched.h:43: return __running_cpu;
    (<0.0>,14288) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14302) fake_sched.h:43: return __running_cpu;
    (<0.0>,14306) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,14315) fake_sched.h:43: return __running_cpu;
    (<0.0>,14322) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,14331) fake_sched.h:43: return __running_cpu;
    (<0.0>,14335) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,14337) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,14343) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14344) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14345) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14350) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14351) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14352) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14353) fake_defs.h:237: switch (size) {
    (<0.0>,14355) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14357) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14358) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14360) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14363) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14364) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14365) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14367) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14369) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14371) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14372) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14374) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14379) tree.c:2046: return true;
    (<0.0>,14381) tree.c:2047: }
    (<0.0>,14385) tree.c:2214: first_gp_fqs = true;
    (<0.0>,14386) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,14387) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,14388) tree.c:2216: if (j > HZ) {
    (<0.0>,14391) tree.c:2220: ret = 0;
    (<0.0>,14393) tree.c:2222: if (!ret) {
    (<0.0>,14396) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14397) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14399) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14401) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14403) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14404) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14407) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14408) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14413) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14414) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14415) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14416) fake_defs.h:237: switch (size) {
    (<0.0>,14418) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14420) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14421) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14423) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14426) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14427) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14428) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14432) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,14434) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,14438) fake_sched.h:43: return __running_cpu;
    (<0.0>,14442) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,14446) fake_sched.h:43: return __running_cpu;
    (<0.0>,14450) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14455) fake_sched.h:43: return __running_cpu;
    (<0.0>,14459) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,14470) fake_sched.h:43: return __running_cpu;
    (<0.0>,14474) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14475) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,14477) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,14478) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,14479) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,14481) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,14483) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,14484) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14485) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14486) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14487) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14488) tree.c:942: if (oldval)
    (<0.0>,14496)
    (<0.0>,14502)
    (<0.0>,14511) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14512) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14513) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14517) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14518) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14519) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14521) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14526) fake_sched.h:43: return __running_cpu;
    (<0.0>,14529) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14531) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14534) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14536) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14538) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14541) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14542) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14543) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14547) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14548) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14549) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14551) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14556) fake_sched.h:43: return __running_cpu;
    (<0.0>,14559) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14561) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14564) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14566) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14568) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14571) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14572) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14573) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14577) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14578) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14579) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14581) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14586) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,14591) fake_sched.h:43: return __running_cpu;
    (<0.0>,14596) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,14604) fake_sched.h:43: return __running_cpu;
    (<0.0>,14610) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,14624) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14625) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14626) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14630) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14631) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14632) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14634) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14638) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14640) fake_sched.h:43: return __running_cpu;
    (<0.0>,14643) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14645) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14667)
    (<0.0>,14668)
    (<0.0>,14669) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14671) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14672) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14673) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14675) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14677) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14678) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,14679) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,14714)
    (<0.0>,14715)
    (<0.0>,14716) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,14719) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,14732)
    (<0.0>,14733) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14738) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14739) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14740) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14743) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14745) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14746) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14748) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14751) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14752) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14753) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14754) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14759) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14760) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14761) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14762) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14767) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14772) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14773) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14774) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14780) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,14805)
    (<0.0>,14806) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,14811) tree.c:1472: j = jiffies;
    (<0.0>,14812) tree.c:1472: j = jiffies;
    (<0.0>,14813) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14818) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14819) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14820) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14821) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14823) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14825) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14826) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14831) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14832) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14833) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14834) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14836) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14841) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14842) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14843) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14844) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14846) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14848) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14849) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14854) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14855) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14856) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14857) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14859) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14864) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14865) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14866) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14867) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14869) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14871) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14872) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14877) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14878) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14879) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14880) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14882) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14887) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14888) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14889) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14890) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14892) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14894) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14895) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14897) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14900) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14901) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14902) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14903) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14904) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,14905) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,14909) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,14910) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,14916) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,14919)
    (<0.0>,14922) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,14925) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14927) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14930) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14934) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14938) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14940) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14943) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,14947) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,14950) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14952) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14954) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14955) tree.c:3511: return 1;
    (<0.0>,14957) tree.c:3548: }
    (<0.0>,14961) tree.c:3561: return 1;
    (<0.0>,14963) tree.c:3563: }
    (<0.0>,14970) fake_sched.h:43: return __running_cpu;
    (<0.0>,14974) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,14978) tree.c:2891: if (user)
    (<0.0>,14986) fake_sched.h:43: return __running_cpu;
    (<0.0>,14990) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,14992) fake_sched.h:43: return __running_cpu;
    (<0.0>,14996) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15002) fake_sched.h:43: return __running_cpu;
    (<0.0>,15006) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,15016)
    (<0.0>,15019) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15020) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15021) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15025) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15026) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15027) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15029) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15033) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,15045)
    (<0.0>,15047) fake_sched.h:43: return __running_cpu;
    (<0.0>,15050) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15052) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15054) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15055) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15057) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15064) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15065) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15067) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15073) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15074) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15075) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15076) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,15077) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,15081)
    (<0.0>,15082)
    (<0.0>,15083) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,15084) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,15109)
    (<0.0>,15110)
    (<0.0>,15111) tree.c:1905: local_irq_save(flags);
    (<0.0>,15114)
    (<0.0>,15116) fake_sched.h:43: return __running_cpu;
    (<0.0>,15120) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15122) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15126) fake_sched.h:43: return __running_cpu;
    (<0.0>,15130) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15135) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15137) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15138) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15139) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15141) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15142) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15147) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15148) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15149) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15150) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15152) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15154) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15155) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15157) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15160) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15161) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15162) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15165) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15167) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15168) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15173) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15174) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15175) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15176) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15178) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15180) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15181) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15183) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15186) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15187) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15188) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15191) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15195) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15196) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15197) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15198) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15200) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15201) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15202) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15203) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15206) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15209) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15210) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15218) tree.c:1911: local_irq_restore(flags);
    (<0.0>,15221)
    (<0.0>,15223) fake_sched.h:43: return __running_cpu;
    (<0.0>,15227) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15229) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15233) fake_sched.h:43: return __running_cpu;
    (<0.0>,15237) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15244) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15246) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15249) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,15253) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,15257) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15259) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15260) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15261) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15279)
    (<0.0>,15280)
    (<0.0>,15281)
    (<0.0>,15282) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,15284) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,15285) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,15289) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15290) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15291) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15293) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15297)
    (<0.0>,15298)
    (<0.0>,15299) fake_sync.h:83: local_irq_save(flags);
    (<0.0>,15302)
    (<0.0>,15304) fake_sched.h:43: return __running_cpu;
    (<0.0>,15308) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15310) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15314) fake_sched.h:43: return __running_cpu;
    (<0.0>,15318) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15324) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,15325) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,15332) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,15336) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,15340) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15342) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15343) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15345) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15348) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15350) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15351) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15353) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15356) tree.c:2491: rdp->gpwrap) {
    (<0.0>,15358) tree.c:2491: rdp->gpwrap) {
    (<0.0>,15361) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,15363) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,15364) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,15365) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,15367) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,15368) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,15372) tree.c:2508: rdp->core_needs_qs = false;
    (<0.0>,15374) tree.c:2508: rdp->core_needs_qs = false;
    (<0.0>,15375) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,15376) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,15377) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,15386)
    (<0.0>,15387)
    (<0.0>,15388)
    (<0.0>,15389) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15392) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15395) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15398) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15399) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15402) tree.c:1750: return false;
    (<0.0>,15404) tree.c:1799: }
    (<0.0>,15407) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,15408) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15409) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15410) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15411) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15413) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15414) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15438)
    (<0.0>,15439)
    (<0.0>,15440)
    (<0.0>,15441)
    (<0.0>,15442)
    (<0.0>,15443) tree.c:2385: unsigned long oldmask = 0;
    (<0.0>,15445) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15447) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15448) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15452) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15454) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15455) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15458) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15463) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15464) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15468) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15469) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15470) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15471) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,15473) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,15475) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,15477) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,15480) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,15482) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,15488) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15489) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15490) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15492) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15496)
    (<0.0>,15497)
    (<0.0>,15498) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,15499) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,15502) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,15505)
    (<0.0>,15507) fake_sched.h:43: return __running_cpu;
    (<0.0>,15511) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15513) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15517) fake_sched.h:43: return __running_cpu;
    (<0.0>,15521) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15531) tree.c:2518: if (needwake)
    (<0.0>,15538) tree.c:3016: local_irq_save(flags);
    (<0.0>,15541)
    (<0.0>,15543) fake_sched.h:43: return __running_cpu;
    (<0.0>,15547) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15549) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15553) fake_sched.h:43: return __running_cpu;
    (<0.0>,15557) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15562) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15563) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15574)
    (<0.0>,15575)
    (<0.0>,15576) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,15589)
    (<0.0>,15590) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15595) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15596) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15597) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15598) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15600) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15602) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15603) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15605) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15608) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15609) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15610) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15611) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15616) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15617) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15618) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15619) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15621) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15623) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15624) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15626) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15629) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15630) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15631) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15637) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,15639) tree.c:666: }
    (<0.0>,15642) tree.c:3024: local_irq_restore(flags);
    (<0.0>,15645)
    (<0.0>,15647) fake_sched.h:43: return __running_cpu;
    (<0.0>,15651) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15653) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15657) fake_sched.h:43: return __running_cpu;
    (<0.0>,15661) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15667) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,15670)
    (<0.0>,15671) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15673) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15676) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15683) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15686)
    (<0.0>,15690) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15693) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15694) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15695) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15699) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15700) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15701) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15703) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15707) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,15719)
    (<0.0>,15721) fake_sched.h:43: return __running_cpu;
    (<0.0>,15724) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15726) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15728) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15729) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15731) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15738) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15739) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15741) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15747) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15748) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15749) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15750) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,15751) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,15755)
    (<0.0>,15756)
    (<0.0>,15757) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,15758) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,15783)
    (<0.0>,15784)
    (<0.0>,15785) tree.c:1905: local_irq_save(flags);
    (<0.0>,15788)
    (<0.0>,15790) fake_sched.h:43: return __running_cpu;
    (<0.0>,15794) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15796) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15800) fake_sched.h:43: return __running_cpu;
    (<0.0>,15804) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15809) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15811) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15812) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15813) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15815) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15816) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15821) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15822) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15823) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15824) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15826) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15834) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15835) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15836) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15839) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15841) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15842) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15847) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15848) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15849) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15850) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15852) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15854) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15855) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15857) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15860) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15861) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15862) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15865) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15869) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15870) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15871) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15872) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15875) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15877) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15880) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15883) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15884) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15892) tree.c:1911: local_irq_restore(flags);
    (<0.0>,15895)
    (<0.0>,15897) fake_sched.h:43: return __running_cpu;
    (<0.0>,15901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15903) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15907) fake_sched.h:43: return __running_cpu;
    (<0.0>,15911) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15918) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15920) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15925) tree.c:3016: local_irq_save(flags);
    (<0.0>,15928)
    (<0.0>,15930) fake_sched.h:43: return __running_cpu;
    (<0.0>,15934) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15936) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15940) fake_sched.h:43: return __running_cpu;
    (<0.0>,15944) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15949) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15950) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15961)
    (<0.0>,15962)
    (<0.0>,15963) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,15976)
    (<0.0>,15977) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15982) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15983) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15984) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15985) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15987) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15989) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15990) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15992) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15995) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15996) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15997) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15998) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16003) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16004) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16005) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16006) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16008) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16010) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16011) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16013) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16016) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16017) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16018) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16024) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,16040)
    (<0.0>,16041) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16044)
    (<0.0>,16045) tree.c:625: return &rsp->node[0];
    (<0.0>,16049) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16050) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16055) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16056) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16057) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16058) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16060) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16062) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16063) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16065) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16068) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16069) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16070) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16074) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16075) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16077) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16080) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16081) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16085) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16086) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16087) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16088) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16090) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16092) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16093) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16095) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16098) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16099) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16100) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16104) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,16107) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,16110) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,16113) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,16114) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,16117) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16119) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16122) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16125) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16128) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16129) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16131) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16134) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16138) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16140) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16142) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16145) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16148) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16151) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16152) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16154) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16157) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16161) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16163) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16165) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16168) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,16170) tree.c:666: }
    (<0.0>,16173) tree.c:3024: local_irq_restore(flags);
    (<0.0>,16176)
    (<0.0>,16178) fake_sched.h:43: return __running_cpu;
    (<0.0>,16182) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,16184) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,16188) fake_sched.h:43: return __running_cpu;
    (<0.0>,16192) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,16198) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,16201)
    (<0.0>,16202) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16204) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16207) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16214) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,16217)
    (<0.0>,16221) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16224) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16225) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16226) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16230) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16231) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16232) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16234) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16242) fake_sched.h:43: return __running_cpu;
    (<0.0>,16246) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,16256) fake_sched.h:43: return __running_cpu;
    (<0.0>,16260) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,16261) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,16263) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,16264) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,16265) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,16267) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,16269) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,16270) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16271) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16272) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16273) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16274) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,16276) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,16284)
    (<0.0>,16290) fake_sched.h:43: return __running_cpu;
    (<0.0>,16294)
    (<0.0>,16297) tree.c:755: local_irq_save(flags);
    (<0.0>,16300)
    (<0.0>,16302) fake_sched.h:43: return __running_cpu;
    (<0.0>,16306) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,16308) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,16312) fake_sched.h:43: return __running_cpu;
    (<0.0>,16316) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,16328)
    (<0.0>,16330) fake_sched.h:43: return __running_cpu;
    (<0.0>,16334) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,16335) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,16337) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,16338) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,16339) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16340) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16341) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16342) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16343) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,16347) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,16349) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,16350) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,16351) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,16367)
    (<0.0>,16369)
    (<0.0>,16371) fake_sched.h:43: return __running_cpu;
    (<0.0>,16375) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,16378) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16379) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16380) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16384) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16385) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16386) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16388) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16393) fake_sched.h:43: return __running_cpu;
    (<0.0>,16396) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16398) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16400) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16401) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,16404)
    (<0.0>,16407) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16410) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16411) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16412) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16416) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16417) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16418) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16420) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16425) fake_sched.h:43: return __running_cpu;
    (<0.0>,16428) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16430) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16432) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16433) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,16436)
    (<0.0>,16439) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16442) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16443) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16444) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16448) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16449) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16450) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16452) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16459) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16462) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16463) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16464) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16466) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16467) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16469) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16470) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16471) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16472) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16486)
    (<0.0>,16488) tree.c:758: local_irq_restore(flags);
    (<0.0>,16491)
    (<0.0>,16493) fake_sched.h:43: return __running_cpu;
    (<0.0>,16497) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,16499) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,16503) fake_sched.h:43: return __running_cpu;
    (<0.0>,16507) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,16513) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,16516) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,16521) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16537)
    (<0.0>,16538)
    (<0.0>,16539) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16542)
    (<0.0>,16543) tree.c:625: return &rsp->node[0];
    (<0.0>,16547) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16548) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16553) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16554) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16555) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16556) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16558) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16560) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16561) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16563) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16566) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16567) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16568) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16570) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16571) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16572) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16573) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16577) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16582) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16583) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16584) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16585) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16587) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16589) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16590) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16592) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16595) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16596) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16597) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16600) tree.c:2066: return false;
    (<0.0>,16602) tree.c:2067: }
    (<0.0>,16607) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16623)
    (<0.0>,16624)
    (<0.0>,16625) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16628)
    (<0.0>,16629) tree.c:625: return &rsp->node[0];
    (<0.0>,16633) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16634) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16639) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16640) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16641) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16642) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16644) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16646) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16647) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16649) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16652) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16653) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16654) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16656) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16657) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16658) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16659) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16663) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16668) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16669) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16670) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16671) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16673) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16675) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16676) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16678) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16681) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16682) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16683) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16686) tree.c:2066: return false;
    (<0.0>,16688) tree.c:2067: }
    (<0.0>,16693) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16709)
    (<0.0>,16710)
    (<0.0>,16711) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16714)
    (<0.0>,16715) tree.c:625: return &rsp->node[0];
    (<0.0>,16719) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16720) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16725) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16726) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16727) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16728) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16730) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16732) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16733) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16735) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16738) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16739) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16740) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16742) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16743) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16744) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16745) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16749) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16754) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16755) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16756) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16757) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16759) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16761) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16762) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16767) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16768) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16769) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16772) tree.c:2066: return false;
    (<0.0>,16774) tree.c:2067: }
    (<0.0>,16779) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16795)
    (<0.0>,16796)
    (<0.0>,16797) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16800)
    (<0.0>,16801) tree.c:625: return &rsp->node[0];
    (<0.0>,16805) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16806) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16811) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16812) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16813) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16814) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16816) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16818) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16819) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16821) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16824) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16825) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16826) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16828) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16829) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16830) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16831) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16835) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16840) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16841) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16842) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16843) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16845) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16847) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16848) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16850) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16853) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16854) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16855) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16858) tree.c:2066: return false;
    (<0.0>,16860) tree.c:2067: }
    (<0.0>,16865) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16881)
    (<0.0>,16882)
    (<0.0>,16883) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16886)
    (<0.0>,16887) tree.c:625: return &rsp->node[0];
    (<0.0>,16891) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16892) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16897) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16898) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16899) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16900) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16902) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16904) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16905) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16907) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16910) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16911) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16912) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16914) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16915) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16916) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16917) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16921) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16926) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16927) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16928) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16929) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16933) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16934) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16936) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16939) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16940) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16941) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16944) tree.c:2066: return false;
    (<0.0>,16946) tree.c:2067: }
      (<0.1>,769) fake_sync.h:281: while (!x->done)
      (<0.1>,776) fake_sched.h:43: return __running_cpu;
      (<0.1>,780)
      (<0.1>,781) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,784) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,789) tree.c:892: local_irq_save(flags);
      (<0.1>,792)
      (<0.1>,794) fake_sched.h:43: return __running_cpu;
      (<0.1>,798) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,804) fake_sched.h:43: return __running_cpu;
      (<0.1>,808) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,820)
      (<0.1>,822) fake_sched.h:43: return __running_cpu;
      (<0.1>,826) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,827) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,829) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,830) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,831) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,832) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,833) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,834) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,835) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,839) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,841) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,842) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,843) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,854)
      (<0.1>,855)
      (<0.1>,857) fake_sched.h:43: return __running_cpu;
      (<0.1>,861) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,865) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,868) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,869) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,870) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,872) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,873) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,875) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,876) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,877) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,878) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,888)
      (<0.1>,890) tree.c:895: local_irq_restore(flags);
      (<0.1>,893)
      (<0.1>,895) fake_sched.h:43: return __running_cpu;
      (<0.1>,899) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,905) fake_sched.h:43: return __running_cpu;
      (<0.1>,909) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,917) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,919) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,924)
      (<0.1>,927) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,929) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,931) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,932) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,940) litmus.c:91: y = 1;
      (<0.1>,942) fake_sched.h:43: return __running_cpu;
      (<0.1>,946)
      (<0.1>,949) tree.c:755: local_irq_save(flags);
      (<0.1>,952)
      (<0.1>,954) fake_sched.h:43: return __running_cpu;
      (<0.1>,958) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,960) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,964) fake_sched.h:43: return __running_cpu;
      (<0.1>,968) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,980)
      (<0.1>,982) fake_sched.h:43: return __running_cpu;
      (<0.1>,986) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,987) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,989) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,990) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,991) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,992) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,993) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,994) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,995) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,999) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1001) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1002) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1003) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1019)
      (<0.1>,1021)
      (<0.1>,1023) fake_sched.h:43: return __running_cpu;
      (<0.1>,1027) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,1030) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1031) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1037) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1038) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1040) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1045) fake_sched.h:43: return __running_cpu;
      (<0.1>,1048) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1050) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1052) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1053) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1056)
      (<0.1>,1059) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1062) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1063) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1069) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1070) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1072) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1077) fake_sched.h:43: return __running_cpu;
      (<0.1>,1080) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1082) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1084) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1085) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1088)
      (<0.1>,1091) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1094) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1095) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1101) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1102) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1104) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1111) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1116) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1118) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1119) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1121) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1122) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1123) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1124) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1138)
      (<0.1>,1140) tree.c:758: local_irq_restore(flags);
      (<0.1>,1143)
      (<0.1>,1145) fake_sched.h:43: return __running_cpu;
      (<0.1>,1149) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1151) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1155) fake_sched.h:43: return __running_cpu;
      (<0.1>,1159) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1165) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1168) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,10607) litmus.c:69: r_y = y;
  (<0>,10608) litmus.c:69: r_y = y;
  (<0>,10618) fake_sched.h:43: return __running_cpu;
  (<0>,10622)
  (<0>,10625) tree.c:755: local_irq_save(flags);
  (<0>,10628)
  (<0>,10630) fake_sched.h:43: return __running_cpu;
  (<0>,10634) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10636) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10640) fake_sched.h:43: return __running_cpu;
  (<0>,10644) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,10656)
  (<0>,10658) fake_sched.h:43: return __running_cpu;
  (<0>,10662) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,10663) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,10665) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,10666) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,10667) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10668) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10669) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10670) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10671) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,10675) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,10677) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,10678) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,10679) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,10695)
  (<0>,10697)
  (<0>,10699) fake_sched.h:43: return __running_cpu;
  (<0>,10703) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,10706) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10707) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10708) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10712) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10713) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10714) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10716) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10721) fake_sched.h:43: return __running_cpu;
  (<0>,10724) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10726) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10728) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10729) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,10732)
  (<0>,10735) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10738) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10739) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10740) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10744) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10745) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10746) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10748) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10753) fake_sched.h:43: return __running_cpu;
  (<0>,10756) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10758) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10760) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10761) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,10764)
  (<0>,10767) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10770) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10771) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10772) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10776) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10777) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10778) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10780) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10787) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10790) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10791) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10792) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10794) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10795) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10797) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10798) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10799) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10800) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10814)
  (<0>,10816) tree.c:758: local_irq_restore(flags);
  (<0>,10819)
  (<0>,10821) fake_sched.h:43: return __running_cpu;
  (<0>,10825) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10827) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10831) fake_sched.h:43: return __running_cpu;
  (<0>,10835) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,10841) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,10844) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,10849) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,10853) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10856) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10860) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10861) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10862) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
              Error: Assertion violation at (<0>,10865): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpqbqcpjpn/tmpx4odemz2.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DFORCE_FAILURE_1 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpqbqcpjpn/tmpx_0vaduy.ll /tmp/tmpqbqcpjpn/tmpx4odemz2.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=4 --preemption-bounding=PB --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpqbqcpjpn/tmpx_0vaduy.ll
Total wall-clock time: 15.42 s
Trace count: 100000 (also 0 sleepset blocked, 0 schedulings and 0 branches were rejected due to the bound)
No errors were detected.
Total wall-clock time: 0.0 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 59 (also 1 sleepset blocked, 0 schedulings and 31 branches were rejected due to the bound) 

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,75) tree_plugin.h:731: pr_info("Hierarchical RCU implementation.\n");
  (<0>,79) tree_plugin.h:76: if (rcu_fanout_exact)
  (<0>,82) tree_plugin.h:87: if (rcu_fanout_leaf != RCU_FANOUT_LEAF)
  (<0>,94) tree.c:4147: ulong d;
  (<0>,95) tree.c:4159: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,98) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,99) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,101) tree.c:4161: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,104) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,105) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,107) tree.c:4165: if (rcu_fanout_leaf == RCU_FANOUT_LEAF &&
  (<0>,120)
  (<0>,121) tree.c:4056: static void __init rcu_init_one(struct rcu_state *rsp)
  (<0>,122) tree.c:4067: int i;
  (<0>,125) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,128) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,130) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,131) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,134) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,137) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,141) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,143) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,145) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,147) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,151) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,153) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,154) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,165)
  (<0>,166) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,167) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,170) tree.c:4041: int ccur;
  (<0>,171) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,173) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,175) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,178) tree.c:4046: ccur = levelcnt[i];
  (<0>,180) tree.c:4046: ccur = levelcnt[i];
  (<0>,182) tree.c:4046: ccur = levelcnt[i];
  (<0>,183) tree.c:4046: ccur = levelcnt[i];
  (<0>,184) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,185) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,188) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,190) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,192) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,194) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,195) tree.c:4048: cprv = ccur;
  (<0>,196) tree.c:4048: cprv = ccur;
  (<0>,198) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,200) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,202) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,207) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,208) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,210) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,211) tree.c:4085: fl_mask <<= 1;
  (<0>,215) tree.c:4085: fl_mask <<= 1;
  (<0>,216) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,218) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,220) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,223) tree.c:4090: cpustride *= levelspread[i];
  (<0>,226) tree.c:4090: cpustride *= levelspread[i];
  (<0>,227) tree.c:4090: cpustride *= levelspread[i];
  (<0>,229) tree.c:4090: cpustride *= levelspread[i];
  (<0>,230) tree.c:4091: rnp = rsp->level[i];
  (<0>,232) tree.c:4091: rnp = rsp->level[i];
  (<0>,235) tree.c:4091: rnp = rsp->level[i];
  (<0>,236) tree.c:4091: rnp = rsp->level[i];
  (<0>,237) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,240) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,243) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,246) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,250)
  (<0>,251) fake_sync.h:75: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,252) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,258) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,262)
  (<0>,263) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,264) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,270) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,273) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,275) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,276) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,278) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,279) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,281) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,282) tree.c:4101: rnp->qsmask = 0;
  (<0>,284) tree.c:4101: rnp->qsmask = 0;
  (<0>,285) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,287) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,288) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,291) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,293) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,294) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,296) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,299) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,301) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,302) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,304) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,307) tree.c:4107: if (i == 0) {
  (<0>,310) tree.c:4108: rnp->grpnum = 0;
  (<0>,312) tree.c:4108: rnp->grpnum = 0;
  (<0>,313) tree.c:4109: rnp->grpmask = 0;
  (<0>,315) tree.c:4109: rnp->grpmask = 0;
  (<0>,316) tree.c:4110: rnp->parent = NULL;
  (<0>,318) tree.c:4110: rnp->parent = NULL;
  (<0>,320) tree.c:4117: rnp->level = i;
  (<0>,322) tree.c:4117: rnp->level = i;
  (<0>,324) tree.c:4117: rnp->level = i;
  (<0>,325) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,329)
  (<0>,330) fake_defs.h:358: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,331) fake_defs.h:360: list->next = list;
  (<0>,333) fake_defs.h:360: list->next = list;
  (<0>,334) fake_defs.h:361: list->prev = list;
  (<0>,335) fake_defs.h:361: list->prev = list;
  (<0>,337) fake_defs.h:361: list->prev = list;
  (<0>,339) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,342) tree_plugin.h:2431: }
  (<0>,352) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,356)
  (<0>,357) fake_sync.h:141: void spin_lock_init(raw_spinlock_t *l)
  (<0>,358) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,363) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,365) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,366) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,368) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,370) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,371) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,374) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,378) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,380) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,382) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,389) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,392) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,395) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,396) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,397) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,399) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,400) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,407)
  (<0>,408) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,409) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,411) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,412) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,415) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,417) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,418) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,421) fake_defs.h:635: if (offset & mask)
  (<0>,422) fake_defs.h:635: if (offset & mask)
  (<0>,426) fake_defs.h:636: return cpu;
  (<0>,427) fake_defs.h:636: return cpu;
  (<0>,429) fake_defs.h:639: }
  (<0>,431) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,432) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,436) tree.c:4132: while (i > rnp->grphi)
  (<0>,437) tree.c:4132: while (i > rnp->grphi)
  (<0>,439) tree.c:4132: while (i > rnp->grphi)
  (<0>,442) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,443) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,445) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,447) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,450) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,451) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,452) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,475)
  (<0>,476) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,477) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,479) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,481) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,483) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,484) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,487)
  (<0>,488) tree.c:623: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,492) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,496) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,497) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,498) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,500) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,504)
  (<0>,505) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,506) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,509) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,511) fake_sched.h:43: return __running_cpu;
  (<0>,515) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,517) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,521) fake_sched.h:43: return __running_cpu;
  (<0>,525) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,531) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,532) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,539) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,540) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,542) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,544) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,548) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,550) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,551) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,554) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,556) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,557) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,559) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,561) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,566) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,567) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,569) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,571) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,575) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,576) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,577) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,578) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,579) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,581) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,584) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,585) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,586) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,591) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,592) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,593) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,595) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,598) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,599) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,600) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,604) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,605) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,606) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,607) tree.c:3776: rdp->cpu = cpu;
  (<0>,608) tree.c:3776: rdp->cpu = cpu;
  (<0>,610) tree.c:3776: rdp->cpu = cpu;
  (<0>,611) tree.c:3777: rdp->rsp = rsp;
  (<0>,612) tree.c:3777: rdp->rsp = rsp;
  (<0>,614) tree.c:3777: rdp->rsp = rsp;
  (<0>,615) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,618) tree_plugin.h:2448: }
  (<0>,623) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,624) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,625) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,627) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,631)
  (<0>,632) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,633) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,634) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,637) fake_sync.h:93: local_irq_restore(flags);
  (<0>,640) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,642) fake_sched.h:43: return __running_cpu;
  (<0>,646) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,648) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,652) fake_sched.h:43: return __running_cpu;
  (<0>,656) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,666) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,667) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,674)
  (<0>,675)
  (<0>,676) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,678) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,679) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,682) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,684) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,685) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,688) fake_defs.h:635: if (offset & mask)
  (<0>,689) fake_defs.h:635: if (offset & mask)
  (<0>,693) fake_defs.h:636: return cpu;
  (<0>,694) fake_defs.h:636: return cpu;
  (<0>,696) fake_defs.h:639: }
  (<0>,698) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,699) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,703) tree.c:4132: while (i > rnp->grphi)
  (<0>,704) tree.c:4132: while (i > rnp->grphi)
  (<0>,706) tree.c:4132: while (i > rnp->grphi)
  (<0>,709) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,710) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,712) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,714) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,717) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,718) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,719) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,742)
  (<0>,743)
  (<0>,744) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,746) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,748) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,750) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,751) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,754)
  (<0>,755) tree.c:625: return &rsp->node[0];
  (<0>,759) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,763) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,764) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,765) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,767) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,771)
  (<0>,772)
  (<0>,773) fake_sync.h:83: local_irq_save(flags);
  (<0>,776) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,778) fake_sched.h:43: return __running_cpu;
  (<0>,782) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,784) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,788) fake_sched.h:43: return __running_cpu;
  (<0>,792) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,798) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,799) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,806) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,807) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,809) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,811) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,815) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,817) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,818) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,821) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,823) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,824) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,826) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,828) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,833) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,834) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,836) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,838) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,842) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,843) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,844) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,845) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,846) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,848) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,851) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,852) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,853) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,858) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,859) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,860) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,862) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,865) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,866) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,867) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,871) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,872) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,873) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,874) tree.c:3776: rdp->cpu = cpu;
  (<0>,875) tree.c:3776: rdp->cpu = cpu;
  (<0>,877) tree.c:3776: rdp->cpu = cpu;
  (<0>,878) tree.c:3777: rdp->rsp = rsp;
  (<0>,879) tree.c:3777: rdp->rsp = rsp;
  (<0>,881) tree.c:3777: rdp->rsp = rsp;
  (<0>,882) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,885) tree_plugin.h:2448: }
  (<0>,890) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,891) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,892) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,894) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,898)
  (<0>,899)
  (<0>,900) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,901) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,904) fake_sync.h:93: local_irq_restore(flags);
  (<0>,907) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,909) fake_sched.h:43: return __running_cpu;
  (<0>,913) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,915) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,919) fake_sched.h:43: return __running_cpu;
  (<0>,923) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,933) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,934) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,941)
  (<0>,942)
  (<0>,943) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,945) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,946) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,949) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,951) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,952) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,955) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,957) fake_defs.h:639: }
  (<0>,959) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,960) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,963) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,968)
  (<0>,969) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,970) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,971) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,972) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,974) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,979)
  (<0>,980) fake_defs.h:364: static inline void __list_add(struct list_head *new,
  (<0>,981) fake_defs.h:365: struct list_head *prev,
  (<0>,982) fake_defs.h:366: struct list_head *next)
  (<0>,983) fake_defs.h:368: next->prev = new;
  (<0>,985) fake_defs.h:368: next->prev = new;
  (<0>,986) fake_defs.h:369: new->next = next;
  (<0>,987) fake_defs.h:369: new->next = next;
  (<0>,989) fake_defs.h:369: new->next = next;
  (<0>,990) fake_defs.h:370: new->prev = prev;
  (<0>,991) fake_defs.h:370: new->prev = prev;
  (<0>,993) fake_defs.h:370: new->prev = prev;
  (<0>,994) fake_defs.h:371: prev->next = new;
  (<0>,995) fake_defs.h:371: prev->next = new;
  (<0>,997) fake_defs.h:371: prev->next = new;
  (<0>,1009)
  (<0>,1010) tree.c:4066: int cpustride = 1;
  (<0>,1011) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1014) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1017) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1019) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1020) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1023) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1026) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1027) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1030) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1032) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1034) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1036) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1037) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1040) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1042) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1043) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1054)
  (<0>,1055)
  (<0>,1056) tree.c:4036: if (rcu_fanout_exact) {
  (<0>,1059) tree.c:4044: cprv = nr_cpu_ids;
  (<0>,1060) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1062) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1064) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1067) tree.c:4046: ccur = levelcnt[i];
  (<0>,1069) tree.c:4046: ccur = levelcnt[i];
  (<0>,1071) tree.c:4046: ccur = levelcnt[i];
  (<0>,1072) tree.c:4046: ccur = levelcnt[i];
  (<0>,1073) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1074) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1077) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1079) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1081) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1083) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1084) tree.c:4048: cprv = ccur;
  (<0>,1085) tree.c:4048: cprv = ccur;
  (<0>,1087) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1089) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1091) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1096) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1097) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1099) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1100) tree.c:4085: fl_mask <<= 1;
  (<0>,1104) tree.c:4085: fl_mask <<= 1;
  (<0>,1105) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1107) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1109) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1112) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1115) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1116) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1118) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1119) tree.c:4091: rnp = rsp->level[i];
  (<0>,1121) tree.c:4091: rnp = rsp->level[i];
  (<0>,1124) tree.c:4091: rnp = rsp->level[i];
  (<0>,1125) tree.c:4091: rnp = rsp->level[i];
  (<0>,1126) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1128) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1129) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1132) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1135) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,1139)
  (<0>,1140) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1141) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1147) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1151)
  (<0>,1152) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1153) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1159) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1161) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1162) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1164) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1165) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1167) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1168) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1170) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1171) tree.c:4101: rnp->qsmask = 0;
  (<0>,1173) tree.c:4101: rnp->qsmask = 0;
  (<0>,1174) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1176) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1177) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1178) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1180) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1182) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1183) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1185) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1188) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1190) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1191) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1193) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1196) tree.c:4107: if (i == 0) {
  (<0>,1199) tree.c:4108: rnp->grpnum = 0;
  (<0>,1201) tree.c:4108: rnp->grpnum = 0;
  (<0>,1202) tree.c:4109: rnp->grpmask = 0;
  (<0>,1204) tree.c:4109: rnp->grpmask = 0;
  (<0>,1205) tree.c:4110: rnp->parent = NULL;
  (<0>,1207) tree.c:4110: rnp->parent = NULL;
  (<0>,1209) tree.c:4117: rnp->level = i;
  (<0>,1211) tree.c:4117: rnp->level = i;
  (<0>,1213) tree.c:4117: rnp->level = i;
  (<0>,1214) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1218)
  (<0>,1219) fake_defs.h:360: list->next = list;
  (<0>,1220) fake_defs.h:360: list->next = list;
  (<0>,1222) fake_defs.h:360: list->next = list;
  (<0>,1223) fake_defs.h:361: list->prev = list;
  (<0>,1224) fake_defs.h:361: list->prev = list;
  (<0>,1226) fake_defs.h:361: list->prev = list;
  (<0>,1228) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,1231) tree_plugin.h:2431: }
  (<0>,1241) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,1245)
  (<0>,1246) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1247) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1252) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1254) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1255) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1257) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1259) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1260) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1263) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1267) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1269) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1271) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1278) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1281) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1284) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1285) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1286) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1288) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1289) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1296)
  (<0>,1297)
  (<0>,1298) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1300) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1301) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1304) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1306) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1307) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1310) fake_defs.h:635: if (offset & mask)
  (<0>,1311) fake_defs.h:635: if (offset & mask)
  (<0>,1315) fake_defs.h:636: return cpu;
  (<0>,1316) fake_defs.h:636: return cpu;
  (<0>,1318) fake_defs.h:639: }
  (<0>,1320) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1321) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1325) tree.c:4132: while (i > rnp->grphi)
  (<0>,1326) tree.c:4132: while (i > rnp->grphi)
  (<0>,1328) tree.c:4132: while (i > rnp->grphi)
  (<0>,1331) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1332) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1334) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1336) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1339) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1340) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1341) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1364)
  (<0>,1365)
  (<0>,1366) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1368) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1370) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1372) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1373) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1376)
  (<0>,1377) tree.c:625: return &rsp->node[0];
  (<0>,1381) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1385) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1386) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1387) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1389) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1393)
  (<0>,1394)
  (<0>,1395) fake_sync.h:83: local_irq_save(flags);
  (<0>,1398) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1400) fake_sched.h:43: return __running_cpu;
  (<0>,1404) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1406) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1410) fake_sched.h:43: return __running_cpu;
  (<0>,1414) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1420) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1421) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1428) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1429) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1431) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1433) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1437) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1439) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1440) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1443) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1445) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1446) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1448) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1450) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1455) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1456) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1458) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1460) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1464) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1465) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1466) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1467) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1468) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1470) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1473) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1474) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1475) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1480) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1481) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1482) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1484) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1487) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1488) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1489) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1493) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1494) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1495) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1496) tree.c:3776: rdp->cpu = cpu;
  (<0>,1497) tree.c:3776: rdp->cpu = cpu;
  (<0>,1499) tree.c:3776: rdp->cpu = cpu;
  (<0>,1500) tree.c:3777: rdp->rsp = rsp;
  (<0>,1501) tree.c:3777: rdp->rsp = rsp;
  (<0>,1503) tree.c:3777: rdp->rsp = rsp;
  (<0>,1504) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1507) tree_plugin.h:2448: }
  (<0>,1512) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1513) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1514) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1516) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1520)
  (<0>,1521)
  (<0>,1522) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1523) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1526) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1529) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1531) fake_sched.h:43: return __running_cpu;
  (<0>,1535) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1541) fake_sched.h:43: return __running_cpu;
  (<0>,1545) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1555) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1556) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1563)
  (<0>,1564)
  (<0>,1565) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1567) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1568) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1571) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1573) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1574) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1577) fake_defs.h:635: if (offset & mask)
  (<0>,1578) fake_defs.h:635: if (offset & mask)
  (<0>,1582) fake_defs.h:636: return cpu;
  (<0>,1583) fake_defs.h:636: return cpu;
  (<0>,1585) fake_defs.h:639: }
  (<0>,1587) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1588) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1592) tree.c:4132: while (i > rnp->grphi)
  (<0>,1593) tree.c:4132: while (i > rnp->grphi)
  (<0>,1595) tree.c:4132: while (i > rnp->grphi)
  (<0>,1598) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1599) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1601) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1603) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1606) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1607) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1608) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1631)
  (<0>,1632)
  (<0>,1633) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1635) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1637) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1639) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1640) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1643)
  (<0>,1644) tree.c:625: return &rsp->node[0];
  (<0>,1648) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1652) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1653) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1654) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1656) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1660)
  (<0>,1661)
  (<0>,1662) fake_sync.h:83: local_irq_save(flags);
  (<0>,1665) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1667) fake_sched.h:43: return __running_cpu;
  (<0>,1671) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1673) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1677) fake_sched.h:43: return __running_cpu;
  (<0>,1681) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1687) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1688) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1695) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1696) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1698) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1700) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1704) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1706) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1707) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1710) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1712) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1713) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1715) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1717) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1722) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1723) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1725) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1727) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1731) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1732) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1733) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1734) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1735) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1737) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1740) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1741) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1742) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1747) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1748) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1749) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1751) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1754) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1755) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1756) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1760) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1761) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1762) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1763) tree.c:3776: rdp->cpu = cpu;
  (<0>,1764) tree.c:3776: rdp->cpu = cpu;
  (<0>,1766) tree.c:3776: rdp->cpu = cpu;
  (<0>,1767) tree.c:3777: rdp->rsp = rsp;
  (<0>,1768) tree.c:3777: rdp->rsp = rsp;
  (<0>,1770) tree.c:3777: rdp->rsp = rsp;
  (<0>,1771) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1774) tree_plugin.h:2448: }
  (<0>,1779) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1780) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1781) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1783) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1787)
  (<0>,1788)
  (<0>,1789) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1790) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1793) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1796) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1798) fake_sched.h:43: return __running_cpu;
  (<0>,1802) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1804) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1808) fake_sched.h:43: return __running_cpu;
  (<0>,1812) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1822) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1823) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1830)
  (<0>,1831)
  (<0>,1832) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1834) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1835) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1838) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1840) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1841) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1844) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,1846) fake_defs.h:639: }
  (<0>,1848) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1849) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1852) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1857)
  (<0>,1858)
  (<0>,1859) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1860) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1861) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1863) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1868)
  (<0>,1869)
  (<0>,1870)
  (<0>,1871) fake_defs.h:368: next->prev = new;
  (<0>,1872) fake_defs.h:368: next->prev = new;
  (<0>,1874) fake_defs.h:368: next->prev = new;
  (<0>,1875) fake_defs.h:369: new->next = next;
  (<0>,1876) fake_defs.h:369: new->next = next;
  (<0>,1878) fake_defs.h:369: new->next = next;
  (<0>,1879) fake_defs.h:370: new->prev = prev;
  (<0>,1880) fake_defs.h:370: new->prev = prev;
  (<0>,1882) fake_defs.h:370: new->prev = prev;
  (<0>,1883) fake_defs.h:371: prev->next = new;
  (<0>,1884) fake_defs.h:371: prev->next = new;
  (<0>,1886) fake_defs.h:371: prev->next = new;
  (<0>,1890) tree.c:4251: if (dump_tree)
  (<0>,1899) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1901) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1902) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1909)
  (<0>,1910)
  (<0>,1911) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1913) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1914) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1917) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1919) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1920) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1923) fake_defs.h:635: if (offset & mask)
  (<0>,1924) fake_defs.h:635: if (offset & mask)
  (<0>,1928) fake_defs.h:636: return cpu;
  (<0>,1929) fake_defs.h:636: return cpu;
  (<0>,1931) fake_defs.h:639: }
  (<0>,1933) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1934) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1937) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,1945)
  (<0>,1946) tree.c:3829: int rcutree_prepare_cpu(unsigned int cpu)
  (<0>,1947) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1948) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1952) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1953) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1954) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1956) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1960) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1961) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1987)
  (<0>,1988) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1989) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1991) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1993) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1995) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1996) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1999)
  (<0>,2000) tree.c:625: return &rsp->node[0];
  (<0>,2004) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2008) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2009) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2010) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2012) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2016)
  (<0>,2017)
  (<0>,2018) fake_sync.h:83: local_irq_save(flags);
  (<0>,2021) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2023) fake_sched.h:43: return __running_cpu;
  (<0>,2027) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2029) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2033) fake_sched.h:43: return __running_cpu;
  (<0>,2037) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2043) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2044) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2051) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2053) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2054) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2056) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2057) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2059) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2060) tree.c:3800: rdp->blimit = blimit;
  (<0>,2061) tree.c:3800: rdp->blimit = blimit;
  (<0>,2063) tree.c:3800: rdp->blimit = blimit;
  (<0>,2064) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2066) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2069) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2072)
  (<0>,2073) tree.c:1551: static void init_callback_list(struct rcu_data *rdp)
  (<0>,2076) tree_plugin.h:2469: return false;
  (<0>,2079) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2083)
  (<0>,2084) tree.c:1539: static void init_default_callback_list(struct rcu_data *rdp)
  (<0>,2086) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2087) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2089) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2092) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2094) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2096) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2099) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2101) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2103) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2105) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2108) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2110) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2112) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2115) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2117) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2119) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2121) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2124) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2126) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2128) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2131) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2133) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2135) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2137) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2140) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2142) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2144) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2147) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2149) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2151) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2153) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2160) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2162) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2164) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2165) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2167) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2170) tree_plugin.h:2902: }
  (<0>,2172) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2173) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2175) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2178) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2179) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2180) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2183) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2185) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2188) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2189) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2192)
  (<0>,2193) tree.h:721: static inline void raw_spin_unlock_rcu_node(struct rcu_node *rnp)
  (<0>,2197)
  (<0>,2198) fake_sync.h:120: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2199) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2205) tree.c:3814: rnp = rdp->mynode;
  (<0>,2207) tree.c:3814: rnp = rdp->mynode;
  (<0>,2208) tree.c:3814: rnp = rdp->mynode;
  (<0>,2209) tree.c:3815: mask = rdp->grpmask;
  (<0>,2211) tree.c:3815: mask = rdp->grpmask;
  (<0>,2212) tree.c:3815: mask = rdp->grpmask;
  (<0>,2213) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2216)
  (<0>,2217) tree.h:715: static inline void raw_spin_lock_rcu_node(struct rcu_node *rnp)
  (<0>,2221) fake_sync.h:115: preempt_disable();
  (<0>,2223) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2224) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2231) tree.c:3817: if (!rdp->beenonline)
  (<0>,2233) tree.c:3817: if (!rdp->beenonline)
  (<0>,2237) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2242) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2243) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2244) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2245) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2247) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2249) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2250) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2252) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2255) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2256) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2257) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2259) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2260) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2265) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2266) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2267) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2268) fake_defs.h:237: switch (size) {
  (<0>,2270) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2272) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2273) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2275) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2278) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2279) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2280) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2282) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2284) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2285) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2287) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2288) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2290) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2291) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2293) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2294) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2296) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2297) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2301) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2302) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2305) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2306) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2308) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2309) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2311) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2317) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2318) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2319) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2321) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2325)
  (<0>,2326)
  (<0>,2327) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2328) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2331) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2334) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2336) fake_sched.h:43: return __running_cpu;
  (<0>,2340) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2342) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2346) fake_sched.h:43: return __running_cpu;
  (<0>,2350) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2360) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2363) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2364) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2365) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2369) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2370) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2371) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2373) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2377) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2378) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2404)
  (<0>,2405)
  (<0>,2406) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2408) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2410) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2412) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2413) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2416)
  (<0>,2417) tree.c:625: return &rsp->node[0];
  (<0>,2421) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2425) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2426) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2427) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2429) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2433)
  (<0>,2434)
  (<0>,2435) fake_sync.h:83: local_irq_save(flags);
  (<0>,2438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2440) fake_sched.h:43: return __running_cpu;
  (<0>,2444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2446) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2450) fake_sched.h:43: return __running_cpu;
  (<0>,2454) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2460) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2461) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2468) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2470) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2471) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2473) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2474) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2476) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2477) tree.c:3800: rdp->blimit = blimit;
  (<0>,2478) tree.c:3800: rdp->blimit = blimit;
  (<0>,2480) tree.c:3800: rdp->blimit = blimit;
  (<0>,2481) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2483) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2486) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2489)
  (<0>,2490) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,2493) tree_plugin.h:2469: return false;
  (<0>,2496) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2500)
  (<0>,2501) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2503) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2504) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2506) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2509) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2511) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2513) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2516) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2518) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2520) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2522) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2525) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2527) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2529) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2532) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2534) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2536) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2538) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2541) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2543) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2545) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2548) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2550) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2552) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2554) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2557) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2559) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2561) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2564) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2566) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2568) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2570) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2577) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2579) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2581) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2582) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2584) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2587) tree_plugin.h:2902: }
  (<0>,2589) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2590) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2592) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2595) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2596) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2597) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2600) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2602) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2605) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2606) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2609)
  (<0>,2610) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2614)
  (<0>,2615) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2616) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2622) tree.c:3814: rnp = rdp->mynode;
  (<0>,2624) tree.c:3814: rnp = rdp->mynode;
  (<0>,2625) tree.c:3814: rnp = rdp->mynode;
  (<0>,2626) tree.c:3815: mask = rdp->grpmask;
  (<0>,2628) tree.c:3815: mask = rdp->grpmask;
  (<0>,2629) tree.c:3815: mask = rdp->grpmask;
  (<0>,2630) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2633)
  (<0>,2634) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2638) fake_sync.h:115: preempt_disable();
  (<0>,2640) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2641) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2648) tree.c:3817: if (!rdp->beenonline)
  (<0>,2650) tree.c:3817: if (!rdp->beenonline)
  (<0>,2654) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2659) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2660) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2661) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2662) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2664) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2666) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2667) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2669) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2672) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2673) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2674) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2676) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2677) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2682) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2683) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2684) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2685) fake_defs.h:237: switch (size) {
  (<0>,2687) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2689) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2690) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2692) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2695) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2696) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2697) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2699) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2701) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2702) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2704) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2705) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2707) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2708) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2710) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2711) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2713) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2714) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2718) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2719) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2722) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2723) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2725) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2726) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2728) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2734) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2735) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2736) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2738) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2742)
  (<0>,2743)
  (<0>,2744) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2745) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2748) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2751) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2753) fake_sched.h:43: return __running_cpu;
  (<0>,2757) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2759) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2763) fake_sched.h:43: return __running_cpu;
  (<0>,2767) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2777) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2780) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2781) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2782) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2786) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2787) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2788) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2790) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2794) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,2797) tree_plugin.h:1243: }
  (<0>,2799) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2802) tree_plugin.h:2461: }
  (<0>,2805) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,2823)
  (<0>,2824) tree.c:3890: void rcu_cpu_starting(unsigned int cpu)
  (<0>,2825) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2826) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2830) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2831) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2832) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2834) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2839) fake_sched.h:43: return __running_cpu;
  (<0>,2842) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2844) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2846) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2847) tree.c:3900: rnp = rdp->mynode;
  (<0>,2849) tree.c:3900: rnp = rdp->mynode;
  (<0>,2850) tree.c:3900: rnp = rdp->mynode;
  (<0>,2851) tree.c:3901: mask = rdp->grpmask;
  (<0>,2853) tree.c:3901: mask = rdp->grpmask;
  (<0>,2854) tree.c:3901: mask = rdp->grpmask;
  (<0>,2858) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2859) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2860) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2862) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2866)
  (<0>,2867)
  (<0>,2868) fake_sync.h:83: local_irq_save(flags);
  (<0>,2871) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2873) fake_sched.h:43: return __running_cpu;
  (<0>,2877) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2879) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2883) fake_sched.h:43: return __running_cpu;
  (<0>,2887) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2893) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2894) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2901) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2902) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2904) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2906) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2907) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2908) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2910) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2912) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2916) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2917) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2918) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2920) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2924)
  (<0>,2925)
  (<0>,2926) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2927) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2930) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2933) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2935) fake_sched.h:43: return __running_cpu;
  (<0>,2939) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2941) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2945) fake_sched.h:43: return __running_cpu;
  (<0>,2949) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2958) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2961) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2962) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2963) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2967) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2968) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2969) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2971) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2976) fake_sched.h:43: return __running_cpu;
  (<0>,2979) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2981) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2983) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2984) tree.c:3900: rnp = rdp->mynode;
  (<0>,2986) tree.c:3900: rnp = rdp->mynode;
  (<0>,2987) tree.c:3900: rnp = rdp->mynode;
  (<0>,2988) tree.c:3901: mask = rdp->grpmask;
  (<0>,2990) tree.c:3901: mask = rdp->grpmask;
  (<0>,2991) tree.c:3901: mask = rdp->grpmask;
  (<0>,2995) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2996) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2997) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2999) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3003)
  (<0>,3004)
  (<0>,3005) fake_sync.h:83: local_irq_save(flags);
  (<0>,3008) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3010) fake_sched.h:43: return __running_cpu;
  (<0>,3014) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3016) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3020) fake_sched.h:43: return __running_cpu;
  (<0>,3024) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3030) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3031) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3038) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3039) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3041) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3043) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3044) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3045) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3047) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3049) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3053) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3054) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3055) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3057) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3061)
  (<0>,3062)
  (<0>,3063) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3064) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3067) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3070) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3072) fake_sched.h:43: return __running_cpu;
  (<0>,3076) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3078) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3082) fake_sched.h:43: return __running_cpu;
  (<0>,3086) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3095) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3098) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3099) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3100) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3104) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3105) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3106) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3108) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3114) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3115) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3122)
  (<0>,3123)
  (<0>,3124) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3126) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3127) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3130) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3132) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3133) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3136) fake_defs.h:635: if (offset & mask)
  (<0>,3137) fake_defs.h:635: if (offset & mask)
  (<0>,3141) fake_defs.h:636: return cpu;
  (<0>,3142) fake_defs.h:636: return cpu;
  (<0>,3144) fake_defs.h:639: }
  (<0>,3146) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3147) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3150) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,3158)
  (<0>,3159) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3160) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3161) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3165) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3166) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3167) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3169) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3173) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3174) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3200)
  (<0>,3201)
  (<0>,3202) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3204) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3206) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3208) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3209) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3212)
  (<0>,3213) tree.c:625: return &rsp->node[0];
  (<0>,3217) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3221) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3222) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3223) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3225) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3229)
  (<0>,3230)
  (<0>,3231) fake_sync.h:83: local_irq_save(flags);
  (<0>,3234) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3236) fake_sched.h:43: return __running_cpu;
  (<0>,3240) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3242) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3246) fake_sched.h:43: return __running_cpu;
  (<0>,3250) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3256) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3257) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3264) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3266) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3267) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3269) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3270) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3272) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3273) tree.c:3800: rdp->blimit = blimit;
  (<0>,3274) tree.c:3800: rdp->blimit = blimit;
  (<0>,3276) tree.c:3800: rdp->blimit = blimit;
  (<0>,3277) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3279) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3282) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3285)
  (<0>,3286) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3289) tree_plugin.h:2469: return false;
  (<0>,3292) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3296)
  (<0>,3297) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3299) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3300) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3302) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3305) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3307) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3309) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3312) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3314) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3316) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3318) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3321) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3323) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3325) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3328) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3330) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3332) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3334) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3337) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3339) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3341) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3344) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3346) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3348) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3350) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3353) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3355) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3357) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3360) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3362) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3364) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3366) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3373) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3375) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3377) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3378) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3380) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3383) tree_plugin.h:2902: }
  (<0>,3385) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3386) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3388) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3391) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3392) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3393) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3396) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3398) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3401) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3402) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3405)
  (<0>,3406) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3410)
  (<0>,3411) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3412) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3418) tree.c:3814: rnp = rdp->mynode;
  (<0>,3420) tree.c:3814: rnp = rdp->mynode;
  (<0>,3421) tree.c:3814: rnp = rdp->mynode;
  (<0>,3422) tree.c:3815: mask = rdp->grpmask;
  (<0>,3424) tree.c:3815: mask = rdp->grpmask;
  (<0>,3425) tree.c:3815: mask = rdp->grpmask;
  (<0>,3426) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3429)
  (<0>,3430) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3434) fake_sync.h:115: preempt_disable();
  (<0>,3436) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3437) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3444) tree.c:3817: if (!rdp->beenonline)
  (<0>,3446) tree.c:3817: if (!rdp->beenonline)
  (<0>,3450) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3455) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3456) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3457) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3458) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3460) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3462) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3463) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3465) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3468) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3469) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3470) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3472) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3473) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3478) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3479) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3480) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3481) fake_defs.h:237: switch (size) {
  (<0>,3483) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3485) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3486) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3488) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3491) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3492) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3493) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3495) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3497) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3498) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3500) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3501) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3503) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3504) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3506) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3507) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3509) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3510) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3514) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3515) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3518) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3519) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3521) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3522) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3524) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3530) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3531) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3532) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3534) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3538)
  (<0>,3539)
  (<0>,3540) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3541) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3544) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3547) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3549) fake_sched.h:43: return __running_cpu;
  (<0>,3553) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3555) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3559) fake_sched.h:43: return __running_cpu;
  (<0>,3563) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3573) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3576) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3577) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3578) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3582) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3583) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3584) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3586) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3590) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3591) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3617)
  (<0>,3618)
  (<0>,3619) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3621) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3623) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3625) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3626) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3629)
  (<0>,3630) tree.c:625: return &rsp->node[0];
  (<0>,3634) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3638) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3639) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3640) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3642) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3646)
  (<0>,3647)
  (<0>,3648) fake_sync.h:83: local_irq_save(flags);
  (<0>,3651) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3653) fake_sched.h:43: return __running_cpu;
  (<0>,3657) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3659) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3663) fake_sched.h:43: return __running_cpu;
  (<0>,3667) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3673) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3674) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3681) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3683) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3684) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3686) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3687) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3689) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3690) tree.c:3800: rdp->blimit = blimit;
  (<0>,3691) tree.c:3800: rdp->blimit = blimit;
  (<0>,3693) tree.c:3800: rdp->blimit = blimit;
  (<0>,3694) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3696) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3699) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3702)
  (<0>,3703) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3706) tree_plugin.h:2469: return false;
  (<0>,3709) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3713)
  (<0>,3714) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3716) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3717) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3719) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3722) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3724) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3726) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3729) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3731) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3733) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3735) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3738) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3740) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3742) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3745) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3747) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3749) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3751) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3754) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3756) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3758) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3761) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3763) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3765) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3767) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3770) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3772) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3774) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3777) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3779) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3781) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3783) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3790) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3792) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3794) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3795) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3797) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3800) tree_plugin.h:2902: }
  (<0>,3802) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3803) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3805) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3808) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3809) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3810) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3813) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3815) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3818) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3819) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3822)
  (<0>,3823) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3827)
  (<0>,3828) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3829) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3835) tree.c:3814: rnp = rdp->mynode;
  (<0>,3837) tree.c:3814: rnp = rdp->mynode;
  (<0>,3838) tree.c:3814: rnp = rdp->mynode;
  (<0>,3839) tree.c:3815: mask = rdp->grpmask;
  (<0>,3841) tree.c:3815: mask = rdp->grpmask;
  (<0>,3842) tree.c:3815: mask = rdp->grpmask;
  (<0>,3843) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3846)
  (<0>,3847) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3851) fake_sync.h:115: preempt_disable();
  (<0>,3853) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3854) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3861) tree.c:3817: if (!rdp->beenonline)
  (<0>,3863) tree.c:3817: if (!rdp->beenonline)
  (<0>,3867) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3872) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3873) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3874) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3875) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3877) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3879) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3880) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3882) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3885) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3886) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3887) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3889) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3890) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3895) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3896) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3897) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3898) fake_defs.h:237: switch (size) {
  (<0>,3900) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3902) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3903) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3905) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3908) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3909) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3910) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3912) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3914) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3915) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3917) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3918) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3920) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3921) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3923) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3924) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3926) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3927) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3931) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3932) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3935) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3936) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3938) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3939) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3941) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3947) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3948) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3949) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3951) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3955)
  (<0>,3956)
  (<0>,3957) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3958) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3961) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3964) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3966) fake_sched.h:43: return __running_cpu;
  (<0>,3970) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3972) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3976) fake_sched.h:43: return __running_cpu;
  (<0>,3980) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3990) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3993) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3994) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3995) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3999) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4000) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4001) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4003) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4007) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,4010) tree_plugin.h:1243: }
  (<0>,4012) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,4015) tree_plugin.h:2461: }
  (<0>,4018) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,4036)
  (<0>,4037) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4038) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4039) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4043) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4044) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4045) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4047) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4052) fake_sched.h:43: return __running_cpu;
  (<0>,4055) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4057) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4059) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4060) tree.c:3900: rnp = rdp->mynode;
  (<0>,4062) tree.c:3900: rnp = rdp->mynode;
  (<0>,4063) tree.c:3900: rnp = rdp->mynode;
  (<0>,4064) tree.c:3901: mask = rdp->grpmask;
  (<0>,4066) tree.c:3901: mask = rdp->grpmask;
  (<0>,4067) tree.c:3901: mask = rdp->grpmask;
  (<0>,4071) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4072) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4073) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4075) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4079)
  (<0>,4080)
  (<0>,4081) fake_sync.h:83: local_irq_save(flags);
  (<0>,4084) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4086) fake_sched.h:43: return __running_cpu;
  (<0>,4090) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4092) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4096) fake_sched.h:43: return __running_cpu;
  (<0>,4100) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4106) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4107) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4114) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4115) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4117) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4119) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4120) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4121) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4123) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4125) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4129) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4130) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4131) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4133) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4137)
  (<0>,4138)
  (<0>,4139) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4140) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4143) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4146) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4148) fake_sched.h:43: return __running_cpu;
  (<0>,4152) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4154) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4158) fake_sched.h:43: return __running_cpu;
  (<0>,4162) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4171) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4175) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4180) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4181) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4182) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4184) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4189) fake_sched.h:43: return __running_cpu;
  (<0>,4192) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4194) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4196) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4197) tree.c:3900: rnp = rdp->mynode;
  (<0>,4199) tree.c:3900: rnp = rdp->mynode;
  (<0>,4200) tree.c:3900: rnp = rdp->mynode;
  (<0>,4201) tree.c:3901: mask = rdp->grpmask;
  (<0>,4203) tree.c:3901: mask = rdp->grpmask;
  (<0>,4204) tree.c:3901: mask = rdp->grpmask;
  (<0>,4208) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4209) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4210) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4212) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4216)
  (<0>,4217)
  (<0>,4218) fake_sync.h:83: local_irq_save(flags);
  (<0>,4221) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4223) fake_sched.h:43: return __running_cpu;
  (<0>,4227) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4229) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4233) fake_sched.h:43: return __running_cpu;
  (<0>,4237) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4243) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4244) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4251) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4252) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4254) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4256) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4257) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4258) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4260) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4262) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4266) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4267) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4268) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4270) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4274)
  (<0>,4275)
  (<0>,4276) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4277) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4280) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4283) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4285) fake_sched.h:43: return __running_cpu;
  (<0>,4289) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4291) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4295) fake_sched.h:43: return __running_cpu;
  (<0>,4299) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4308) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4313) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4317) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4319) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4321) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4327) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4328) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4335)
  (<0>,4336)
  (<0>,4337) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4339) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4340) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4343) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4345) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4346) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4349) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,4351) fake_defs.h:639: }
  (<0>,4353) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4354) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4358) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4360) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4363) litmus.c:123: set_cpu(i);
  (<0>,4366)
  (<0>,4367) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,4368) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4370) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4388)
  (<0>,4389) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4390) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4391) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4395) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4396) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4397) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4399) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4404) fake_sched.h:43: return __running_cpu;
  (<0>,4407) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4409) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4411) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4412) tree.c:3900: rnp = rdp->mynode;
  (<0>,4414) tree.c:3900: rnp = rdp->mynode;
  (<0>,4415) tree.c:3900: rnp = rdp->mynode;
  (<0>,4416) tree.c:3901: mask = rdp->grpmask;
  (<0>,4418) tree.c:3901: mask = rdp->grpmask;
  (<0>,4419) tree.c:3901: mask = rdp->grpmask;
  (<0>,4423) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4424) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4425) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4427) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4431)
  (<0>,4432)
  (<0>,4433) fake_sync.h:83: local_irq_save(flags);
  (<0>,4436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4438) fake_sched.h:43: return __running_cpu;
  (<0>,4442) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4448) fake_sched.h:43: return __running_cpu;
  (<0>,4452) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4458) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4459) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4466) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4467) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4469) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4471) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4472) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4473) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4475) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4477) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4481) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4482) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4483) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4485) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4489)
  (<0>,4490)
  (<0>,4491) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4492) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4495) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4498) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4500) fake_sched.h:43: return __running_cpu;
  (<0>,4504) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4506) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4510) fake_sched.h:43: return __running_cpu;
  (<0>,4514) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4523) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4526) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4527) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4528) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4532) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4533) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4534) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4541) fake_sched.h:43: return __running_cpu;
  (<0>,4544) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4546) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4548) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4549) tree.c:3900: rnp = rdp->mynode;
  (<0>,4551) tree.c:3900: rnp = rdp->mynode;
  (<0>,4552) tree.c:3900: rnp = rdp->mynode;
  (<0>,4553) tree.c:3901: mask = rdp->grpmask;
  (<0>,4555) tree.c:3901: mask = rdp->grpmask;
  (<0>,4556) tree.c:3901: mask = rdp->grpmask;
  (<0>,4560) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4561) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4562) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4564) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4568)
  (<0>,4569)
  (<0>,4570) fake_sync.h:83: local_irq_save(flags);
  (<0>,4573) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4575) fake_sched.h:43: return __running_cpu;
  (<0>,4579) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4581) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4585) fake_sched.h:43: return __running_cpu;
  (<0>,4589) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4595) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4596) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4603) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4604) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4606) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4608) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4609) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4610) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4612) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4614) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4618) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4619) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4620) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4622) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4626)
  (<0>,4627)
  (<0>,4628) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4629) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4632) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4635) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4637) fake_sched.h:43: return __running_cpu;
  (<0>,4641) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4643) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4647) fake_sched.h:43: return __running_cpu;
  (<0>,4651) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4660) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4663) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4664) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4665) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4669) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4670) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4671) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4673) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4680) tree.c:753: unsigned long flags;
  (<0>,4683) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4685) fake_sched.h:43: return __running_cpu;
  (<0>,4689) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4691) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4695) fake_sched.h:43: return __running_cpu;
  (<0>,4699) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4711) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4713) fake_sched.h:43: return __running_cpu;
  (<0>,4717) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4718) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4720) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4721) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4722) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4723) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4724) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4725) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4726) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4730) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4732) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4733) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4734) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4750)
  (<0>,4752) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4754) fake_sched.h:43: return __running_cpu;
  (<0>,4758) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4761) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4762) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4763) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4767) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4768) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4769) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4771) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4779) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4781) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4783) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4784) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4787) tree_plugin.h:2457: }
  (<0>,4790) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4793) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4794) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4795) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4799) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4800) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4801) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4803) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4808) fake_sched.h:43: return __running_cpu;
  (<0>,4811) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4813) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4815) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4816) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4819) tree_plugin.h:2457: }
  (<0>,4822) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4825) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4826) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4827) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4831) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4832) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4833) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4835) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4842) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4845) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4846) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4847) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4849) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4850) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4852) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4853) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4854) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4855) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4869) tree_plugin.h:2879: }
  (<0>,4871) tree.c:758: local_irq_restore(flags);
  (<0>,4874) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4876) fake_sched.h:43: return __running_cpu;
  (<0>,4880) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4882) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4886) fake_sched.h:43: return __running_cpu;
  (<0>,4890) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4897) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4899) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4901) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4904) litmus.c:123: set_cpu(i);
  (<0>,4907)
  (<0>,4908) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4909) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4911) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4929)
  (<0>,4930) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4931) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4932) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4936) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4937) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4938) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4940) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4948) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4950) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4952) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4953) tree.c:3900: rnp = rdp->mynode;
  (<0>,4955) tree.c:3900: rnp = rdp->mynode;
  (<0>,4956) tree.c:3900: rnp = rdp->mynode;
  (<0>,4957) tree.c:3901: mask = rdp->grpmask;
  (<0>,4959) tree.c:3901: mask = rdp->grpmask;
  (<0>,4960) tree.c:3901: mask = rdp->grpmask;
  (<0>,4964) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4965) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4966) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4968) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4972)
  (<0>,4973)
  (<0>,4974) fake_sync.h:83: local_irq_save(flags);
  (<0>,4977) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:43: return __running_cpu;
  (<0>,4983) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4985) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4989) fake_sched.h:43: return __running_cpu;
  (<0>,4993) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4999) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5000) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5007) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5008) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5010) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5012) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5013) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5014) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5016) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5018) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5022) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5023) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5024) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5026) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5030)
  (<0>,5031)
  (<0>,5032) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5033) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5036) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5039) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5041) fake_sched.h:43: return __running_cpu;
  (<0>,5045) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5047) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5051) fake_sched.h:43: return __running_cpu;
  (<0>,5055) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5064) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5067) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5068) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5069) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5073) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5074) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5075) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5077) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5082) fake_sched.h:43: return __running_cpu;
  (<0>,5085) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5087) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5089) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5090) tree.c:3900: rnp = rdp->mynode;
  (<0>,5092) tree.c:3900: rnp = rdp->mynode;
  (<0>,5093) tree.c:3900: rnp = rdp->mynode;
  (<0>,5094) tree.c:3901: mask = rdp->grpmask;
  (<0>,5096) tree.c:3901: mask = rdp->grpmask;
  (<0>,5097) tree.c:3901: mask = rdp->grpmask;
  (<0>,5101) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5102) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5103) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5105) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5109)
  (<0>,5110)
  (<0>,5111) fake_sync.h:83: local_irq_save(flags);
  (<0>,5114) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5116) fake_sched.h:43: return __running_cpu;
  (<0>,5120) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5122) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5126) fake_sched.h:43: return __running_cpu;
  (<0>,5130) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5136) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5137) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5144) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5145) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5147) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5149) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5150) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5151) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5153) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5155) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5159) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5160) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5161) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5163) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5167)
  (<0>,5168)
  (<0>,5169) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5170) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5173) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5176) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5178) fake_sched.h:43: return __running_cpu;
  (<0>,5182) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5184) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5188) fake_sched.h:43: return __running_cpu;
  (<0>,5192) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5201) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5204) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5205) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5206) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5210) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5211) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5212) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5214) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5221) tree.c:755: local_irq_save(flags);
  (<0>,5224) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5226) fake_sched.h:43: return __running_cpu;
  (<0>,5230) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5232) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5236) fake_sched.h:43: return __running_cpu;
  (<0>,5240) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5252) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5254) fake_sched.h:43: return __running_cpu;
  (<0>,5258) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5259) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5261) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5262) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5263) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5264) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5265) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5266) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5267) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,5271) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5273) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5274) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5275) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5291)
  (<0>,5293) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5295) fake_sched.h:43: return __running_cpu;
  (<0>,5299) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5302) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5303) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5304) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5308) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5309) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5310) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5312) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5317) fake_sched.h:43: return __running_cpu;
  (<0>,5320) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5322) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5324) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5325) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5328) tree_plugin.h:2457: }
  (<0>,5331) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5334) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5335) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5336) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5340) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5341) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5342) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5344) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5349) fake_sched.h:43: return __running_cpu;
  (<0>,5352) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5354) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5356) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5357) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5360) tree_plugin.h:2457: }
  (<0>,5363) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5366) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5367) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5368) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5372) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5373) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5374) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5376) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5383) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5386) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5387) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5388) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5390) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5391) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5393) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5394) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5395) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5396) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5410) tree_plugin.h:2879: }
  (<0>,5412) tree.c:758: local_irq_restore(flags);
  (<0>,5415) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5417) fake_sched.h:43: return __running_cpu;
  (<0>,5421) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5423) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5427) fake_sched.h:43: return __running_cpu;
  (<0>,5431) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5438) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5440) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5442) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5462) tree.c:3971: unsigned long flags;
  (<0>,5463) tree.c:3972: int kthread_prio_in = kthread_prio;
  (<0>,5464) tree.c:3973: struct rcu_node *rnp;
  (<0>,5467) tree.c:3983: else if (kthread_prio > 99)
  (<0>,5471) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5472) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5475) tree.c:3989: rcu_scheduler_fully_active = 1;
  (<0>,5476) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5477) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5478) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5482) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5483) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5484) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5486) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5490) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5492) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5494) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5503)
  (<0>,5504) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5505) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5506) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5507) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5508) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5511)
  (<0>,5512)
  (<0>,5519)
  (<0>,5520)
  (<0>,5527)
  (<0>,5528)
  (<0>,5535)
  (<0>,5536)
  (<0>,5543)
  (<0>,5544)
  (<0>,5551)
  (<0>,5552)
  (<0>,5559)
  (<0>,5560)
  (<0>,5567)
  (<0>,5568)
  (<0>,5575)
  (<0>,5576)
  (<0>,5583)
  (<0>,5584) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5593) fake_defs.h:868: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,5599) fake_defs.h:870: task = malloc(sizeof(*task));
  (<0>,5600) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5602) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5604) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5605) fake_defs.h:872: task->tid = t;
  (<0>,5606) fake_defs.h:872: task->tid = t;
  (<0>,5608) fake_defs.h:872: task->tid = t;
  (<0>,5609) fake_defs.h:873: return task;
  (<0>,5610) fake_defs.h:873: return task;
  (<0>,5612) fake_defs.h:876: }
  (<0>,5614) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5615) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5618)
  (<0>,5619) tree.c:625: return &rsp->node[0];
  (<0>,5623) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5627) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5628) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5629) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5631) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5635)
  (<0>,5636)
  (<0>,5637) fake_sync.h:83: local_irq_save(flags);
  (<0>,5640) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5642) fake_sched.h:43: return __running_cpu;
  (<0>,5646) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5648) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5652) fake_sched.h:43: return __running_cpu;
  (<0>,5656) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5662) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5663) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5670) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5671) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5673) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5674) tree.c:3996: if (kthread_prio) {
  (<0>,5680) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5681) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5682) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5684) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5688)
  (<0>,5689)
  (<0>,5690) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5691) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5694) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5697) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5699) fake_sched.h:43: return __running_cpu;
  (<0>,5703) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5705) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5709) fake_sched.h:43: return __running_cpu;
  (<0>,5713) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5724) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5727) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5728) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5729) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5733) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5734) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5735) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5737) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5741) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5743) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5745) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5754)
  (<0>,5755)
  (<0>,5756)
  (<0>,5757)
  (<0>,5758) fake_defs.h:865: struct task_struct *task = NULL;
  (<0>,5759) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5762)
  (<0>,5763)
  (<0>,5770)
  (<0>,5771)
  (<0>,5778)
  (<0>,5779)
  (<0>,5786)
  (<0>,5787)
  (<0>,5794)
  (<0>,5795) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5808) fake_defs.h:875: return NULL;
  (<0>,5810) fake_defs.h:876: }
  (<0>,5812) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5813) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5816)
  (<0>,5817) tree.c:625: return &rsp->node[0];
  (<0>,5821) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5825) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5826) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5827) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5829) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5833)
  (<0>,5834)
  (<0>,5835) fake_sync.h:83: local_irq_save(flags);
  (<0>,5838) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5840) fake_sched.h:43: return __running_cpu;
  (<0>,5844) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5846) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5850) fake_sched.h:43: return __running_cpu;
  (<0>,5854) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5860) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5861) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5868) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5869) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5871) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5872) tree.c:3996: if (kthread_prio) {
  (<0>,5878) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5879) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5880) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5882) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5886)
  (<0>,5887)
  (<0>,5888) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5889) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5892) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5895) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5897) fake_sched.h:43: return __running_cpu;
  (<0>,5901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5903) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5907) fake_sched.h:43: return __running_cpu;
  (<0>,5911) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5922) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5925) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5926) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5927) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5931) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5932) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5933) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5935) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5949)
  (<0>,5950) litmus.c:51: void *thread_reader(void *arg)
  (<0>,5953)
  (<0>,5954) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5955) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5958) fake_sched.h:43: return __running_cpu;
  (<0>,5962)
  (<0>,5963) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,5966) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,5971) tree.c:890: unsigned long flags;
  (<0>,5974) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5976) fake_sched.h:43: return __running_cpu;
  (<0>,5980) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5982) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5986) fake_sched.h:43: return __running_cpu;
  (<0>,5990) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6002) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6004) fake_sched.h:43: return __running_cpu;
  (<0>,6008) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6009) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6011) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6012) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6013) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6014) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6015) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6016) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6017) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,6021) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6023) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6024) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6025) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6036)
  (<0>,6037) tree.c:830: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,6039) fake_sched.h:43: return __running_cpu;
  (<0>,6043) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6047) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6050) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6051) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6052) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6054) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6055) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6057) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6058) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6059) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6060) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6070) tree_plugin.h:2883: }
  (<0>,6072) tree.c:895: local_irq_restore(flags);
  (<0>,6075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6077) fake_sched.h:43: return __running_cpu;
  (<0>,6081) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6083) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6087) fake_sched.h:43: return __running_cpu;
  (<0>,6091) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6106) litmus.c:57: r_x = x;
  (<0>,6107) litmus.c:57: r_x = x;
  (<0>,6111) fake_sched.h:43: return __running_cpu;
  (<0>,6115) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
  (<0>,6119) fake_sched.h:43: return __running_cpu;
  (<0>,6123) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6128) fake_sched.h:43: return __running_cpu;
  (<0>,6132) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
  (<0>,6143) fake_sched.h:43: return __running_cpu;
  (<0>,6147) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6148) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6150) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6151) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6152) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6154) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6156) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6157) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6158) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6159) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6160) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6161) tree.c:942: if (oldval)
  (<0>,6169) tree_plugin.h:2883: }
  (<0>,6175) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,6184) tree_plugin.h:1669: struct rcu_state *rsp;
  (<0>,6185) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6186) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6190) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6191) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6192) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6194) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6199) fake_sched.h:43: return __running_cpu;
  (<0>,6202) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6204) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6207) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6209) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6211) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6214) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6215) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6216) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6220) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6221) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6222) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6224) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6229) fake_sched.h:43: return __running_cpu;
  (<0>,6232) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6234) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6237) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6239) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6241) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6244) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6245) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6246) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6250) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6251) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6252) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6254) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6259) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,6264) fake_sched.h:43: return __running_cpu;
  (<0>,6269) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,6277) fake_sched.h:43: return __running_cpu;
  (<0>,6283) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
  (<0>,6289) fake_sched.h:43: return __running_cpu;
  (<0>,6296) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
  (<0>,6309) tree.c:3557: struct rcu_state *rsp;
  (<0>,6310) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6311) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6315) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6316) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6317) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6319) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6323) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6325) fake_sched.h:43: return __running_cpu;
  (<0>,6328) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6330) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6352)
  (<0>,6353) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6354) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6356) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6357) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6358) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6360) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6362) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6363) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6364) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6399)
  (<0>,6400) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6401) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6404) tree.c:1469: !rcu_gp_in_progress(rsp))
  (<0>,6417)
  (<0>,6418) tree.c:237: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,6423) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6424) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6425) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6426) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6428) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6430) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6431) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6433) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6436) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6437) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6438) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6439) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6444) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6445) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6446) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6447) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6449) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6451) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6452) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6454) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6457) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6458) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6459) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6467) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
  (<0>,6470) tree_plugin.h:2923: return false;
  (<0>,6473) tree.c:3503: if (rcu_scheduler_fully_active &&
  (<0>,6476) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6478) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6481) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6483) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6487) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,6490)
  (<0>,6491) tree.c:614: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,6493) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6496) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6503) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6504) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6515)
  (<0>,6516) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6517) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6530)
  (<0>,6531) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6536) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6537) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6538) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6539) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6541) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6543) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6544) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6546) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6549) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6550) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6551) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6552) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6557) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6560) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6562) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6564) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6565) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6567) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6570) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6571) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6572) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6578) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,6594)
  (<0>,6595) tree.c:633: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,6598)
  (<0>,6599) tree.c:625: return &rsp->node[0];
  (<0>,6603) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6604) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6609) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6610) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6611) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6612) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6614) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6616) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6617) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6619) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6622) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6623) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6624) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6628) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6629) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6631) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6634) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6635) tree.c:639: return READ_ONCE(*fp);
  (<0>,6639) tree.c:639: return READ_ONCE(*fp);
  (<0>,6640) tree.c:639: return READ_ONCE(*fp);
  (<0>,6641) tree.c:639: return READ_ONCE(*fp);
  (<0>,6642) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6644) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6646) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6647) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6649) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6652) tree.c:639: return READ_ONCE(*fp);
  (<0>,6653) tree.c:639: return READ_ONCE(*fp);
  (<0>,6654) tree.c:639: return READ_ONCE(*fp);
  (<0>,6658) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6661) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6664) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6667) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6668) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6671) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6673) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6676) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6679) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6682) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6683) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6685) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6688) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6692) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6694) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6696) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6699) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6702) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6705) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6706) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6708) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6711) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6715) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6717) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6719) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6722) tree.c:665: return false; /* No grace period needed. */
  (<0>,6724) tree.c:666: }
  (<0>,6727) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6732) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6733) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6734) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6735) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6737) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6739) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6740) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6742) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6745) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6746) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6747) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6748) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6750) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6753) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6758) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6759) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6760) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6761) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6763) fake_defs.h:266: __READ_ONCE_SIZE;
      (<0.1>,1)
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:892: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,55) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,57) fake_sched.h:43: return __running_cpu;
      (<0.1>,61) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,62) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,64) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,67) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,68) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,69) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,70) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,74) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,76) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,77) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,78) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,89)
      (<0.1>,90) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,92) fake_sched.h:43: return __running_cpu;
      (<0.1>,96) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,100) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,103) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,104) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,105) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,107) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,110) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,111) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,112) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,113) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,123) tree_plugin.h:2883: }
      (<0.1>,125) tree.c:895: local_irq_restore(flags);
      (<0.1>,128) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,130) fake_sched.h:43: return __running_cpu;
      (<0.1>,134) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,140) fake_sched.h:43: return __running_cpu;
      (<0.1>,144) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,151) litmus.c:87: x = 1;
      (<0.1>,163) tree.c:3255: ret = num_online_cpus() <= 1;
      (<0.1>,165) tree.c:3257: return ret;
      (<0.1>,172) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,175) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,176) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,177) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,178) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,181) update.c:148: rcu_scheduler_active == RCU_SCHEDULER_INIT;
      (<0.1>,198)
      (<0.1>,199)
      (<0.1>,200)
      (<0.1>,201)
      (<0.1>,202) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,204) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,205) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,208) update.c:352: if (checktiny &&
      (<0.1>,211) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,213) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,218) rcupdate.h:476: }
      (<0.1>,220) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,222) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,227)
      (<0.1>,228) fake_sync.h:272: x->done = 0;
      (<0.1>,230) fake_sync.h:272: x->done = 0;
      (<0.1>,234) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,236) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,238) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,239) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,241) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,247)
      (<0.1>,248)
      (<0.1>,249) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,250) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,281)
      (<0.1>,282)
      (<0.1>,283)
      (<0.1>,284)
      (<0.1>,286)
      (<0.1>,287) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,294) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,295) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,301) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,302) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,303) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,304) tree.c:3147: if (debug_rcu_head_queue(head)) {
      (<0.1>,307) rcu.h:92: return 0;
      (<0.1>,311) tree.c:3153: head->func = func;
      (<0.1>,312) tree.c:3153: head->func = func;
      (<0.1>,315) tree.c:3153: head->func = func;
      (<0.1>,316) tree.c:3154: head->next = NULL;
      (<0.1>,318) tree.c:3154: head->next = NULL;
      (<0.1>,319) tree.c:3162: local_irq_save(flags);
      (<0.1>,322) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,324) fake_sched.h:43: return __running_cpu;
      (<0.1>,328) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,330) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,334) fake_sched.h:43: return __running_cpu;
      (<0.1>,338) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,344) fake_sched.h:43: return __running_cpu;
      (<0.1>,347) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,349) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,351) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,352) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,355) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,363) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,367) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,369) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,371) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,372) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,377) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,378) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,379) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,380) fake_defs.h:237: switch (size) {
      (<0.1>,382) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,384) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,385) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,387) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,390) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,391) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,392) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,393) tree.c:3189: if (lazy)
      (<0.1>,400) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,401) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,404) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,405) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,406) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,408) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,411) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,412) tree.c:3197: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,419) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,420) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,421) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,422) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,430)
      (<0.1>,431)
      (<0.1>,432)
      (<0.1>,433) tree.c:3086: if (!rcu_is_watching())
      (<0.1>,440) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,442) fake_sched.h:43: return __running_cpu;
      (<0.1>,448) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,449) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,450) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,455) tree.c:1060: ret = __rcu_is_watching();
      (<0.1>,457) tree.c:1062: return ret;
      (<0.1>,461) tree.c:3090: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,464) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,466) fake_sched.h:43: return __running_cpu;
      (<0.1>,470) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,480) tree.c:3205: local_irq_restore(flags);
      (<0.1>,483) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,485) fake_sched.h:43: return __running_cpu;
      (<0.1>,489) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,491) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,495) fake_sched.h:43: return __running_cpu;
      (<0.1>,499) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,508) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,510) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,512) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,513) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,516) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,518) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,519) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,522) update.c:365: if (checktiny &&
      (<0.1>,525) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,527) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,532) fake_sync.h:278: might_sleep();
      (<0.1>,536) fake_sched.h:43: return __running_cpu;
      (<0.1>,540) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,543) tree.c:755: local_irq_save(flags);
      (<0.1>,546) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,548) fake_sched.h:43: return __running_cpu;
      (<0.1>,552) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,554) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,558) fake_sched.h:43: return __running_cpu;
      (<0.1>,562) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,574) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,576) fake_sched.h:43: return __running_cpu;
      (<0.1>,580) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,581) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,583) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,584) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,585) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,586) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,587) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,588) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,589) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,593) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,595) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,596) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,597) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,613)
      (<0.1>,615) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,617) fake_sched.h:43: return __running_cpu;
      (<0.1>,621) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,624) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,625) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,626) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,630) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,631) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,632) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,639) fake_sched.h:43: return __running_cpu;
      (<0.1>,642) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,644) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,646) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,647) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,650) tree_plugin.h:2457: }
      (<0.1>,653) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,656) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,657) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,658) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,662) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,663) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,664) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,666) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,671) fake_sched.h:43: return __running_cpu;
      (<0.1>,674) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,676) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,678) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,679) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,682) tree_plugin.h:2457: }
      (<0.1>,685) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,688) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,689) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,690) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,694) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,695) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,696) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,698) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,705) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,708) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,709) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,710) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,712) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,713) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,715) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,716) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,717) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,718) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,732) tree_plugin.h:2879: }
      (<0.1>,734) tree.c:758: local_irq_restore(flags);
      (<0.1>,737) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,739) fake_sched.h:43: return __running_cpu;
      (<0.1>,743) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,745) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,749) fake_sched.h:43: return __running_cpu;
      (<0.1>,753) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,759) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,762) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,767) fake_sync.h:281: while (!x->done)
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:892: local_irq_save(flags);
    (<0.0>,35) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,63) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,65) fake_sched.h:43: return __running_cpu;
    (<0.0>,69) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,70) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,72) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,75) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,76) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,77) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,78) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,82) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,84) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,85) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,86) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,97)
    (<0.0>,98) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,100) fake_sched.h:43: return __running_cpu;
    (<0.0>,104) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,111) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,112) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,113) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,115) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,116) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,118) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,119) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,120) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,121) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,131) tree_plugin.h:2883: }
    (<0.0>,133) tree.c:895: local_irq_restore(flags);
    (<0.0>,136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,138) fake_sched.h:43: return __running_cpu;
    (<0.0>,142) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,144) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,148) fake_sched.h:43: return __running_cpu;
    (<0.0>,152) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,159) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,207)
    (<0.0>,208) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,211) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,214)
    (<0.0>,215) tree.c:625: return &rsp->node[0];
    (<0.0>,219) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,227) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,233) fake_sched.h:43: return __running_cpu;
    (<0.0>,237) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,241) fake_sched.h:43: return __running_cpu;
    (<0.0>,245) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,250) fake_sched.h:43: return __running_cpu;
    (<0.0>,254) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,270) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,272) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,273) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,274) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,276) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,278) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,279) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,280) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,281) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,282) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,283) tree.c:942: if (oldval)
    (<0.0>,291) tree_plugin.h:2883: }
    (<0.0>,297) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,306) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,307) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,308) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,312) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,313) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,314) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,316) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,321) fake_sched.h:43: return __running_cpu;
    (<0.0>,324) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,326) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,329) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,331) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,333) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,336) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,337) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,338) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,342) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,343) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,344) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,346) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,351) fake_sched.h:43: return __running_cpu;
    (<0.0>,354) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,356) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,359) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,361) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,363) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,366) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,367) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,368) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,372) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,373) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,374) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,376) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,381) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,386) fake_sched.h:43: return __running_cpu;
    (<0.0>,391) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,399) fake_sched.h:43: return __running_cpu;
    (<0.0>,405) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,411) fake_sched.h:43: return __running_cpu;
    (<0.0>,418) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,431) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,432) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,433) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,437) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,438) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,439) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,441) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,445) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,447) fake_sched.h:43: return __running_cpu;
    (<0.0>,450) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,452) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,474)
    (<0.0>,475)
    (<0.0>,476) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,478) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,479) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,480) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,482) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,484) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,485) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,486) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,521)
    (<0.0>,522)
    (<0.0>,523) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,526) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,539)
    (<0.0>,540) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,545) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,546) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,547) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,548) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,550) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,552) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,553) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,555) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,560) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,561) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,566) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,567) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,569) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,571) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,573) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,574) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,576) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,579) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,580) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,581) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,589) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,592) tree_plugin.h:2923: return false;
    (<0.0>,595) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,598) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,600) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,603) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,605) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,609) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,612)
    (<0.0>,613) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,615) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,618) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,625) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,626) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,637)
    (<0.0>,638)
    (<0.0>,639) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,652)
    (<0.0>,653) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,658) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,659) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,660) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,661) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,663) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,665) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,666) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,668) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,671) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,672) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,673) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,674) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,679) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,680) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,681) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,682) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,684) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,686) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,692) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,693) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,694) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,700) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,716)
    (<0.0>,717) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,720)
    (<0.0>,721) tree.c:625: return &rsp->node[0];
    (<0.0>,725) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,726) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,731) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,732) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,733) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,734) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,736) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,739) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,744) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,745) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,746) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,750) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,751) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,753) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,756) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,757) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,761) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,762) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,763) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,774) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,775) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,776) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,780) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,783) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,786) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,789) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,790) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,793) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,795) tree.c:666: }
    (<0.0>,798) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,800) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,802) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,803) tree.c:3523: return 1;
    (<0.0>,805) tree.c:3548: }
    (<0.0>,809) tree.c:3561: return 1;
    (<0.0>,811) tree.c:3563: }
    (<0.0>,818) fake_sched.h:43: return __running_cpu;
    (<0.0>,822) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,826) tree.c:2891: if (user)
    (<0.0>,834) fake_sched.h:43: return __running_cpu;
    (<0.0>,838) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,840) fake_sched.h:43: return __running_cpu;
    (<0.0>,844) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,850) fake_sched.h:43: return __running_cpu;
    (<0.0>,854) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,864) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,867) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,868) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,869) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,873) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,874) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,875) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,877) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,881) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,893) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,895) fake_sched.h:43: return __running_cpu;
    (<0.0>,898) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,900) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,902) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,903) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,905) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,912) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,913) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,915) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,921) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,922) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,923) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,924) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,925) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,929)
    (<0.0>,930)
    (<0.0>,931) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,932) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,957)
    (<0.0>,958)
    (<0.0>,959) tree.c:1905: local_irq_save(flags);
    (<0.0>,962) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,964) fake_sched.h:43: return __running_cpu;
    (<0.0>,968) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,970) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,974) fake_sched.h:43: return __running_cpu;
    (<0.0>,978) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,983) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,985) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,986) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,987) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,989) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,990) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,995) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,996) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,997) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,998) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1000) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1002) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1003) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1005) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1008) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1009) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1010) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1013) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1015) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1016) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1021) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1022) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1023) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1024) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1026) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1028) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1029) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1031) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1034) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1035) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1036) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1039) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1043) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1044) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1045) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1046) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1048) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1050) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1054) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1057) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1058) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1066) tree.c:1911: local_irq_restore(flags);
    (<0.0>,1069) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1071) fake_sched.h:43: return __running_cpu;
    (<0.0>,1075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1077) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1081) fake_sched.h:43: return __running_cpu;
    (<0.0>,1085) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1092) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1094) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1099) tree.c:3016: local_irq_save(flags);
    (<0.0>,1102) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1104) fake_sched.h:43: return __running_cpu;
    (<0.0>,1108) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1110) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1114) fake_sched.h:43: return __running_cpu;
    (<0.0>,1118) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1123) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1124) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1135)
    (<0.0>,1136)
    (<0.0>,1137) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,1150)
    (<0.0>,1151) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1156) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1157) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1158) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1161) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1163) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1164) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1166) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1169) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1170) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1171) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1172) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1177) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1178) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1179) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1180) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1182) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1184) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1185) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1187) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1190) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1191) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1192) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1198) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,1214)
    (<0.0>,1215) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1218)
    (<0.0>,1219) tree.c:625: return &rsp->node[0];
    (<0.0>,1223) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1224) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1229) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1230) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1231) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1232) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1234) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1236) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1237) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1239) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1242) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1243) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1244) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1248) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1249) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1251) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1254) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1255) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1259) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1260) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1261) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1262) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1264) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1266) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1267) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1269) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1272) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1273) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1274) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1278) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1281) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1284) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1287) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1288) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1291) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,1293) tree.c:666: }
    (<0.0>,1296) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,1299)
    (<0.0>,1300) tree.c:625: return &rsp->node[0];
    (<0.0>,1306)
    (<0.0>,1307) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,1311) fake_sync.h:115: preempt_disable();
    (<0.0>,1313) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1314) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1321) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,1327) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1329) fake_sched.h:43: return __running_cpu;
    (<0.0>,1332) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1334) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1336) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1337) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1340)
    (<0.0>,1341) tree.c:625: return &rsp->node[0];
    (<0.0>,1345) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1346) tree.c:2334: bool ret = false;
    (<0.0>,1347) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1348) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1349) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1357)
    (<0.0>,1358)
    (<0.0>,1359)
    (<0.0>,1360) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1363) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1366) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1369) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1370) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1373) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1375) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1378) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1380) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1381) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1383) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1386) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1391) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1393) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1394) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1397) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1399) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1402) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1404) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1407) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1408) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1411) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1414) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1416) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1419) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1420) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1422) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1425) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1426) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1428) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1431) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1432) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1434) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1437) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1439) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1441) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1442) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1444) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1446) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1449) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1451) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1454) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1455) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1458) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1461) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1463) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1466) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1467) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1469) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1472) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1473) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1475) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1478) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1479) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1481) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1484) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1486) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1488) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1489) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1491) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1493) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1496) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1497) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1498) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1507)
    (<0.0>,1508)
    (<0.0>,1509)
    (<0.0>,1510) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1513) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1516) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1519) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1520) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1523) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1524) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1529)
    (<0.0>,1530)
    (<0.0>,1531) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1534)
    (<0.0>,1535) tree.c:625: return &rsp->node[0];
    (<0.0>,1539) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1542) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1544) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1545) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1547) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1550) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1552) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1554) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1556) tree.c:1585: }
    (<0.0>,1558) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1559) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1561) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1564) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1566) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1569) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1570) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1573) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1576) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1580) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1582) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1584) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1587) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1589) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1592) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1593) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1596) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1599) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1603) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1605) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1607) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1610) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1612) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1616) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1619) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1622) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1623) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1625) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1628) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1629) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1630) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1632) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1635) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1637) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1639) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1641) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1644) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1647) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1648) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1650) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1653) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1654) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1655) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1657) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1660) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1662) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1664) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1666) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1669) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1672) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1673) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1675) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1678) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1679) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1680) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1682) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1685) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1687) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1689) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1691) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1694) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1695) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1714)
    (<0.0>,1715)
    (<0.0>,1716)
    (<0.0>,1717) tree.c:1613: bool ret = false;
    (<0.0>,1718) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1720) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1723)
    (<0.0>,1724) tree.c:625: return &rsp->node[0];
    (<0.0>,1728) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1729) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1731) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1732) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1737)
    (<0.0>,1738)
    (<0.0>,1739) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1742)
    (<0.0>,1743) tree.c:625: return &rsp->node[0];
    (<0.0>,1747) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1750) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1752) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1753) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1755) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1758) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1760) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1762) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1764) tree.c:1585: }
    (<0.0>,1766) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1767) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1768) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1769) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1775)
    (<0.0>,1776)
    (<0.0>,1777)
    (<0.0>,1778) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1782) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1784) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1787) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1790) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1792) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1793) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1795) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1798) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1803) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1804) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1805) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1806) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1808) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1810) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1811) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1813) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1816) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1817) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1818) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1819) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1824) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1825) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1826) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1834) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1837) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1838) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1839) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1842) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1843) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1846) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1848) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1849) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1854)
    (<0.0>,1855)
    (<0.0>,1856) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1859)
    (<0.0>,1860) tree.c:625: return &rsp->node[0];
    (<0.0>,1864) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1867) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1869) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1870) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1872) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1875) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1877) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1879) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1881) tree.c:1585: }
    (<0.0>,1883) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1884) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1886) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1889) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1890) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1892) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1895) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1899) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1900) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1902) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1905) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1908) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1910) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1912) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1915) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1916) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1918) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1921) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1926) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1928) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1930) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1933) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1934) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1936) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1939) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1944) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1946) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1948) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1951) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1953) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1956) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1959) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1961) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1964) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1966) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1967) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1969) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1970) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1972) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1975) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1976) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1977) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1983)
    (<0.0>,1984)
    (<0.0>,1985)
    (<0.0>,1986) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1990) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1992) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1993) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1994) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2005)
    (<0.0>,2006)
    (<0.0>,2007)
    (<0.0>,2008) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2010) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2013) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2014) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2025)
    (<0.0>,2026)
    (<0.0>,2027) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2040)
    (<0.0>,2041) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2046) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2047) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2048) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2053) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2054) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2056) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2059) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2060) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2061) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2062) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2067) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2068) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2069) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2070) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2072) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2074) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2075) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2077) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2080) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2081) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2082) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2088) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2104)
    (<0.0>,2105) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2108)
    (<0.0>,2109) tree.c:625: return &rsp->node[0];
    (<0.0>,2113) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2114) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2119) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2120) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2121) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2122) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2124) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2126) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2127) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2129) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2132) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2133) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2134) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2138) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2139) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2141) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2144) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2145) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2149) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2150) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2151) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2152) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2154) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2156) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2157) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2162) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2163) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2164) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2168) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2170) tree.c:666: }
    (<0.0>,2175) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2180) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2181) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2182) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2183) fake_defs.h:237: switch (size) {
    (<0.0>,2185) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2187) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2188) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2190) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2193) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2194) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2195) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2198) tree.c:2318: return true;
    (<0.0>,2200) tree.c:2319: }
    (<0.0>,2203) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2206) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2207) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2211) tree.c:1689: if (c_out != NULL)
    (<0.0>,2214) tree.c:1691: return ret;
    (<0.0>,2218) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,2219) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2222) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2223) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2229) tree.c:1798: return ret;
    (<0.0>,2231) tree.c:1798: return ret;
    (<0.0>,2233) tree.c:1799: }
    (<0.0>,2235) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,2237) tree.c:1843: }
    (<0.0>,2241) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,2242) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2243) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2244) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2255)
    (<0.0>,2256)
    (<0.0>,2257)
    (<0.0>,2258) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2260) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2263) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2264) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2275)
    (<0.0>,2276)
    (<0.0>,2277) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2290)
    (<0.0>,2291) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2296) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2297) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2298) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2299) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2301) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2303) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2304) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2306) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2309) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2310) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2311) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2312) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2317) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2318) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2320) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2322) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2324) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2325) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2327) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2330) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2331) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2332) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2338) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2354)
    (<0.0>,2355) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2358)
    (<0.0>,2359) tree.c:625: return &rsp->node[0];
    (<0.0>,2363) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2364) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2369) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2370) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2371) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2372) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2374) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2376) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2377) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2379) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2382) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2383) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2384) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2388) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2389) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2391) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2394) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2395) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2399) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2400) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2401) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2407) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2409) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2412) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2413) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2414) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2418) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2420) tree.c:666: }
    (<0.0>,2425) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2430) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2431) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2432) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2433) fake_defs.h:237: switch (size) {
    (<0.0>,2435) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2437) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2438) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2440) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2443) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2444) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2445) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2448) tree.c:2318: return true;
    (<0.0>,2450) tree.c:2319: }
    (<0.0>,2454) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2455) tree.c:2346: return ret;
    (<0.0>,2459) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,2463) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2464) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2465) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2468)
    (<0.0>,2469) tree.c:625: return &rsp->node[0];
    (<0.0>,2474) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2478)
    (<0.0>,2479)
    (<0.0>,2480) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2481) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2484) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,2487) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2489) fake_sched.h:43: return __running_cpu;
    (<0.0>,2493) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2495) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2499) fake_sched.h:43: return __running_cpu;
    (<0.0>,2503) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2511) tree.c:3021: if (needwake)
    (<0.0>,2514) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,2522)
    (<0.0>,2523) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2524) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2526) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2533) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2536)
    (<0.0>,2537) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2539) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2542) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2549) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2552) tree_plugin.h:2457: }
    (<0.0>,2556) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2559) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2560) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2561) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2565) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2566) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2567) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2569) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2573) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,2585) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2587) fake_sched.h:43: return __running_cpu;
    (<0.0>,2590) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2592) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2594) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2595) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2597) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2604) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2605) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2607) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2613) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2614) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2615) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2616) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2617) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2621)
    (<0.0>,2622)
    (<0.0>,2623) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2624) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2649)
    (<0.0>,2650)
    (<0.0>,2651) tree.c:1905: local_irq_save(flags);
    (<0.0>,2654) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2656) fake_sched.h:43: return __running_cpu;
    (<0.0>,2660) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2662) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2666) fake_sched.h:43: return __running_cpu;
    (<0.0>,2670) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2675) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2677) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2678) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2679) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2681) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2682) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2687) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2688) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2689) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2690) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2695) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2697) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2700) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2701) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2702) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2705) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2707) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2708) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2713) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2714) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2715) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2716) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2718) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2720) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2721) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2723) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2726) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2727) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2728) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2731) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2735) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2736) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2737) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2740) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2742) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2743) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2746) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2749) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2750) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2758) tree.c:1911: local_irq_restore(flags);
    (<0.0>,2761) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2763) fake_sched.h:43: return __running_cpu;
    (<0.0>,2767) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2769) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2773) fake_sched.h:43: return __running_cpu;
    (<0.0>,2777) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2784) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2786) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2791) tree.c:3016: local_irq_save(flags);
    (<0.0>,2794) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2802) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2806) fake_sched.h:43: return __running_cpu;
    (<0.0>,2810) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2815) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2816) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2827)
    (<0.0>,2828)
    (<0.0>,2829) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2842)
    (<0.0>,2843) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2849) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2850) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2855) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2858) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2863) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2864) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2870) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2871) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2872) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2877) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2879) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2882) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2883) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2890) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2906)
    (<0.0>,2907) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2910)
    (<0.0>,2911) tree.c:625: return &rsp->node[0];
    (<0.0>,2915) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2916) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2921) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2922) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2923) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2924) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2926) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2929) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2934) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2935) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2936) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2940) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2941) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2943) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2946) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2947) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2951) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2952) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2953) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2954) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2956) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2959) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2964) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2965) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2966) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2970) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2973) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2976) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2979) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2980) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2983) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2985) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2988) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2991) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2994) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2995) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2997) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3000) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3004) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3006) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3008) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3011) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3014) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3017) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3018) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3020) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3023) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3027) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3029) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3031) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3034) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,3036) tree.c:666: }
    (<0.0>,3039) tree.c:3024: local_irq_restore(flags);
    (<0.0>,3042) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3044) fake_sched.h:43: return __running_cpu;
    (<0.0>,3048) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3050) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3054) fake_sched.h:43: return __running_cpu;
    (<0.0>,3058) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3064) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,3067)
    (<0.0>,3068) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3070) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3073) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3080) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3083) tree_plugin.h:2457: }
    (<0.0>,3087) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3090) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3091) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3092) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3096) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3097) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3098) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3100) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3108) fake_sched.h:43: return __running_cpu;
    (<0.0>,3112) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,3122) fake_sched.h:43: return __running_cpu;
    (<0.0>,3126) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3127) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3129) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3130) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3131) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3133) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3135) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3136) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3137) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3138) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3139) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3140) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3142) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3150) tree_plugin.h:2879: }
    (<0.0>,3156) fake_sched.h:43: return __running_cpu;
    (<0.0>,3160) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,3163) tree.c:755: local_irq_save(flags);
    (<0.0>,3166) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3168) fake_sched.h:43: return __running_cpu;
    (<0.0>,3172) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3174) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3178) fake_sched.h:43: return __running_cpu;
    (<0.0>,3182) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3194) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3196) fake_sched.h:43: return __running_cpu;
    (<0.0>,3200) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3201) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3203) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3204) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3205) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3206) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3207) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3208) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3209) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3213) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3215) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3216) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3217) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3233)
    (<0.0>,3235) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3237) fake_sched.h:43: return __running_cpu;
    (<0.0>,3241) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3244) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3245) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3246) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3250) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3251) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3252) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3254) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3259) fake_sched.h:43: return __running_cpu;
    (<0.0>,3262) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3264) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3266) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3267) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3270) tree_plugin.h:2457: }
    (<0.0>,3273) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3276) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3277) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3278) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3282) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3283) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3284) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3286) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3291) fake_sched.h:43: return __running_cpu;
    (<0.0>,3294) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3296) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3298) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3299) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3302) tree_plugin.h:2457: }
    (<0.0>,3305) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3308) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3309) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3310) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3314) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3315) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3316) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3318) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3325) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3328) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3329) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3330) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3332) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3333) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3335) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3336) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3337) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3338) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3352) tree_plugin.h:2879: }
    (<0.0>,3354) tree.c:758: local_irq_restore(flags);
    (<0.0>,3357) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3359) fake_sched.h:43: return __running_cpu;
    (<0.0>,3363) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3365) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3369) fake_sched.h:43: return __running_cpu;
    (<0.0>,3373) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3379) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3382) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3387) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3392) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3393) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3394) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3395) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3397) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3399) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3400) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3405) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3406) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3407) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3414) fake_sched.h:43: return __running_cpu;
    (<0.0>,3418)
    (<0.0>,3419) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3422) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3427) tree.c:892: local_irq_save(flags);
    (<0.0>,3430) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3432) fake_sched.h:43: return __running_cpu;
    (<0.0>,3436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3442) fake_sched.h:43: return __running_cpu;
    (<0.0>,3446) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3458) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3460) fake_sched.h:43: return __running_cpu;
    (<0.0>,3464) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3465) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3467) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3468) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3469) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3470) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3471) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3472) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3473) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3477) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3479) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3480) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3481) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3492)
    (<0.0>,3493) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3495) fake_sched.h:43: return __running_cpu;
    (<0.0>,3499) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3503) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3506) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3507) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3508) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3510) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3511) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3513) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3514) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3515) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3516) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3526) tree_plugin.h:2883: }
    (<0.0>,3528) tree.c:895: local_irq_restore(flags);
    (<0.0>,3531) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3533) fake_sched.h:43: return __running_cpu;
    (<0.0>,3537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3539) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3543) fake_sched.h:43: return __running_cpu;
    (<0.0>,3547) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3554) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3556) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3557) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,3602)
    (<0.0>,3603) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3606)
    (<0.0>,3607) tree.c:625: return &rsp->node[0];
    (<0.0>,3611) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3613) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3614) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3615) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3620) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3621) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3622) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3623) fake_defs.h:237: switch (size) {
    (<0.0>,3625) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3627) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3628) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3630) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3633) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3634) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3635) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3636) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,3639)
    (<0.0>,3640) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,3644) fake_sync.h:99: local_irq_disable();
    (<0.0>,3647) fake_sched.h:43: return __running_cpu;
    (<0.0>,3651) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,3655) fake_sched.h:43: return __running_cpu;
    (<0.0>,3659) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3664) fake_sched.h:43: return __running_cpu;
    (<0.0>,3668) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3671) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3672) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3679) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3684) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3685) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3686) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3691) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3697) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3698) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3699) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3710)
    (<0.0>,3716)
    (<0.0>,3721) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3726) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3727) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3728) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3729) fake_defs.h:237: switch (size) {
    (<0.0>,3731) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3733) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3734) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3736) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3739) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3740) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3741) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3742) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3755)
    (<0.0>,3756) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3761) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3762) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3763) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3774) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3775) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3776) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3777) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3782) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3783) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3784) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3785) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3787) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3789) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3790) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3792) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3795) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3796) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3797) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3805) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3806) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3819)
    (<0.0>,3820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3825) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3826) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3827) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3830) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3833) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3835) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3838) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3839) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3840) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3841) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3846) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3847) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3849) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3854) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3859) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3860) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3868) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3869) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3870) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3873) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,3888)
    (<0.0>,3889) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3890) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3891) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3892) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3894) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3915) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3916) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3917) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3918) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3920) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3922) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3923) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3925) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3928) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3929) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3930) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3931) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3932) update.c:466: if (till_stall_check < 3) {
    (<0.0>,3935) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,3939) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,3944) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,3946) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3947) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3949) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3950) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3955) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3956) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3957) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3958) fake_defs.h:237: switch (size) {
    (<0.0>,3960) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3962) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3963) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3965) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3968) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3969) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3970) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3971) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3972) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3975) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3977) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3978) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3983) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3984) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3985) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3986) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3988) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3990) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3991) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3993) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3996) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3997) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3998) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3999) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4001) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4005) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4007) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4009) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4010) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4012) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4013) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4014) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4018) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4021)
    (<0.0>,4022) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4026)
    (<0.0>,4027) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4028) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4033) fake_sched.h:43: return __running_cpu;
    (<0.0>,4037) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4039) fake_sched.h:43: return __running_cpu;
    (<0.0>,4043) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4050) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4053) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4056) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4057) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4059) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4060) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4062) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4067) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4068) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4072)
    (<0.0>,4073)
    (<0.0>,4074) tree.c:1922: if (delay > 0 &&
    (<0.0>,4078) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4081)
    (<0.0>,4082) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4086) fake_sync.h:99: local_irq_disable();
    (<0.0>,4089) fake_sched.h:43: return __running_cpu;
    (<0.0>,4093) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4097) fake_sched.h:43: return __running_cpu;
    (<0.0>,4101) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4106) fake_sched.h:43: return __running_cpu;
    (<0.0>,4110) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4113) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4114) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4121) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4123) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4124) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4126) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4129) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4131) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4132) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4133) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4135) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4136) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4138) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4139) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4143) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4145) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4151) tree.c:1983: if (!oldmask) /* First online CPU for this rcu_node. */
    (<0.0>,4154) tree.c:1984: rcu_init_new_rnp(rnp);
    (<0.0>,4159)
    (<0.0>,4160) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4161) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4163) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4165) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4166) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4167) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4169) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4170) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4171) tree.c:3752: if (rnp == NULL)
    (<0.0>,4177) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4179) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4182) tree.c:2007: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4185)
    (<0.0>,4186) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4190)
    (<0.0>,4191) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4192) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4197) fake_sched.h:43: return __running_cpu;
    (<0.0>,4201) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4203) fake_sched.h:43: return __running_cpu;
    (<0.0>,4207) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4215) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4217) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4219) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4220) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4222) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4227) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4230) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4232) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4233) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4235) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4240) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4241) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4245)
    (<0.0>,4246)
    (<0.0>,4247) tree.c:1922: if (delay > 0 &&
    (<0.0>,4251) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4254)
    (<0.0>,4255) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4259) fake_sync.h:99: local_irq_disable();
    (<0.0>,4262) fake_sched.h:43: return __running_cpu;
    (<0.0>,4266) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4270) fake_sched.h:43: return __running_cpu;
    (<0.0>,4274) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4279) fake_sched.h:43: return __running_cpu;
    (<0.0>,4283) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4286) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4287) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4295) fake_sched.h:43: return __running_cpu;
    (<0.0>,4298) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4300) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4302) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4303) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,4309)
    (<0.0>,4310) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4312) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4317) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4318) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4320) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4324) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4325) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4326) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4328) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4330) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4331) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4333) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4335) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4337) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4338) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4339) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4344) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4345) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4346) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4347) fake_defs.h:237: switch (size) {
    (<0.0>,4349) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4351) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4352) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4354) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4357) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4358) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4359) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4360) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4362) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4363) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4365) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4370) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4371) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4373) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4374) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4376) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4380) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4381) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4382) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4385) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4386) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4388) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4391) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4392) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4393) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4415)
    (<0.0>,4416)
    (<0.0>,4417)
    (<0.0>,4418) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4420) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4421) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4423) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4426) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4430) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4431) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4432) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4433) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4435) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4436) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4437) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4438) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4441) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4444) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4445) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4453) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4454) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4455) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4464)
    (<0.0>,4465)
    (<0.0>,4466)
    (<0.0>,4467) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4470) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4473) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4476) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4477) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4480) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4481) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4486)
    (<0.0>,4487)
    (<0.0>,4488) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4491)
    (<0.0>,4492) tree.c:625: return &rsp->node[0];
    (<0.0>,4496) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4499) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4501) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4502) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4504) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4507) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4509) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4511) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4513) tree.c:1585: }
    (<0.0>,4515) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4516) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4518) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4521) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4523) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4526) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4527) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4530) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4533) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4537) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4539) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4541) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4544) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4546) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4549) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4550) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4553) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4556) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4559) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4561) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4564) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4565) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4570) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4572) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4576) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4579) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4582) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4583) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4585) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4588) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4589) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4590) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4592) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4595) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4597) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4599) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4601) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4604) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4607) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4608) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4610) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4613) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4614) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4615) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4617) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4620) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4622) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4624) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4626) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4629) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4630) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4649)
    (<0.0>,4650)
    (<0.0>,4651)
    (<0.0>,4652) tree.c:1613: bool ret = false;
    (<0.0>,4653) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4655) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4658)
    (<0.0>,4659) tree.c:625: return &rsp->node[0];
    (<0.0>,4663) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4664) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4666) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4667) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4672)
    (<0.0>,4673)
    (<0.0>,4674) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4677)
    (<0.0>,4678) tree.c:625: return &rsp->node[0];
    (<0.0>,4682) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4685) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4687) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4688) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4690) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4693) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4695) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4697) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4699) tree.c:1585: }
    (<0.0>,4701) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4702) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4703) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4704) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4710)
    (<0.0>,4711)
    (<0.0>,4712)
    (<0.0>,4713) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4717) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4719) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4722) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4725) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4727) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4728) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4730) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4733) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4735) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4738) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4740) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4741) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4742) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4743) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4749)
    (<0.0>,4750)
    (<0.0>,4751)
    (<0.0>,4752) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4757) tree.c:1689: if (c_out != NULL)
    (<0.0>,4760) tree.c:1691: return ret;
    (<0.0>,4764) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4765) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4768) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4769) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4775) tree.c:1798: return ret;
    (<0.0>,4777) tree.c:1798: return ret;
    (<0.0>,4779) tree.c:1799: }
    (<0.0>,4782) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4784) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4786) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4787) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4789) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4792) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4794) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4795) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4797) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4800) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4802) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4803) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4805) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4811) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4812) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4815) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4819) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4821) fake_sched.h:43: return __running_cpu;
    (<0.0>,4825) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4826) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4828) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4829) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4831) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4834) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4835) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,4837) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,4839) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,4841) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,4843) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,4844) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,4847)
    (<0.0>,4848) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4850) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4851) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4853) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4863)
    (<0.0>,4868) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4872) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4873) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4874) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4875) fake_defs.h:237: switch (size) {
    (<0.0>,4877) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4878) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4879) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4880) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4883) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4886) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4887) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4890) tree.c:1896: return ret;
    (<0.0>,4894) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,4897) tree_plugin.h:1231: }
    (<0.0>,4901) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4904)
    (<0.0>,4905) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4909)
    (<0.0>,4910) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4911) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4916) fake_sched.h:43: return __running_cpu;
    (<0.0>,4920) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4922) fake_sched.h:43: return __running_cpu;
    (<0.0>,4926) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4941) fake_sched.h:43: return __running_cpu;
    (<0.0>,4947) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,4953) fake_sched.h:43: return __running_cpu;
    (<0.0>,4960) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,4962) fake_sched.h:43: return __running_cpu;
    (<0.0>,4969) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
    (<0.0>,4977) fake_sched.h:43: return __running_cpu;
    (<0.0>,4981) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,4994) fake_sched.h:43: return __running_cpu;
    (<0.0>,4998) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,5001) tree.c:755: local_irq_save(flags);
    (<0.0>,5004) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5006) fake_sched.h:43: return __running_cpu;
    (<0.0>,5010) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5012) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5016) fake_sched.h:43: return __running_cpu;
    (<0.0>,5020) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5032) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5034) fake_sched.h:43: return __running_cpu;
    (<0.0>,5038) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5039) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5041) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5042) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5043) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5044) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5045) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5046) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5047) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5051) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5053) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5054) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5055) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5071)
    (<0.0>,5073) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5075) fake_sched.h:43: return __running_cpu;
    (<0.0>,5079) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5082) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5083) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5084) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5088) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5089) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5090) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5092) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5097) fake_sched.h:43: return __running_cpu;
    (<0.0>,5100) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5102) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5104) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5105) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5108) tree_plugin.h:2457: }
    (<0.0>,5111) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5114) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5115) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5116) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5120) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5121) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5122) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5124) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5129) fake_sched.h:43: return __running_cpu;
    (<0.0>,5132) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5134) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5136) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5137) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5140) tree_plugin.h:2457: }
    (<0.0>,5143) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5146) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5147) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5148) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5152) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5153) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5154) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5156) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5163) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5166) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5167) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5168) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5170) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5171) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5173) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5174) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5175) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5176) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5190) tree_plugin.h:2879: }
    (<0.0>,5192) tree.c:758: local_irq_restore(flags);
    (<0.0>,5195) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5197) fake_sched.h:43: return __running_cpu;
    (<0.0>,5201) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5203) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5207) fake_sched.h:43: return __running_cpu;
    (<0.0>,5211) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5217) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5220) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5225) fake_sched.h:43: return __running_cpu;
    (<0.0>,5229)
    (<0.0>,5230) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5233) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5238) tree.c:892: local_irq_save(flags);
    (<0.0>,5241) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5243) fake_sched.h:43: return __running_cpu;
    (<0.0>,5247) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5249) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5253) fake_sched.h:43: return __running_cpu;
    (<0.0>,5257) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5269) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5271) fake_sched.h:43: return __running_cpu;
    (<0.0>,5275) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5276) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5278) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5279) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5280) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5281) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5282) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5283) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5284) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,5288) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5290) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5291) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5292) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5303)
    (<0.0>,5304) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5306) fake_sched.h:43: return __running_cpu;
    (<0.0>,5310) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5314) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5317) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5318) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5319) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5321) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5322) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5324) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5325) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5326) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5327) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5337) tree_plugin.h:2883: }
    (<0.0>,5339) tree.c:895: local_irq_restore(flags);
    (<0.0>,5342) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5344) fake_sched.h:43: return __running_cpu;
    (<0.0>,5348) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5350) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5354) fake_sched.h:43: return __running_cpu;
    (<0.0>,5358) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5372) fake_sched.h:43: return __running_cpu;
    (<0.0>,5376) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,5385) fake_sched.h:43: return __running_cpu;
    (<0.0>,5392) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,5401) fake_sched.h:43: return __running_cpu;
    (<0.0>,5405) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5407) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5413) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5414) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5415) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5420) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5421) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5422) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5423) fake_defs.h:237: switch (size) {
    (<0.0>,5425) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5427) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5428) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5430) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5433) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5434) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5435) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5437) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5439) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5441) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5442) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5444) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5449) tree.c:2046: return true;
    (<0.0>,5451) tree.c:2047: }
    (<0.0>,5455) tree.c:2214: first_gp_fqs = true;
    (<0.0>,5456) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5457) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5458) tree.c:2216: if (j > HZ) {
    (<0.0>,5461) tree.c:2220: ret = 0;
    (<0.0>,5463) tree.c:2222: if (!ret) {
    (<0.0>,5466) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5467) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5469) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5471) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5473) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5474) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5477) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5478) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5483) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5484) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5485) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5486) fake_defs.h:237: switch (size) {
    (<0.0>,5488) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5490) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5491) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5493) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5496) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5497) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5498) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5502) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5504) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5508) fake_sched.h:43: return __running_cpu;
    (<0.0>,5512) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5516) fake_sched.h:43: return __running_cpu;
    (<0.0>,5520) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5525) fake_sched.h:43: return __running_cpu;
    (<0.0>,5529) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5540) fake_sched.h:43: return __running_cpu;
    (<0.0>,5544) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5545) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5547) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5548) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5549) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5551) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5553) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5554) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5555) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5556) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5557) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5558) tree.c:942: if (oldval)
    (<0.0>,5566) tree_plugin.h:2883: }
    (<0.0>,5572) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,5581) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5582) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5583) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5587) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5588) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5589) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5591) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5596) fake_sched.h:43: return __running_cpu;
    (<0.0>,5599) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5601) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5604) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5606) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5608) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5611) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5612) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5613) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5617) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5618) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5619) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5621) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5626) fake_sched.h:43: return __running_cpu;
    (<0.0>,5629) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5631) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5634) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5636) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5638) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5641) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5642) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5643) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5647) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5648) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5649) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5651) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5656) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,5661) fake_sched.h:43: return __running_cpu;
    (<0.0>,5666) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,5674) fake_sched.h:43: return __running_cpu;
    (<0.0>,5680) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,5694) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5695) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5696) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5700) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5701) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5702) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5704) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5708) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5710) fake_sched.h:43: return __running_cpu;
    (<0.0>,5713) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5715) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5737)
    (<0.0>,5738)
    (<0.0>,5739) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5741) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5742) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5743) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5745) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5747) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5748) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5749) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5784)
    (<0.0>,5785)
    (<0.0>,5786) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,5789) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,5802)
    (<0.0>,5803) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5808) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5809) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5810) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5811) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5813) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5815) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5816) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5818) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5821) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5822) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5823) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5824) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5829) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5830) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5831) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5834) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5836) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5837) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5839) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5842) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5843) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5844) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5850) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,5875)
    (<0.0>,5876) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,5881) tree.c:1472: j = jiffies;
    (<0.0>,5882) tree.c:1472: j = jiffies;
    (<0.0>,5883) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5888) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5889) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5890) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5891) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5893) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5895) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5896) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5898) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5901) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5902) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5903) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5904) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5906) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5911) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5912) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5913) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5914) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5916) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5918) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5919) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5921) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5924) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5925) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5926) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5927) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5929) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5934) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5935) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5936) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5937) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5939) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5941) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5944) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5947) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5948) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5949) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5950) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5952) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5957) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5958) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5959) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5960) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5962) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5964) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5965) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5967) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5970) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5971) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5972) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5973) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5974) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5975) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5979) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5980) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5986) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,5989) tree_plugin.h:2923: return false;
    (<0.0>,5992) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,5995) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5997) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,6000) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,6004) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,6008) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6010) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6013) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,6017) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,6020) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6022) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6024) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6025) tree.c:3511: return 1;
    (<0.0>,6027) tree.c:3548: }
    (<0.0>,6031) tree.c:3561: return 1;
    (<0.0>,6033) tree.c:3563: }
    (<0.0>,6040) fake_sched.h:43: return __running_cpu;
    (<0.0>,6044) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,6048) tree.c:2891: if (user)
    (<0.0>,6056) fake_sched.h:43: return __running_cpu;
    (<0.0>,6060) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6062) fake_sched.h:43: return __running_cpu;
    (<0.0>,6066) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6072) fake_sched.h:43: return __running_cpu;
    (<0.0>,6076) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,6086) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,6089) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6090) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6091) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6095) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6096) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6097) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6099) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6103) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,6115) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6117) fake_sched.h:43: return __running_cpu;
    (<0.0>,6120) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6122) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6124) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6125) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6127) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6134) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6135) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6137) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6143) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6144) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6145) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6146) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6147) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6151)
    (<0.0>,6152)
    (<0.0>,6153) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6154) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6179)
    (<0.0>,6180)
    (<0.0>,6181) tree.c:1905: local_irq_save(flags);
    (<0.0>,6184) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6186) fake_sched.h:43: return __running_cpu;
    (<0.0>,6190) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6192) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6196) fake_sched.h:43: return __running_cpu;
    (<0.0>,6200) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6205) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6207) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6208) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6209) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6211) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6212) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6217) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6218) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6219) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6220) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6222) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6224) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6225) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6227) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6230) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6231) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6232) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6235) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6237) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6238) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6243) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6244) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6245) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6246) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6248) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6250) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6251) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6253) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6256) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6257) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6258) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6261) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6265) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6266) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6267) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6268) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6270) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6271) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6272) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6273) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6276) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6279) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6280) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6288) tree.c:1911: local_irq_restore(flags);
    (<0.0>,6291) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6293) fake_sched.h:43: return __running_cpu;
    (<0.0>,6297) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6299) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6303) fake_sched.h:43: return __running_cpu;
    (<0.0>,6307) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6314) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6316) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6319) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,6323) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,6327) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6329) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6330) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6331) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6349)
    (<0.0>,6350)
    (<0.0>,6351)
    (<0.0>,6352) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6354) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6355) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6359) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6360) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6361) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6363) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6367)
    (<0.0>,6368)
    (<0.0>,6369) fake_sync.h:83: local_irq_save(flags);
    (<0.0>,6372) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6374) fake_sched.h:43: return __running_cpu;
    (<0.0>,6378) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6380) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6384) fake_sched.h:43: return __running_cpu;
    (<0.0>,6388) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6394) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,6395) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,6402) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,6406) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,6410) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6412) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6413) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6415) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6418) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6420) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6421) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6423) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6426) tree.c:2491: rdp->gpwrap) {
    (<0.0>,6428) tree.c:2491: rdp->gpwrap) {
    (<0.0>,6431) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6433) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6434) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6435) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6437) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6438) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6445) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6446) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6447) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6449) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6453)
    (<0.0>,6454)
    (<0.0>,6455) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,6456) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,6459) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,6462) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6464) fake_sched.h:43: return __running_cpu;
    (<0.0>,6468) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6470) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6474) fake_sched.h:43: return __running_cpu;
    (<0.0>,6478) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6490) tree.c:3016: local_irq_save(flags);
    (<0.0>,6493) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6495) fake_sched.h:43: return __running_cpu;
    (<0.0>,6499) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6501) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6505) fake_sched.h:43: return __running_cpu;
    (<0.0>,6509) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6514) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6515) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6526)
    (<0.0>,6527)
    (<0.0>,6528) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,6541)
    (<0.0>,6542) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6547) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6548) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6549) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6550) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6552) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6554) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6555) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6557) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6560) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6561) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6562) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6563) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6569) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6570) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6571) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6573) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6575) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6576) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6578) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6581) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6582) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6583) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6589) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,6591) tree.c:666: }
    (<0.0>,6594) tree.c:3024: local_irq_restore(flags);
    (<0.0>,6597) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6599) fake_sched.h:43: return __running_cpu;
    (<0.0>,6603) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6605) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6609) fake_sched.h:43: return __running_cpu;
    (<0.0>,6613) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6619) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,6622)
    (<0.0>,6623) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6625) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6628) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6635) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6638) tree_plugin.h:2457: }
    (<0.0>,6642) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6645) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6646) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6647) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6651) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6652) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6653) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6655) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6659) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,6671) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6673) fake_sched.h:43: return __running_cpu;
    (<0.0>,6676) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6678) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6680) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6681) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6683) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6690) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6691) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6693) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6699) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6700) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6701) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6702) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6703) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6707)
    (<0.0>,6708)
    (<0.0>,6709) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6710) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6735)
    (<0.0>,6736)
    (<0.0>,6737) tree.c:1905: local_irq_save(flags);
    (<0.0>,6740) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6742) fake_sched.h:43: return __running_cpu;
    (<0.0>,6746) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6748) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6752) fake_sched.h:43: return __running_cpu;
    (<0.0>,6756) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6761) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6763) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6764) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6765) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6767) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6768) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6773) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6774) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6775) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6776) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6778) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6780) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6781) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6783) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6786) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6787) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6788) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6791) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6793) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6794) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6799) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6800) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6801) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6802) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6804) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6806) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6807) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6809) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6812) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6813) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6814) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6817) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6821) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6822) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6823) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6824) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6826) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6832) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6835) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6836) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6844) tree.c:1911: local_irq_restore(flags);
    (<0.0>,6847) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6849) fake_sched.h:43: return __running_cpu;
    (<0.0>,6853) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6855) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6859) fake_sched.h:43: return __running_cpu;
    (<0.0>,6863) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6870) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6872) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6877) tree.c:3016: local_irq_save(flags);
    (<0.0>,6880) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6882) fake_sched.h:43: return __running_cpu;
    (<0.0>,6886) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6888) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6892) fake_sched.h:43: return __running_cpu;
    (<0.0>,6896) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6901) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6902) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6913)
    (<0.0>,6914)
    (<0.0>,6915) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,6928)
    (<0.0>,6929) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6934) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6935) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6936) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6937) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6939) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6941) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6944) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6947) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6948) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6949) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6950) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6955) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6956) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6957) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6960) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6962) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6963) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6965) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6968) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6969) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6970) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6976) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,6992)
    (<0.0>,6993) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6996)
    (<0.0>,6997) tree.c:625: return &rsp->node[0];
    (<0.0>,7001) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7002) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7007) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7008) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7009) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7010) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7012) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7014) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7015) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7017) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7020) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7021) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7022) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7026) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7027) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7029) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7032) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7033) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7037) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7038) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7039) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7040) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7042) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7044) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7045) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7047) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7050) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7051) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7052) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7056) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7059) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7062) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7065) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7066) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7069) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7071) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7074) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7077) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7080) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7081) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7083) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7086) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7090) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7092) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7094) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7097) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7100) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7103) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7104) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7106) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7109) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7113) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7115) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7117) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7120) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,7122) tree.c:666: }
    (<0.0>,7125) tree.c:3024: local_irq_restore(flags);
    (<0.0>,7128) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7130) fake_sched.h:43: return __running_cpu;
    (<0.0>,7134) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7140) fake_sched.h:43: return __running_cpu;
    (<0.0>,7144) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7150) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7153)
    (<0.0>,7154) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7156) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7159) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7166) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7169) tree_plugin.h:2457: }
    (<0.0>,7173) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7176) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7177) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7178) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7182) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7183) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7184) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7186) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7194) fake_sched.h:43: return __running_cpu;
    (<0.0>,7198) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,7208) fake_sched.h:43: return __running_cpu;
    (<0.0>,7212) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7213) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7215) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7216) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7217) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7219) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7221) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7222) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7223) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7224) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7225) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7226) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,7228) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,7236) tree_plugin.h:2879: }
    (<0.0>,7242) fake_sched.h:43: return __running_cpu;
    (<0.0>,7246) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,7249) tree.c:755: local_irq_save(flags);
    (<0.0>,7252) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7254) fake_sched.h:43: return __running_cpu;
    (<0.0>,7258) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7260) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7264) fake_sched.h:43: return __running_cpu;
    (<0.0>,7268) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7280) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7282) fake_sched.h:43: return __running_cpu;
    (<0.0>,7286) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7287) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7289) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7290) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7291) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7292) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7293) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7294) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7295) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,7299) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,7301) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,7302) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,7303) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,7319)
    (<0.0>,7321) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7323) fake_sched.h:43: return __running_cpu;
    (<0.0>,7327) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7330) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7331) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7332) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7336) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7337) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7338) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7340) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7345) fake_sched.h:43: return __running_cpu;
    (<0.0>,7348) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7350) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7352) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7353) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7356) tree_plugin.h:2457: }
    (<0.0>,7359) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7362) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7363) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7364) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7368) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7369) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7370) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7372) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7377) fake_sched.h:43: return __running_cpu;
    (<0.0>,7380) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7382) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7384) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7385) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7388) tree_plugin.h:2457: }
    (<0.0>,7391) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7394) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7395) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7396) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7400) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7401) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7402) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7404) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7411) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7414) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7415) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7416) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7418) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7419) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7421) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7422) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7423) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7424) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7438) tree_plugin.h:2879: }
    (<0.0>,7440) tree.c:758: local_irq_restore(flags);
    (<0.0>,7443) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7445) fake_sched.h:43: return __running_cpu;
    (<0.0>,7449) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7451) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7455) fake_sched.h:43: return __running_cpu;
    (<0.0>,7459) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7465) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,7468) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,7473) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7489)
    (<0.0>,7490)
    (<0.0>,7491) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7494)
    (<0.0>,7495) tree.c:625: return &rsp->node[0];
    (<0.0>,7499) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7500) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7505) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7506) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7507) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7508) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7510) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7512) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7513) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7515) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7518) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7519) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7520) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7522) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7523) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7524) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7525) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7529) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7534) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7535) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7536) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7537) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7539) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7541) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7542) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7544) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7547) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7548) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7549) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7552) tree.c:2066: return false;
    (<0.0>,7554) tree.c:2067: }
    (<0.0>,7559) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7575)
    (<0.0>,7576)
    (<0.0>,7577) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7580)
    (<0.0>,7581) tree.c:625: return &rsp->node[0];
    (<0.0>,7585) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7586) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7591) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7592) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7593) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7594) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7596) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7598) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7599) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7601) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7604) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7605) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7606) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7608) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7609) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7610) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7611) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7615) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7620) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7621) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7622) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7623) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7625) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7627) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7628) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7630) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7633) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7634) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7635) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7638) tree.c:2066: return false;
    (<0.0>,7640) tree.c:2067: }
    (<0.0>,7645) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7661)
    (<0.0>,7662)
    (<0.0>,7663) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7666)
    (<0.0>,7667) tree.c:625: return &rsp->node[0];
    (<0.0>,7671) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7672) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7677) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7678) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7679) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7680) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7682) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7684) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7685) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7690) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7691) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7692) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7694) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7695) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7696) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7697) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7701) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7706) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7707) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7708) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7709) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7711) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7713) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7714) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7716) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7719) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7720) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7721) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7724) tree.c:2066: return false;
    (<0.0>,7726) tree.c:2067: }
    (<0.0>,7731) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7747)
    (<0.0>,7748)
    (<0.0>,7749) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7752)
    (<0.0>,7753) tree.c:625: return &rsp->node[0];
    (<0.0>,7757) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7758) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7763) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7764) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7765) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7770) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7773) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7776) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7777) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7778) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7780) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7781) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7782) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7783) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7787) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7792) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7793) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7794) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7795) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7797) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7799) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7800) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7802) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7805) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7806) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7807) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7810) tree.c:2066: return false;
    (<0.0>,7812) tree.c:2067: }
    (<0.0>,7817) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7833)
    (<0.0>,7834)
    (<0.0>,7835) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7838)
    (<0.0>,7839) tree.c:625: return &rsp->node[0];
    (<0.0>,7843) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7844) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7849) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7850) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7851) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7852) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7854) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7857) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7859) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7862) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7863) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7864) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7866) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7867) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7868) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7869) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7873) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7878) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7879) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7880) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7881) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7883) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6765) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6766) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6768) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6771) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6772) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6773) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6774) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6776) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6779) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6781) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6783) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6784) tree.c:3536: return 1;
  (<0>,6786) tree.c:3548: }
  (<0>,6790) tree.c:3561: return 1;
  (<0>,6792) tree.c:3563: }
  (<0>,6799) fake_sched.h:43: return __running_cpu;
  (<0>,6803) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
  (<0>,6807) tree.c:2891: if (user)
  (<0>,6815) fake_sched.h:43: return __running_cpu;
  (<0>,6819) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
  (<0>,6821) fake_sched.h:43: return __running_cpu;
  (<0>,6825) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6831) fake_sched.h:43: return __running_cpu;
  (<0>,6835) fake_sched.h:183: if (need_softirq[get_cpu()]) {
  (<0>,6845) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,6848) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6849) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6850) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6854) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6855) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6856) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6858) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6862) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,6874) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6876) fake_sched.h:43: return __running_cpu;
  (<0>,6879) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6881) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6883) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6884) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6886) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6893) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6894) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6896) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6902) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6903) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6904) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6905) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6906) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6910)
  (<0>,6911)
  (<0>,6912) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,6913) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,6938)
  (<0>,6939)
  (<0>,6940) tree.c:1905: local_irq_save(flags);
  (<0>,6943) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6945) fake_sched.h:43: return __running_cpu;
  (<0>,6949) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6951) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6955) fake_sched.h:43: return __running_cpu;
  (<0>,6959) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6964) tree.c:1906: rnp = rdp->mynode;
  (<0>,6966) tree.c:1906: rnp = rdp->mynode;
  (<0>,6967) tree.c:1906: rnp = rdp->mynode;
  (<0>,6968) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6970) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6971) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6976) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6977) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6978) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6979) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6981) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6983) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6984) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6986) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6989) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6990) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6991) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6994) tree.c:1910: !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */
  (<0>,6998)
  (<0>,6999) tree.h:752: bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,7004) fake_sync.h:129: preempt_disable();
  (<0>,7006) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
  (<0>,7007) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
  (<0>,7010) fake_sync.h:134: return 1;
  (<0>,7012) fake_sync.h:135: }
  (<0>,7016) tree.h:752: bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,7017) tree.h:754: if (locked)
  (<0>,7023) tree.h:756: return locked;
  (<0>,7027) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7028) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7029) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7051)
  (<0>,7052)
  (<0>,7053)
  (<0>,7054) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7056) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7057) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7059) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7062) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7066) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7067) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7068) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7069) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7071) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7072) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7073) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7074) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7077) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7080) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7081) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7089) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7090) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7091) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7100)
  (<0>,7101)
  (<0>,7102)
  (<0>,7103) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7106) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7109) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7112) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7113) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7116) tree.c:1750: return false;
  (<0>,7118) tree.c:1799: }
  (<0>,7121) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7123) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7125) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7126) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7128) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7131) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7133) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7134) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7136) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7139) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7141) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7142) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7144) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7150) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7151) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7154) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7158) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7160) fake_sched.h:43: return __running_cpu;
  (<0>,7164) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7165) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7167) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7168) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7170) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7173) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7174) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
  (<0>,7176) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
  (<0>,7178) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
  (<0>,7180) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
  (<0>,7182) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
  (<0>,7183) tree.c:1893: zero_cpu_stall_ticks(rdp);
  (<0>,7186)
  (<0>,7187) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,7189) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,7190) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,7192) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,7202)
  (<0>,7207) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7211) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7212) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7213) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7214) fake_defs.h:237: switch (size) {
  (<0>,7216) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7217) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7218) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7219) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7222) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7225) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7226) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7229) tree.c:1896: return ret;
  (<0>,7233) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7237) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7238) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7239) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7241) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7245)
  (<0>,7246)
  (<0>,7247) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,7248) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,7251) fake_sync.h:93: local_irq_restore(flags);
  (<0>,7254) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7256) fake_sched.h:43: return __running_cpu;
  (<0>,7260) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7262) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7266) fake_sched.h:43: return __running_cpu;
  (<0>,7270) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7278) tree.c:1916: if (needwake)
  (<0>,7282) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7284) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7287) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,7291) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,7295) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7297) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7299) fake_sched.h:43: return __running_cpu;
  (<0>,7303) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7308) tree.c:3016: local_irq_save(flags);
  (<0>,7311) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7313) fake_sched.h:43: return __running_cpu;
  (<0>,7317) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7319) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7323) fake_sched.h:43: return __running_cpu;
  (<0>,7327) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7332) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7333) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7344)
  (<0>,7345)
  (<0>,7346) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7359)
  (<0>,7360) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7365) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7366) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7367) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7368) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7370) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7372) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7373) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7375) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7378) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7379) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7380) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7381) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7386) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7387) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7388) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7389) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7391) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7393) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7394) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7396) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7399) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7400) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7401) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7407) tree.c:653: return false;  /* No, a grace period is already in progress. */
  (<0>,7409) tree.c:666: }
  (<0>,7412) tree.c:3024: local_irq_restore(flags);
  (<0>,7415) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7417) fake_sched.h:43: return __running_cpu;
  (<0>,7421) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7423) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7427) fake_sched.h:43: return __running_cpu;
  (<0>,7431) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7437) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7440)
  (<0>,7441) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7443) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7446) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7453) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,7456) tree_plugin.h:2457: }
  (<0>,7460) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7463) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7464) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7465) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7469) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7470) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7471) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7473) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7477) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,7489) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7491) fake_sched.h:43: return __running_cpu;
  (<0>,7494) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7496) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7498) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7499) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7501) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7508) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7509) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7511) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7517) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7518) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7519) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7520) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7521) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7525)
  (<0>,7526)
  (<0>,7527) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,7528) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,7553)
  (<0>,7554)
  (<0>,7555) tree.c:1905: local_irq_save(flags);
  (<0>,7558) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7560) fake_sched.h:43: return __running_cpu;
  (<0>,7564) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7566) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7570) fake_sched.h:43: return __running_cpu;
  (<0>,7574) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7579) tree.c:1906: rnp = rdp->mynode;
  (<0>,7581) tree.c:1906: rnp = rdp->mynode;
  (<0>,7582) tree.c:1906: rnp = rdp->mynode;
  (<0>,7583) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7585) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7586) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7591) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7592) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7593) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7594) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7596) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7598) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7599) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7601) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7604) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7605) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7606) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7609) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7611) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7612) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7617) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7618) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7619) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7620) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7622) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7624) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7625) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7627) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7630) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7631) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7632) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7635) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7639) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7640) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7641) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7642) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7644) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7645) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7646) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7647) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7650) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7653) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7654) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7662) tree.c:1911: local_irq_restore(flags);
  (<0>,7665) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7667) fake_sched.h:43: return __running_cpu;
  (<0>,7671) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7673) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7677) fake_sched.h:43: return __running_cpu;
  (<0>,7681) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7688) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7690) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7695) tree.c:3016: local_irq_save(flags);
  (<0>,7698) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7700) fake_sched.h:43: return __running_cpu;
  (<0>,7704) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7706) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7710) fake_sched.h:43: return __running_cpu;
  (<0>,7714) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7719) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7720) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7731)
  (<0>,7732)
  (<0>,7733) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7746)
  (<0>,7747) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7752) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7753) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7754) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7755) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7757) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7759) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7760) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7762) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7765) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7766) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7767) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7768) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7773) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7774) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7775) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7776) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7778) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7780) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7781) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7783) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7786) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7787) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7788) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7794) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,7810)
  (<0>,7811) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7814)
  (<0>,7815) tree.c:625: return &rsp->node[0];
  (<0>,7819) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7820) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7825) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7826) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7827) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7828) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7830) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7832) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7833) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7835) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7838) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7839) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7840) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7844) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7845) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7847) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7850) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7851) tree.c:639: return READ_ONCE(*fp);
  (<0>,7855) tree.c:639: return READ_ONCE(*fp);
  (<0>,7856) tree.c:639: return READ_ONCE(*fp);
  (<0>,7857) tree.c:639: return READ_ONCE(*fp);
  (<0>,7858) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7860) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7862) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7863) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7865) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7868) tree.c:639: return READ_ONCE(*fp);
  (<0>,7869) tree.c:639: return READ_ONCE(*fp);
  (<0>,7870) tree.c:639: return READ_ONCE(*fp);
  (<0>,7874) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7877) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7880) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7883) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7884) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7887) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7889) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7892) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7895) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7898) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7899) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7901) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7904) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7908) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7910) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7912) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7915) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7918) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7921) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7922) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7924) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7927) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7931) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7933) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7935) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7938) tree.c:665: return false; /* No grace period needed. */
  (<0>,7940) tree.c:666: }
  (<0>,7943) tree.c:3024: local_irq_restore(flags);
  (<0>,7946) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7948) fake_sched.h:43: return __running_cpu;
  (<0>,7952) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7954) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7958) fake_sched.h:43: return __running_cpu;
  (<0>,7962) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7968) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7971)
  (<0>,7972) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7974) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7977) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7984) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,7987) tree_plugin.h:2457: }
  (<0>,7991) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7994) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7995) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7996) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8000) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8001) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8002) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8004) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8012) fake_sched.h:43: return __running_cpu;
  (<0>,8016) fake_sched.h:185: need_softirq[get_cpu()] = 0;
  (<0>,8026) fake_sched.h:43: return __running_cpu;
  (<0>,8030) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8031) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8033) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8034) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8035) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8037) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8039) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8040) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8041) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8042) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8043) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8044) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,8046) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,8054) tree_plugin.h:2879: }
    (<0.0>,7885) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7886) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7888) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7891) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7892) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7893) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7896) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7899)
    (<0.0>,7903) tree.c:2064: return true;
    (<0.0>,7905) tree.c:2067: }
    (<0.0>,7910) fake_sched.h:43: return __running_cpu;
    (<0.0>,7914)
    (<0.0>,7915) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,7918) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,7923) tree.c:892: local_irq_save(flags);
    (<0.0>,7926)
    (<0.0>,7928) fake_sched.h:43: return __running_cpu;
    (<0.0>,7932) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7934) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7938) fake_sched.h:43: return __running_cpu;
    (<0.0>,7942) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7954)
    (<0.0>,7956) fake_sched.h:43: return __running_cpu;
    (<0.0>,7960) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7961) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7963) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7964) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7965) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7966) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7967) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7968) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7969) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,7973) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,7975) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,7976) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,7977) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,7988)
    (<0.0>,7989)
    (<0.0>,7991) fake_sched.h:43: return __running_cpu;
    (<0.0>,7995) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7999) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8002) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8003) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8004) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8006) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8007) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8009) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8010) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8011) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8012) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8022)
    (<0.0>,8024) tree.c:895: local_irq_restore(flags);
    (<0.0>,8027)
    (<0.0>,8029) fake_sched.h:43: return __running_cpu;
    (<0.0>,8033) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8035) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8039) fake_sched.h:43: return __running_cpu;
    (<0.0>,8043) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8050) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8051) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8052) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8053) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,8055) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,8056) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8061) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8062) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8063) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8064) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8066) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8068) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8069) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8071) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8074) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8075) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8076) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8079) tree.c:2237: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8082)
    (<0.0>,8087) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,8089) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,8090) tree.c:2280: rcu_gp_cleanup(rsp);
    (<0.0>,8130)
    (<0.0>,8131) tree.c:2109: bool needgp = false;
    (<0.0>,8132) tree.c:2110: int nocb = 0;
    (<0.0>,8133) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8136)
    (<0.0>,8137) tree.c:625: return &rsp->node[0];
    (<0.0>,8141) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8143) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8144) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8145) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8150) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8151) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8152) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8153) fake_defs.h:237: switch (size) {
    (<0.0>,8155) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8157) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8158) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8160) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8163) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8164) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8165) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8166) tree.c:2116: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,8169)
    (<0.0>,8170) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8174)
    (<0.0>,8177) fake_sched.h:43: return __running_cpu;
    (<0.0>,8181) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8185) fake_sched.h:43: return __running_cpu;
    (<0.0>,8189) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8194) fake_sched.h:43: return __running_cpu;
    (<0.0>,8198) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8201) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8202) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8209) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8210) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8212) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8214) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8215) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,8216) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,8218) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,8221) tree.c:2129: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,8224)
    (<0.0>,8225) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8229)
    (<0.0>,8230) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8231) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8236) fake_sched.h:43: return __running_cpu;
    (<0.0>,8240) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,8242) fake_sched.h:43: return __running_cpu;
    (<0.0>,8246) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8253) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8256) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8258) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8259) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8261) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8266) tree.c:2141: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,8269)
    (<0.0>,8270) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8274)
    (<0.0>,8277) fake_sched.h:43: return __running_cpu;
    (<0.0>,8281) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8285) fake_sched.h:43: return __running_cpu;
    (<0.0>,8289) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8294) fake_sched.h:43: return __running_cpu;
    (<0.0>,8298) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8301) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8302) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8309) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8312)
    (<0.0>,8318) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8319) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8322)
    (<0.0>,8327) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8328) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8329) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8330) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8332) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8337) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8338) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8340) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8344) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8345) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8346) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8348) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8350) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8351) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8352) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8357) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8358) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8359) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8360) fake_defs.h:237: switch (size) {
    (<0.0>,8362) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8364) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8365) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8367) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8370) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8371) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8372) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8374) fake_sched.h:43: return __running_cpu;
    (<0.0>,8377) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8379) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8381) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8382) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8383) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8385) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8388) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8389) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8390) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8412)
    (<0.0>,8413)
    (<0.0>,8414)
    (<0.0>,8415) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8417) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8418) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8420) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8423) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8424) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8425) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8433)
    (<0.0>,8434)
    (<0.0>,8435)
    (<0.0>,8436) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8439) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8442) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8445) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8446) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8449) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8451) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8454) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8456) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8457) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8459) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8462) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8466) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8468) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8471) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8472) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8475) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8477) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8479) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8481) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8484) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8486) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8487) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8489) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8492) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8497) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8499) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8500) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8503) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8506) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8507) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8509) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8512) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8514) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8516) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8518) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8519) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8522) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,8524) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,8527) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8529) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8532) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8533) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8536) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8540) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8541) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8542) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8551)
    (<0.0>,8552)
    (<0.0>,8553)
    (<0.0>,8554) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8557) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8560) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8563) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8564) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8567) tree.c:1750: return false;
    (<0.0>,8569) tree.c:1799: }
    (<0.0>,8571) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8573) tree.c:1843: }
    (<0.0>,8576) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8577) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8579) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8580) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8582) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8586) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8588) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8589) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8591) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8594) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8598) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8599) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8600) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8601) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8603) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8604) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8605) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8606) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8609) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8612) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8613) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8621) tree.c:1896: return ret;
    (<0.0>,8625) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8629) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8631) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8632) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8639)
    (<0.0>,8640)
    (<0.0>,8641) tree.c:1702: int c = rnp->completed;
    (<0.0>,8643) tree.c:1702: int c = rnp->completed;
    (<0.0>,8645) tree.c:1702: int c = rnp->completed;
    (<0.0>,8647) fake_sched.h:43: return __running_cpu;
    (<0.0>,8650) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8652) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8654) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8655) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,8658) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,8661) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,8662) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8666) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8669) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8670) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8671) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,8672) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,8673) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,8675) tree.c:1709: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,8683)
    (<0.0>,8684)
    (<0.0>,8685)
    (<0.0>,8686)
    (<0.0>,8690) tree.c:1710: return needmore;
    (<0.0>,8692) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8694) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8695) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,8698)
    (<0.0>,8700) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,8701) tree.c:2151: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,8704)
    (<0.0>,8705) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8709)
    (<0.0>,8710) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8711) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8716) fake_sched.h:43: return __running_cpu;
    (<0.0>,8720) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,8722) fake_sched.h:43: return __running_cpu;
    (<0.0>,8726) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8733) tree.c:2152: rcu_nocb_gp_cleanup(sq);
    (<0.0>,8736)
    (<0.0>,8746) fake_sched.h:43: return __running_cpu;
    (<0.0>,8752) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,8760) fake_sched.h:43: return __running_cpu;
    (<0.0>,8764) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,8777) fake_sched.h:43: return __running_cpu;
    (<0.0>,8781)
    (<0.0>,8784) tree.c:755: local_irq_save(flags);
    (<0.0>,8787)
    (<0.0>,8789) fake_sched.h:43: return __running_cpu;
    (<0.0>,8793) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8795) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8799) fake_sched.h:43: return __running_cpu;
    (<0.0>,8803) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8815)
    (<0.0>,8817) fake_sched.h:43: return __running_cpu;
    (<0.0>,8821) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8822) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8824) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8825) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8826) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8827) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8828) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8829) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8830) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,8834) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,8836) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,8837) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8838) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8854)
    (<0.0>,8856)
    (<0.0>,8858) fake_sched.h:43: return __running_cpu;
    (<0.0>,8862) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8865) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8866) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8867) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8871) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8872) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8873) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8875) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8880) fake_sched.h:43: return __running_cpu;
    (<0.0>,8883) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8885) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8887) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8888) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8891)
    (<0.0>,8894) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8897) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8898) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8899) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8903) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8904) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8905) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8907) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8912) fake_sched.h:43: return __running_cpu;
    (<0.0>,8915) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8917) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8919) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8920) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8923)
    (<0.0>,8926) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8929) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8930) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8931) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8935) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8936) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8937) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8939) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8946) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8949) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8950) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8951) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8953) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8954) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8956) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8957) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8958) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8959) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8973)
    (<0.0>,8975) tree.c:758: local_irq_restore(flags);
    (<0.0>,8978)
    (<0.0>,8980) fake_sched.h:43: return __running_cpu;
    (<0.0>,8984) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8986) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8990) fake_sched.h:43: return __running_cpu;
    (<0.0>,8994) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9000) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9003) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9008) fake_sched.h:43: return __running_cpu;
    (<0.0>,9012)
    (<0.0>,9013) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9016) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9021) tree.c:892: local_irq_save(flags);
    (<0.0>,9024)
    (<0.0>,9026) fake_sched.h:43: return __running_cpu;
    (<0.0>,9030) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9032) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9036) fake_sched.h:43: return __running_cpu;
    (<0.0>,9040) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9052)
    (<0.0>,9054) fake_sched.h:43: return __running_cpu;
    (<0.0>,9058) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9059) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,9061) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,9062) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,9063) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9064) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9065) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9066) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9067) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,9071) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9073) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9074) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9075) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9086)
    (<0.0>,9087)
    (<0.0>,9089) fake_sched.h:43: return __running_cpu;
    (<0.0>,9093) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9097) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9100) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9101) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9102) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9104) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9105) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9107) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9108) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9109) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9110) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9120)
    (<0.0>,9122) tree.c:895: local_irq_restore(flags);
    (<0.0>,9125)
    (<0.0>,9127) fake_sched.h:43: return __running_cpu;
    (<0.0>,9131) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9133) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9137) fake_sched.h:43: return __running_cpu;
    (<0.0>,9141) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9155) fake_sched.h:43: return __running_cpu;
    (<0.0>,9159) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,9168) fake_sched.h:43: return __running_cpu;
    (<0.0>,9175) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,9184) fake_sched.h:43: return __running_cpu;
    (<0.0>,9188) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,9190) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,9196) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9197) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9198) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9203) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9204) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9205) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9206) fake_defs.h:237: switch (size) {
    (<0.0>,9208) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9210) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9211) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9213) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9216) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9217) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9218) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9219) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,9220) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,9224)
    (<0.0>,9225)
    (<0.0>,9226) tree.c:1922: if (delay > 0 &&
    (<0.0>,9231) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9233) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9235) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9236) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9238) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9243) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,9246)
    (<0.0>,9247) tree.c:625: return &rsp->node[0];
    (<0.0>,9251) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,9252) tree.c:2158: raw_spin_lock_irq_rcu_node(rnp); /* Order GP before ->completed update. */
    (<0.0>,9255)
    (<0.0>,9256) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,9260)
    (<0.0>,9263) fake_sched.h:43: return __running_cpu;
    (<0.0>,9267) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9271) fake_sched.h:43: return __running_cpu;
    (<0.0>,9275) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9280) fake_sched.h:43: return __running_cpu;
    (<0.0>,9284) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9287) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,9288) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,9295) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,9296) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,9300)
    (<0.0>,9301)
    (<0.0>,9304) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9306) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9307) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9308) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9313) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9314) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9315) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9316) fake_defs.h:237: switch (size) {
    (<0.0>,9318) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9320) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9321) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9323) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9326) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9327) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9328) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9331) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,9333) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,9335) fake_sched.h:43: return __running_cpu;
    (<0.0>,9338) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9340) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9342) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9343) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9344) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9345) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9353)
    (<0.0>,9354)
    (<0.0>,9355)
    (<0.0>,9356) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9359) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9362) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9365) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9366) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9369) tree.c:1818: return false;
    (<0.0>,9371) tree.c:1843: }
    (<0.0>,9374) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9378) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9379) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9382) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9383) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9394)
    (<0.0>,9395)
    (<0.0>,9396) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,9409)
    (<0.0>,9410) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9415) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9416) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9417) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9418) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9420) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9422) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9423) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9425) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9428) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9429) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9430) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9431) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9436) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9437) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9438) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9439) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9441) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9443) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9444) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9446) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9449) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9450) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9451) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9457) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,9473)
    (<0.0>,9474) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9477)
    (<0.0>,9478) tree.c:625: return &rsp->node[0];
    (<0.0>,9482) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9483) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9488) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9489) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9490) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9491) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9493) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9495) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9496) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9498) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9501) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9502) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9503) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9507) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9508) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9510) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9513) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9514) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9518) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9519) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9520) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9521) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9523) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9525) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9526) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9528) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9531) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9532) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9533) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9537) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,9539) tree.c:666: }
    (<0.0>,9544) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9549) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9550) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9551) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9552) fake_defs.h:237: switch (size) {
    (<0.0>,9554) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9556) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9557) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9559) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9562) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9563) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9564) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9568) tree.c:2174: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,9571)
    (<0.0>,9572) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,9576)
    (<0.0>,9577) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,9578) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,9583) fake_sched.h:43: return __running_cpu;
    (<0.0>,9587) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9589) fake_sched.h:43: return __running_cpu;
    (<0.0>,9593) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9601) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,9603) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,9608) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,9610) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,9614) fake_sched.h:43: return __running_cpu;
    (<0.0>,9618) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9622) fake_sched.h:43: return __running_cpu;
    (<0.0>,9626) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9631) fake_sched.h:43: return __running_cpu;
    (<0.0>,9635) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9646) fake_sched.h:43: return __running_cpu;
    (<0.0>,9650) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9651) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,9653) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,9654) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,9655) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,9657) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,9659) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,9660) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9661) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9662) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9663) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9664) tree.c:942: if (oldval)
    (<0.0>,9672)
    (<0.0>,9678)
    (<0.0>,9687) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9688) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9689) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9693) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9694) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9695) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9697) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9702) fake_sched.h:43: return __running_cpu;
    (<0.0>,9705) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9707) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9710) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9712) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9714) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9717) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9718) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9719) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9723) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9724) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9725) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9727) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9732) fake_sched.h:43: return __running_cpu;
    (<0.0>,9735) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9737) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9740) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9742) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9744) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9747) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9748) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9749) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9753) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9754) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9755) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9757) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9762) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,9767) fake_sched.h:43: return __running_cpu;
    (<0.0>,9772) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,9780) fake_sched.h:43: return __running_cpu;
    (<0.0>,9786) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,9800) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9801) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9802) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9806) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9807) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9808) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9810) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9814) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9816) fake_sched.h:43: return __running_cpu;
    (<0.0>,9819) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9821) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9843)
    (<0.0>,9844)
    (<0.0>,9845) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9847) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9848) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9849) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9851) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9853) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9854) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,9855) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,9890)
    (<0.0>,9891)
    (<0.0>,9892) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,9895) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,9908)
    (<0.0>,9909) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9914) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9915) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9916) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9917) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9919) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9921) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9922) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9924) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9927) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9928) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9929) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9930) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9935) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9936) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9937) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9938) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9940) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9943) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9945) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9948) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9949) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9950) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9958) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,9961)
    (<0.0>,9964) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,9967) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9969) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9972) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9976) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9980) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,9982) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,9985) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,9989) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,9992) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,9994) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,9996) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,9997) tree.c:3511: return 1;
    (<0.0>,9999) tree.c:3548: }
    (<0.0>,10003) tree.c:3561: return 1;
    (<0.0>,10005) tree.c:3563: }
    (<0.0>,10012) fake_sched.h:43: return __running_cpu;
    (<0.0>,10016) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,10020) tree.c:2891: if (user)
    (<0.0>,10028) fake_sched.h:43: return __running_cpu;
    (<0.0>,10032) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,10034) fake_sched.h:43: return __running_cpu;
    (<0.0>,10038) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10044) fake_sched.h:43: return __running_cpu;
    (<0.0>,10048) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,10058)
    (<0.0>,10061) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10062) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10063) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10067) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10068) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10069) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10071) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10075) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,10087)
    (<0.0>,10089) fake_sched.h:43: return __running_cpu;
    (<0.0>,10092) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10094) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10096) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10097) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10099) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10106) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10107) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10109) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10115) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10116) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10117) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10118) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10119) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10123)
    (<0.0>,10124)
    (<0.0>,10125) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,10126) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,10151)
    (<0.0>,10152)
    (<0.0>,10153) tree.c:1905: local_irq_save(flags);
    (<0.0>,10156)
    (<0.0>,10158) fake_sched.h:43: return __running_cpu;
    (<0.0>,10162) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10164) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10168) fake_sched.h:43: return __running_cpu;
    (<0.0>,10172) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10177) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10179) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10180) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10181) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10183) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10184) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10189) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10190) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10191) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10192) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10194) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10196) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10197) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10199) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10202) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10203) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10204) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10207) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10209) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10210) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10215) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10216) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10217) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10218) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10220) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10222) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10223) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10225) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10228) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10229) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10230) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10233) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10237) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10238) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10239) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10240) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10242) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10243) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10244) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10245) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10248) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10251) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10252) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10260) tree.c:1911: local_irq_restore(flags);
    (<0.0>,10263)
    (<0.0>,10265) fake_sched.h:43: return __running_cpu;
    (<0.0>,10269) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10271) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10275) fake_sched.h:43: return __running_cpu;
    (<0.0>,10279) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10286) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,10288) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,10291) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,10295) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,10299) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10301) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10302) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10303) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10321)
    (<0.0>,10322)
    (<0.0>,10323)
    (<0.0>,10324) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,10326) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,10327) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,10331) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,10332) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,10333) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,10335) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,10339)
    (<0.0>,10340)
    (<0.0>,10341) fake_sync.h:83: local_irq_save(flags);
    (<0.0>,10344)
    (<0.0>,10346) fake_sched.h:43: return __running_cpu;
    (<0.0>,10350) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10352) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10356) fake_sched.h:43: return __running_cpu;
    (<0.0>,10360) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10366) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,10367) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,10374) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,10378) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,10382) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10384) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10385) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10387) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10390) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10392) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10393) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10395) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10398) tree.c:2499: rdp->cpu_no_qs.b.norm = true;	/* need qs for new gp. */
    (<0.0>,10402) tree.c:2499: rdp->cpu_no_qs.b.norm = true;	/* need qs for new gp. */
    (<0.0>,10404) fake_sched.h:43: return __running_cpu;
    (<0.0>,10408) tree.c:2500: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,10409) tree.c:2500: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,10411) tree.c:2500: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,10415) tree.c:2501: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,10416) tree.c:2501: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,10417) tree.c:2501: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,10419) tree.c:2501: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,10423)
    (<0.0>,10424)
    (<0.0>,10425) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10426) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10429) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,10432)
    (<0.0>,10434) fake_sched.h:43: return __running_cpu;
    (<0.0>,10438) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10440) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10444) fake_sched.h:43: return __running_cpu;
    (<0.0>,10448) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10460) tree.c:3016: local_irq_save(flags);
    (<0.0>,10463)
    (<0.0>,10465) fake_sched.h:43: return __running_cpu;
    (<0.0>,10469) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10471) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10475) fake_sched.h:43: return __running_cpu;
    (<0.0>,10479) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10484) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10485) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10496)
    (<0.0>,10497)
    (<0.0>,10498) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,10511)
    (<0.0>,10512) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10517) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10518) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10519) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10520) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10522) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10524) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10525) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10527) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10530) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10531) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10532) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10533) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10538) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10539) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10540) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10541) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10543) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10545) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10546) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10548) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10551) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10552) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10553) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10559) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,10575)
    (<0.0>,10576) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10579)
    (<0.0>,10580) tree.c:625: return &rsp->node[0];
    (<0.0>,10584) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10585) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10590) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10591) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10592) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10593) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10595) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10597) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10598) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10600) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10603) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10604) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10605) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10609) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10610) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10612) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10615) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10616) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10620) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10621) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10622) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10623) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10625) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10627) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10628) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10630) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10633) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10634) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10635) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10639) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,10641) tree.c:666: }
    (<0.0>,10644) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,10647)
    (<0.0>,10648) tree.c:625: return &rsp->node[0];
    (<0.0>,10654)
    (<0.0>,10655) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,10659)
    (<0.0>,10661) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,10662) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,10669) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,10675)
    (<0.0>,10677) fake_sched.h:43: return __running_cpu;
    (<0.0>,10680) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10682) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10684) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10685) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10688)
    (<0.0>,10689) tree.c:625: return &rsp->node[0];
    (<0.0>,10693) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10694) tree.c:2334: bool ret = false;
    (<0.0>,10695) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10696) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10697) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10705)
    (<0.0>,10706)
    (<0.0>,10707)
    (<0.0>,10708) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10711) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10714) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10717) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10718) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10721) tree.c:1818: return false;
    (<0.0>,10723) tree.c:1843: }
    (<0.0>,10726) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10730) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10731) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10732) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10733) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10744)
    (<0.0>,10745)
    (<0.0>,10746)
    (<0.0>,10747) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10749) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10752) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10753) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10764)
    (<0.0>,10765)
    (<0.0>,10766) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,10779)
    (<0.0>,10780) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10785) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10786) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10787) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10788) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10790) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10792) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10793) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10795) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10798) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10799) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10800) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10801) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10806) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10807) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10808) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10809) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10811) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10813) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10814) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10816) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10819) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10821) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10827) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,10843)
    (<0.0>,10844) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10847)
    (<0.0>,10848) tree.c:625: return &rsp->node[0];
    (<0.0>,10852) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10853) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10858) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10859) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10860) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10861) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10863) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10865) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10866) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10868) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10871) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10872) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10873) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10877) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10878) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10880) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10883) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10884) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10888) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10889) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10890) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10891) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10893) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10895) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10896) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10898) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10901) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10902) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10903) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10907) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,10909) tree.c:666: }
    (<0.0>,10914) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10919) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10920) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10921) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10922) fake_defs.h:237: switch (size) {
    (<0.0>,10924) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,10926) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,10927) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,10929) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,10932) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10933) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10934) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10937) tree.c:2318: return true;
    (<0.0>,10939) tree.c:2319: }
    (<0.0>,10943) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10944) tree.c:2346: return ret;
    (<0.0>,10948) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,10952) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,10953) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,10954) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,10957)
    (<0.0>,10958) tree.c:625: return &rsp->node[0];
    (<0.0>,10963) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,10967)
    (<0.0>,10968)
    (<0.0>,10969) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10970) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10973) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,10976)
    (<0.0>,10978) fake_sched.h:43: return __running_cpu;
    (<0.0>,10982) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10984) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10988) fake_sched.h:43: return __running_cpu;
    (<0.0>,10992) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11000) tree.c:3021: if (needwake)
    (<0.0>,11003) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,11011)
    (<0.0>,11012) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,11013) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,11015) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,11022) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11025)
    (<0.0>,11026) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11028) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11031) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11034) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11037) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11044) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,11045) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,11054)
    (<0.0>,11055)
    (<0.0>,11058) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11059) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11060) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11061) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11063) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11065) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11066) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11068) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11071) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11072) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11073) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11082) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,11084) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,11093) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,11094) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,11116)
    (<0.0>,11117)
    (<0.0>,11118) tree.c:2767: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,11121)
    (<0.0>,11122) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11124) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11127) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11130) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11133) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11140) tree.c:2779: local_irq_save(flags);
    (<0.0>,11143)
    (<0.0>,11145) fake_sched.h:43: return __running_cpu;
    (<0.0>,11149) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11151) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11155) fake_sched.h:43: return __running_cpu;
    (<0.0>,11159) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11164) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11165) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11166) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11167) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11168) tree.c:2781: bl = rdp->blimit;
    (<0.0>,11170) tree.c:2781: bl = rdp->blimit;
    (<0.0>,11171) tree.c:2781: bl = rdp->blimit;
    (<0.0>,11174) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,11176) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,11177) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,11178) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11181) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11182) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11183) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11185) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11186) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,11189) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,11190) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,11191) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11194) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11195) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11196) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11198) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11201) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11203) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11206) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11207) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11210) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11213) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11215) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11217) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11220) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11223) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11225) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11227) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11230) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11232) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11235) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11236) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11239) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11242) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11244) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11246) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11249) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11252) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11254) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11256) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11259) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11261) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11264) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11265) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11268) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11271) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11273) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11275) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11278) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11281) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11283) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11285) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11288) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11290) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11293) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11294) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11297) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11300) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11302) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11304) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11307) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11310) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11312) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11314) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11317) tree.c:2790: local_irq_restore(flags);
    (<0.0>,11320)
    (<0.0>,11322) fake_sched.h:43: return __running_cpu;
    (<0.0>,11326) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11328) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11332) fake_sched.h:43: return __running_cpu;
    (<0.0>,11336) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11341) tree.c:2793: count = count_lazy = 0;
    (<0.0>,11342) tree.c:2793: count = count_lazy = 0;
    (<0.0>,11344) tree.c:2794: while (list) {
    (<0.0>,11347) tree.c:2795: next = list->next;
    (<0.0>,11349) tree.c:2795: next = list->next;
    (<0.0>,11350) tree.c:2795: next = list->next;
    (<0.0>,11353) tree.c:2797: debug_rcu_head_unqueue(list);
    (<0.0>,11356)
    (<0.0>,11358) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11360) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11361) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11367)
    (<0.0>,11368)
    (<0.0>,11369) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11372) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11374) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11377) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,11380) rcu.h:118: head->func(head);
    (<0.0>,11383) rcu.h:118: head->func(head);
    (<0.0>,11384) rcu.h:118: head->func(head);
    (<0.0>,11390)
    (<0.0>,11391) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11392) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11393) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11397) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11398) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11399) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11400) update.c:341: complete(&rcu->completion);
    (<0.0>,11404)
    (<0.0>,11405) fake_sync.h:288: x->done++;
    (<0.0>,11407) fake_sync.h:288: x->done++;
    (<0.0>,11409) fake_sync.h:288: x->done++;
    (<0.0>,11414) rcu.h:120: return false;
    (<0.0>,11416) rcu.h:122: }
    (<0.0>,11419) tree.c:2800: list = next;
    (<0.0>,11420) tree.c:2800: list = next;
    (<0.0>,11421) tree.c:2802: if (++count >= bl &&
    (<0.0>,11423) tree.c:2802: if (++count >= bl &&
    (<0.0>,11424) tree.c:2802: if (++count >= bl &&
    (<0.0>,11428) tree.c:2794: while (list) {
    (<0.0>,11431) tree.c:2808: local_irq_save(flags);
    (<0.0>,11434)
    (<0.0>,11436) fake_sched.h:43: return __running_cpu;
    (<0.0>,11440) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11442) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11446) fake_sched.h:43: return __running_cpu;
    (<0.0>,11450) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11457) tree.c:2814: if (list != NULL) {
    (<0.0>,11461) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11462) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11464) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11466) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11468) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11470) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11471) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11473) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11474) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11479) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11480) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11481) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11482) fake_defs.h:237: switch (size) {
    (<0.0>,11484) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11486) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11487) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11489) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11492) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11493) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11494) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11495) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11496) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11498) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11500) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11501) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,11503) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,11506) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11508) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11511) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11513) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11516) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11518) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11519) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11521) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11522) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11527) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11529) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11532) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11534) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11541) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11542) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11544) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11547) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11549) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11555) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11556) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11557) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11558) tree.c:2840: local_irq_restore(flags);
    (<0.0>,11561)
    (<0.0>,11563) fake_sched.h:43: return __running_cpu;
    (<0.0>,11567) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11569) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11573) fake_sched.h:43: return __running_cpu;
    (<0.0>,11577) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11582) tree.c:2843: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11585)
    (<0.0>,11586) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11588) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11591) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11602) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11605)
    (<0.0>,11609) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11612) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11613) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11614) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11618) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11619) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11620) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11622) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11626) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,11638)
    (<0.0>,11640) fake_sched.h:43: return __running_cpu;
    (<0.0>,11643) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11645) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11647) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11648) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11650) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11657) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11658) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11660) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11666) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11667) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11668) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11669) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11670) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11674)
    (<0.0>,11675)
    (<0.0>,11676) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,11677) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,11702)
    (<0.0>,11703)
    (<0.0>,11704) tree.c:1905: local_irq_save(flags);
    (<0.0>,11707)
    (<0.0>,11709) fake_sched.h:43: return __running_cpu;
    (<0.0>,11713) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11715) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11719) fake_sched.h:43: return __running_cpu;
    (<0.0>,11723) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11728) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11730) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11731) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11732) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11734) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11735) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11740) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11741) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11742) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11743) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11745) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11747) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11748) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11750) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11753) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11754) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11755) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11758) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11760) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11761) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11766) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11767) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11768) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11773) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11774) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11776) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11779) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11780) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11781) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11784) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11788) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11789) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11790) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11791) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11793) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11794) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11795) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11796) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11799) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11802) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11803) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11811) tree.c:1911: local_irq_restore(flags);
    (<0.0>,11814)
    (<0.0>,11816) fake_sched.h:43: return __running_cpu;
    (<0.0>,11820) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11822) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11826) fake_sched.h:43: return __running_cpu;
    (<0.0>,11830) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11837) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,11839) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,11844) tree.c:3016: local_irq_save(flags);
    (<0.0>,11847)
    (<0.0>,11849) fake_sched.h:43: return __running_cpu;
    (<0.0>,11853) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11855) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11859) fake_sched.h:43: return __running_cpu;
    (<0.0>,11863) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11868) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11869) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11880)
    (<0.0>,11881)
    (<0.0>,11882) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,11895)
    (<0.0>,11896) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11901) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11902) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11903) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11904) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11906) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11908) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11909) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11911) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11914) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11915) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11916) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11917) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11922) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11923) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11924) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11925) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11927) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11929) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11930) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11932) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11935) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11936) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11937) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11943) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,11959)
    (<0.0>,11960) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11963)
    (<0.0>,11964) tree.c:625: return &rsp->node[0];
    (<0.0>,11968) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11969) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11974) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11975) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11976) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11977) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11979) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11981) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11982) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11984) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11987) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11988) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11989) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11993) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11994) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11996) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11999) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12000) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12004) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12005) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12006) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12007) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12009) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12011) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12012) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12014) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12017) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12018) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12019) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12023) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12026) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12029) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12032) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12033) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12036) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12038) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12041) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12044) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12047) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12048) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12050) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12053) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12057) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12059) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12061) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12064) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12067) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12070) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12071) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12073) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12076) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12080) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12082) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12084) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12087) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,12089) tree.c:666: }
    (<0.0>,12092) tree.c:3024: local_irq_restore(flags);
    (<0.0>,12095)
    (<0.0>,12097) fake_sched.h:43: return __running_cpu;
    (<0.0>,12101) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12103) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12107) fake_sched.h:43: return __running_cpu;
    (<0.0>,12111) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12117) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,12120)
    (<0.0>,12121) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12123) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12126) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12133) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12136)
    (<0.0>,12140) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12143) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12144) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12145) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12149) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12150) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12151) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12153) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12161) fake_sched.h:43: return __running_cpu;
    (<0.0>,12165) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,12175) fake_sched.h:43: return __running_cpu;
    (<0.0>,12179) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12180) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,12182) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,12183) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,12184) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,12186) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,12188) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,12189) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12190) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12191) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12192) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12193) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,12195) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,12203)
    (<0.0>,12209) fake_sched.h:43: return __running_cpu;
    (<0.0>,12213)
    (<0.0>,12216) tree.c:755: local_irq_save(flags);
    (<0.0>,12219)
    (<0.0>,12221) fake_sched.h:43: return __running_cpu;
    (<0.0>,12225) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12227) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12231) fake_sched.h:43: return __running_cpu;
    (<0.0>,12235) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12247)
    (<0.0>,12249) fake_sched.h:43: return __running_cpu;
    (<0.0>,12253) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12254) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12256) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12257) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12258) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12259) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12260) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12261) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12262) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,12266) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,12268) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,12269) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12270) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12286)
    (<0.0>,12288)
    (<0.0>,12290) fake_sched.h:43: return __running_cpu;
    (<0.0>,12294) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12297) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12298) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12299) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12303) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12304) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12305) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12307) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12312) fake_sched.h:43: return __running_cpu;
    (<0.0>,12315) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12317) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12319) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12320) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12323)
    (<0.0>,12326) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12329) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12330) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12331) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12335) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12336) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12337) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12339) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12344) fake_sched.h:43: return __running_cpu;
    (<0.0>,12347) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12349) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12351) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12352) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12355)
    (<0.0>,12358) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12361) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12362) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12363) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12367) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12368) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12369) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12371) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12378) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12381) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12382) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12383) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12385) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12386) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12388) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12389) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12390) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12391) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12405)
    (<0.0>,12407) tree.c:758: local_irq_restore(flags);
    (<0.0>,12410)
    (<0.0>,12412) fake_sched.h:43: return __running_cpu;
    (<0.0>,12416) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12418) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12422) fake_sched.h:43: return __running_cpu;
    (<0.0>,12426) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12432) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12435) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12440) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12445) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12446) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12447) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12448) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12450) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12452) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12453) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12455) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12458) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12459) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12460) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12467) fake_sched.h:43: return __running_cpu;
    (<0.0>,12471)
    (<0.0>,12472) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12475) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12480) tree.c:892: local_irq_save(flags);
    (<0.0>,12483)
    (<0.0>,12485) fake_sched.h:43: return __running_cpu;
    (<0.0>,12489) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12491) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12495) fake_sched.h:43: return __running_cpu;
    (<0.0>,12499) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12511)
    (<0.0>,12513) fake_sched.h:43: return __running_cpu;
    (<0.0>,12517) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12518) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12520) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12521) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12522) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12523) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12524) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12525) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12526) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,12530) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,12532) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,12533) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,12534) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,12545)
    (<0.0>,12546)
    (<0.0>,12548) fake_sched.h:43: return __running_cpu;
    (<0.0>,12552) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12556) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12559) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12560) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12561) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12563) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12564) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12566) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12567) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12568) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12569) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12579)
    (<0.0>,12581) tree.c:895: local_irq_restore(flags);
    (<0.0>,12584)
    (<0.0>,12586) fake_sched.h:43: return __running_cpu;
    (<0.0>,12590) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12592) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12596) fake_sched.h:43: return __running_cpu;
    (<0.0>,12600) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12607) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,12609) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,12610) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,12655)
    (<0.0>,12656) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12659)
    (<0.0>,12660) tree.c:625: return &rsp->node[0];
    (<0.0>,12664) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12666) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12667) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12668) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12673) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12674) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12675) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12676) fake_defs.h:237: switch (size) {
    (<0.0>,12678) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12680) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12681) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12683) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12686) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12687) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12688) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12689) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,12692)
    (<0.0>,12693) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12697)
    (<0.0>,12700) fake_sched.h:43: return __running_cpu;
    (<0.0>,12704) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12708) fake_sched.h:43: return __running_cpu;
    (<0.0>,12712) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12717) fake_sched.h:43: return __running_cpu;
    (<0.0>,12721) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12724) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12725) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12732) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12737) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12738) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12739) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12740) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12742) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12744) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12745) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12747) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12750) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12751) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12752) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12763)
    (<0.0>,12769)
    (<0.0>,12774) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12779) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12780) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12781) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12782) fake_defs.h:237: switch (size) {
    (<0.0>,12784) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12786) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12787) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12789) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12792) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12793) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12794) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12795) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12808)
    (<0.0>,12809) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12814) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12815) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12816) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12817) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12819) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12821) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12822) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12824) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12827) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12828) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12829) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12830) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12835) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12836) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12837) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12838) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12840) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12842) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12843) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12845) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12849) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12850) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12858) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12859) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12872)
    (<0.0>,12873) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12878) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12879) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12880) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12881) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12883) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12885) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12886) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12888) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12891) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12892) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12893) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12894) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12899) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12900) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12901) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12902) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12904) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12906) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12907) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12909) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12912) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12913) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12914) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12921) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12922) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12923) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12926) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,12941)
    (<0.0>,12942) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,12943) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,12944) tree.c:1240: rsp->gp_start = j;
    (<0.0>,12945) tree.c:1240: rsp->gp_start = j;
    (<0.0>,12947) tree.c:1240: rsp->gp_start = j;
    (<0.0>,12968) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12969) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12970) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12971) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12973) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12975) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12976) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12978) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12981) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12982) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12983) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12984) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12985) update.c:466: if (till_stall_check < 3) {
    (<0.0>,12988) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,12992) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,12997) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,12999) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13000) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13002) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13003) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13008) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13009) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13010) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13011) fake_defs.h:237: switch (size) {
    (<0.0>,13013) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13015) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13016) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13018) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13021) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13022) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13023) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13024) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13025) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13028) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13030) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13031) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13036) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13037) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13038) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13039) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13041) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13043) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13044) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13046) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13049) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13050) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13051) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13052) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13054) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13058) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13060) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13062) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13063) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13065) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13066) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13067) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13071) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,13074)
    (<0.0>,13075) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13079)
    (<0.0>,13080) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13081) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13086) fake_sched.h:43: return __running_cpu;
    (<0.0>,13090) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,13092) fake_sched.h:43: return __running_cpu;
    (<0.0>,13096) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13103) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13106) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13109) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13110) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13112) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13113) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13115) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13120) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,13121) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,13125)
    (<0.0>,13126)
    (<0.0>,13127) tree.c:1922: if (delay > 0 &&
    (<0.0>,13131) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,13134)
    (<0.0>,13135) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13139)
    (<0.0>,13142) fake_sched.h:43: return __running_cpu;
    (<0.0>,13146) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,13150) fake_sched.h:43: return __running_cpu;
    (<0.0>,13154) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13159) fake_sched.h:43: return __running_cpu;
    (<0.0>,13163) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,13166) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13167) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13174) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13176) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13177) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13179) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13182) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,13184) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,13187) tree.c:1973: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,13190)
    (<0.0>,13191) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13195)
    (<0.0>,13196) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13197) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13202) fake_sched.h:43: return __running_cpu;
    (<0.0>,13206) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,13208) fake_sched.h:43: return __running_cpu;
    (<0.0>,13212) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13220) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13222) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13224) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13225) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13227) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13232) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13235) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13237) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13238) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13240) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13245) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,13246) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,13250)
    (<0.0>,13251)
    (<0.0>,13252) tree.c:1922: if (delay > 0 &&
    (<0.0>,13256) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,13259)
    (<0.0>,13260) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13264)
    (<0.0>,13267) fake_sched.h:43: return __running_cpu;
    (<0.0>,13271) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,13275) fake_sched.h:43: return __running_cpu;
    (<0.0>,13279) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13284) fake_sched.h:43: return __running_cpu;
    (<0.0>,13288) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,13291) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13292) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13300) fake_sched.h:43: return __running_cpu;
    (<0.0>,13303) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13305) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13307) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13308) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,13314)
    (<0.0>,13315) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13317) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13322) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13323) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13325) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13329) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13330) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13331) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13333) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13335) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13336) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13338) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13340) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13342) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13343) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13344) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13349) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13350) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13351) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13352) fake_defs.h:237: switch (size) {
    (<0.0>,13354) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13356) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13357) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13359) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13362) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13363) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13364) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13365) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13367) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13368) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13370) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13375) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13376) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13378) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13379) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13381) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13385) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13386) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13387) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13390) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13391) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13393) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13396) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13397) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13398) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13420)
    (<0.0>,13421)
    (<0.0>,13422)
    (<0.0>,13423) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13425) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13426) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13428) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13431) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13435) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13436) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13437) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13438) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13440) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13441) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13442) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13443) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13446) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13449) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13450) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13458) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13459) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13460) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13469)
    (<0.0>,13470)
    (<0.0>,13471)
    (<0.0>,13472) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13475) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13478) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13481) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13482) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13485) tree.c:1750: return false;
    (<0.0>,13487) tree.c:1799: }
    (<0.0>,13490) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13492) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13494) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13495) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13497) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13500) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13502) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13503) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13505) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13508) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13510) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13511) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13513) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13519) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13520) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13523) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13527) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13529) fake_sched.h:43: return __running_cpu;
    (<0.0>,13533) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13534) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13536) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13537) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13539) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13542) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13543) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,13545) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,13547) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,13549) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,13551) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,13552) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,13555)
    (<0.0>,13556) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,13558) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,13559) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,13561) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,13571)
    (<0.0>,13576) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13580) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13581) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13582) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13583) fake_defs.h:237: switch (size) {
    (<0.0>,13585) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13586) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13587) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13588) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13591) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13594) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13595) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13598) tree.c:1896: return ret;
    (<0.0>,13602) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,13605)
    (<0.0>,13609) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,13612)
    (<0.0>,13613) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13617)
    (<0.0>,13618) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13619) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13624) fake_sched.h:43: return __running_cpu;
    (<0.0>,13628) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,13630) fake_sched.h:43: return __running_cpu;
    (<0.0>,13634) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13649) fake_sched.h:43: return __running_cpu;
    (<0.0>,13655) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,13661) fake_sched.h:43: return __running_cpu;
    (<0.0>,13668) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,13670) fake_sched.h:43: return __running_cpu;
    (<0.0>,13677) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
    (<0.0>,13685) fake_sched.h:43: return __running_cpu;
    (<0.0>,13689) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,13702) fake_sched.h:43: return __running_cpu;
    (<0.0>,13706)
    (<0.0>,13709) tree.c:755: local_irq_save(flags);
    (<0.0>,13712)
    (<0.0>,13714) fake_sched.h:43: return __running_cpu;
    (<0.0>,13718) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13720) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13724) fake_sched.h:43: return __running_cpu;
    (<0.0>,13728) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13740)
    (<0.0>,13742) fake_sched.h:43: return __running_cpu;
    (<0.0>,13746) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13747) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13749) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13750) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13751) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13752) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13753) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13754) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13755) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,13759) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,13761) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,13762) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,13763) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,13779)
    (<0.0>,13781)
    (<0.0>,13783) fake_sched.h:43: return __running_cpu;
    (<0.0>,13787) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13790) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13791) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13792) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13796) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13797) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13798) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13800) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13805) fake_sched.h:43: return __running_cpu;
    (<0.0>,13808) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13810) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13812) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13813) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,13816)
    (<0.0>,13819) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13822) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13823) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13824) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13828) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13829) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13830) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13832) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13837) fake_sched.h:43: return __running_cpu;
    (<0.0>,13840) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13842) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13844) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13845) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,13848)
    (<0.0>,13851) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13854) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13855) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13856) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13860) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13861) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13862) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13864) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13871) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13874) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13875) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13876) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13878) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13879) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13881) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13882) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13883) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13884) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13898)
    (<0.0>,13900) tree.c:758: local_irq_restore(flags);
    (<0.0>,13903)
    (<0.0>,13905) fake_sched.h:43: return __running_cpu;
    (<0.0>,13909) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13911) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13915) fake_sched.h:43: return __running_cpu;
    (<0.0>,13919) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13925) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,13928) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,13933) fake_sched.h:43: return __running_cpu;
    (<0.0>,13937)
    (<0.0>,13938) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,13941) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,13946) tree.c:892: local_irq_save(flags);
    (<0.0>,13949)
    (<0.0>,13951) fake_sched.h:43: return __running_cpu;
    (<0.0>,13955) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13957) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13961) fake_sched.h:43: return __running_cpu;
    (<0.0>,13965) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13977)
    (<0.0>,13979) fake_sched.h:43: return __running_cpu;
    (<0.0>,13983) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13984) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,13986) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,13987) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,13988) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,13989) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,13990) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,13991) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,13992) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,13996) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,13998) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,13999) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,14000) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,14011)
    (<0.0>,14012)
    (<0.0>,14014) fake_sched.h:43: return __running_cpu;
    (<0.0>,14018) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14022) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14025) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14026) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14027) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14029) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14030) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14032) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14033) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14034) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14035) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14045)
    (<0.0>,14047) tree.c:895: local_irq_restore(flags);
    (<0.0>,14050)
    (<0.0>,14052) fake_sched.h:43: return __running_cpu;
    (<0.0>,14056) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14058) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14062) fake_sched.h:43: return __running_cpu;
    (<0.0>,14066) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14080) fake_sched.h:43: return __running_cpu;
    (<0.0>,14084) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,14093) fake_sched.h:43: return __running_cpu;
    (<0.0>,14100) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,14109) fake_sched.h:43: return __running_cpu;
    (<0.0>,14113) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,14115) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,14121) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14122) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14123) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14128) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14129) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14130) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14131) fake_defs.h:237: switch (size) {
    (<0.0>,14133) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14135) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14136) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14138) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14141) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14142) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14143) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14145) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14147) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14149) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14150) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14152) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14157) tree.c:2046: return true;
    (<0.0>,14159) tree.c:2047: }
    (<0.0>,14163) tree.c:2214: first_gp_fqs = true;
    (<0.0>,14164) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,14165) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,14166) tree.c:2216: if (j > HZ) {
    (<0.0>,14169) tree.c:2220: ret = 0;
    (<0.0>,14171) tree.c:2222: if (!ret) {
    (<0.0>,14174) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14175) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14177) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14179) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14181) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14182) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14185) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14186) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14191) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14192) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14193) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14194) fake_defs.h:237: switch (size) {
    (<0.0>,14196) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14198) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14199) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14201) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14204) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14205) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14206) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14210) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,14212) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,14216) fake_sched.h:43: return __running_cpu;
    (<0.0>,14220) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,14224) fake_sched.h:43: return __running_cpu;
    (<0.0>,14228) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14233) fake_sched.h:43: return __running_cpu;
    (<0.0>,14237) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,14248) fake_sched.h:43: return __running_cpu;
    (<0.0>,14252) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14253) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,14255) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,14256) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,14257) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,14259) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,14261) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,14262) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14263) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14264) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14265) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14266) tree.c:942: if (oldval)
    (<0.0>,14274)
    (<0.0>,14280)
    (<0.0>,14289) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14290) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14291) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14295) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14296) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14297) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14299) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14304) fake_sched.h:43: return __running_cpu;
    (<0.0>,14307) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14309) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14312) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14314) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14316) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14319) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14320) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14321) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14325) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14326) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14327) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14329) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14334) fake_sched.h:43: return __running_cpu;
    (<0.0>,14337) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14339) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14342) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14344) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14346) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14349) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14350) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14351) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14355) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14356) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14357) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14359) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14364) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,14369) fake_sched.h:43: return __running_cpu;
    (<0.0>,14374) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,14382) fake_sched.h:43: return __running_cpu;
    (<0.0>,14388) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,14402) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14403) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14404) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14408) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14409) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14410) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14412) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14416) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14418) fake_sched.h:43: return __running_cpu;
    (<0.0>,14421) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14423) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14445)
    (<0.0>,14446)
    (<0.0>,14447) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14449) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14450) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14451) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14453) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14455) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14456) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,14457) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,14492)
    (<0.0>,14493)
    (<0.0>,14494) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,14497) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,14510)
    (<0.0>,14511) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14516) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14517) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14518) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14519) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14521) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14523) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14524) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14526) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14529) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14530) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14531) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14532) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14537) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14538) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14539) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14540) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14542) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14544) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14545) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14547) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14550) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14551) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14552) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14558) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,14583)
    (<0.0>,14584) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,14589) tree.c:1472: j = jiffies;
    (<0.0>,14590) tree.c:1472: j = jiffies;
    (<0.0>,14591) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14596) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14597) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14598) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14599) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14601) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14603) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14604) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14606) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14609) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14610) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14611) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14612) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14614) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14619) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14620) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14621) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14622) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14624) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14626) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14627) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14629) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14632) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14633) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14634) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14635) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14637) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14642) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14643) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14644) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14645) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14647) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14649) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14650) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14652) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14655) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14656) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14657) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14658) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14660) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14665) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14666) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14667) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14668) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14670) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14672) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14673) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14675) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14678) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14679) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14680) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14681) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14682) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,14683) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,14687) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,14688) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,14694) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,14697)
    (<0.0>,14700) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,14703) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14705) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14708) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14712) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14716) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14718) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14721) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,14725) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,14728) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14730) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14732) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14733) tree.c:3511: return 1;
    (<0.0>,14735) tree.c:3548: }
    (<0.0>,14739) tree.c:3561: return 1;
    (<0.0>,14741) tree.c:3563: }
    (<0.0>,14748) fake_sched.h:43: return __running_cpu;
    (<0.0>,14752) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,14756) tree.c:2891: if (user)
    (<0.0>,14764) fake_sched.h:43: return __running_cpu;
    (<0.0>,14768) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,14770) fake_sched.h:43: return __running_cpu;
    (<0.0>,14774) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14780) fake_sched.h:43: return __running_cpu;
    (<0.0>,14784) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,14794)
    (<0.0>,14797) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14798) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14799) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14803) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14804) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14805) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14807) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14811) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,14823)
    (<0.0>,14825) fake_sched.h:43: return __running_cpu;
    (<0.0>,14828) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,14830) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,14832) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,14833) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14835) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14842) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14843) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14845) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14851) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14852) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14853) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14854) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,14855) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,14859)
    (<0.0>,14860)
    (<0.0>,14861) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,14862) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,14887)
    (<0.0>,14888)
    (<0.0>,14889) tree.c:1905: local_irq_save(flags);
    (<0.0>,14892)
    (<0.0>,14894) fake_sched.h:43: return __running_cpu;
    (<0.0>,14898) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14900) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14904) fake_sched.h:43: return __running_cpu;
    (<0.0>,14908) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14913) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,14915) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,14916) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,14917) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14919) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14920) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14925) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14926) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14927) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14930) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14932) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14933) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14935) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14938) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14939) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14940) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14943) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14945) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14946) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14951) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14952) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14953) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14954) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14956) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14959) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14964) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14965) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14966) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14969) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14973) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14974) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14975) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14976) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14978) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14979) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14980) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14981) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14984) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14987) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14988) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14996) tree.c:1911: local_irq_restore(flags);
    (<0.0>,14999)
    (<0.0>,15001) fake_sched.h:43: return __running_cpu;
    (<0.0>,15005) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15007) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15011) fake_sched.h:43: return __running_cpu;
    (<0.0>,15015) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15022) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15024) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15027) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,15031) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,15035) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15037) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15038) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15039) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15057)
    (<0.0>,15058)
    (<0.0>,15059)
    (<0.0>,15060) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,15062) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,15063) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,15067) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15068) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15069) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15071) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15075)
    (<0.0>,15076)
    (<0.0>,15077) fake_sync.h:83: local_irq_save(flags);
    (<0.0>,15080)
    (<0.0>,15082) fake_sched.h:43: return __running_cpu;
    (<0.0>,15086) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15088) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15092) fake_sched.h:43: return __running_cpu;
    (<0.0>,15096) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15102) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,15103) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,15110) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,15114) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,15118) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15120) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15121) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15123) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15126) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15128) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15129) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15131) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15134) tree.c:2491: rdp->gpwrap) {
    (<0.0>,15136) tree.c:2491: rdp->gpwrap) {
    (<0.0>,15139) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,15141) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,15142) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,15143) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,15145) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,15146) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,15153) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15154) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15155) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15157) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15161)
    (<0.0>,15162)
    (<0.0>,15163) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,15164) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,15167) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,15170)
    (<0.0>,15172) fake_sched.h:43: return __running_cpu;
    (<0.0>,15176) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15178) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15182) fake_sched.h:43: return __running_cpu;
    (<0.0>,15186) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15198) tree.c:3016: local_irq_save(flags);
    (<0.0>,15201)
    (<0.0>,15203) fake_sched.h:43: return __running_cpu;
    (<0.0>,15207) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15209) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15213) fake_sched.h:43: return __running_cpu;
    (<0.0>,15217) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15222) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15223) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15234)
    (<0.0>,15235)
    (<0.0>,15236) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,15249)
    (<0.0>,15250) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15255) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15256) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15257) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15258) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15260) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15262) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15263) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15265) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15268) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15269) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15270) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15271) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15276) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15277) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15278) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15279) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15281) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15283) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15284) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15286) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15289) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15290) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15291) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15297) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,15299) tree.c:666: }
    (<0.0>,15302) tree.c:3024: local_irq_restore(flags);
    (<0.0>,15305)
    (<0.0>,15307) fake_sched.h:43: return __running_cpu;
    (<0.0>,15311) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15313) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15317) fake_sched.h:43: return __running_cpu;
    (<0.0>,15321) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15327) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,15330)
    (<0.0>,15331) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15333) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15336) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15343) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15346)
    (<0.0>,15350) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15353) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15354) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15355) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15359) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15360) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15361) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15363) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15367) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,15379)
    (<0.0>,15381) fake_sched.h:43: return __running_cpu;
    (<0.0>,15384) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15386) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15388) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15389) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15391) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15398) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15399) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15401) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15407) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15408) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15409) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15410) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,15411) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,15415)
    (<0.0>,15416)
    (<0.0>,15417) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,15418) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,15443)
    (<0.0>,15444)
    (<0.0>,15445) tree.c:1905: local_irq_save(flags);
    (<0.0>,15448)
    (<0.0>,15450) fake_sched.h:43: return __running_cpu;
    (<0.0>,15454) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15456) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15460) fake_sched.h:43: return __running_cpu;
    (<0.0>,15464) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15469) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15471) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15472) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15473) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15475) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15476) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15481) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15482) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15483) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15484) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15486) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15488) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15489) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15491) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15494) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15495) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15496) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15499) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15501) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15502) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15507) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15508) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15509) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15510) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15512) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15514) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15515) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15517) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15520) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15521) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15522) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15525) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15529) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15530) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15531) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15532) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15534) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15535) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15536) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15537) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15540) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15543) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15544) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15552) tree.c:1911: local_irq_restore(flags);
    (<0.0>,15555)
    (<0.0>,15557) fake_sched.h:43: return __running_cpu;
    (<0.0>,15561) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15563) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15567) fake_sched.h:43: return __running_cpu;
    (<0.0>,15571) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15578) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15580) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15585) tree.c:3016: local_irq_save(flags);
    (<0.0>,15588)
    (<0.0>,15590) fake_sched.h:43: return __running_cpu;
    (<0.0>,15594) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15596) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15600) fake_sched.h:43: return __running_cpu;
    (<0.0>,15604) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15609) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15610) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15621)
    (<0.0>,15622)
    (<0.0>,15623) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,15636)
    (<0.0>,15637) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15642) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15643) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15644) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15645) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15647) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15649) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15650) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15652) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15655) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15656) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15657) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15658) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15663) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15664) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15665) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15666) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15668) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15670) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15671) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15673) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15676) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15677) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15678) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15684) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,15700)
    (<0.0>,15701) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15704)
    (<0.0>,15705) tree.c:625: return &rsp->node[0];
    (<0.0>,15709) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15710) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15715) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15716) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15717) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15718) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15720) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15722) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15723) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15725) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15728) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15729) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15730) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15734) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15735) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,15737) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,15740) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,15741) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15745) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15746) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15747) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15748) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15750) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15752) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15753) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15755) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15758) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15759) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15760) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15764) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,15767) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,15770) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,15773) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,15774) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,15777) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15779) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15782) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15785) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15788) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15789) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15791) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15794) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15798) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15800) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15802) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15805) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15808) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15811) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15812) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15814) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15817) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15821) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15823) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15825) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15828) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,15830) tree.c:666: }
    (<0.0>,15833) tree.c:3024: local_irq_restore(flags);
    (<0.0>,15836)
    (<0.0>,15838) fake_sched.h:43: return __running_cpu;
    (<0.0>,15842) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15844) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15848) fake_sched.h:43: return __running_cpu;
    (<0.0>,15852) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15858) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,15861)
    (<0.0>,15862) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15864) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15867) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15874) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15877)
    (<0.0>,15881) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15884) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15885) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15886) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15890) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15891) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15892) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15894) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15902) fake_sched.h:43: return __running_cpu;
    (<0.0>,15906) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,15916) fake_sched.h:43: return __running_cpu;
    (<0.0>,15920) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15921) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,15923) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,15924) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,15925) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,15927) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,15929) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,15930) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15931) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15932) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15933) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15934) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,15936) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,15944)
    (<0.0>,15950) fake_sched.h:43: return __running_cpu;
    (<0.0>,15954)
    (<0.0>,15957) tree.c:755: local_irq_save(flags);
    (<0.0>,15960)
    (<0.0>,15962) fake_sched.h:43: return __running_cpu;
    (<0.0>,15966) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15968) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15972) fake_sched.h:43: return __running_cpu;
    (<0.0>,15976) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15988)
    (<0.0>,15990) fake_sched.h:43: return __running_cpu;
    (<0.0>,15994) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15995) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15997) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15998) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15999) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16000) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16001) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16002) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16003) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,16007) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,16009) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,16010) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,16011) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,16027)
    (<0.0>,16029)
    (<0.0>,16031) fake_sched.h:43: return __running_cpu;
    (<0.0>,16035) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,16038) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16039) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16040) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16044) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16045) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16046) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16048) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16053) fake_sched.h:43: return __running_cpu;
    (<0.0>,16056) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16058) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16060) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16061) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,16064)
    (<0.0>,16067) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16070) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16071) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16072) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16076) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16077) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16078) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16080) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16085) fake_sched.h:43: return __running_cpu;
    (<0.0>,16088) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16090) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16092) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16093) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,16096)
    (<0.0>,16099) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16102) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16103) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16104) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16108) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16109) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16110) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16112) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16119) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16122) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16123) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16124) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16126) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16127) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16129) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16130) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16131) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16132) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16146)
    (<0.0>,16148) tree.c:758: local_irq_restore(flags);
    (<0.0>,16151)
    (<0.0>,16153) fake_sched.h:43: return __running_cpu;
    (<0.0>,16157) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,16159) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,16163) fake_sched.h:43: return __running_cpu;
    (<0.0>,16167) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,16173) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,16176) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,16181) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16197)
    (<0.0>,16198)
    (<0.0>,16199) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16202)
    (<0.0>,16203) tree.c:625: return &rsp->node[0];
    (<0.0>,16207) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16208) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16213) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16214) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16215) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16216) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16218) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16220) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16221) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16223) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16226) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16227) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16228) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16230) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16231) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16232) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16233) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16237) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16242) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16243) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16244) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16245) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16247) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16249) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16250) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16252) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16255) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16256) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16257) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16260) tree.c:2066: return false;
    (<0.0>,16262) tree.c:2067: }
    (<0.0>,16267) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16283)
    (<0.0>,16284)
    (<0.0>,16285) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16288)
    (<0.0>,16289) tree.c:625: return &rsp->node[0];
    (<0.0>,16293) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16294) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16299) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16300) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16301) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16302) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16304) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16306) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16307) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16309) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16312) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16313) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16314) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16316) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16317) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16318) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16319) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16323) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16328) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16329) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16330) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16331) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16333) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16335) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16336) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16338) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16341) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16342) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16343) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16346) tree.c:2066: return false;
    (<0.0>,16348) tree.c:2067: }
    (<0.0>,16353) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16369)
    (<0.0>,16370)
    (<0.0>,16371) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16374)
    (<0.0>,16375) tree.c:625: return &rsp->node[0];
    (<0.0>,16379) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16380) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16385) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16386) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16387) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16388) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16390) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16392) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16393) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16395) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16398) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16399) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16400) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16402) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16403) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16404) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16405) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16409) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16414) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16415) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16416) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16417) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16419) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16421) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16422) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16424) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16427) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16428) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16429) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16432) tree.c:2066: return false;
    (<0.0>,16434) tree.c:2067: }
    (<0.0>,16439) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16455)
    (<0.0>,16456)
    (<0.0>,16457) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16460)
    (<0.0>,16461) tree.c:625: return &rsp->node[0];
    (<0.0>,16465) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16466) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16471) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16472) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16473) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16474) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16476) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16478) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16479) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16481) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16484) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16485) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16486) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16488) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16489) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16490) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16491) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16495) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16500) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16501) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16502) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16503) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16505) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16507) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16508) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16510) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16513) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16514) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16515) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16518) tree.c:2066: return false;
    (<0.0>,16520) tree.c:2067: }
    (<0.0>,16525) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16541)
    (<0.0>,16542)
    (<0.0>,16543) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16546)
    (<0.0>,16547) tree.c:625: return &rsp->node[0];
    (<0.0>,16551) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16552) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16557) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16558) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16559) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16560) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16562) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16564) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16565) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16567) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16570) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16571) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16572) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16574) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16575) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16576) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16577) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16581) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16586) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16587) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16588) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16589) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16591) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16593) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16594) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16596) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16599) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16600) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16601) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16604) tree.c:2066: return false;
    (<0.0>,16606) tree.c:2067: }
      (<0.1>,769) fake_sync.h:281: while (!x->done)
      (<0.1>,776) fake_sched.h:43: return __running_cpu;
      (<0.1>,780)
      (<0.1>,781) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,784) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,789) tree.c:892: local_irq_save(flags);
      (<0.1>,792)
      (<0.1>,794) fake_sched.h:43: return __running_cpu;
      (<0.1>,798) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,804) fake_sched.h:43: return __running_cpu;
      (<0.1>,808) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,820)
      (<0.1>,822) fake_sched.h:43: return __running_cpu;
      (<0.1>,826) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,827) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,829) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,830) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,831) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,832) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,833) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,834) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,835) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,839) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,841) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,842) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,843) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,854)
      (<0.1>,855)
      (<0.1>,857) fake_sched.h:43: return __running_cpu;
      (<0.1>,861) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,865) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,868) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,869) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,870) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,872) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,873) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,875) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,876) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,877) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,878) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,888)
      (<0.1>,890) tree.c:895: local_irq_restore(flags);
      (<0.1>,893)
      (<0.1>,895) fake_sched.h:43: return __running_cpu;
      (<0.1>,899) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,905) fake_sched.h:43: return __running_cpu;
      (<0.1>,909) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,917) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,919) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,924)
      (<0.1>,927) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,929) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,931) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,932) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,940) litmus.c:91: y = 1;
      (<0.1>,942) fake_sched.h:43: return __running_cpu;
      (<0.1>,946)
      (<0.1>,949) tree.c:755: local_irq_save(flags);
      (<0.1>,952)
      (<0.1>,954) fake_sched.h:43: return __running_cpu;
      (<0.1>,958) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,960) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,964) fake_sched.h:43: return __running_cpu;
      (<0.1>,968) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,980)
      (<0.1>,982) fake_sched.h:43: return __running_cpu;
      (<0.1>,986) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,987) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,989) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,990) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,991) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,992) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,993) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,994) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,995) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,999) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1001) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1002) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1003) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1019)
      (<0.1>,1021)
      (<0.1>,1023) fake_sched.h:43: return __running_cpu;
      (<0.1>,1027) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,1030) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1031) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1037) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1038) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1040) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1045) fake_sched.h:43: return __running_cpu;
      (<0.1>,1048) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1050) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1052) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1053) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1056)
      (<0.1>,1059) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1062) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1063) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1069) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1070) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1072) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1077) fake_sched.h:43: return __running_cpu;
      (<0.1>,1080) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1082) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1084) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1085) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1088)
      (<0.1>,1091) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1094) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1095) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1101) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1102) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1104) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1111) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1116) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1118) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1119) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1121) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1122) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1123) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1124) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1138)
      (<0.1>,1140) tree.c:758: local_irq_restore(flags);
      (<0.1>,1143)
      (<0.1>,1145) fake_sched.h:43: return __running_cpu;
      (<0.1>,1149) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1151) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1155) fake_sched.h:43: return __running_cpu;
      (<0.1>,1159) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1165) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1168) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8059) litmus.c:69: r_y = y;
  (<0>,8060) litmus.c:69: r_y = y;
  (<0>,8070) fake_sched.h:43: return __running_cpu;
  (<0>,8074)
  (<0>,8077) tree.c:755: local_irq_save(flags);
  (<0>,8080)
  (<0>,8082) fake_sched.h:43: return __running_cpu;
  (<0>,8086) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8088) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8092) fake_sched.h:43: return __running_cpu;
  (<0>,8096) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8108)
  (<0>,8110) fake_sched.h:43: return __running_cpu;
  (<0>,8114) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8115) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8117) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8118) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8119) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8120) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8121) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8122) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8123) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,8127) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,8129) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,8130) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,8131) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,8147)
  (<0>,8149)
  (<0>,8151) fake_sched.h:43: return __running_cpu;
  (<0>,8155) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8158) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8159) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8160) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8164) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8165) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8166) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8168) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8173) fake_sched.h:43: return __running_cpu;
  (<0>,8176) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8178) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8180) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8181) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,8184)
  (<0>,8187) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8190) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8191) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8192) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8196) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8197) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8198) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8200) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8205) fake_sched.h:43: return __running_cpu;
  (<0>,8208) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8210) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8212) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8213) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,8216)
  (<0>,8219) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8222) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8223) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8224) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8228) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8229) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8230) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8232) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8239) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8242) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8243) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8244) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8246) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8247) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8249) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8250) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8251) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8252) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8266)
  (<0>,8268) tree.c:758: local_irq_restore(flags);
  (<0>,8271)
  (<0>,8273) fake_sched.h:43: return __running_cpu;
  (<0>,8277) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8279) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8283) fake_sched.h:43: return __running_cpu;
  (<0>,8287) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8293) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8296) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8301) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,8305) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8308) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8312) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8313) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8314) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
             Error: Assertion violation at (<0>,8317): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpx5mt6wbq/tmp98offx0y.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DFORCE_FAILURE_5 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpx5mt6wbq/tmpcvi9mxux.ll /tmp/tmpx5mt6wbq/tmp98offx0y.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=4 --preemption-bounding=PB --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpx5mt6wbq/tmpcvi9mxux.ll
Total wall-clock time: 3.92 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 388 (also 16 sleepset blocked, 0 schedulings and 161 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpe7mehbz3/tmp_ue9gv_y.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DLIVENESS_CHECK_1 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpe7mehbz3/tmpcrtz_ny5.ll /tmp/tmpe7mehbz3/tmp_ue9gv_y.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpe7mehbz3/tmpcrtz_ny5.ll
Total wall-clock time: 13.58 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 566 (also 16 sleepset blocked, 0 schedulings and 182 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpz2nznnux/tmp6edi9u5f.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DLIVENESS_CHECK_2 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpz2nznnux/tmp196eowph.ll /tmp/tmpz2nznnux/tmp6edi9u5f.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpz2nznnux/tmp196eowph.ll
Total wall-clock time: 20.28 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 490 (also 16 sleepset blocked, 0 schedulings and 182 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmphi9wud9w/tmpp9tlgpic.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DLIVENESS_CHECK_3 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmphi9wud9w/tmpvi698vlj.ll /tmp/tmphi9wud9w/tmpp9tlgpic.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=PB --bound=4 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmphi9wud9w/tmpvi698vlj.ll
Total wall-clock time: 17.32 s
--------------------------------------------------------------------
---  Verification proceeded as expected
--------------------------------------------------------------------
