--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 823 (also 71 sleepset blocked, 0 schedulings and 768 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpfnk1e6mw/tmp_l_r139w.ll -S -emit-llvm -g -I v3.0 -std=gnu99 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpfnk1e6mw/tmp0fhzhmu2.ll /tmp/tmpfnk1e6mw/tmp_l_r139w.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpfnk1e6mw/tmp0fhzhmu2.ll
Total wall-clock time: 13.17 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 823 (also 71 sleepset blocked, 0 schedulings and 848 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp1xsnmo9r/tmphwxgbsoa.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DFORCE_FAILURE_1 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp1xsnmo9r/tmp0hesnzm2.ll /tmp/tmp1xsnmo9r/tmphwxgbsoa.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=3 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp1xsnmo9r/tmp0hesnzm2.ll
Total wall-clock time: 13.25 s
^^^ Unexpected verification success
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 286 (also 18 sleepset blocked, 0 schedulings and 204 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpnxumsxf1/tmpo6ddrf8q.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DFORCE_FAILURE_3 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpnxumsxf1/tmpcw3lzi4g.ll /tmp/tmpnxumsxf1/tmpo6ddrf8q.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpnxumsxf1/tmpcw3lzi4g.ll
Total wall-clock time: 4.33 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 597 (also 18 sleepset blocked, 0 schedulings and 393 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpe5f70hja/tmpg9t0q4uc.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DFORCE_FAILURE_5 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpe5f70hja/tmp28u9gsfs.ll /tmp/tmpe5f70hja/tmpg9t0q4uc.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpe5f70hja/tmp28u9gsfs.ll
Total wall-clock time: 8.54 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 260 (also 18 sleepset blocked, 0 schedulings and 180 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpaae0s_8h/tmppi_hsvig.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DLIVENESS_CHECK_1 -DASSERT_0 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpaae0s_8h/tmpm2aapilv.ll /tmp/tmpaae0s_8h/tmppi_hsvig.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpaae0s_8h/tmpm2aapilv.ll
Total wall-clock time: 3.99 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 338 (also 22 sleepset blocked, 0 schedulings and 281 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpkt9l0rfa/tmphq860bx8.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DLIVENESS_CHECK_2 -DASSERT_0 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpkt9l0rfa/tmph6xrdmnt.ll /tmp/tmpkt9l0rfa/tmphq860bx8.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpkt9l0rfa/tmph6xrdmnt.ll
Total wall-clock time: 5.16 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 303 (also 18 sleepset blocked, 0 schedulings and 227 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpy4swbh0m/tmpz3dfmoce.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DLIVENESS_CHECK_3 -DASSERT_0 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpy4swbh0m/tmpn44rti9f.ll /tmp/tmpy4swbh0m/tmpz3dfmoce.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpy4swbh0m/tmpn44rti9f.ll
Total wall-clock time: 4.44 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 272 (also 33 sleepset blocked, 0 schedulings and 320 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp4ar0ukxq/tmprexb7sy1.ll -S -emit-llvm -g -I v3.19 -std=gnu99 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp4ar0ukxq/tmp14ibgd5c.ll /tmp/tmp4ar0ukxq/tmprexb7sy1.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp4ar0ukxq/tmp14ibgd5c.ll
Total wall-clock time: 8.19 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 272 (also 33 sleepset blocked, 0 schedulings and 344 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp6wmzy94u/tmpvjh8u04_.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DFORCE_FAILURE_1 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp6wmzy94u/tmprn8yq66r.ll /tmp/tmp6wmzy94u/tmpvjh8u04_.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=3 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp6wmzy94u/tmprn8yq66r.ll
Total wall-clock time: 8.20 s
^^^ Unexpected verification success
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 478 (also 8 sleepset blocked, 0 schedulings and 437 branches were rejected due to the bound) 

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,73) tree_plugin.h:931: pr_info("Hierarchical RCU implementation.\n");
  (<0>,77) tree_plugin.h:91: if (rcu_fanout_leaf != CONFIG_RCU_FANOUT_LEAF)
  (<0>,89) tree.c:3718: ulong d;
  (<0>,90) tree.c:3722: int rcu_capacity[MAX_RCU_LVLS + 1];
  (<0>,91) tree.c:3732: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,94) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,95) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,97) tree.c:3734: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,100) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,101) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,103) tree.c:3738: if (rcu_fanout_leaf == CONFIG_RCU_FANOUT_LEAF &&
  (<0>,115)
  (<0>,116) tree.c:3628: static void __init rcu_init_one(struct rcu_state *rsp,
  (<0>,117) tree.c:3629: struct rcu_data __percpu *rda)
  (<0>,118) tree.c:3643: int i;
  (<0>,121) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,123) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,124) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,127) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,130) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,133) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,136) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,140) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,142) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,143) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,146) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,149) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,152) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,158)
  (<0>,159) tree.c:3610: static void __init rcu_init_levelspread(struct rcu_state *rsp)
  (<0>,160) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,162) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,164) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,169) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,172) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,173) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,174) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,175) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,183) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,186) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,187) tree.c:3620: cprv = ccur;
  (<0>,188) tree.c:3620: cprv = ccur;
  (<0>,190) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,192) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,194) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,198) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,199) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,201) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,202) tree.c:3661: fl_mask <<= 1;
  (<0>,206) tree.c:3661: fl_mask <<= 1;
  (<0>,207) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,209) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,211) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,219) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,221) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,223) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,224) tree.c:3667: rnp = rsp->level[i];
  (<0>,226) tree.c:3667: rnp = rsp->level[i];
  (<0>,229) tree.c:3667: rnp = rsp->level[i];
  (<0>,230) tree.c:3667: rnp = rsp->level[i];
  (<0>,231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,233) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,236) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,242) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,246)
  (<0>,247) fake_sync.h:68: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,248) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,254) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,258)
  (<0>,259) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,260) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,268) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,269) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,271) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,274) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,275) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,277) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,278) tree.c:3677: rnp->qsmask = 0;
  (<0>,280) tree.c:3677: rnp->qsmask = 0;
  (<0>,281) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,283) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,285) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,287) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,295) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,297) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,298) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,300) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,303) tree.c:3683: if (i == 0) {
  (<0>,306) tree.c:3684: rnp->grpnum = 0;
  (<0>,308) tree.c:3684: rnp->grpnum = 0;
  (<0>,309) tree.c:3685: rnp->grpmask = 0;
  (<0>,311) tree.c:3685: rnp->grpmask = 0;
  (<0>,312) tree.c:3686: rnp->parent = NULL;
  (<0>,314) tree.c:3686: rnp->parent = NULL;
  (<0>,316) tree.c:3693: rnp->level = i;
  (<0>,318) tree.c:3693: rnp->level = i;
  (<0>,320) tree.c:3693: rnp->level = i;
  (<0>,321) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,325)
  (<0>,326) fake_defs.h:273: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,327) fake_defs.h:275: list->next = list;
  (<0>,329) fake_defs.h:275: list->next = list;
  (<0>,330) fake_defs.h:276: list->prev = list;
  (<0>,331) fake_defs.h:276: list->prev = list;
  (<0>,333) fake_defs.h:276: list->prev = list;
  (<0>,335) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,338) tree_plugin.h:2694: }
  (<0>,341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,348) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,351) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,354) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,358) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,360) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,362) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,365) tree.c:3699: rsp->rda = rda;
  (<0>,366) tree.c:3699: rsp->rda = rda;
  (<0>,368) tree.c:3699: rsp->rda = rda;
  (<0>,371) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,374) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,377) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,378) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,379) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,381) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,382) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,389)
  (<0>,390) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,391) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,393) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,394) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,397) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,399) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,400) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,403) fake_defs.h:536: if (offset & mask)
  (<0>,404) fake_defs.h:536: if (offset & mask)
  (<0>,408) fake_defs.h:537: return cpu;
  (<0>,409) fake_defs.h:537: return cpu;
  (<0>,411) fake_defs.h:540: }
  (<0>,413) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,414) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,418) tree.c:3703: while (i > rnp->grphi)
  (<0>,419) tree.c:3703: while (i > rnp->grphi)
  (<0>,421) tree.c:3703: while (i > rnp->grphi)
  (<0>,424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,425) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,429) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,432) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,433) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,434) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,447)
  (<0>,448) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,449) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,451) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,453) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,455) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,456) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,459)
  (<0>,460) tree.c:451: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,464) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,465) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,467) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,471)
  (<0>,472) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,473) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,476) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,478) fake_sched.h:43: return __running_cpu;
  (<0>,482) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,484) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,488) fake_sched.h:43: return __running_cpu;
  (<0>,492) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,498) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,499) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,504) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,506) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,508) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,512) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,514) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,515) tree.c:3412: init_callback_list(rdp);
  (<0>,519)
  (<0>,520) tree.c:1223: static void init_callback_list(struct rcu_data *rdp)
  (<0>,523) tree_plugin.h:2732: return false;
  (<0>,526) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,528) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,529) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,531) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,534) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,538) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,541) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,543) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,545) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,547) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,550) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,554) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,557) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,559) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,561) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,563) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,566) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,570) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,573) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,575) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,577) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,579) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,582) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,586) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,589) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,591) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,593) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,595) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,599) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,601) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,602) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,604) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,608) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,610) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,611) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,613) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,621) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,623) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,629) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,630) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,631) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,634) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,642) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,645) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,652) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,653) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,654) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,655) tree.c:3418: rdp->cpu = cpu;
  (<0>,656) tree.c:3418: rdp->cpu = cpu;
  (<0>,658) tree.c:3418: rdp->cpu = cpu;
  (<0>,659) tree.c:3419: rdp->rsp = rsp;
  (<0>,660) tree.c:3419: rdp->rsp = rsp;
  (<0>,662) tree.c:3419: rdp->rsp = rsp;
  (<0>,663) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,666) tree_plugin.h:2711: }
  (<0>,668) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,670) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,674)
  (<0>,675) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,676) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,677) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,680) fake_sync.h:86: local_irq_restore(flags);
  (<0>,683) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,685) fake_sched.h:43: return __running_cpu;
  (<0>,689) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,691) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,695) fake_sched.h:43: return __running_cpu;
  (<0>,699) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,708) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,709) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,716)
  (<0>,717)
  (<0>,718) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,720) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,721) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,724) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,726) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,727) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,730) fake_defs.h:536: if (offset & mask)
  (<0>,731) fake_defs.h:536: if (offset & mask)
  (<0>,735) fake_defs.h:537: return cpu;
  (<0>,736) fake_defs.h:537: return cpu;
  (<0>,738) fake_defs.h:540: }
  (<0>,740) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,741) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,745) tree.c:3703: while (i > rnp->grphi)
  (<0>,746) tree.c:3703: while (i > rnp->grphi)
  (<0>,748) tree.c:3703: while (i > rnp->grphi)
  (<0>,751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,752) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,756) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,759) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,760) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,761) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,774)
  (<0>,775)
  (<0>,776) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,778) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,780) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,782) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,783) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,786)
  (<0>,787) tree.c:453: return &rsp->node[0];
  (<0>,791) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,792) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,794) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,798)
  (<0>,799)
  (<0>,800) fake_sync.h:76: local_irq_save(flags);
  (<0>,803) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,805) fake_sched.h:43: return __running_cpu;
  (<0>,809) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,811) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,815) fake_sched.h:43: return __running_cpu;
  (<0>,819) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,825) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,826) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,831) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,833) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,835) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,839) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,841) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,842) tree.c:3412: init_callback_list(rdp);
  (<0>,846)
  (<0>,847) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,850) tree_plugin.h:2732: return false;
  (<0>,853) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,855) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,856) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,858) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,861) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,865) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,868) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,870) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,872) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,874) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,877) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,881) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,884) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,886) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,888) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,890) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,893) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,897) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,900) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,902) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,904) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,906) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,909) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,913) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,916) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,918) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,920) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,922) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,926) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,928) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,929) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,931) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,935) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,937) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,938) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,940) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,948) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,950) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,956) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,957) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,958) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,961) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,969) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,972) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,979) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,980) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,981) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,982) tree.c:3418: rdp->cpu = cpu;
  (<0>,983) tree.c:3418: rdp->cpu = cpu;
  (<0>,985) tree.c:3418: rdp->cpu = cpu;
  (<0>,986) tree.c:3419: rdp->rsp = rsp;
  (<0>,987) tree.c:3419: rdp->rsp = rsp;
  (<0>,989) tree.c:3419: rdp->rsp = rsp;
  (<0>,990) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,993) tree_plugin.h:2711: }
  (<0>,995) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,997) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1001)
  (<0>,1002)
  (<0>,1003) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1004) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1007) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1010) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1012) fake_sched.h:43: return __running_cpu;
  (<0>,1016) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1018) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1022) fake_sched.h:43: return __running_cpu;
  (<0>,1026) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1035) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1036) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1043)
  (<0>,1044)
  (<0>,1045) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1047) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1048) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1051) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1053) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1054) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1057) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,1059) fake_defs.h:540: }
  (<0>,1061) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1062) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1065) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1070)
  (<0>,1071) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1072) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1073) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1074) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1076) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1081)
  (<0>,1082) fake_defs.h:279: static inline void __list_add(struct list_head *new,
  (<0>,1083) fake_defs.h:280: struct list_head *prev,
  (<0>,1084) fake_defs.h:281: struct list_head *next)
  (<0>,1085) fake_defs.h:283: next->prev = new;
  (<0>,1087) fake_defs.h:283: next->prev = new;
  (<0>,1088) fake_defs.h:284: new->next = next;
  (<0>,1089) fake_defs.h:284: new->next = next;
  (<0>,1091) fake_defs.h:284: new->next = next;
  (<0>,1092) fake_defs.h:285: new->prev = prev;
  (<0>,1093) fake_defs.h:285: new->prev = prev;
  (<0>,1095) fake_defs.h:285: new->prev = prev;
  (<0>,1096) fake_defs.h:286: prev->next = new;
  (<0>,1097) fake_defs.h:286: prev->next = new;
  (<0>,1099) fake_defs.h:286: prev->next = new;
  (<0>,1110)
  (<0>,1111)
  (<0>,1112) tree.c:3642: int cpustride = 1;
  (<0>,1113) tree.c:3650: if (rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1116) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1118) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1119) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1122) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1125) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1126) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1128) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1133) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1135) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1137) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1141) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1143) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1144) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1147) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,1153)
  (<0>,1154) tree.c:3616: cprv = nr_cpu_ids;
  (<0>,1155) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1157) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1159) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1162) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1164) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1168) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1169) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1170) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1173) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1176) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1182) tree.c:3620: cprv = ccur;
  (<0>,1183) tree.c:3620: cprv = ccur;
  (<0>,1185) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1187) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1189) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1193) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1194) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1196) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1197) tree.c:3661: fl_mask <<= 1;
  (<0>,1201) tree.c:3661: fl_mask <<= 1;
  (<0>,1202) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1204) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1206) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1209) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1211) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1218) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1219) tree.c:3667: rnp = rsp->level[i];
  (<0>,1221) tree.c:3667: rnp = rsp->level[i];
  (<0>,1224) tree.c:3667: rnp = rsp->level[i];
  (<0>,1225) tree.c:3667: rnp = rsp->level[i];
  (<0>,1226) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1228) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1229) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1237) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,1241)
  (<0>,1242) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1243) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1249) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1253)
  (<0>,1254) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1255) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1261) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1263) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1264) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1267) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1269) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1270) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1273) tree.c:3677: rnp->qsmask = 0;
  (<0>,1275) tree.c:3677: rnp->qsmask = 0;
  (<0>,1276) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1278) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1279) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1280) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1282) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1285) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1287) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1293) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1295) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1298) tree.c:3683: if (i == 0) {
  (<0>,1301) tree.c:3684: rnp->grpnum = 0;
  (<0>,1303) tree.c:3684: rnp->grpnum = 0;
  (<0>,1304) tree.c:3685: rnp->grpmask = 0;
  (<0>,1306) tree.c:3685: rnp->grpmask = 0;
  (<0>,1307) tree.c:3686: rnp->parent = NULL;
  (<0>,1309) tree.c:3686: rnp->parent = NULL;
  (<0>,1311) tree.c:3693: rnp->level = i;
  (<0>,1313) tree.c:3693: rnp->level = i;
  (<0>,1315) tree.c:3693: rnp->level = i;
  (<0>,1316) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1320)
  (<0>,1321) fake_defs.h:275: list->next = list;
  (<0>,1322) fake_defs.h:275: list->next = list;
  (<0>,1324) fake_defs.h:275: list->next = list;
  (<0>,1325) fake_defs.h:276: list->prev = list;
  (<0>,1326) fake_defs.h:276: list->prev = list;
  (<0>,1328) fake_defs.h:276: list->prev = list;
  (<0>,1330) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,1333) tree_plugin.h:2694: }
  (<0>,1336) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1338) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1339) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1353) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1355) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1357) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1360) tree.c:3699: rsp->rda = rda;
  (<0>,1361) tree.c:3699: rsp->rda = rda;
  (<0>,1363) tree.c:3699: rsp->rda = rda;
  (<0>,1366) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1369) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1372) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1373) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1374) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1376) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1377) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1384)
  (<0>,1385)
  (<0>,1386) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1388) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1389) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1392) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1394) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1395) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1398) fake_defs.h:536: if (offset & mask)
  (<0>,1399) fake_defs.h:536: if (offset & mask)
  (<0>,1403) fake_defs.h:537: return cpu;
  (<0>,1404) fake_defs.h:537: return cpu;
  (<0>,1406) fake_defs.h:540: }
  (<0>,1408) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1409) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1413) tree.c:3703: while (i > rnp->grphi)
  (<0>,1414) tree.c:3703: while (i > rnp->grphi)
  (<0>,1416) tree.c:3703: while (i > rnp->grphi)
  (<0>,1419) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1420) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1422) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1428) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1429) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1442)
  (<0>,1443)
  (<0>,1444) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1446) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1448) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1450) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1451) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1454)
  (<0>,1455) tree.c:453: return &rsp->node[0];
  (<0>,1459) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1460) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1462) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1466)
  (<0>,1467)
  (<0>,1468) fake_sync.h:76: local_irq_save(flags);
  (<0>,1471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1473) fake_sched.h:43: return __running_cpu;
  (<0>,1477) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1479) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1483) fake_sched.h:43: return __running_cpu;
  (<0>,1487) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1493) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1494) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1498) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1499) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1501) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1507) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1509) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1510) tree.c:3412: init_callback_list(rdp);
  (<0>,1514)
  (<0>,1515) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1518) tree_plugin.h:2732: return false;
  (<0>,1521) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1523) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1529) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1531) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1533) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1538) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1540) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1542) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1545) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1547) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1549) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1554) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1556) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1558) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1561) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1563) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1565) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1570) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1572) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1574) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1577) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1579) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1581) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1586) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1588) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1590) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1594) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1596) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1597) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1599) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1600) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1603) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1606) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1608) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1610) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1616) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1618) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1624) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1626) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1627) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1629) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1638) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1640) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1647) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1649) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1650) tree.c:3418: rdp->cpu = cpu;
  (<0>,1651) tree.c:3418: rdp->cpu = cpu;
  (<0>,1653) tree.c:3418: rdp->cpu = cpu;
  (<0>,1654) tree.c:3419: rdp->rsp = rsp;
  (<0>,1655) tree.c:3419: rdp->rsp = rsp;
  (<0>,1657) tree.c:3419: rdp->rsp = rsp;
  (<0>,1658) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1661) tree_plugin.h:2711: }
  (<0>,1663) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1665) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1669)
  (<0>,1670)
  (<0>,1671) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1672) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1675) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1678) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1680) fake_sched.h:43: return __running_cpu;
  (<0>,1684) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1686) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1690) fake_sched.h:43: return __running_cpu;
  (<0>,1694) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1703) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1704) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1711)
  (<0>,1712)
  (<0>,1713) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1715) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1716) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1719) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1721) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1722) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1725) fake_defs.h:536: if (offset & mask)
  (<0>,1726) fake_defs.h:536: if (offset & mask)
  (<0>,1730) fake_defs.h:537: return cpu;
  (<0>,1731) fake_defs.h:537: return cpu;
  (<0>,1733) fake_defs.h:540: }
  (<0>,1735) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1736) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1740) tree.c:3703: while (i > rnp->grphi)
  (<0>,1741) tree.c:3703: while (i > rnp->grphi)
  (<0>,1743) tree.c:3703: while (i > rnp->grphi)
  (<0>,1746) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1747) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1749) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1755) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1756) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1769)
  (<0>,1770)
  (<0>,1771) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1773) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1775) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1777) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1778) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1781)
  (<0>,1782) tree.c:453: return &rsp->node[0];
  (<0>,1786) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1787) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1789) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1793)
  (<0>,1794)
  (<0>,1795) fake_sync.h:76: local_irq_save(flags);
  (<0>,1798) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1800) fake_sched.h:43: return __running_cpu;
  (<0>,1804) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1806) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1810) fake_sched.h:43: return __running_cpu;
  (<0>,1814) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1820) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1821) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1825) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1826) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1828) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1834) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1836) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1837) tree.c:3412: init_callback_list(rdp);
  (<0>,1841)
  (<0>,1842) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1845) tree_plugin.h:2732: return false;
  (<0>,1848) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1850) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1851) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1853) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1856) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1858) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1860) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1865) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1867) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1869) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1872) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1874) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1876) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1881) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1883) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1885) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1888) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1890) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1892) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1897) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1899) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1901) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1904) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1906) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1908) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1913) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1915) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1917) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1921) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1923) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1924) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1926) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1927) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1930) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1933) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1935) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1937) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1943) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1945) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1951) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1953) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1954) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1956) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1965) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1967) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1974) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1976) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1977) tree.c:3418: rdp->cpu = cpu;
  (<0>,1978) tree.c:3418: rdp->cpu = cpu;
  (<0>,1980) tree.c:3418: rdp->cpu = cpu;
  (<0>,1981) tree.c:3419: rdp->rsp = rsp;
  (<0>,1982) tree.c:3419: rdp->rsp = rsp;
  (<0>,1984) tree.c:3419: rdp->rsp = rsp;
  (<0>,1985) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1988) tree_plugin.h:2711: }
  (<0>,1990) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1992) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1996)
  (<0>,1997)
  (<0>,1998) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1999) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,2002) fake_sync.h:86: local_irq_restore(flags);
  (<0>,2005) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2007) fake_sched.h:43: return __running_cpu;
  (<0>,2011) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2013) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2017) fake_sched.h:43: return __running_cpu;
  (<0>,2021) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2030) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2031) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2038)
  (<0>,2039)
  (<0>,2040) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2042) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2043) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2046) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2048) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2049) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2052) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,2054) fake_defs.h:540: }
  (<0>,2056) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2057) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2060) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,2065)
  (<0>,2066)
  (<0>,2067) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2068) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2069) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2071) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2076)
  (<0>,2077)
  (<0>,2078)
  (<0>,2079) fake_defs.h:283: next->prev = new;
  (<0>,2080) fake_defs.h:283: next->prev = new;
  (<0>,2082) fake_defs.h:283: next->prev = new;
  (<0>,2083) fake_defs.h:284: new->next = next;
  (<0>,2084) fake_defs.h:284: new->next = next;
  (<0>,2086) fake_defs.h:284: new->next = next;
  (<0>,2087) fake_defs.h:285: new->prev = prev;
  (<0>,2088) fake_defs.h:285: new->prev = prev;
  (<0>,2090) fake_defs.h:285: new->prev = prev;
  (<0>,2091) fake_defs.h:286: prev->next = new;
  (<0>,2092) fake_defs.h:286: prev->next = new;
  (<0>,2094) fake_defs.h:286: prev->next = new;
  (<0>,2106) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2108) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2109) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2116)
  (<0>,2117)
  (<0>,2118) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2120) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2121) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2124) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2126) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2127) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2130) fake_defs.h:536: if (offset & mask)
  (<0>,2131) fake_defs.h:536: if (offset & mask)
  (<0>,2135) fake_defs.h:537: return cpu;
  (<0>,2136) fake_defs.h:537: return cpu;
  (<0>,2138) fake_defs.h:540: }
  (<0>,2140) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2141) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2144) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2163)
  (<0>,2164) tree.c:3493: static int rcu_cpu_notify(struct notifier_block *self,
  (<0>,2165) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2166) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2168) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2169) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2170) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2172) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2174) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2175) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2177) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2178) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2181) tree.c:3502: switch (action) {
  (<0>,2183) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2192)
  (<0>,2193) tree.c:3482: static void rcu_prepare_cpu(int cpu)
  (<0>,2194) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2195) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2199) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2200) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2201) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2203) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2207) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2208) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2216)
  (<0>,2217) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2218) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2220) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2222) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2224) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2225) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2228)
  (<0>,2229) tree.c:453: return &rsp->node[0];
  (<0>,2233) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2234) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2238)
  (<0>,2239) fake_sync.h:172: void mutex_lock(struct mutex *l)
  (<0>,2241) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2245) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2247) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2251)
  (<0>,2252)
  (<0>,2253) fake_sync.h:76: local_irq_save(flags);
  (<0>,2256) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2258) fake_sched.h:43: return __running_cpu;
  (<0>,2262) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2264) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2268) fake_sched.h:43: return __running_cpu;
  (<0>,2272) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2278) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2279) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2283) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2285) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2286) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2288) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2289) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2291) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2292) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2294) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2295) tree.c:3446: rdp->blimit = blimit;
  (<0>,2296) tree.c:3446: rdp->blimit = blimit;
  (<0>,2298) tree.c:3446: rdp->blimit = blimit;
  (<0>,2299) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2303)
  (<0>,2304) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2307) tree_plugin.h:2732: return false;
  (<0>,2310) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2312) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2313) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2315) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2318) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2320) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2322) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2325) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2327) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2329) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2331) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2334) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2336) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2338) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2341) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2343) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2345) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2347) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2350) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2352) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2354) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2357) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2359) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2361) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2363) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2366) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2368) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2370) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2373) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2375) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2377) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2379) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2383) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2385) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2387) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2388) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2390) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2393) tree_plugin.h:3164: }
  (<0>,2395) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2397) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2400) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2403) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2405) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2408) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2409) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2413)
  (<0>,2414) fake_sync.h:113: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2415) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2420) tree.c:3455: rnp = rdp->mynode;
  (<0>,2422) tree.c:3455: rnp = rdp->mynode;
  (<0>,2423) tree.c:3455: rnp = rdp->mynode;
  (<0>,2424) tree.c:3456: mask = rdp->grpmask;
  (<0>,2426) tree.c:3456: mask = rdp->grpmask;
  (<0>,2427) tree.c:3456: mask = rdp->grpmask;
  (<0>,2429) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2433) fake_sync.h:108: preempt_disable();
  (<0>,2435) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2436) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2440) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2441) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2443) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2445) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2446) tree.c:3461: mask = rnp->grpmask;
  (<0>,2448) tree.c:3461: mask = rnp->grpmask;
  (<0>,2449) tree.c:3461: mask = rnp->grpmask;
  (<0>,2450) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2451) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2453) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2456) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2458) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2459) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2461) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2462) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2464) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2465) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2467) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2468) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2470) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2471) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2473) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2477) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2481)
  (<0>,2482) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2483) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2488) tree.c:3475: rnp = rnp->parent;
  (<0>,2490) tree.c:3475: rnp = rnp->parent;
  (<0>,2491) tree.c:3475: rnp = rnp->parent;
  (<0>,2493) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2497) tree.c:3477: local_irq_restore(flags);
  (<0>,2500) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2502) fake_sched.h:43: return __running_cpu;
  (<0>,2506) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2508) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2512) fake_sched.h:43: return __running_cpu;
  (<0>,2516) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2521) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2525)
  (<0>,2526) fake_sync.h:178: void mutex_unlock(struct mutex *l)
  (<0>,2528) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2534) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2537) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2538) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2539) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2543) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2544) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2545) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2547) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2551) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2552) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2560)
  (<0>,2561)
  (<0>,2562) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2564) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2566) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2568) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2569) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2572)
  (<0>,2573) tree.c:453: return &rsp->node[0];
  (<0>,2577) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2578) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2582)
  (<0>,2583) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2585) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2589) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2591) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2595)
  (<0>,2596)
  (<0>,2597) fake_sync.h:76: local_irq_save(flags);
  (<0>,2600) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2602) fake_sched.h:43: return __running_cpu;
  (<0>,2606) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2608) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2612) fake_sched.h:43: return __running_cpu;
  (<0>,2616) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2622) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2623) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2627) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2629) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2630) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2632) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2633) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2635) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2636) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2638) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2639) tree.c:3446: rdp->blimit = blimit;
  (<0>,2640) tree.c:3446: rdp->blimit = blimit;
  (<0>,2642) tree.c:3446: rdp->blimit = blimit;
  (<0>,2643) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2647)
  (<0>,2648) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2651) tree_plugin.h:2732: return false;
  (<0>,2654) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2656) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2657) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2659) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2662) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2664) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2666) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2669) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2671) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2673) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2675) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2678) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2680) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2682) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2685) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2687) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2689) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2691) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2694) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2696) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2698) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2701) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2703) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2705) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2707) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2710) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2712) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2714) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2717) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2719) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2721) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2723) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2727) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2729) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2731) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2732) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2734) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2737) tree_plugin.h:3164: }
  (<0>,2739) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2741) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2744) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2747) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2749) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2752) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2753) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2757)
  (<0>,2758) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2759) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2764) tree.c:3455: rnp = rdp->mynode;
  (<0>,2766) tree.c:3455: rnp = rdp->mynode;
  (<0>,2767) tree.c:3455: rnp = rdp->mynode;
  (<0>,2768) tree.c:3456: mask = rdp->grpmask;
  (<0>,2770) tree.c:3456: mask = rdp->grpmask;
  (<0>,2771) tree.c:3456: mask = rdp->grpmask;
  (<0>,2773) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2777) fake_sync.h:108: preempt_disable();
  (<0>,2779) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2780) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2784) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2785) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2787) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2789) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2790) tree.c:3461: mask = rnp->grpmask;
  (<0>,2792) tree.c:3461: mask = rnp->grpmask;
  (<0>,2793) tree.c:3461: mask = rnp->grpmask;
  (<0>,2794) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2795) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2797) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2800) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2802) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2803) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2805) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2806) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2808) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2809) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2811) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2812) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2814) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2815) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2817) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2821) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2825)
  (<0>,2826) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2827) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2832) tree.c:3475: rnp = rnp->parent;
  (<0>,2834) tree.c:3475: rnp = rnp->parent;
  (<0>,2835) tree.c:3475: rnp = rnp->parent;
  (<0>,2837) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2841) tree.c:3477: local_irq_restore(flags);
  (<0>,2844) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2846) fake_sched.h:43: return __running_cpu;
  (<0>,2850) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2852) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2856) fake_sched.h:43: return __running_cpu;
  (<0>,2860) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2865) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2869)
  (<0>,2870) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2872) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2878) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2881) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2882) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2883) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2887) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2888) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2889) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2891) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2896) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,2900) tree_plugin.h:1499: }
  (<0>,2902) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2906) tree_plugin.h:2724: }
  (<0>,2913) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2914) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2921)
  (<0>,2922)
  (<0>,2923) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2925) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2926) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2929) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2931) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2932) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2935) fake_defs.h:536: if (offset & mask)
  (<0>,2936) fake_defs.h:536: if (offset & mask)
  (<0>,2940) fake_defs.h:537: return cpu;
  (<0>,2941) fake_defs.h:537: return cpu;
  (<0>,2943) fake_defs.h:540: }
  (<0>,2945) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2946) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2949) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2968)
  (<0>,2969)
  (<0>,2970)
  (<0>,2971) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2973) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2974) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2975) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2977) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2979) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2980) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2982) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2983) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2986) tree.c:3502: switch (action) {
  (<0>,2988) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2997)
  (<0>,2998) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2999) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3000) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3004) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3005) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3006) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3008) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3012) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3013) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3021)
  (<0>,3022)
  (<0>,3023) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3025) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3027) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3029) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3030) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3033)
  (<0>,3034) tree.c:453: return &rsp->node[0];
  (<0>,3038) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3039) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3043)
  (<0>,3044) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3046) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3050) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3052) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3056)
  (<0>,3057)
  (<0>,3058) fake_sync.h:76: local_irq_save(flags);
  (<0>,3061) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3063) fake_sched.h:43: return __running_cpu;
  (<0>,3067) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3069) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3073) fake_sched.h:43: return __running_cpu;
  (<0>,3077) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3083) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3084) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3088) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3090) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3091) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3093) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3094) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3096) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3097) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3099) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3100) tree.c:3446: rdp->blimit = blimit;
  (<0>,3101) tree.c:3446: rdp->blimit = blimit;
  (<0>,3103) tree.c:3446: rdp->blimit = blimit;
  (<0>,3104) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3108)
  (<0>,3109) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3112) tree_plugin.h:2732: return false;
  (<0>,3115) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3117) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3118) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3120) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3123) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3125) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3127) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3130) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3132) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3134) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3136) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3139) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3141) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3143) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3146) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3148) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3150) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3152) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3155) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3157) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3159) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3162) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3164) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3166) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3168) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3171) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3173) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3175) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3178) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3180) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3182) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3184) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3188) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3190) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3192) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3193) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3195) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3198) tree_plugin.h:3164: }
  (<0>,3200) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3202) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3205) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3208) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3210) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3213) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3214) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3218)
  (<0>,3219) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3220) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3225) tree.c:3455: rnp = rdp->mynode;
  (<0>,3227) tree.c:3455: rnp = rdp->mynode;
  (<0>,3228) tree.c:3455: rnp = rdp->mynode;
  (<0>,3229) tree.c:3456: mask = rdp->grpmask;
  (<0>,3231) tree.c:3456: mask = rdp->grpmask;
  (<0>,3232) tree.c:3456: mask = rdp->grpmask;
  (<0>,3234) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3238) fake_sync.h:108: preempt_disable();
  (<0>,3240) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3241) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3245) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3246) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3248) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3250) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3251) tree.c:3461: mask = rnp->grpmask;
  (<0>,3253) tree.c:3461: mask = rnp->grpmask;
  (<0>,3254) tree.c:3461: mask = rnp->grpmask;
  (<0>,3255) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3256) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3258) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3261) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3263) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3264) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3266) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3267) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3269) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3270) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3272) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3273) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3275) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3276) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3278) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3282) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3286)
  (<0>,3287) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3288) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3293) tree.c:3475: rnp = rnp->parent;
  (<0>,3295) tree.c:3475: rnp = rnp->parent;
  (<0>,3296) tree.c:3475: rnp = rnp->parent;
  (<0>,3298) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3302) tree.c:3477: local_irq_restore(flags);
  (<0>,3305) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3307) fake_sched.h:43: return __running_cpu;
  (<0>,3311) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3313) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3317) fake_sched.h:43: return __running_cpu;
  (<0>,3321) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3326) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3330)
  (<0>,3331) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3333) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3339) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3342) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3343) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3344) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3348) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3349) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3350) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3352) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3356) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3357) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3365)
  (<0>,3366)
  (<0>,3367) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3369) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3371) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3373) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3374) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3377)
  (<0>,3378) tree.c:453: return &rsp->node[0];
  (<0>,3382) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3383) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3387)
  (<0>,3388) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3390) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3394) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3396) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3400)
  (<0>,3401)
  (<0>,3402) fake_sync.h:76: local_irq_save(flags);
  (<0>,3405) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3407) fake_sched.h:43: return __running_cpu;
  (<0>,3411) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3413) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3417) fake_sched.h:43: return __running_cpu;
  (<0>,3421) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3427) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3428) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3432) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3434) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3435) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3437) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3438) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3440) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3441) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3443) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3444) tree.c:3446: rdp->blimit = blimit;
  (<0>,3445) tree.c:3446: rdp->blimit = blimit;
  (<0>,3447) tree.c:3446: rdp->blimit = blimit;
  (<0>,3448) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3452)
  (<0>,3453) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3456) tree_plugin.h:2732: return false;
  (<0>,3459) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3461) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3462) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3464) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3467) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3469) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3471) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3474) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3476) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3478) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3480) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3483) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3485) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3487) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3490) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3492) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3494) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3496) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3499) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3501) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3503) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3506) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3508) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3510) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3512) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3515) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3517) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3519) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3522) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3528) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3532) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3534) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3536) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3537) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3539) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3542) tree_plugin.h:3164: }
  (<0>,3544) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3546) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3549) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3552) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3554) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3557) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3558) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3562)
  (<0>,3563) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3564) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3569) tree.c:3455: rnp = rdp->mynode;
  (<0>,3571) tree.c:3455: rnp = rdp->mynode;
  (<0>,3572) tree.c:3455: rnp = rdp->mynode;
  (<0>,3573) tree.c:3456: mask = rdp->grpmask;
  (<0>,3575) tree.c:3456: mask = rdp->grpmask;
  (<0>,3576) tree.c:3456: mask = rdp->grpmask;
  (<0>,3578) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3582) fake_sync.h:108: preempt_disable();
  (<0>,3584) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3585) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3589) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3590) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3592) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3594) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3595) tree.c:3461: mask = rnp->grpmask;
  (<0>,3597) tree.c:3461: mask = rnp->grpmask;
  (<0>,3598) tree.c:3461: mask = rnp->grpmask;
  (<0>,3599) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3600) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3602) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3605) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3607) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3608) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3610) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3611) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3613) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3614) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3616) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3617) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3619) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3620) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3622) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3626) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3630)
  (<0>,3631) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3632) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3637) tree.c:3475: rnp = rnp->parent;
  (<0>,3639) tree.c:3475: rnp = rnp->parent;
  (<0>,3640) tree.c:3475: rnp = rnp->parent;
  (<0>,3642) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3646) tree.c:3477: local_irq_restore(flags);
  (<0>,3649) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3651) fake_sched.h:43: return __running_cpu;
  (<0>,3655) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3657) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3661) fake_sched.h:43: return __running_cpu;
  (<0>,3665) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3670) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3674)
  (<0>,3675) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3677) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3683) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3686) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3687) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3688) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3692) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3693) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3694) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3696) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3701) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,3705) tree_plugin.h:1499: }
  (<0>,3707) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,3711) tree_plugin.h:2724: }
  (<0>,3718) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3719) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3726)
  (<0>,3727)
  (<0>,3728) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3730) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3731) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3734) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3736) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3737) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3740) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,3742) fake_defs.h:540: }
  (<0>,3744) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3745) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3751) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3753) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3756) litmus.c:123: set_cpu(i);
  (<0>,3759)
  (<0>,3760) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,3761) fake_sched.h:56: __running_cpu = cpu;
  (<0>,3765) tree.c:578: unsigned long flags;
  (<0>,3768) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3770) fake_sched.h:43: return __running_cpu;
  (<0>,3774) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3776) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3780) fake_sched.h:43: return __running_cpu;
  (<0>,3784) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3797) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3799) fake_sched.h:43: return __running_cpu;
  (<0>,3803) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3804) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3806) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3807) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3808) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3814) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3815) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3820) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3821) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3822) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3823) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,3827) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3829) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3830) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3831) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3850)
  (<0>,3852) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3854) fake_sched.h:43: return __running_cpu;
  (<0>,3858) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3861) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,3865) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3866) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3867) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3871) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3872) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3873) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3875) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3880) fake_sched.h:43: return __running_cpu;
  (<0>,3883) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3885) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3887) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3888) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3891) tree_plugin.h:2720: }
  (<0>,3894) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3897) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3898) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3899) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3903) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3904) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3905) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3907) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3912) fake_sched.h:43: return __running_cpu;
  (<0>,3915) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3917) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3919) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3920) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3923) tree_plugin.h:2720: }
  (<0>,3926) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3929) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3930) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3931) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3935) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3936) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3937) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3939) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3946) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3949) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3950) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3951) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3953) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3954) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3956) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3959) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3965) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3966) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3969) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3974) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3975) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3976) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3990) tree_plugin.h:3141: }
  (<0>,3992) tree.c:583: local_irq_restore(flags);
  (<0>,3995) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3997) fake_sched.h:43: return __running_cpu;
  (<0>,4001) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4003) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4007) fake_sched.h:43: return __running_cpu;
  (<0>,4011) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4018) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4020) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4022) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4025) litmus.c:123: set_cpu(i);
  (<0>,4028)
  (<0>,4029) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4030) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4034) tree.c:580: local_irq_save(flags);
  (<0>,4037) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4039) fake_sched.h:43: return __running_cpu;
  (<0>,4043) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4045) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4049) fake_sched.h:43: return __running_cpu;
  (<0>,4053) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4066) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4068) fake_sched.h:43: return __running_cpu;
  (<0>,4072) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4073) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4075) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4076) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4077) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4083) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4084) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4089) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4090) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4091) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4092) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4096) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4098) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4099) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4100) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4119)
  (<0>,4121) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4123) fake_sched.h:43: return __running_cpu;
  (<0>,4127) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4130) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,4134) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4135) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4136) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4140) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4141) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4142) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4144) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4149) fake_sched.h:43: return __running_cpu;
  (<0>,4152) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4154) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4156) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4157) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4160) tree_plugin.h:2720: }
  (<0>,4163) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4166) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4167) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4168) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4172) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4173) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4181) fake_sched.h:43: return __running_cpu;
  (<0>,4184) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4186) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4188) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4189) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4192) tree_plugin.h:2720: }
  (<0>,4195) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4198) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4199) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4200) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4204) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4205) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4206) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4208) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4215) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4218) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4219) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4220) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4222) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4223) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4225) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4228) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4234) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4235) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4238) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4243) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4244) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4245) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4259) tree_plugin.h:3141: }
  (<0>,4261) tree.c:583: local_irq_restore(flags);
  (<0>,4264) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4266) fake_sched.h:43: return __running_cpu;
  (<0>,4270) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4272) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4276) fake_sched.h:43: return __running_cpu;
  (<0>,4280) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4287) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4289) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4291) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4303) tree.c:3561: unsigned long flags;
  (<0>,4304) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4305) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4306) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4310) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4314) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4320) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4322) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4331)
  (<0>,4332) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4333) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4334) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4335) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4336) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4339)
  (<0>,4340)
  (<0>,4347)
  (<0>,4348)
  (<0>,4355)
  (<0>,4356)
  (<0>,4363)
  (<0>,4364)
  (<0>,4371)
  (<0>,4372)
  (<0>,4379)
  (<0>,4380)
  (<0>,4387)
  (<0>,4388)
  (<0>,4395)
  (<0>,4396)
  (<0>,4403)
  (<0>,4404)
  (<0>,4411)
  (<0>,4412) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4421) fake_defs.h:730: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,4427) fake_defs.h:732: task = malloc(sizeof(*task));
  (<0>,4428) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4430) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4432) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4433) fake_defs.h:734: task->tid = t;
  (<0>,4434) fake_defs.h:734: task->tid = t;
  (<0>,4436) fake_defs.h:734: task->tid = t;
  (<0>,4437) fake_defs.h:735: return task;
  (<0>,4438) fake_defs.h:735: return task;
  (<0>,4440) fake_defs.h:738: }
  (<0>,4442) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4443) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4446)
  (<0>,4447) tree.c:453: return &rsp->node[0];
  (<0>,4451) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4452) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4454) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4458)
  (<0>,4459)
  (<0>,4460) fake_sync.h:76: local_irq_save(flags);
  (<0>,4463) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4465) fake_sched.h:43: return __running_cpu;
  (<0>,4469) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4475) fake_sched.h:43: return __running_cpu;
  (<0>,4479) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4485) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4486) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4490) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4491) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4493) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4494) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4496) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4500)
  (<0>,4501)
  (<0>,4502) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4503) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4506) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4509) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4511) fake_sched.h:43: return __running_cpu;
  (<0>,4515) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4517) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4521) fake_sched.h:43: return __running_cpu;
  (<0>,4525) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4533) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4537) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4538) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4542) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4543) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4544) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4546) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4550) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4552) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4554) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4563)
  (<0>,4564)
  (<0>,4565)
  (<0>,4566)
  (<0>,4567) fake_defs.h:727: struct task_struct *task = NULL;
  (<0>,4568) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4571)
  (<0>,4572)
  (<0>,4579)
  (<0>,4580)
  (<0>,4587)
  (<0>,4588)
  (<0>,4595)
  (<0>,4596)
  (<0>,4603)
  (<0>,4604) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4617) fake_defs.h:737: return NULL;
  (<0>,4619) fake_defs.h:738: }
  (<0>,4621) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4622) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4625)
  (<0>,4626) tree.c:453: return &rsp->node[0];
  (<0>,4630) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4631) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4633) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4637)
  (<0>,4638)
  (<0>,4639) fake_sync.h:76: local_irq_save(flags);
  (<0>,4642) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4644) fake_sched.h:43: return __running_cpu;
  (<0>,4648) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4650) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4654) fake_sched.h:43: return __running_cpu;
  (<0>,4658) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4664) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4665) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4669) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4670) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4672) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4673) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4675) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4679)
  (<0>,4680)
  (<0>,4681) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4682) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4685) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4688) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4690) fake_sched.h:43: return __running_cpu;
  (<0>,4694) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4696) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4700) fake_sched.h:43: return __running_cpu;
  (<0>,4704) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4712) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4715) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4716) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4717) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4721) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4722) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4723) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4725) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4739)
  (<0>,4740) litmus.c:51: void *thread_reader(void *arg)
  (<0>,4743)
  (<0>,4744) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4745) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4748) fake_sched.h:43: return __running_cpu;
  (<0>,4752)
  (<0>,4753) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,4756) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,4761) tree.c:702: unsigned long flags;
  (<0>,4764) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4766) fake_sched.h:43: return __running_cpu;
  (<0>,4770) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4772) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4780) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4793) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4795) fake_sched.h:43: return __running_cpu;
  (<0>,4799) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4800) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4802) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4803) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4804) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4809) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4810) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4814) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4815) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4816) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4817) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,4821) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4823) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4824) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4825) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4839)
  (<0>,4840) tree.c:644: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,4842) fake_sched.h:43: return __running_cpu;
  (<0>,4846) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4850) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4853) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4854) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4855) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4857) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4858) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4860) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4870) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4871) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4874) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4879) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4880) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4881) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4886) tree.c:656: if (!user && !is_idle_task(current)) {
  (<0>,4895) tree_plugin.h:3145: }
  (<0>,4897) tree.c:707: local_irq_restore(flags);
  (<0>,4900) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4902) fake_sched.h:43: return __running_cpu;
  (<0>,4906) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4908) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4912) fake_sched.h:43: return __running_cpu;
  (<0>,4916) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4932) litmus.c:57: r_x = x;
  (<0>,4933) litmus.c:57: r_x = x;
  (<0>,4937) fake_sched.h:43: return __running_cpu;
  (<0>,4941) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4949) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4954) fake_sched.h:43: return __running_cpu;
  (<0>,4958) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
  (<0>,4968) tree.c:745: unsigned long flags;
  (<0>,4971) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4973) fake_sched.h:43: return __running_cpu;
  (<0>,4977) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4984) fake_sched.h:43: return __running_cpu;
  (<0>,4988) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4989) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4991) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4992) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4993) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4995) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4997) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4998) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5000) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5005) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5006) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5008) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5012) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5013) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5014) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5015) tree.c:754: if (oldval)
  (<0>,5023) tree_plugin.h:3145: }
  (<0>,5025) tree.c:759: local_irq_restore(flags);
  (<0>,5028) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5030) fake_sched.h:43: return __running_cpu;
  (<0>,5034) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5036) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5044) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,5049) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,5054) fake_sched.h:43: return __running_cpu;
  (<0>,5059) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,5067) fake_sched.h:43: return __running_cpu;
  (<0>,5072) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
  (<0>,5078) fake_sched.h:43: return __running_cpu;
  (<0>,5083) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
  (<0>,5096) tree.c:3185: struct rcu_state *rsp;
  (<0>,5097) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5098) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5102) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5103) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5104) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5106) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5110) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5112) fake_sched.h:43: return __running_cpu;
  (<0>,5115) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5117) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5124)
  (<0>,5125) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5126) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5128) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5129) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5130) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5132) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5134) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5135) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5136) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5146)
  (<0>,5147) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5148) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5153) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
  (<0>,5156) tree_plugin.h:3185: return 0;
  (<0>,5159) tree.c:3135: if (rcu_scheduler_fully_active &&
  (<0>,5162) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5164) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5167) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5169) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5173) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,5176)
  (<0>,5177) tree.c:442: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,5179) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5182) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5189) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5190) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5196)
  (<0>,5197) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5198) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5201)
  (<0>,5202) tree.c:176: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,5204) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5205) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5207) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5213) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5219)
  (<0>,5220) tree.c:461: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,5223)
  (<0>,5224) tree.c:453: return &rsp->node[0];
  (<0>,5228) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5229) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5231) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5235) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5236) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5238) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5241) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5242) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5243) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5247) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5250) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5253) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5256) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5257) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5260) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5262) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5265) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5268) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5271) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5272) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5274) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5277) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5281) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5283) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5285) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5288) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5291) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5294) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5295) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5297) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5300) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5304) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5306) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5308) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5311) tree.c:493: return 0; /* No grace period needed. */
  (<0>,5313) tree.c:494: }
  (<0>,5317) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5319) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5320) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5322) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5325) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5327) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5328) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5330) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5333) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
  (<0>,5336) tree_plugin.h:2715: return false;
  (<0>,5340) tree.c:3174: rdp->n_rp_need_nothing++;
  (<0>,5342) tree.c:3174: rdp->n_rp_need_nothing++;
  (<0>,5344) tree.c:3174: rdp->n_rp_need_nothing++;
  (<0>,5345) tree.c:3175: return 0;
  (<0>,5347) tree.c:3176: }
  (<0>,5352) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5355) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5356) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5357) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5361) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5362) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5363) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5365) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5369) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5371) fake_sched.h:43: return __running_cpu;
  (<0>,5374) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5376) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5383)
  (<0>,5384)
  (<0>,5385) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5387) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5388) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5389) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5391) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5393) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5394) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5395) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5405)
  (<0>,5406)
  (<0>,5407) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
  (<0>,5412) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
  (<0>,5415) tree_plugin.h:3185: return 0;
  (<0>,5418) tree.c:3135: if (rcu_scheduler_fully_active &&
  (<0>,5421) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5423) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5426) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5428) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5432) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,5435)
  (<0>,5436) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5438) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5441) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5448) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5449) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5455)
  (<0>,5456)
  (<0>,5457) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,5460)
  (<0>,5461) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5463) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5464) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5466) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5472) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5478)
  (<0>,5479) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5482)
  (<0>,5483) tree.c:453: return &rsp->node[0];
  (<0>,5487) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5488) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5490) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5494) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5495) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5497) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5500) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5501) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5502) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5506) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5509) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5512) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5515) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5516) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5519) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5521) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5524) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5527) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5530) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5531) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5533) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5536) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5540) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5542) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5544) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5547) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5550) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5553) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5554) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5556) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5559) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5563) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5565) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5567) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5570) tree.c:493: return 0; /* No grace period needed. */
  (<0>,5572) tree.c:494: }
  (<0>,5576) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5578) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5579) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5581) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5584) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5586) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5587) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5589) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5592) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
  (<0>,5595) tree_plugin.h:2715: return false;
  (<0>,5599) tree.c:3174: rdp->n_rp_need_nothing++;
  (<0>,5601) tree.c:3174: rdp->n_rp_need_nothing++;
  (<0>,5603) tree.c:3174: rdp->n_rp_need_nothing++;
  (<0>,5604) tree.c:3175: return 0;
  (<0>,5606) tree.c:3176: }
  (<0>,5611) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5614) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5615) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5616) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5620) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5621) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5622) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5624) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5628) tree.c:3190: return 0;
  (<0>,5630) tree.c:3191: }
  (<0>,5634) tree.c:2437: if (user)
  (<0>,5642) fake_sched.h:43: return __running_cpu;
  (<0>,5646) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
  (<0>,5648) fake_sched.h:43: return __running_cpu;
  (<0>,5652) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5658) fake_sched.h:43: return __running_cpu;
  (<0>,5662) fake_sched.h:182: if (need_softirq[get_cpu()]) {
  (<0>,5672) tree.c:620: unsigned long flags;
  (<0>,5675) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5677) fake_sched.h:43: return __running_cpu;
  (<0>,5681) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5683) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5687) fake_sched.h:43: return __running_cpu;
  (<0>,5691) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5697) fake_sched.h:43: return __running_cpu;
  (<0>,5701) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,5702) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,5704) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,5705) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,5706) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,5708) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,5710) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,5711) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,5713) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,5718) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,5719) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,5721) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,5725) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,5726) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,5727) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,5728) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,5730) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,5738) tree_plugin.h:3141: }
  (<0>,5740) tree.c:634: local_irq_restore(flags);
  (<0>,5743) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5745) fake_sched.h:43: return __running_cpu;
  (<0>,5749) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5751) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5755) fake_sched.h:43: return __running_cpu;
  (<0>,5759) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,1)
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:704: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,56) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,58) fake_sched.h:43: return __running_cpu;
      (<0.1>,62) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,63) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,67) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,72) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,73) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,77) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,78) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,79) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,80) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,84) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,86) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,87) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,88) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,102)
      (<0.1>,103) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,105) fake_sched.h:43: return __running_cpu;
      (<0.1>,109) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,113) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,116) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,117) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,118) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,120) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,123) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,126) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,133) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,137) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,143) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,144) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,149) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,158) tree_plugin.h:3145: }
      (<0.1>,160) tree.c:707: local_irq_restore(flags);
      (<0.1>,163) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,165) fake_sched.h:43: return __running_cpu;
      (<0.1>,169) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,171) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,175) fake_sched.h:43: return __running_cpu;
      (<0.1>,179) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,186) litmus.c:87: x = 1;
      (<0.1>,196) tree.c:2787: ret = num_online_cpus() <= 1;
      (<0.1>,198) tree.c:2789: return ret;
      (<0.1>,202) tree.c:2841: if (rcu_expedited)
      (<0.1>,208) update.c:223: init_rcu_head_on_stack(&rcu.head);
      (<0.1>,212) rcupdate.h:401: }
      (<0.1>,217)
      (<0.1>,218) fake_sync.h:232: x->done = 0;
      (<0.1>,220) fake_sync.h:232: x->done = 0;
      (<0.1>,224) update.c:226: crf(&rcu.head, wakeme_after_rcu);
      (<0.1>,229)
      (<0.1>,230)
      (<0.1>,231) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,232) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,249)
      (<0.1>,250)
      (<0.1>,251)
      (<0.1>,252)
      (<0.1>,254)
      (<0.1>,255) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,262) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,263) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,269) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,270) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,271) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,272) tree.c:2690: if (debug_rcu_head_queue(head)) {
      (<0.1>,275) rcu.h:92: return 0;
      (<0.1>,279) tree.c:2696: head->func = func;
      (<0.1>,280) tree.c:2696: head->func = func;
      (<0.1>,282) tree.c:2696: head->func = func;
      (<0.1>,283) tree.c:2697: head->next = NULL;
      (<0.1>,285) tree.c:2697: head->next = NULL;
      (<0.1>,286) tree.c:2705: local_irq_save(flags);
      (<0.1>,289) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,291) fake_sched.h:43: return __running_cpu;
      (<0.1>,295) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,297) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,301) fake_sched.h:43: return __running_cpu;
      (<0.1>,305) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,311) fake_sched.h:43: return __running_cpu;
      (<0.1>,314) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,316) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,318) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,319) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,322) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,330) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,333) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,335) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,337) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,339) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,340) tree.c:2721: if (lazy)
      (<0.1>,347) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,348) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,351) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,352) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,353) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,355) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,358) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,359) tree.c:2729: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,366) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,367) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,368) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,369) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,377)
      (<0.1>,378)
      (<0.1>,379)
      (<0.1>,380) tree.c:2628: if (!rcu_is_watching() && cpu_online(smp_processor_id()))
      (<0.1>,386) fake_sched.h:43: return __running_cpu;
      (<0.1>,392) tree.c:815: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,397) tree.c:829: ret = __rcu_is_watching();
      (<0.1>,399) tree.c:831: return ret;
      (<0.1>,403) tree.c:2632: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,406) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,408) fake_sched.h:43: return __running_cpu;
      (<0.1>,412) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,422) tree.c:2737: local_irq_restore(flags);
      (<0.1>,425) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,427) fake_sched.h:43: return __running_cpu;
      (<0.1>,431) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,433) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,437) fake_sched.h:43: return __running_cpu;
      (<0.1>,441) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,452) fake_sync.h:238: might_sleep();
      (<0.1>,456) fake_sched.h:43: return __running_cpu;
      (<0.1>,460) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,463) tree.c:580: local_irq_save(flags);
      (<0.1>,466) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,468) fake_sched.h:43: return __running_cpu;
      (<0.1>,472) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,474) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,478) fake_sched.h:43: return __running_cpu;
      (<0.1>,482) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,495) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,497) fake_sched.h:43: return __running_cpu;
      (<0.1>,501) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,502) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,504) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,505) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,506) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,512) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,513) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,518) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,519) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,520) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,521) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,525) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,527) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,528) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,529) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,548)
      (<0.1>,550) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,552) fake_sched.h:43: return __running_cpu;
      (<0.1>,556) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,559) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,563) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,564) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,565) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,569) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,570) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,571) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,573) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,578) fake_sched.h:43: return __running_cpu;
      (<0.1>,581) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,583) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,585) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,586) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,589) tree_plugin.h:2720: }
      (<0.1>,592) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,595) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,596) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,597) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,601) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,602) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,603) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,605) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,610) fake_sched.h:43: return __running_cpu;
      (<0.1>,613) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,615) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,617) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,618) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,621) tree_plugin.h:2720: }
      (<0.1>,624) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,627) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,628) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,629) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,633) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,635) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,637) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,644) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,647) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,648) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,649) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,651) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,652) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,654) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,657) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,663) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,664) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,667) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,672) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,673) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,674) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,688) tree_plugin.h:3141: }
      (<0.1>,690) tree.c:583: local_irq_restore(flags);
      (<0.1>,693) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,695) fake_sched.h:43: return __running_cpu;
      (<0.1>,699) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,701) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,705) fake_sched.h:43: return __running_cpu;
      (<0.1>,709) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,715) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,718) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,723) fake_sync.h:241: while (!x->done)
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:704: local_irq_save(flags);
    (<0.0>,35)
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,64)
    (<0.0>,66) fake_sched.h:43: return __running_cpu;
    (<0.0>,70) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,71) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,75) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,80) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,81) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,85) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,86) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,87) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,88) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,92) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,94) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,95) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,96) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,110)
    (<0.0>,111)
    (<0.0>,113) fake_sched.h:43: return __running_cpu;
    (<0.0>,117) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,124) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,125) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,126) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,128) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,129) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,131) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,141) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,145) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,150) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,151) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,152) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,157) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,166)
    (<0.0>,168) tree.c:707: local_irq_restore(flags);
    (<0.0>,171)
    (<0.0>,173) fake_sched.h:43: return __running_cpu;
    (<0.0>,177) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,179) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,183) fake_sched.h:43: return __running_cpu;
    (<0.0>,187) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,194) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,206)
    (<0.0>,207) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,209) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,213)
    (<0.0>,214) tree.c:453: return &rsp->node[0];
    (<0.0>,218) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,223) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,225) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) fake_sched.h:43: return __running_cpu;
    (<0.0>,233) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,237) fake_sched.h:43: return __running_cpu;
    (<0.0>,241) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,246) fake_sched.h:43: return __running_cpu;
    (<0.0>,250) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,260) tree.c:749: local_irq_save(flags);
    (<0.0>,263)
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,271) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,276) fake_sched.h:43: return __running_cpu;
    (<0.0>,280) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,281) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,283) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,284) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,285) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,287) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,289) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,290) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,292) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,297) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,298) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,300) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,304) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,305) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,306) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,307) tree.c:754: if (oldval)
    (<0.0>,315)
    (<0.0>,317) tree.c:759: local_irq_restore(flags);
    (<0.0>,320)
    (<0.0>,322) fake_sched.h:43: return __running_cpu;
    (<0.0>,326) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,328) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,336)
    (<0.0>,341) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,346) fake_sched.h:43: return __running_cpu;
    (<0.0>,351) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,359) fake_sched.h:43: return __running_cpu;
    (<0.0>,364) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,370) fake_sched.h:43: return __running_cpu;
    (<0.0>,375) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,388) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,389) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,390) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,394) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,395) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,396) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,398) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,402) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,404) fake_sched.h:43: return __running_cpu;
    (<0.0>,407) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,409) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,416)
    (<0.0>,417)
    (<0.0>,418) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,420) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,421) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,422) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,424) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,426) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,427) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,428) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,438)
    (<0.0>,439)
    (<0.0>,440) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,445) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,448)
    (<0.0>,451) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,454) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,456) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,459) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,461) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,465) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,468)
    (<0.0>,469) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,471) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,474) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,481) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,482) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,488)
    (<0.0>,489)
    (<0.0>,490) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,493)
    (<0.0>,494) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,496) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,497) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,499) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,505) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,511)
    (<0.0>,512) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,515)
    (<0.0>,516) tree.c:453: return &rsp->node[0];
    (<0.0>,520) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,521) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,523) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,527) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,528) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,530) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,533) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,534) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,535) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,539) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,542) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,545) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,548) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,549) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,552) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,554) tree.c:494: }
    (<0.0>,558) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,560) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,562) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,563) tree.c:3152: return 1;
    (<0.0>,565) tree.c:3176: }
    (<0.0>,569) tree.c:3189: return 1;
    (<0.0>,571) tree.c:3191: }
    (<0.0>,578) fake_sched.h:43: return __running_cpu;
    (<0.0>,582) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,586) tree.c:2437: if (user)
    (<0.0>,594) fake_sched.h:43: return __running_cpu;
    (<0.0>,598) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,600) fake_sched.h:43: return __running_cpu;
    (<0.0>,604) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,610) fake_sched.h:43: return __running_cpu;
    (<0.0>,614) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,624)
    (<0.0>,627) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,628) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,629) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,633) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,634) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,635) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,637) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,641) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,650)
    (<0.0>,652) fake_sched.h:43: return __running_cpu;
    (<0.0>,655) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,657) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,659) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,660) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,662) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,669) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,670) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,672) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,678) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,679) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,680) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,681) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,682) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,686)
    (<0.0>,687)
    (<0.0>,688) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,689) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,696)
    (<0.0>,697)
    (<0.0>,698) tree.c:1584: local_irq_save(flags);
    (<0.0>,701)
    (<0.0>,703) fake_sched.h:43: return __running_cpu;
    (<0.0>,707) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,709) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,713) fake_sched.h:43: return __running_cpu;
    (<0.0>,717) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,722) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,724) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,725) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,726) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,728) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,729) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,731) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,734) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,736) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,737) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,739) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,742) tree.c:1589: local_irq_restore(flags);
    (<0.0>,745)
    (<0.0>,747) fake_sched.h:43: return __running_cpu;
    (<0.0>,751) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,753) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,757) fake_sched.h:43: return __running_cpu;
    (<0.0>,761) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,768) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,770) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,775) tree.c:2558: local_irq_save(flags);
    (<0.0>,778)
    (<0.0>,780) fake_sched.h:43: return __running_cpu;
    (<0.0>,784) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,786) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,790) fake_sched.h:43: return __running_cpu;
    (<0.0>,794) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,799) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,800) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,806)
    (<0.0>,807)
    (<0.0>,808) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,811)
    (<0.0>,812) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,814) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,815) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,817) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,823) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,829)
    (<0.0>,830) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,833)
    (<0.0>,834) tree.c:453: return &rsp->node[0];
    (<0.0>,838) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,839) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,841) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,845) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,846) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,848) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,851) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,852) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,853) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,857) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,860) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,863) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,866) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,867) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,870) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,872) tree.c:494: }
    (<0.0>,876) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,879)
    (<0.0>,880) tree.c:453: return &rsp->node[0];
    (<0.0>,887)
    (<0.0>,889) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,890) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,894) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,900)
    (<0.0>,902) fake_sched.h:43: return __running_cpu;
    (<0.0>,905) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,907) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,909) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,910) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,913)
    (<0.0>,914) tree.c:453: return &rsp->node[0];
    (<0.0>,918) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,919) tree.c:1925: bool ret = false;
    (<0.0>,920) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,921) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,922) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,930)
    (<0.0>,931)
    (<0.0>,932)
    (<0.0>,933) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,936) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,939) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,942) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,943) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,946) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,948) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,951) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,953) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,954) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,956) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,959) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,964) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,966) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,967) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,970) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,972) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,975) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,977) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,980) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,981) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,984) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,987) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,989) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,992) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,993) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,995) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,998) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,999) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1001) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1004) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1005) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1007) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1010) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1012) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1014) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1015) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1017) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1019) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1022) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1024) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1027) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1028) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1031) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1034) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1036) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1039) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1040) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1042) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1045) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1046) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1048) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1051) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1052) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1054) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1057) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1059) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1061) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1062) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1064) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1066) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1069) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1070) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1071) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1080)
    (<0.0>,1081)
    (<0.0>,1082)
    (<0.0>,1083) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1086) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1089) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1092) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1093) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1096) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1097) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1102)
    (<0.0>,1103)
    (<0.0>,1104) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1107)
    (<0.0>,1108) tree.c:453: return &rsp->node[0];
    (<0.0>,1112) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1115) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1117) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1118) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1120) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1123) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1125) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1127) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1129) tree.c:1261: }
    (<0.0>,1131) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1132) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1134) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1137) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1139) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1142) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1143) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1146) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1149) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1153) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1155) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1157) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1160) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1162) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1165) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1166) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1169) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1172) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1176) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1178) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1180) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1183) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1185) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1189) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1192) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1195) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1196) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1198) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1201) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1202) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1203) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1205) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1208) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1210) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1212) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1214) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1217) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1220) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1221) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1223) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1226) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1227) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1228) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1230) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1233) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1235) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1237) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1239) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1242) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1245) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1246) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1248) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1251) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1252) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1253) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1255) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1258) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1260) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1262) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1264) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1267) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1268) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1277)
    (<0.0>,1278)
    (<0.0>,1279)
    (<0.0>,1280) tree.c:1289: bool ret = false;
    (<0.0>,1281) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1283) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1286)
    (<0.0>,1287) tree.c:453: return &rsp->node[0];
    (<0.0>,1291) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1292) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1294) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1295) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1300)
    (<0.0>,1301)
    (<0.0>,1302) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1305)
    (<0.0>,1306) tree.c:453: return &rsp->node[0];
    (<0.0>,1310) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1313) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1315) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1316) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1318) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1321) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1323) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1325) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1327) tree.c:1261: }
    (<0.0>,1329) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1330) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1331) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1332) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1338)
    (<0.0>,1339)
    (<0.0>,1340)
    (<0.0>,1341)
    (<0.0>,1345) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1347) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1350) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1353) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1355) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1356) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1358) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1361) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1363) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1364) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1366) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1369) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1370) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1373) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1375) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1376) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1381)
    (<0.0>,1382)
    (<0.0>,1383) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1386)
    (<0.0>,1387) tree.c:453: return &rsp->node[0];
    (<0.0>,1391) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1394) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1396) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1397) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1399) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1402) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1404) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1406) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1408) tree.c:1261: }
    (<0.0>,1410) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1411) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1413) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1416) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1417) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1419) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1422) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1426) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1427) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1429) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1432) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1435) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1437) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1439) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1442) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1443) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1445) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1448) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1453) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1455) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1457) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1460) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1461) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1463) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1466) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1471) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1473) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1475) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1478) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1480) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1483) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1486) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1488) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1491) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1493) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1494) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1496) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1497) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1499) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1502) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1503) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1504) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1510)
    (<0.0>,1511)
    (<0.0>,1512)
    (<0.0>,1513)
    (<0.0>,1517) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1519) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1520) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1521) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1527)
    (<0.0>,1528)
    (<0.0>,1529)
    (<0.0>,1530) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1532) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1535) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1536) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1542)
    (<0.0>,1543)
    (<0.0>,1544) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1547)
    (<0.0>,1548) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1550) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1551) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1553) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1559) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1565)
    (<0.0>,1566) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1569)
    (<0.0>,1570) tree.c:453: return &rsp->node[0];
    (<0.0>,1574) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1575) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1577) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1581) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1582) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1584) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1587) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1588) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1589) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1593) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1595) tree.c:494: }
    (<0.0>,1599) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1601) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1604) tree.c:1909: return true;
    (<0.0>,1606) tree.c:1910: }
    (<0.0>,1609) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1612) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1613) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1617) tree.c:1372: if (c_out != NULL)
    (<0.0>,1620) tree.c:1374: return ret;
    (<0.0>,1624) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1625) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1628) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1629) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1635) tree.c:1482: return ret;
    (<0.0>,1637) tree.c:1482: return ret;
    (<0.0>,1639) tree.c:1483: }
    (<0.0>,1641) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1643) tree.c:1527: }
    (<0.0>,1647) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1648) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1649) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1650) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1656)
    (<0.0>,1657)
    (<0.0>,1658)
    (<0.0>,1659) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1661) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1664) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1665) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1671)
    (<0.0>,1672)
    (<0.0>,1673) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1676)
    (<0.0>,1677) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1679) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1680) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1682) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1688) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1694)
    (<0.0>,1695) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1698)
    (<0.0>,1699) tree.c:453: return &rsp->node[0];
    (<0.0>,1703) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1704) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1706) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1710) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1711) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1713) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1716) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1717) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1718) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1722) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1724) tree.c:494: }
    (<0.0>,1728) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1730) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1733) tree.c:1909: return true;
    (<0.0>,1735) tree.c:1910: }
    (<0.0>,1739) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1740) tree.c:1937: return ret;
    (<0.0>,1744) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,1745) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1748)
    (<0.0>,1749) tree.c:453: return &rsp->node[0];
    (<0.0>,1754) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1758)
    (<0.0>,1759)
    (<0.0>,1760) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1761) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1764) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,1767)
    (<0.0>,1769) fake_sched.h:43: return __running_cpu;
    (<0.0>,1773) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1775) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1779) fake_sched.h:43: return __running_cpu;
    (<0.0>,1783) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1790) tree.c:2563: if (needwake)
    (<0.0>,1793) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,1796)
    (<0.0>,1797) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1798) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1800) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1807) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,1810)
    (<0.0>,1811) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1813) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1816) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1823) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,1826)
    (<0.0>,1830) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1833) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1834) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1835) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1839) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1840) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1841) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1843) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1847) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,1856)
    (<0.0>,1858) fake_sched.h:43: return __running_cpu;
    (<0.0>,1861) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1863) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1865) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1866) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1868) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1875) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1876) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1878) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1884) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1885) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1886) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1887) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1888) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1892)
    (<0.0>,1893)
    (<0.0>,1894) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1895) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1902)
    (<0.0>,1903)
    (<0.0>,1904) tree.c:1584: local_irq_save(flags);
    (<0.0>,1907)
    (<0.0>,1909) fake_sched.h:43: return __running_cpu;
    (<0.0>,1913) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1915) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1919) fake_sched.h:43: return __running_cpu;
    (<0.0>,1923) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1928) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1930) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1931) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1932) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1934) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1935) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1937) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1940) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1942) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1943) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1945) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1948) tree.c:1589: local_irq_restore(flags);
    (<0.0>,1951)
    (<0.0>,1953) fake_sched.h:43: return __running_cpu;
    (<0.0>,1957) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1959) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1963) fake_sched.h:43: return __running_cpu;
    (<0.0>,1967) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1974) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1976) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1981) tree.c:2558: local_irq_save(flags);
    (<0.0>,1984)
    (<0.0>,1986) fake_sched.h:43: return __running_cpu;
    (<0.0>,1990) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1992) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1996) fake_sched.h:43: return __running_cpu;
    (<0.0>,2000) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2005) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2006) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2012)
    (<0.0>,2013)
    (<0.0>,2014) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,2017)
    (<0.0>,2018) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2020) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2021) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2023) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2029) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,2035)
    (<0.0>,2036) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2039)
    (<0.0>,2040) tree.c:453: return &rsp->node[0];
    (<0.0>,2044) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2045) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2047) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2051) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2052) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2054) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2057) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2058) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2059) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2063) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2066) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2069) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2072) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2073) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2076) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2078) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2081) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2084) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2087) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2088) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2090) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2093) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2097) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2099) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2101) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2104) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2107) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2110) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2111) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2113) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2116) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2120) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2122) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2124) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2127) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,2129) tree.c:494: }
    (<0.0>,2133) tree.c:2566: local_irq_restore(flags);
    (<0.0>,2136)
    (<0.0>,2138) fake_sched.h:43: return __running_cpu;
    (<0.0>,2142) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2144) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2148) fake_sched.h:43: return __running_cpu;
    (<0.0>,2152) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2158) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2161)
    (<0.0>,2162) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2164) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2167) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2174) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2177)
    (<0.0>,2181) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2184) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2185) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2186) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2190) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2191) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2192) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2194) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2202) fake_sched.h:43: return __running_cpu;
    (<0.0>,2206) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,2215) tree.c:624: local_irq_save(flags);
    (<0.0>,2218)
    (<0.0>,2220) fake_sched.h:43: return __running_cpu;
    (<0.0>,2224) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2226) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2230) fake_sched.h:43: return __running_cpu;
    (<0.0>,2234) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2240) fake_sched.h:43: return __running_cpu;
    (<0.0>,2244) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2245) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2247) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2248) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2249) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2251) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2253) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2254) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2256) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2261) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2262) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2264) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2268) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2269) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2270) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2271) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2273) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2281)
    (<0.0>,2283) tree.c:634: local_irq_restore(flags);
    (<0.0>,2286)
    (<0.0>,2288) fake_sched.h:43: return __running_cpu;
    (<0.0>,2292) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2294) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2298) fake_sched.h:43: return __running_cpu;
    (<0.0>,2302) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2311) fake_sched.h:43: return __running_cpu;
    (<0.0>,2315)
    (<0.0>,2318) tree.c:580: local_irq_save(flags);
    (<0.0>,2321)
    (<0.0>,2323) fake_sched.h:43: return __running_cpu;
    (<0.0>,2327) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2329) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2333) fake_sched.h:43: return __running_cpu;
    (<0.0>,2337) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2350)
    (<0.0>,2352) fake_sched.h:43: return __running_cpu;
    (<0.0>,2356) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2357) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2359) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2360) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2361) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2367) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2368) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2373) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2374) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2375) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2376) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,2380) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2382) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2383) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2384) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2403)
    (<0.0>,2405)
    (<0.0>,2407) fake_sched.h:43: return __running_cpu;
    (<0.0>,2411) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2414) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,2418) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2419) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2420) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2424) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2425) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2426) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2428) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2433) fake_sched.h:43: return __running_cpu;
    (<0.0>,2436) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2438) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2440) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2441) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2444)
    (<0.0>,2447) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2450) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2451) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2452) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2456) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2457) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2458) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2460) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2465) fake_sched.h:43: return __running_cpu;
    (<0.0>,2468) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2470) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2472) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2473) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2476)
    (<0.0>,2479) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2482) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2483) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2484) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2488) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2489) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2490) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2492) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2499) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2502) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2503) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2504) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2506) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2507) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2509) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2512) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2518) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2519) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2522) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2527) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2528) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2529) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2543)
    (<0.0>,2545) tree.c:583: local_irq_restore(flags);
    (<0.0>,2548)
    (<0.0>,2550) fake_sched.h:43: return __running_cpu;
    (<0.0>,2554) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2556) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2560) fake_sched.h:43: return __running_cpu;
    (<0.0>,2564) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2570) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2573) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2578) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2580) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2589) fake_sched.h:43: return __running_cpu;
    (<0.0>,2593)
    (<0.0>,2594) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2597) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2602) tree.c:704: local_irq_save(flags);
    (<0.0>,2605)
    (<0.0>,2607) fake_sched.h:43: return __running_cpu;
    (<0.0>,2611) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2613) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2617) fake_sched.h:43: return __running_cpu;
    (<0.0>,2621) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2634)
    (<0.0>,2636) fake_sched.h:43: return __running_cpu;
    (<0.0>,2640) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2641) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2643) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2644) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2645) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2650) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2651) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2655) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2656) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2657) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2658) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,2662) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2664) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2665) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2666) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2680)
    (<0.0>,2681)
    (<0.0>,2683) fake_sched.h:43: return __running_cpu;
    (<0.0>,2687) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2691) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2694) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2695) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2696) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2698) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2699) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2701) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2704) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2711) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2712) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2715) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2720) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2721) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2722) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2727) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,2736)
    (<0.0>,2738) tree.c:707: local_irq_restore(flags);
    (<0.0>,2741)
    (<0.0>,2743) fake_sched.h:43: return __running_cpu;
    (<0.0>,2747) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2749) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2753) fake_sched.h:43: return __running_cpu;
    (<0.0>,2757) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2764) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,2776)
    (<0.0>,2777) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2780)
    (<0.0>,2781) tree.c:453: return &rsp->node[0];
    (<0.0>,2785) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2789) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2793)
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2804) fake_sched.h:43: return __running_cpu;
    (<0.0>,2808) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2813) fake_sched.h:43: return __running_cpu;
    (<0.0>,2817) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,2820) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2821) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2827) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2829) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2832) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2834) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2835) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2838)
    (<0.0>,2839) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2841) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2842) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2844) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2852) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2853) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2856)
    (<0.0>,2857) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2859) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2860) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2870) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2871) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2874) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,2879)
    (<0.0>,2880) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2881) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2882) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2883) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2885) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2889) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2890) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2891) update.c:344: if (till_stall_check < 3) {
    (<0.0>,2894) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,2898) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,2903) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,2904) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2905) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2907) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2909) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2910) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2911) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2914) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2916) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2920) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2922) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2924) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2926) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2930) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,2934)
    (<0.0>,2935) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2936) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2941) fake_sched.h:43: return __running_cpu;
    (<0.0>,2945) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,2947) fake_sched.h:43: return __running_cpu;
    (<0.0>,2951) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2957) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,2961)
    (<0.0>,2962) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2964) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2970) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2973) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2975) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2976) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2978) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2983) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2987)
    (<0.0>,2990) fake_sched.h:43: return __running_cpu;
    (<0.0>,2994) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2998) fake_sched.h:43: return __running_cpu;
    (<0.0>,3002) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3007) fake_sched.h:43: return __running_cpu;
    (<0.0>,3011) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3014) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3015) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3022) fake_sched.h:43: return __running_cpu;
    (<0.0>,3025) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3027) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3029) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3030) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,3036)
    (<0.0>,3037) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3039) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3044) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3045) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3047) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3051) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3052) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3053) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3055) tree.c:1656: rnp->qsmask = 0;
    (<0.0>,3057) tree.c:1656: rnp->qsmask = 0;
    (<0.0>,3058) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3060) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3061) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3063) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3064) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3066) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3067) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3069) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3074) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3075) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3077) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3078) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3080) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3084) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3085) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3086) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3087) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3089) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3090) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3092) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3093) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3094) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3096) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3099) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3100) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3101) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3107)
    (<0.0>,3108)
    (<0.0>,3109)
    (<0.0>,3110) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3112) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3113) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3115) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3118) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3119) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3120) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3129)
    (<0.0>,3130)
    (<0.0>,3131)
    (<0.0>,3132) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3135) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3138) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3141) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3142) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3145) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3146) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3151)
    (<0.0>,3152)
    (<0.0>,3153) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3156)
    (<0.0>,3157) tree.c:453: return &rsp->node[0];
    (<0.0>,3161) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3164) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3166) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3167) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3169) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3172) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3174) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3176) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3178) tree.c:1261: }
    (<0.0>,3180) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3181) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3183) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3186) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3188) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3191) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3192) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3195) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3198) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3202) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3204) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3206) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3209) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3211) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3214) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3215) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3218) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3221) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3224) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3226) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3229) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3230) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3235) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3237) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3241) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3244) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3247) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3248) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3250) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3253) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3254) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3255) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3257) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3260) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3262) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3264) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3266) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3269) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3272) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3273) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3275) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3278) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3279) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3280) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3282) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3285) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3287) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3289) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3291) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3294) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3295) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3304)
    (<0.0>,3305)
    (<0.0>,3306)
    (<0.0>,3307) tree.c:1289: bool ret = false;
    (<0.0>,3308) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3310) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3313)
    (<0.0>,3314) tree.c:453: return &rsp->node[0];
    (<0.0>,3318) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3319) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3321) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3322) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3327)
    (<0.0>,3328)
    (<0.0>,3329) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3332)
    (<0.0>,3333) tree.c:453: return &rsp->node[0];
    (<0.0>,3337) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3340) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3342) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3343) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3345) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3348) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3350) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3352) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3354) tree.c:1261: }
    (<0.0>,3356) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3357) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3358) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3359) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3365)
    (<0.0>,3366)
    (<0.0>,3367)
    (<0.0>,3368)
    (<0.0>,3372) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3374) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3377) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3380) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3382) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3383) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3385) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3388) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3390) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3393) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3395) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3396) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3397) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3398) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3404)
    (<0.0>,3405)
    (<0.0>,3406)
    (<0.0>,3407)
    (<0.0>,3412) tree.c:1372: if (c_out != NULL)
    (<0.0>,3415) tree.c:1374: return ret;
    (<0.0>,3419) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3420) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3423) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3424) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3430) tree.c:1482: return ret;
    (<0.0>,3432) tree.c:1482: return ret;
    (<0.0>,3434) tree.c:1483: }
    (<0.0>,3437) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3439) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3441) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3442) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3444) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3447) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3449) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3450) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3452) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3455) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3457) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3458) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3460) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3461) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3463) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3468) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3471) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3472) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,3475)
    (<0.0>,3478) tree.c:1575: return ret;
    (<0.0>,3482) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,3485)
    (<0.0>,3489) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,3493)
    (<0.0>,3494) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3495) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3500) fake_sched.h:43: return __running_cpu;
    (<0.0>,3504) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,3506) fake_sched.h:43: return __running_cpu;
    (<0.0>,3510) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3523) fake_sched.h:43: return __running_cpu;
    (<0.0>,3528) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,3534) fake_sched.h:43: return __running_cpu;
    (<0.0>,3539) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,3545) fake_sched.h:43: return __running_cpu;
    (<0.0>,3549) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,3561) fake_sched.h:43: return __running_cpu;
    (<0.0>,3565)
    (<0.0>,3568) tree.c:580: local_irq_save(flags);
    (<0.0>,3571)
    (<0.0>,3573) fake_sched.h:43: return __running_cpu;
    (<0.0>,3577) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3579) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3583) fake_sched.h:43: return __running_cpu;
    (<0.0>,3587) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3600)
    (<0.0>,3602) fake_sched.h:43: return __running_cpu;
    (<0.0>,3606) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3607) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3609) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3610) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3611) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3617) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3618) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3623) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3624) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3625) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3626) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3630) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3632) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3633) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3634) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3653)
    (<0.0>,3655)
    (<0.0>,3657) fake_sched.h:43: return __running_cpu;
    (<0.0>,3661) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3664) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,3668) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3669) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3670) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3674) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3675) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3676) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3678) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3683) fake_sched.h:43: return __running_cpu;
    (<0.0>,3686) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3688) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3690) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3691) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3694)
    (<0.0>,3697) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3700) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3701) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3702) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3706) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3707) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3708) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3710) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3715) fake_sched.h:43: return __running_cpu;
    (<0.0>,3718) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3720) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3722) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3723) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3726)
    (<0.0>,3729) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3732) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3733) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3734) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3738) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3739) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3740) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3742) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3749) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3752) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3753) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3754) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3756) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3757) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3759) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3762) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3768) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3769) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3772) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3777) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3778) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3779) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3793)
    (<0.0>,3795) tree.c:583: local_irq_restore(flags);
    (<0.0>,3798)
    (<0.0>,3800) fake_sched.h:43: return __running_cpu;
    (<0.0>,3804) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3806) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3810) fake_sched.h:43: return __running_cpu;
    (<0.0>,3814) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3820) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3823) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3828) fake_sched.h:43: return __running_cpu;
    (<0.0>,3832)
    (<0.0>,3833) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3836) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3841) tree.c:704: local_irq_save(flags);
    (<0.0>,3844)
    (<0.0>,3846) fake_sched.h:43: return __running_cpu;
    (<0.0>,3850) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3852) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3856) fake_sched.h:43: return __running_cpu;
    (<0.0>,3860) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3873)
    (<0.0>,3875) fake_sched.h:43: return __running_cpu;
    (<0.0>,3879) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3880) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3882) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3883) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3884) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3889) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3890) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3894) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3895) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3896) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3897) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3901) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3903) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3904) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3905) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3919)
    (<0.0>,3920)
    (<0.0>,3922) fake_sched.h:43: return __running_cpu;
    (<0.0>,3926) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3930) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3933) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3934) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3935) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3937) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3938) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3940) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3943) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3950) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3951) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3954) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3959) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3960) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3961) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3966) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,3975)
    (<0.0>,3977) tree.c:707: local_irq_restore(flags);
    (<0.0>,3980)
    (<0.0>,3982) fake_sched.h:43: return __running_cpu;
    (<0.0>,3986) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3988) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3992) fake_sched.h:43: return __running_cpu;
    (<0.0>,3996) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4011) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4013) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4015) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4016) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4018) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4023) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,4027)
    (<0.0>,4028) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4030) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4034) tree.c:1674: return 1;
    (<0.0>,4036) tree.c:1675: }
    (<0.0>,4041) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,4042) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4043) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4044) tree.c:1819: if (j > HZ) {
    (<0.0>,4047) tree.c:1823: ret = 0;
    (<0.0>,4049) tree.c:1825: if (!ret)
    (<0.0>,4052) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4053) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4055) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4057) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4061) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4063) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4067) fake_sched.h:43: return __running_cpu;
    (<0.0>,4071) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4075) fake_sched.h:43: return __running_cpu;
    (<0.0>,4079) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4084) fake_sched.h:43: return __running_cpu;
    (<0.0>,4088) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4098) tree.c:749: local_irq_save(flags);
    (<0.0>,4101)
    (<0.0>,4103) fake_sched.h:43: return __running_cpu;
    (<0.0>,4107) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4109) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4114) fake_sched.h:43: return __running_cpu;
    (<0.0>,4118) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4119) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4121) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4122) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4123) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4125) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4127) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4128) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4130) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4135) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4136) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4138) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4142) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4143) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4144) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4145) tree.c:754: if (oldval)
    (<0.0>,4153)
    (<0.0>,4155) tree.c:759: local_irq_restore(flags);
    (<0.0>,4158)
    (<0.0>,4160) fake_sched.h:43: return __running_cpu;
    (<0.0>,4164) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4166) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4174)
    (<0.0>,4179) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,4184) fake_sched.h:43: return __running_cpu;
    (<0.0>,4189) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,4197) fake_sched.h:43: return __running_cpu;
    (<0.0>,4202) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,4216) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4217) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4218) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4222) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4223) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4224) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4226) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4230) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4232) fake_sched.h:43: return __running_cpu;
    (<0.0>,4235) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4237) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4244)
    (<0.0>,4245)
    (<0.0>,4246) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4248) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4249) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4250) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4252) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4254) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4255) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4256) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4266)
    (<0.0>,4267)
    (<0.0>,4268) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,4273) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,4276)
    (<0.0>,4279) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,4282) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4284) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4287) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4289) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4293) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,4296)
    (<0.0>,4297) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4299) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4302) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4309) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4310) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4316)
    (<0.0>,4317)
    (<0.0>,4318) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,4321)
    (<0.0>,4322) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4324) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4325) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4327) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4333) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,4335) tree.c:494: }
    (<0.0>,4339) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4341) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4342) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4344) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4347) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4349) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4350) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4352) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4355) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,4358)
    (<0.0>,4362) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4364) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4366) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4367) tree.c:3175: return 0;
    (<0.0>,4369) tree.c:3176: }
    (<0.0>,4374) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4377) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4378) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4379) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4383) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4384) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4385) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4387) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4391) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4393) fake_sched.h:43: return __running_cpu;
    (<0.0>,4396) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4398) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4405)
    (<0.0>,4406)
    (<0.0>,4407) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4409) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4410) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4411) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4413) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4415) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4416) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4417) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4427)
    (<0.0>,4428)
    (<0.0>,4429) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,4434) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,4437)
    (<0.0>,4440) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,4443) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4445) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4448) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4450) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4454) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,4457)
    (<0.0>,4458) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4460) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4463) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4470) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4471) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4477)
    (<0.0>,4478)
    (<0.0>,4479) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,4482)
    (<0.0>,4483) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4485) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4486) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4488) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4494) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,4500)
    (<0.0>,4501) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,4504)
    (<0.0>,4505) tree.c:453: return &rsp->node[0];
    (<0.0>,4509) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,4510) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4512) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4516) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4517) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,4519) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,4522) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,4523) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,4524) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,4528) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,4531) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,4534) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,4537) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,4538) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,4541) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4543) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4546) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4549) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4552) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4553) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4555) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4558) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4562) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4564) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4566) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4569) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4572) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4575) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4576) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4578) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4581) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4585) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4587) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4589) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4592) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,4594) tree.c:494: }
    (<0.0>,4598) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4600) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4601) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4603) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4606) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4608) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4609) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4611) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4614) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,4617)
    (<0.0>,4621) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4623) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4625) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4626) tree.c:3175: return 0;
    (<0.0>,4628) tree.c:3176: }
    (<0.0>,4633) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4636) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4637) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4638) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4642) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4643) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4644) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4646) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4650) tree.c:3190: return 0;
    (<0.0>,4652) tree.c:3191: }
    (<0.0>,4656) tree.c:2437: if (user)
    (<0.0>,4664) fake_sched.h:43: return __running_cpu;
    (<0.0>,4668) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4670) fake_sched.h:43: return __running_cpu;
    (<0.0>,4674) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4680) fake_sched.h:43: return __running_cpu;
    (<0.0>,4684) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,4694) tree.c:624: local_irq_save(flags);
    (<0.0>,4697)
    (<0.0>,4699) fake_sched.h:43: return __running_cpu;
    (<0.0>,4703) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4705) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4709) fake_sched.h:43: return __running_cpu;
    (<0.0>,4713) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4719) fake_sched.h:43: return __running_cpu;
    (<0.0>,4723) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4724) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,4726) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,4727) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,4728) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,4730) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,4732) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,4733) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4735) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4740) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4741) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4743) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4747) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4748) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4749) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4750) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,4752) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,4760)
    (<0.0>,4762) tree.c:634: local_irq_restore(flags);
    (<0.0>,4765)
    (<0.0>,4767) fake_sched.h:43: return __running_cpu;
    (<0.0>,4771) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4773) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4777) fake_sched.h:43: return __running_cpu;
    (<0.0>,4781) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4790) fake_sched.h:43: return __running_cpu;
    (<0.0>,4794)
    (<0.0>,4797) tree.c:580: local_irq_save(flags);
    (<0.0>,4800)
    (<0.0>,4802) fake_sched.h:43: return __running_cpu;
    (<0.0>,4806) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4808) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4812) fake_sched.h:43: return __running_cpu;
    (<0.0>,4816) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4829)
    (<0.0>,4831) fake_sched.h:43: return __running_cpu;
    (<0.0>,4835) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4836) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,4838) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,4839) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,4840) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4846) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4847) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4852) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4853) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4854) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4855) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,4859) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,4861) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,4862) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,4863) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,4882)
    (<0.0>,4884)
    (<0.0>,4886) fake_sched.h:43: return __running_cpu;
    (<0.0>,4890) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4893) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,4897) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4898) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4899) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4903) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4904) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4905) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4907) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4912) fake_sched.h:43: return __running_cpu;
    (<0.0>,4915) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4917) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4919) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4920) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,4923)
    (<0.0>,4926) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4929) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4930) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4931) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4935) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4936) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4937) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4939) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4944) fake_sched.h:43: return __running_cpu;
    (<0.0>,4947) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4949) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4951) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4952) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,4955)
    (<0.0>,4958) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4961) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4962) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4963) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4967) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4968) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4969) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4971) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4978) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4981) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4982) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4983) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4985) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4986) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4988) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,4991) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,4997) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,4998) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5001) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5006) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5007) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5008) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5022)
    (<0.0>,5024) tree.c:583: local_irq_restore(flags);
    (<0.0>,5027)
    (<0.0>,5029) fake_sched.h:43: return __running_cpu;
    (<0.0>,5033) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5035) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5039) fake_sched.h:43: return __running_cpu;
    (<0.0>,5043) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5049) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5052) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5057) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5059) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5061) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5065) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5067) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5070) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5073)
    (<0.0>,5082) fake_sched.h:43: return __running_cpu;
    (<0.0>,5086)
    (<0.0>,5087) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5090) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5095) tree.c:704: local_irq_save(flags);
    (<0.0>,5098)
    (<0.0>,5100) fake_sched.h:43: return __running_cpu;
    (<0.0>,5104) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5106) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5110) fake_sched.h:43: return __running_cpu;
    (<0.0>,5114) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5127)
    (<0.0>,5129) fake_sched.h:43: return __running_cpu;
    (<0.0>,5133) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5134) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5136) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5137) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5138) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5143) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5144) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5148) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5149) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5150) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5151) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,5155) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5157) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5158) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5159) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5173)
    (<0.0>,5174)
    (<0.0>,5176) fake_sched.h:43: return __running_cpu;
    (<0.0>,5180) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5184) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5187) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5188) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5189) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5191) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5192) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5194) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5197) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5204) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5205) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5208) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5213) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5214) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5215) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5220) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,5229)
    (<0.0>,5231) tree.c:707: local_irq_restore(flags);
    (<0.0>,5234)
    (<0.0>,5236) fake_sched.h:43: return __running_cpu;
    (<0.0>,5240) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5242) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5246) fake_sched.h:43: return __running_cpu;
    (<0.0>,5250) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5257) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5258) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5259) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5260) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,5262) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,5265) tree.c:1840: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,5268)
    (<0.0>,5273) tree.c:1872: rcu_gp_cleanup(rsp);
    (<0.0>,5281)
    (<0.0>,5282) tree.c:1720: bool needgp = false;
    (<0.0>,5283) tree.c:1721: int nocb = 0;
    (<0.0>,5284) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5287)
    (<0.0>,5288) tree.c:453: return &rsp->node[0];
    (<0.0>,5292) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5293) tree.c:1725: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,5297)
    (<0.0>,5300) fake_sched.h:43: return __running_cpu;
    (<0.0>,5304) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5308) fake_sched.h:43: return __running_cpu;
    (<0.0>,5312) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5317) fake_sched.h:43: return __running_cpu;
    (<0.0>,5321) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5324) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5325) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5331) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5332) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5334) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5336) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5337) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5338) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5340) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5343) tree.c:1739: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,5347)
    (<0.0>,5348) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5349) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5354) fake_sched.h:43: return __running_cpu;
    (<0.0>,5358) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,5360) fake_sched.h:43: return __running_cpu;
    (<0.0>,5364) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5370) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5373) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5375) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5376) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5378) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5383) tree.c:1751: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,5387)
    (<0.0>,5390) fake_sched.h:43: return __running_cpu;
    (<0.0>,5394) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5398) fake_sched.h:43: return __running_cpu;
    (<0.0>,5402) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5407) fake_sched.h:43: return __running_cpu;
    (<0.0>,5411) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5414) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5415) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5421) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5423) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5424) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5426) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5428) fake_sched.h:43: return __running_cpu;
    (<0.0>,5431) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5433) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5435) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5436) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5437) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5439) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5442) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5443) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5444) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5450)
    (<0.0>,5451)
    (<0.0>,5452)
    (<0.0>,5453) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5455) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5456) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5458) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5461) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5462) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5463) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5471)
    (<0.0>,5472)
    (<0.0>,5473)
    (<0.0>,5474) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5477) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5480) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5483) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5484) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5487) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5489) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5492) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5494) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5495) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5497) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5500) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5504) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5506) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5509) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5510) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5513) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5515) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5517) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5519) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5522) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5524) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5525) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5527) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5530) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5535) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5537) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5538) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5541) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,5544) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,5545) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,5547) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,5550) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,5552) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5554) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5556) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5557) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5560) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,5562) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,5565) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5567) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5570) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5571) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5574) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5578) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,5579) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,5580) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,5589)
    (<0.0>,5590)
    (<0.0>,5591)
    (<0.0>,5592) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5595) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5598) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5601) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5602) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5605) tree.c:1434: return false;
    (<0.0>,5607) tree.c:1483: }
    (<0.0>,5609) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,5611) tree.c:1527: }
    (<0.0>,5614) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5615) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,5617) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,5618) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,5620) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,5624) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,5626) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,5627) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,5629) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,5632) tree.c:1575: return ret;
    (<0.0>,5636) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5640) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5642) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,5643) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,5650)
    (<0.0>,5651)
    (<0.0>,5652) tree.c:1385: int c = rnp->completed;
    (<0.0>,5654) tree.c:1385: int c = rnp->completed;
    (<0.0>,5656) tree.c:1385: int c = rnp->completed;
    (<0.0>,5658) fake_sched.h:43: return __running_cpu;
    (<0.0>,5661) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5663) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5665) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5666) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,5667) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,5671)
    (<0.0>,5672)
    (<0.0>,5674) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,5677) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,5680) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,5681) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,5685) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,5688) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,5689) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,5690) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,5691) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,5692) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,5694) tree.c:1393: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,5702)
    (<0.0>,5703)
    (<0.0>,5704)
    (<0.0>,5705)
    (<0.0>,5709) tree.c:1394: return needmore;
    (<0.0>,5711) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,5713) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,5714) tree.c:1759: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,5718)
    (<0.0>,5719) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5720) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5725) fake_sched.h:43: return __running_cpu;
    (<0.0>,5729) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,5731) fake_sched.h:43: return __running_cpu;
    (<0.0>,5735) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5748) fake_sched.h:43: return __running_cpu;
    (<0.0>,5753) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,5760) fake_sched.h:43: return __running_cpu;
    (<0.0>,5764) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,5776) fake_sched.h:43: return __running_cpu;
    (<0.0>,5780)
    (<0.0>,5783) tree.c:580: local_irq_save(flags);
    (<0.0>,5786)
    (<0.0>,5788) fake_sched.h:43: return __running_cpu;
    (<0.0>,5792) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5794) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5798) fake_sched.h:43: return __running_cpu;
    (<0.0>,5802) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5815)
    (<0.0>,5817) fake_sched.h:43: return __running_cpu;
    (<0.0>,5821) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5822) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5824) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5825) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5826) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5832) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5833) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5838) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5839) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5840) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5841) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5845) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5847) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5848) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5849) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5868)
    (<0.0>,5870)
    (<0.0>,5872) fake_sched.h:43: return __running_cpu;
    (<0.0>,5876) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5879) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,5883) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5884) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5885) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5889) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5890) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5891) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5893) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5898) fake_sched.h:43: return __running_cpu;
    (<0.0>,5901) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5903) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5905) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5906) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5909)
    (<0.0>,5912) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5915) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5916) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5917) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5921) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5922) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5923) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5925) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5930) fake_sched.h:43: return __running_cpu;
    (<0.0>,5933) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5935) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5937) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5938) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5941)
    (<0.0>,5944) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5947) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5948) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5949) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5953) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5954) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5955) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5957) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5964) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5967) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5968) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5969) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5971) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5972) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5974) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5977) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5983) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5984) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5987) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5992) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5993) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5994) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6008)
    (<0.0>,6010) tree.c:583: local_irq_restore(flags);
    (<0.0>,6013)
    (<0.0>,6015) fake_sched.h:43: return __running_cpu;
    (<0.0>,6019) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6021) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6025) fake_sched.h:43: return __running_cpu;
    (<0.0>,6029) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6035) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6038) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6043) fake_sched.h:43: return __running_cpu;
    (<0.0>,6047)
    (<0.0>,6048) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6051) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6056) tree.c:704: local_irq_save(flags);
    (<0.0>,6059)
    (<0.0>,6061) fake_sched.h:43: return __running_cpu;
    (<0.0>,6065) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6067) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6071) fake_sched.h:43: return __running_cpu;
    (<0.0>,6075) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6088)
    (<0.0>,6090) fake_sched.h:43: return __running_cpu;
    (<0.0>,6094) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6095) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6097) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6098) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6099) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6104) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6105) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6109) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6110) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6111) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6112) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,6116) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6118) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6119) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6120) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6134)
    (<0.0>,6135)
    (<0.0>,6137) fake_sched.h:43: return __running_cpu;
    (<0.0>,6141) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6145) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6148) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6149) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6150) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6152) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6153) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6155) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6158) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6165) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6166) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6169) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6174) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6175) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6176) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6181) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,6190)
    (<0.0>,6192) tree.c:707: local_irq_restore(flags);
    (<0.0>,6195)
    (<0.0>,6197) fake_sched.h:43: return __running_cpu;
    (<0.0>,6201) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6203) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6207) fake_sched.h:43: return __running_cpu;
    (<0.0>,6211) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6226) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6228) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6230) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6231) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6233) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6238) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,6241)
    (<0.0>,6242) tree.c:453: return &rsp->node[0];
    (<0.0>,6246) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,6247) tree.c:1763: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,6251)
    (<0.0>,6254) fake_sched.h:43: return __running_cpu;
    (<0.0>,6258) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6262) fake_sched.h:43: return __running_cpu;
    (<0.0>,6266) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6271) fake_sched.h:43: return __running_cpu;
    (<0.0>,6275) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,6278) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6279) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6285) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,6286) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,6290)
    (<0.0>,6291)
    (<0.0>,6293) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6295) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6296) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6298) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6301) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,6303) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,6305) fake_sched.h:43: return __running_cpu;
    (<0.0>,6308) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6310) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6312) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6313) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6314) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6315) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6323)
    (<0.0>,6324)
    (<0.0>,6325)
    (<0.0>,6326) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6329) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6332) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6335) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6336) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6339) tree.c:1502: return false;
    (<0.0>,6341) tree.c:1527: }
    (<0.0>,6344) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6348) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6349) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6352) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6353) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6359)
    (<0.0>,6360)
    (<0.0>,6361) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,6364)
    (<0.0>,6365) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6367) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6368) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6370) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6376) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,6382)
    (<0.0>,6383) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6386)
    (<0.0>,6387) tree.c:453: return &rsp->node[0];
    (<0.0>,6391) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6392) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6394) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6398) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6399) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6401) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6404) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6405) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,6406) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,6410) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,6412) tree.c:494: }
    (<0.0>,6416) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,6418) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,6422) tree.c:1780: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,6426)
    (<0.0>,6427) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6428) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6433) fake_sched.h:43: return __running_cpu;
    (<0.0>,6437) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6439) fake_sched.h:43: return __running_cpu;
    (<0.0>,6443) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6454) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,6456) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,6460) fake_sched.h:43: return __running_cpu;
    (<0.0>,6464) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6468) fake_sched.h:43: return __running_cpu;
    (<0.0>,6472) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6477) fake_sched.h:43: return __running_cpu;
    (<0.0>,6481) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,6491) tree.c:749: local_irq_save(flags);
    (<0.0>,6494)
    (<0.0>,6496) fake_sched.h:43: return __running_cpu;
    (<0.0>,6500) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6502) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6507) fake_sched.h:43: return __running_cpu;
    (<0.0>,6511) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6512) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,6514) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,6515) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,6516) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,6518) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,6520) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,6521) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6523) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6528) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6529) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6531) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6535) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6536) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6537) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6538) tree.c:754: if (oldval)
    (<0.0>,6546)
    (<0.0>,6548) tree.c:759: local_irq_restore(flags);
    (<0.0>,6551)
    (<0.0>,6553) fake_sched.h:43: return __running_cpu;
    (<0.0>,6557) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6559) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6567)
    (<0.0>,6572) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,6577) fake_sched.h:43: return __running_cpu;
    (<0.0>,6582) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,6590) fake_sched.h:43: return __running_cpu;
    (<0.0>,6595) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,6609) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6610) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6611) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6615) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6616) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6617) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6619) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6623) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,6625) fake_sched.h:43: return __running_cpu;
    (<0.0>,6628) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,6630) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,6637)
    (<0.0>,6638)
    (<0.0>,6639) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6641) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6642) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6643) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,6645) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,6647) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,6648) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,6649) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,6659)
    (<0.0>,6660)
    (<0.0>,6661) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,6666) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,6669)
    (<0.0>,6672) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,6675) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,6677) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,6680) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,6682) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,6686) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,6689)
    (<0.0>,6690) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6692) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6695) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6698) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,6701) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,6708) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,6710) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,6712) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,6713) tree.c:3146: return 1;
    (<0.0>,6715) tree.c:3176: }
    (<0.0>,6719) tree.c:3189: return 1;
    (<0.0>,6721) tree.c:3191: }
    (<0.0>,6728) fake_sched.h:43: return __running_cpu;
    (<0.0>,6732) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,6736) tree.c:2437: if (user)
    (<0.0>,6744) fake_sched.h:43: return __running_cpu;
    (<0.0>,6748) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6750) fake_sched.h:43: return __running_cpu;
    (<0.0>,6754) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6760) fake_sched.h:43: return __running_cpu;
    (<0.0>,6764) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,6774)
    (<0.0>,6777) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6778) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6779) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6783) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6784) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6785) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6787) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6791) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,6800)
    (<0.0>,6802) fake_sched.h:43: return __running_cpu;
    (<0.0>,6805) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6807) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6809) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6810) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6812) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6819) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6820) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6822) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6828) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6829) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6830) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6831) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6832) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6836)
    (<0.0>,6837)
    (<0.0>,6838) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,6839) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,6846)
    (<0.0>,6847)
    (<0.0>,6848) tree.c:1584: local_irq_save(flags);
    (<0.0>,6851)
    (<0.0>,6853) fake_sched.h:43: return __running_cpu;
    (<0.0>,6857) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6859) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6863) fake_sched.h:43: return __running_cpu;
    (<0.0>,6867) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6872) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,6874) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,6875) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,6876) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,6878) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,6879) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,6881) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,6884) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,6886) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,6887) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,6889) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,6892) tree.c:1589: local_irq_restore(flags);
    (<0.0>,6895)
    (<0.0>,6897) fake_sched.h:43: return __running_cpu;
    (<0.0>,6901) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6903) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6907) fake_sched.h:43: return __running_cpu;
    (<0.0>,6911) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6918) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,6920) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,6925) tree.c:2558: local_irq_save(flags);
    (<0.0>,6928)
    (<0.0>,6930) fake_sched.h:43: return __running_cpu;
    (<0.0>,6934) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6936) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6940) fake_sched.h:43: return __running_cpu;
    (<0.0>,6944) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6949) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6950) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6956)
    (<0.0>,6957)
    (<0.0>,6958) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,6961)
    (<0.0>,6962) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6964) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6965) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6967) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6973) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,6979)
    (<0.0>,6980) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6983)
    (<0.0>,6984) tree.c:453: return &rsp->node[0];
    (<0.0>,6988) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6989) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6991) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6995) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6996) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6998) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7001) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7002) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7003) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7007) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7009) tree.c:494: }
    (<0.0>,7013) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,7016)
    (<0.0>,7017) tree.c:453: return &rsp->node[0];
    (<0.0>,7024)
    (<0.0>,7026) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7027) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7031) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,7037)
    (<0.0>,7039) fake_sched.h:43: return __running_cpu;
    (<0.0>,7042) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7044) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7046) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7047) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7050)
    (<0.0>,7051) tree.c:453: return &rsp->node[0];
    (<0.0>,7055) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7056) tree.c:1925: bool ret = false;
    (<0.0>,7057) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7058) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7059) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7067)
    (<0.0>,7068)
    (<0.0>,7069)
    (<0.0>,7070) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7073) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7076) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7079) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7080) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7083) tree.c:1502: return false;
    (<0.0>,7085) tree.c:1527: }
    (<0.0>,7088) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7092) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7093) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7094) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7095) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7101)
    (<0.0>,7102)
    (<0.0>,7103)
    (<0.0>,7104) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7106) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7109) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7110) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7116)
    (<0.0>,7117)
    (<0.0>,7118) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7121)
    (<0.0>,7122) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7124) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7125) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7127) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7133) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7139)
    (<0.0>,7140) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7143)
    (<0.0>,7144) tree.c:453: return &rsp->node[0];
    (<0.0>,7148) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7149) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7151) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7155) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7156) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7158) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7161) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7162) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7163) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7167) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7169) tree.c:494: }
    (<0.0>,7173) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7175) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7178) tree.c:1909: return true;
    (<0.0>,7180) tree.c:1910: }
    (<0.0>,7184) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7185) tree.c:1937: return ret;
    (<0.0>,7189) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,7190) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,7193)
    (<0.0>,7194) tree.c:453: return &rsp->node[0];
    (<0.0>,7199) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,7203)
    (<0.0>,7204)
    (<0.0>,7205) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7206) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7209) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,7212)
    (<0.0>,7214) fake_sched.h:43: return __running_cpu;
    (<0.0>,7218) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7220) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7224) fake_sched.h:43: return __running_cpu;
    (<0.0>,7228) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7235) tree.c:2563: if (needwake)
    (<0.0>,7238) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,7241)
    (<0.0>,7242) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7243) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7245) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7252) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7255)
    (<0.0>,7256) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7258) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7261) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7264) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7267) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7274) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,7275) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,7279)
    (<0.0>,7280)
    (<0.0>,7281) tree.c:2601: if (unlikely(!ACCESS_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,7290) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,7292) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,7301) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,7302) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,7319)
    (<0.0>,7320)
    (<0.0>,7321) tree.c:2313: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,7324)
    (<0.0>,7325) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7327) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7330) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7333) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7336) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7343) tree.c:2325: local_irq_save(flags);
    (<0.0>,7346)
    (<0.0>,7348) fake_sched.h:43: return __running_cpu;
    (<0.0>,7352) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7354) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7358) fake_sched.h:43: return __running_cpu;
    (<0.0>,7362) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7367) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7368) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7369) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7370) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7371) tree.c:2327: bl = rdp->blimit;
    (<0.0>,7373) tree.c:2327: bl = rdp->blimit;
    (<0.0>,7374) tree.c:2327: bl = rdp->blimit;
    (<0.0>,7377) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,7379) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,7380) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,7381) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7384) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7385) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7386) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7388) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7389) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,7392) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,7393) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,7394) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7397) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7398) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7399) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7401) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7404) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7406) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7409) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7410) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7413) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7416) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7418) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7420) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7423) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7426) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7428) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7430) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7433) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7435) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7438) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7439) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7442) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7445) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7447) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7449) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7452) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7455) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7457) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7459) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7462) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7464) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7467) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7468) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7471) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7474) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7476) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7478) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7481) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7484) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7486) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7488) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7491) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7493) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7496) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7497) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7500) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7503) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7505) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7507) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7510) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7513) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7515) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7517) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7520) tree.c:2336: local_irq_restore(flags);
    (<0.0>,7523)
    (<0.0>,7525) fake_sched.h:43: return __running_cpu;
    (<0.0>,7529) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7531) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7535) fake_sched.h:43: return __running_cpu;
    (<0.0>,7539) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7544) tree.c:2339: count = count_lazy = 0;
    (<0.0>,7545) tree.c:2339: count = count_lazy = 0;
    (<0.0>,7547) tree.c:2340: while (list) {
    (<0.0>,7550) tree.c:2341: next = list->next;
    (<0.0>,7552) tree.c:2341: next = list->next;
    (<0.0>,7553) tree.c:2341: next = list->next;
    (<0.0>,7556) tree.c:2343: debug_rcu_head_unqueue(list);
    (<0.0>,7559)
    (<0.0>,7561) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,7563) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,7564) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,7570)
    (<0.0>,7571)
    (<0.0>,7572) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,7574) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,7576) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,7579) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,7582) rcu.h:118: head->func(head);
    (<0.0>,7584) rcu.h:118: head->func(head);
    (<0.0>,7585) rcu.h:118: head->func(head);
    (<0.0>,7591)
    (<0.0>,7592) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7593) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7594) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7598) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7599) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7600) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7601) update.c:216: complete(&rcu->completion);
    (<0.0>,7605)
    (<0.0>,7606) fake_sync.h:248: x->done++;
    (<0.0>,7608) fake_sync.h:248: x->done++;
    (<0.0>,7610) fake_sync.h:248: x->done++;
    (<0.0>,7615) rcu.h:120: return false;
    (<0.0>,7617) rcu.h:122: }
    (<0.0>,7620) tree.c:2346: list = next;
    (<0.0>,7621) tree.c:2346: list = next;
    (<0.0>,7622) tree.c:2348: if (++count >= bl &&
    (<0.0>,7624) tree.c:2348: if (++count >= bl &&
    (<0.0>,7625) tree.c:2348: if (++count >= bl &&
    (<0.0>,7629) tree.c:2340: while (list) {
    (<0.0>,7632) tree.c:2354: local_irq_save(flags);
    (<0.0>,7635)
    (<0.0>,7637) fake_sched.h:43: return __running_cpu;
    (<0.0>,7641) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7643) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7647) fake_sched.h:43: return __running_cpu;
    (<0.0>,7651) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7658) tree.c:2360: if (list != NULL) {
    (<0.0>,7662) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,7663) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,7665) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,7667) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,7668) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,7670) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,7671) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,7673) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,7675) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,7676) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,7677) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,7679) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,7681) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,7682) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,7684) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,7687) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,7689) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,7692) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,7694) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,7697) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,7699) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,7700) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,7702) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,7703) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,7708) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7710) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7713) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7715) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7722) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7723) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7725) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7728) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7730) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7736) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7737) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7738) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7739) tree.c:2386: local_irq_restore(flags);
    (<0.0>,7742)
    (<0.0>,7744) fake_sched.h:43: return __running_cpu;
    (<0.0>,7748) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7750) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7754) fake_sched.h:43: return __running_cpu;
    (<0.0>,7758) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7763) tree.c:2389: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7766)
    (<0.0>,7767) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7769) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7772) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7783) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7786)
    (<0.0>,7790) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7793) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7794) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7795) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7799) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7800) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7801) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7803) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7807) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,7816)
    (<0.0>,7818) fake_sched.h:43: return __running_cpu;
    (<0.0>,7821) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7823) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7825) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7826) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7828) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7835) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7836) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7838) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7844) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7845) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7846) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7847) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7848) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7852)
    (<0.0>,7853)
    (<0.0>,7854) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7855) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7862)
    (<0.0>,7863)
    (<0.0>,7864) tree.c:1584: local_irq_save(flags);
    (<0.0>,7867)
    (<0.0>,7869) fake_sched.h:43: return __running_cpu;
    (<0.0>,7873) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7875) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7879) fake_sched.h:43: return __running_cpu;
    (<0.0>,7883) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7888) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7890) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7891) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7892) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7894) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7895) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7897) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7900) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7902) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7903) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7905) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7908) tree.c:1589: local_irq_restore(flags);
    (<0.0>,7911)
    (<0.0>,7913) fake_sched.h:43: return __running_cpu;
    (<0.0>,7917) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7919) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7923) fake_sched.h:43: return __running_cpu;
    (<0.0>,7927) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7934) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7936) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7941) tree.c:2558: local_irq_save(flags);
    (<0.0>,7944)
    (<0.0>,7946) fake_sched.h:43: return __running_cpu;
    (<0.0>,7950) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7952) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7956) fake_sched.h:43: return __running_cpu;
    (<0.0>,7960) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7965) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7966) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7972)
    (<0.0>,7973)
    (<0.0>,7974) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7977)
    (<0.0>,7978) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7980) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7981) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7983) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7989) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7995)
    (<0.0>,7996) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7999)
    (<0.0>,8000) tree.c:453: return &rsp->node[0];
    (<0.0>,8004) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8005) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8007) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8011) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8012) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8014) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8017) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8018) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8019) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8023) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8026) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8029) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8032) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8033) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8036) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8038) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8041) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8044) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8047) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8048) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8050) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8053) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8057) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8059) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8061) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8064) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8067) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8070) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8071) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8073) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8076) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8080) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8082) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8084) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8087) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,8089) tree.c:494: }
    (<0.0>,8093) tree.c:2566: local_irq_restore(flags);
    (<0.0>,8096)
    (<0.0>,8098) fake_sched.h:43: return __running_cpu;
    (<0.0>,8102) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8104) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8108) fake_sched.h:43: return __running_cpu;
    (<0.0>,8112) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8118) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,8121)
    (<0.0>,8122) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8124) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8127) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8134) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8137)
    (<0.0>,8141) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8144) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8145) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8146) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8150) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8151) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8152) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8154) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8162) fake_sched.h:43: return __running_cpu;
    (<0.0>,8166) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,8175) tree.c:624: local_irq_save(flags);
    (<0.0>,8178)
    (<0.0>,8180) fake_sched.h:43: return __running_cpu;
    (<0.0>,8184) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8186) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8190) fake_sched.h:43: return __running_cpu;
    (<0.0>,8194) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8200) fake_sched.h:43: return __running_cpu;
    (<0.0>,8204) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8205) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8207) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8208) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8209) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8211) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8213) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8214) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8216) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8221) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8222) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8224) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8228) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8229) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8230) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8231) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,8233) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,8241)
    (<0.0>,8243) tree.c:634: local_irq_restore(flags);
    (<0.0>,8246)
    (<0.0>,8248) fake_sched.h:43: return __running_cpu;
    (<0.0>,8252) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8254) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8258) fake_sched.h:43: return __running_cpu;
    (<0.0>,8262) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8271) fake_sched.h:43: return __running_cpu;
    (<0.0>,8275)
    (<0.0>,8278) tree.c:580: local_irq_save(flags);
    (<0.0>,8281)
    (<0.0>,8283) fake_sched.h:43: return __running_cpu;
    (<0.0>,8287) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8289) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8293) fake_sched.h:43: return __running_cpu;
    (<0.0>,8297) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8310)
    (<0.0>,8312) fake_sched.h:43: return __running_cpu;
    (<0.0>,8316) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8317) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8319) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8320) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8321) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8327) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8328) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8333) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8334) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8335) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8336) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,8340) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,8342) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,8343) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8344) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8363)
    (<0.0>,8365)
    (<0.0>,8367) fake_sched.h:43: return __running_cpu;
    (<0.0>,8371) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8374) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,8378) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8379) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8380) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8384) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8385) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8386) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8388) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8393) fake_sched.h:43: return __running_cpu;
    (<0.0>,8396) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8398) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8400) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8401) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8404)
    (<0.0>,8407) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8410) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8411) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8412) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8416) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8417) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8418) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8420) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8425) fake_sched.h:43: return __running_cpu;
    (<0.0>,8428) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8430) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8432) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8433) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8436)
    (<0.0>,8439) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8442) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8443) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8444) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8448) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8449) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8450) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8452) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8459) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8462) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8463) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8464) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8466) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8467) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8469) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8472) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8478) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8479) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8482) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8487) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8488) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8489) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8503)
    (<0.0>,8505) tree.c:583: local_irq_restore(flags);
    (<0.0>,8508)
    (<0.0>,8510) fake_sched.h:43: return __running_cpu;
    (<0.0>,8514) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8516) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8520) fake_sched.h:43: return __running_cpu;
    (<0.0>,8524) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8530) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8533) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8538) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,8540) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,8549) fake_sched.h:43: return __running_cpu;
    (<0.0>,8553)
    (<0.0>,8554) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8557) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8562) tree.c:704: local_irq_save(flags);
    (<0.0>,8565)
    (<0.0>,8567) fake_sched.h:43: return __running_cpu;
    (<0.0>,8571) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8573) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8577) fake_sched.h:43: return __running_cpu;
    (<0.0>,8581) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8594)
    (<0.0>,8596) fake_sched.h:43: return __running_cpu;
    (<0.0>,8600) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8601) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,8603) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,8604) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,8605) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8610) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8611) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8615) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8616) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8617) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8618) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,8622) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8624) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8625) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8626) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8640)
    (<0.0>,8641)
    (<0.0>,8643) fake_sched.h:43: return __running_cpu;
    (<0.0>,8647) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8651) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8654) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8655) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8656) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8658) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8659) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8661) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8664) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8671) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8672) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8675) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8680) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8681) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8682) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8687) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,8696)
    (<0.0>,8698) tree.c:707: local_irq_restore(flags);
    (<0.0>,8701)
    (<0.0>,8703) fake_sched.h:43: return __running_cpu;
    (<0.0>,8707) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8709) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8713) fake_sched.h:43: return __running_cpu;
    (<0.0>,8717) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8724) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,8736)
    (<0.0>,8737) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8740)
    (<0.0>,8741) tree.c:453: return &rsp->node[0];
    (<0.0>,8745) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8749) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,8753)
    (<0.0>,8756) fake_sched.h:43: return __running_cpu;
    (<0.0>,8760) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8764) fake_sched.h:43: return __running_cpu;
    (<0.0>,8768) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8773) fake_sched.h:43: return __running_cpu;
    (<0.0>,8777) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8780) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,8781) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,8787) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,8789) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,8792) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,8794) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,8795) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8798)
    (<0.0>,8799) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8801) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8802) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8804) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8812) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8813) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8816)
    (<0.0>,8817) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8819) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8820) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8822) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8829) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8830) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8831) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8834) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,8839)
    (<0.0>,8840) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,8841) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,8842) tree.c:1012: rsp->gp_start = j;
    (<0.0>,8843) tree.c:1012: rsp->gp_start = j;
    (<0.0>,8845) tree.c:1012: rsp->gp_start = j;
    (<0.0>,8849) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,8850) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,8851) update.c:344: if (till_stall_check < 3) {
    (<0.0>,8854) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,8858) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,8863) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,8864) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,8865) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,8867) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,8869) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,8870) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,8871) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,8874) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,8876) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,8880) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,8882) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,8884) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,8886) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,8890) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,8894)
    (<0.0>,8895) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,8896) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,8901) fake_sched.h:43: return __running_cpu;
    (<0.0>,8905) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,8907) fake_sched.h:43: return __running_cpu;
    (<0.0>,8911) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8917) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,8921)
    (<0.0>,8922) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,8924) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,8930) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8933) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8935) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8936) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8938) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8943) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,8947)
    (<0.0>,8950) fake_sched.h:43: return __running_cpu;
    (<0.0>,8954) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8958) fake_sched.h:43: return __running_cpu;
    (<0.0>,8962) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8967) fake_sched.h:43: return __running_cpu;
    (<0.0>,8971) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8974) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,8975) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,8982) fake_sched.h:43: return __running_cpu;
    (<0.0>,8985) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8987) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8989) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8990) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,8996)
    (<0.0>,8997) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8999) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9004) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9005) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9007) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9011) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9012) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9013) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9015) tree.c:1656: rnp->qsmask = 0;
    (<0.0>,9017) tree.c:1656: rnp->qsmask = 0;
    (<0.0>,9018) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9020) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9021) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9023) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9024) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9026) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9027) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9029) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9034) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9035) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9037) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9038) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9040) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9044) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9045) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9046) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9047) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9049) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9050) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9052) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9053) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9054) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9056) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9059) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9060) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9061) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9067)
    (<0.0>,9068)
    (<0.0>,9069)
    (<0.0>,9070) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9072) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9073) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9075) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9078) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9079) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9080) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9089)
    (<0.0>,9090)
    (<0.0>,9091)
    (<0.0>,9092) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9095) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9098) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9101) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9102) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9105) tree.c:1434: return false;
    (<0.0>,9107) tree.c:1483: }
    (<0.0>,9110) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9112) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9114) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9115) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9117) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9120) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9122) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9123) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9125) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9128) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,9130) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,9131) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9133) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9134) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9136) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9141) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9144) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9145) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,9148)
    (<0.0>,9151) tree.c:1575: return ret;
    (<0.0>,9155) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,9158)
    (<0.0>,9162) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,9166)
    (<0.0>,9167) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9168) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9173) fake_sched.h:43: return __running_cpu;
    (<0.0>,9177) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9179) fake_sched.h:43: return __running_cpu;
    (<0.0>,9183) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9196) fake_sched.h:43: return __running_cpu;
    (<0.0>,9201) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,9207) fake_sched.h:43: return __running_cpu;
    (<0.0>,9212) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,9218) fake_sched.h:43: return __running_cpu;
    (<0.0>,9222) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,9234) fake_sched.h:43: return __running_cpu;
    (<0.0>,9238)
    (<0.0>,9241) tree.c:580: local_irq_save(flags);
    (<0.0>,9244)
    (<0.0>,9246) fake_sched.h:43: return __running_cpu;
    (<0.0>,9250) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9252) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9256) fake_sched.h:43: return __running_cpu;
    (<0.0>,9260) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9273)
    (<0.0>,9275) fake_sched.h:43: return __running_cpu;
    (<0.0>,9279) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9280) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9282) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9283) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9284) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9290) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9291) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9296) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9297) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9298) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9299) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,9303) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9305) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9306) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9307) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9326)
    (<0.0>,9328)
    (<0.0>,9330) fake_sched.h:43: return __running_cpu;
    (<0.0>,9334) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9337) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,9341) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9342) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9343) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9347) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9348) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9349) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9351) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9356) fake_sched.h:43: return __running_cpu;
    (<0.0>,9359) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9361) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9363) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9364) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9367)
    (<0.0>,9370) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9373) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9374) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9375) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9379) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9380) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9381) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9383) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9388) fake_sched.h:43: return __running_cpu;
    (<0.0>,9391) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9393) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9395) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9396) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9399)
    (<0.0>,9402) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9405) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9406) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9407) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9411) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9412) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9413) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9415) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9422) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9425) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9426) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9427) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9429) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9430) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9432) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9435) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9441) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9442) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9445) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9450) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9451) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9452) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9466)
    (<0.0>,9468) tree.c:583: local_irq_restore(flags);
    (<0.0>,9471)
    (<0.0>,9473) fake_sched.h:43: return __running_cpu;
    (<0.0>,9477) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9479) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9483) fake_sched.h:43: return __running_cpu;
    (<0.0>,9487) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9493) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9496) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9501) fake_sched.h:43: return __running_cpu;
    (<0.0>,9505)
    (<0.0>,9506) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9509) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9514) tree.c:704: local_irq_save(flags);
    (<0.0>,9517)
    (<0.0>,9519) fake_sched.h:43: return __running_cpu;
    (<0.0>,9523) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9525) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9529) fake_sched.h:43: return __running_cpu;
    (<0.0>,9533) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9546)
    (<0.0>,9548) fake_sched.h:43: return __running_cpu;
    (<0.0>,9552) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9553) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9555) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9556) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9557) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9562) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9563) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9567) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9568) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9569) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9570) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,9574) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9576) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9577) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9578) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9592)
    (<0.0>,9593)
    (<0.0>,9595) fake_sched.h:43: return __running_cpu;
    (<0.0>,9599) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9603) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9606) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9607) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9608) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9610) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9611) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9613) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9616) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9623) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9624) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9627) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9632) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9633) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9634) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9639) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,9648)
    (<0.0>,9650) tree.c:707: local_irq_restore(flags);
    (<0.0>,9653)
    (<0.0>,9655) fake_sched.h:43: return __running_cpu;
    (<0.0>,9659) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9661) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9665) fake_sched.h:43: return __running_cpu;
    (<0.0>,9669) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9684) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9686) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9688) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9689) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9691) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9696) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,9700)
    (<0.0>,9701) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,9703) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,9707) tree.c:1674: return 1;
    (<0.0>,9709) tree.c:1675: }
    (<0.0>,9714) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,9715) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,9716) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,9717) tree.c:1819: if (j > HZ) {
    (<0.0>,9720) tree.c:1823: ret = 0;
    (<0.0>,9722) tree.c:1825: if (!ret)
    (<0.0>,9725) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,9726) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,9728) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,9730) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,9734) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,9736) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,9740) fake_sched.h:43: return __running_cpu;
    (<0.0>,9744) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9748) fake_sched.h:43: return __running_cpu;
    (<0.0>,9752) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9757) fake_sched.h:43: return __running_cpu;
    (<0.0>,9761) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9771) tree.c:749: local_irq_save(flags);
    (<0.0>,9774)
    (<0.0>,9776) fake_sched.h:43: return __running_cpu;
    (<0.0>,9780) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9782) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9787) fake_sched.h:43: return __running_cpu;
    (<0.0>,9791) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9792) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,9794) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,9795) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,9796) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,9798) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,9800) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,9801) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9803) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9808) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9809) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9811) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9815) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9816) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9817) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9818) tree.c:754: if (oldval)
    (<0.0>,9826)
    (<0.0>,9828) tree.c:759: local_irq_restore(flags);
    (<0.0>,9831)
    (<0.0>,9833) fake_sched.h:43: return __running_cpu;
    (<0.0>,9837) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9839) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9847)
    (<0.0>,9852) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,9857) fake_sched.h:43: return __running_cpu;
    (<0.0>,9862) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,9870) fake_sched.h:43: return __running_cpu;
    (<0.0>,9875) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,9889) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9890) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9891) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9895) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9896) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9897) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9899) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9903) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,9905) fake_sched.h:43: return __running_cpu;
    (<0.0>,9908) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,9910) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,9917)
    (<0.0>,9918)
    (<0.0>,9919) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9921) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9922) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9923) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,9925) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,9927) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,9928) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,9929) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,9939)
    (<0.0>,9940)
    (<0.0>,9941) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,9946) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,9949)
    (<0.0>,9952) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,9955) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,9957) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,9960) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,9962) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,9966) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,9969)
    (<0.0>,9970) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9972) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9975) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9982) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9983) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9989)
    (<0.0>,9990)
    (<0.0>,9991) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,9994)
    (<0.0>,9995) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9997) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9998) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10000) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10006) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,10008) tree.c:494: }
    (<0.0>,10012) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10014) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10015) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10017) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10020) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10022) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10023) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10025) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10028) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,10031)
    (<0.0>,10035) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10037) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10039) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10040) tree.c:3175: return 0;
    (<0.0>,10042) tree.c:3176: }
    (<0.0>,10047) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10050) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10051) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10052) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10056) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10057) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10058) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10060) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10064) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10066) fake_sched.h:43: return __running_cpu;
    (<0.0>,10069) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10071) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10078)
    (<0.0>,10079)
    (<0.0>,10080) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10082) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10083) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10084) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10086) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10088) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10089) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10090) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10100)
    (<0.0>,10101)
    (<0.0>,10102) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,10107) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,10110)
    (<0.0>,10113) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,10116) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10118) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10121) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10123) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10127) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,10130)
    (<0.0>,10131) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10133) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10136) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10143) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10144) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10150)
    (<0.0>,10151)
    (<0.0>,10152) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,10155)
    (<0.0>,10156) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10158) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10159) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10161) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10167) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,10173)
    (<0.0>,10174) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10177)
    (<0.0>,10178) tree.c:453: return &rsp->node[0];
    (<0.0>,10182) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10183) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10185) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10189) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10190) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10192) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10195) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10196) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,10197) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,10201) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,10204) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,10207) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,10210) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,10211) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,10214) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10216) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10219) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10222) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10225) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10226) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10228) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10231) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10235) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10237) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10239) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10242) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10245) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10248) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10249) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10251) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10254) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10258) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10260) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10262) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10265) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,10267) tree.c:494: }
    (<0.0>,10271) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10273) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10274) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10276) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10279) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10281) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10282) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10284) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10287) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,10290)
    (<0.0>,10294) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10296) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10298) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10299) tree.c:3175: return 0;
    (<0.0>,10301) tree.c:3176: }
    (<0.0>,10306) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10309) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10310) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10311) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10315) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10316) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10317) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10319) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10323) tree.c:3190: return 0;
    (<0.0>,10325) tree.c:3191: }
    (<0.0>,10329) tree.c:2437: if (user)
    (<0.0>,10337) fake_sched.h:43: return __running_cpu;
    (<0.0>,10341) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,10343) fake_sched.h:43: return __running_cpu;
    (<0.0>,10347) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10353) fake_sched.h:43: return __running_cpu;
    (<0.0>,10357) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,10367) tree.c:624: local_irq_save(flags);
    (<0.0>,10370)
    (<0.0>,10372) fake_sched.h:43: return __running_cpu;
    (<0.0>,10376) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10378) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10382) fake_sched.h:43: return __running_cpu;
    (<0.0>,10386) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10392) fake_sched.h:43: return __running_cpu;
    (<0.0>,10396) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10397) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,10399) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,10400) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,10401) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,10403) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,10405) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,10406) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10408) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10413) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10414) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10416) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10420) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10421) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10422) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10423) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,10425) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,10433)
    (<0.0>,10435) tree.c:634: local_irq_restore(flags);
    (<0.0>,10438)
    (<0.0>,10440) fake_sched.h:43: return __running_cpu;
    (<0.0>,10444) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10446) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10450) fake_sched.h:43: return __running_cpu;
    (<0.0>,10454) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10463) fake_sched.h:43: return __running_cpu;
    (<0.0>,10467)
    (<0.0>,10470) tree.c:580: local_irq_save(flags);
    (<0.0>,10473)
    (<0.0>,10475) fake_sched.h:43: return __running_cpu;
    (<0.0>,10479) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10481) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10485) fake_sched.h:43: return __running_cpu;
    (<0.0>,10489) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10502)
    (<0.0>,10504) fake_sched.h:43: return __running_cpu;
    (<0.0>,10508) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10509) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10511) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10512) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10513) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10519) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10520) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10525) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10526) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10527) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10528) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,10532) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,10534) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,10535) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,10536) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,10555)
    (<0.0>,10557)
    (<0.0>,10559) fake_sched.h:43: return __running_cpu;
    (<0.0>,10563) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10566) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,10570) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10571) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10572) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10576) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10577) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10578) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10580) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10585) fake_sched.h:43: return __running_cpu;
    (<0.0>,10588) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10590) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10592) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10593) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10596)
    (<0.0>,10599) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10602) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10603) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10604) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10608) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10609) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10610) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10612) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10617) fake_sched.h:43: return __running_cpu;
    (<0.0>,10620) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10622) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10624) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10625) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10628)
    (<0.0>,10631) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10634) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10635) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10636) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10640) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10641) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10642) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10644) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10651) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10654) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10655) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10656) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10658) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10659) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10661) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10664) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10670) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10671) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10674) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10679) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10680) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10681) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10695)
    (<0.0>,10697) tree.c:583: local_irq_restore(flags);
    (<0.0>,10700)
    (<0.0>,10702) fake_sched.h:43: return __running_cpu;
    (<0.0>,10706) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10708) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10712) fake_sched.h:43: return __running_cpu;
    (<0.0>,10716) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10722) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10725) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10730) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10732) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10734) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10738) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10740) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10743) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10746)
    (<0.0>,10755) fake_sched.h:43: return __running_cpu;
    (<0.0>,10759)
    (<0.0>,10760) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10763) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10768) tree.c:704: local_irq_save(flags);
    (<0.0>,10771)
    (<0.0>,10773) fake_sched.h:43: return __running_cpu;
    (<0.0>,10777) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10779) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10783) fake_sched.h:43: return __running_cpu;
    (<0.0>,10787) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10800)
    (<0.0>,10802) fake_sched.h:43: return __running_cpu;
    (<0.0>,10806) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10807) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10809) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10810) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10811) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10816) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10817) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10821) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10822) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10823) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10824) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,10828) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10830) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10831) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10832) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10846)
    (<0.0>,10847)
    (<0.0>,10849) fake_sched.h:43: return __running_cpu;
    (<0.0>,10853) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10857) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10860) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10861) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10862) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10864) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10865) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10867) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10870) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10877) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10878) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10881) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10886) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10887) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10888) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10893) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,10902)
    (<0.0>,10904) tree.c:707: local_irq_restore(flags);
    (<0.0>,10907)
    (<0.0>,10909) fake_sched.h:43: return __running_cpu;
    (<0.0>,10913) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10915) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10919) fake_sched.h:43: return __running_cpu;
    (<0.0>,10923) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10930) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10931) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10932) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10933) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,10935) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,10938) tree.c:1840: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,10941)
    (<0.0>,10946) tree.c:1872: rcu_gp_cleanup(rsp);
    (<0.0>,10954)
    (<0.0>,10955) tree.c:1720: bool needgp = false;
    (<0.0>,10956) tree.c:1721: int nocb = 0;
    (<0.0>,10957) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10960)
    (<0.0>,10961) tree.c:453: return &rsp->node[0];
    (<0.0>,10965) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10966) tree.c:1725: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,10970)
    (<0.0>,10973) fake_sched.h:43: return __running_cpu;
    (<0.0>,10977) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,10981) fake_sched.h:43: return __running_cpu;
    (<0.0>,10985) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10990) fake_sched.h:43: return __running_cpu;
    (<0.0>,10994) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,10997) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,10998) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,11004) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,11005) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,11007) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,11009) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,11010) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,11011) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,11013) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,11016) tree.c:1739: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,11020)
    (<0.0>,11021) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,11022) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,11027) fake_sched.h:43: return __running_cpu;
    (<0.0>,11031) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,11033) fake_sched.h:43: return __running_cpu;
    (<0.0>,11037) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11043) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11046) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11048) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11049) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11051) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11056) tree.c:1751: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,11060)
    (<0.0>,11063) fake_sched.h:43: return __running_cpu;
    (<0.0>,11067) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,11071) fake_sched.h:43: return __running_cpu;
    (<0.0>,11075) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11080) fake_sched.h:43: return __running_cpu;
    (<0.0>,11084) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,11087) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,11088) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,11094) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,11096) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,11097) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,11099) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,11101) fake_sched.h:43: return __running_cpu;
    (<0.0>,11104) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11106) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11108) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11109) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,11110) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,11112) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,11115) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,11116) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,11117) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,11123)
    (<0.0>,11124)
    (<0.0>,11125)
    (<0.0>,11126) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,11128) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,11129) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,11131) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,11134) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,11135) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,11136) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,11144)
    (<0.0>,11145)
    (<0.0>,11146)
    (<0.0>,11147) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11150) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11153) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11156) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11157) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11160) tree.c:1502: return false;
    (<0.0>,11162) tree.c:1527: }
    (<0.0>,11165) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,11166) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,11168) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,11169) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,11171) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,11175) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,11177) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,11178) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,11180) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,11183) tree.c:1575: return ret;
    (<0.0>,11187) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,11191) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,11193) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,11194) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,11201)
    (<0.0>,11202)
    (<0.0>,11203) tree.c:1385: int c = rnp->completed;
    (<0.0>,11205) tree.c:1385: int c = rnp->completed;
    (<0.0>,11207) tree.c:1385: int c = rnp->completed;
    (<0.0>,11209) fake_sched.h:43: return __running_cpu;
    (<0.0>,11212) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11214) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11216) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11217) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,11218) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,11222)
    (<0.0>,11223)
    (<0.0>,11225) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,11228) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,11231) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,11232) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,11236) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,11239) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,11240) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,11241) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,11242) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,11243) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,11245) tree.c:1393: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,11253)
    (<0.0>,11254)
    (<0.0>,11255)
    (<0.0>,11256)
    (<0.0>,11260) tree.c:1394: return needmore;
    (<0.0>,11262) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,11264) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,11265) tree.c:1759: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,11269)
    (<0.0>,11270) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,11271) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,11276) fake_sched.h:43: return __running_cpu;
    (<0.0>,11280) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,11282) fake_sched.h:43: return __running_cpu;
    (<0.0>,11286) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11299) fake_sched.h:43: return __running_cpu;
    (<0.0>,11304) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,11311) fake_sched.h:43: return __running_cpu;
    (<0.0>,11315) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,11327) fake_sched.h:43: return __running_cpu;
    (<0.0>,11331)
    (<0.0>,11334) tree.c:580: local_irq_save(flags);
    (<0.0>,11337)
    (<0.0>,11339) fake_sched.h:43: return __running_cpu;
    (<0.0>,11343) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11345) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11349) fake_sched.h:43: return __running_cpu;
    (<0.0>,11353) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11366)
    (<0.0>,11368) fake_sched.h:43: return __running_cpu;
    (<0.0>,11372) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11373) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11375) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11376) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11377) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11383) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11384) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11389) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11390) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11391) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11392) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,11396) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11398) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11399) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11400) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11419)
    (<0.0>,11421)
    (<0.0>,11423) fake_sched.h:43: return __running_cpu;
    (<0.0>,11427) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11430) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,11434) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11435) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11436) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11440) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11441) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11442) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11444) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11449) fake_sched.h:43: return __running_cpu;
    (<0.0>,11452) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11454) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11456) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11457) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11460)
    (<0.0>,11463) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11466) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11467) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11468) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11472) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11473) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11474) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11476) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11481) fake_sched.h:43: return __running_cpu;
    (<0.0>,11484) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11486) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11488) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11489) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11492)
    (<0.0>,11495) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11498) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11499) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11500) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11504) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11505) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11506) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11508) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11515) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11518) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11519) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11520) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11522) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11523) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11525) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11528) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11534) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11535) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11538) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11543) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11544) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11545) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11559)
    (<0.0>,11561) tree.c:583: local_irq_restore(flags);
    (<0.0>,11564)
    (<0.0>,11566) fake_sched.h:43: return __running_cpu;
    (<0.0>,11570) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11572) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11576) fake_sched.h:43: return __running_cpu;
    (<0.0>,11580) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11586) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11589) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11594) fake_sched.h:43: return __running_cpu;
    (<0.0>,11598)
    (<0.0>,11599) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,11602) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,11607) tree.c:704: local_irq_save(flags);
    (<0.0>,11610)
    (<0.0>,11612) fake_sched.h:43: return __running_cpu;
    (<0.0>,11616) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11618) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11622) fake_sched.h:43: return __running_cpu;
    (<0.0>,11626) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11639)
    (<0.0>,11641) fake_sched.h:43: return __running_cpu;
    (<0.0>,11645) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11646) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,11648) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,11649) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,11650) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11655) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11656) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11660) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11661) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11662) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11663) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,11667) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,11669) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,11670) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,11671) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,11685)
    (<0.0>,11686)
    (<0.0>,11688) fake_sched.h:43: return __running_cpu;
    (<0.0>,11692) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11696) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11699) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11700) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11701) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11703) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11704) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11706) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11709) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11716) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11717) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11720) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11725) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11726) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11727) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11732) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,11741)
    (<0.0>,11743) tree.c:707: local_irq_restore(flags);
    (<0.0>,11746)
    (<0.0>,11748) fake_sched.h:43: return __running_cpu;
    (<0.0>,11752) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11754) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11758) fake_sched.h:43: return __running_cpu;
    (<0.0>,11762) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11777) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11779) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11781) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11782) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11784) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11789) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,11792)
    (<0.0>,11793) tree.c:453: return &rsp->node[0];
    (<0.0>,11797) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,11798) tree.c:1763: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,11802)
    (<0.0>,11805) fake_sched.h:43: return __running_cpu;
    (<0.0>,11809) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,11813) fake_sched.h:43: return __running_cpu;
    (<0.0>,11817) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11822) fake_sched.h:43: return __running_cpu;
    (<0.0>,11826) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,11829) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,11830) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,11836) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,11837) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,11841)
    (<0.0>,11842)
    (<0.0>,11844) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,11846) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,11847) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,11849) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,11852) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,11854) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,11856) fake_sched.h:43: return __running_cpu;
    (<0.0>,11859) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11861) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11863) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11864) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,11865) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,11866) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,11874)
    (<0.0>,11875)
    (<0.0>,11876)
    (<0.0>,11877) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11880) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11883) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11886) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11887) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11890) tree.c:1502: return false;
    (<0.0>,11892) tree.c:1527: }
    (<0.0>,11895) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,11899) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,11900) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11903) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11904) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11910)
    (<0.0>,11911)
    (<0.0>,11912) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,11915)
    (<0.0>,11916) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11918) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11919) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11921) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11927) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,11933)
    (<0.0>,11934) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11937)
    (<0.0>,11938) tree.c:453: return &rsp->node[0];
    (<0.0>,11942) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11943) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11945) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11949) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11950) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11952) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11955) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11956) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11957) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11961) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11964) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11967) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11970) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11971) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11974) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11976) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11979) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11982) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11985) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11986) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11988) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11991) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11995) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11997) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11999) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12002) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12005) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12008) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12009) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12011) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12014) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12018) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12020) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12022) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12025) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,12027) tree.c:494: }
    (<0.0>,12031) tree.c:1780: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,12035)
    (<0.0>,12036) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,12037) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,12042) fake_sched.h:43: return __running_cpu;
    (<0.0>,12046) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12048) fake_sched.h:43: return __running_cpu;
    (<0.0>,12052) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12063) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,12065) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,12069) fake_sched.h:43: return __running_cpu;
    (<0.0>,12073) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12077) fake_sched.h:43: return __running_cpu;
    (<0.0>,12081) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12086) fake_sched.h:43: return __running_cpu;
    (<0.0>,12090) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12100) tree.c:749: local_irq_save(flags);
    (<0.0>,12103)
    (<0.0>,12105) fake_sched.h:43: return __running_cpu;
    (<0.0>,12109) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12111) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12116) fake_sched.h:43: return __running_cpu;
    (<0.0>,12120) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,12121) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,12123) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,12124) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,12125) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,12127) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,12129) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,12130) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12132) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12137) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12138) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12140) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12144) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12145) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12146) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12147) tree.c:754: if (oldval)
    (<0.0>,12155)
    (<0.0>,12157) tree.c:759: local_irq_restore(flags);
    (<0.0>,12160)
    (<0.0>,12162) fake_sched.h:43: return __running_cpu;
    (<0.0>,12166) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12168) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12176)
    (<0.0>,12181) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,12186) fake_sched.h:43: return __running_cpu;
    (<0.0>,12191) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,12199) fake_sched.h:43: return __running_cpu;
    (<0.0>,12204) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,12218) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12219) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12220) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12224) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12225) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12226) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12228) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12232) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12234) fake_sched.h:43: return __running_cpu;
    (<0.0>,12237) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12239) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12246)
    (<0.0>,12247)
    (<0.0>,12248) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12250) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12251) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12252) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12254) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12256) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12257) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,12258) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,12268)
    (<0.0>,12269)
    (<0.0>,12270) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,12275) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,12278)
    (<0.0>,12281) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,12284) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,12286) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,12289) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,12291) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,12295) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,12298)
    (<0.0>,12299) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12301) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12304) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12311) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,12312) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,12318)
    (<0.0>,12319)
    (<0.0>,12320) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,12323)
    (<0.0>,12324) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12326) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12327) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12329) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12335) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,12341)
    (<0.0>,12342) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12345)
    (<0.0>,12346) tree.c:453: return &rsp->node[0];
    (<0.0>,12350) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12351) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12353) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12357) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12358) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12360) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12363) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12364) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,12365) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,12369) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12372) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12375) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12378) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12379) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12382) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12384) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12387) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12390) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12393) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12394) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12396) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12399) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12403) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12405) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12407) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12410) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12413) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12416) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12417) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12419) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12422) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12426) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12428) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12430) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12433) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,12435) tree.c:494: }
    (<0.0>,12439) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12441) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12442) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12444) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12447) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12449) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12450) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12452) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12455) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,12458)
    (<0.0>,12462) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12464) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12466) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12467) tree.c:3175: return 0;
    (<0.0>,12469) tree.c:3176: }
    (<0.0>,12474) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12477) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12478) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12479) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12483) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12484) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12485) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12487) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12491) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12493) fake_sched.h:43: return __running_cpu;
    (<0.0>,12496) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12498) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12505)
    (<0.0>,12506)
    (<0.0>,12507) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12509) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12510) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12511) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12513) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12515) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12516) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,12517) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,12527)
    (<0.0>,12528)
    (<0.0>,12529) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,12534) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,12537)
    (<0.0>,12540) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,12543) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,12545) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,12548) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,12550) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,12554) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,12557)
    (<0.0>,12558) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12560) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12563) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12570) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,12571) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,12577)
    (<0.0>,12578)
    (<0.0>,12579) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,12582)
    (<0.0>,12583) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12585) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12586) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12588) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12594) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,12600)
    (<0.0>,12601) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12604)
    (<0.0>,12605) tree.c:453: return &rsp->node[0];
    (<0.0>,12609) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12610) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12612) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12616) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12617) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12619) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12622) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12623) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,12624) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,12628) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12631) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12634) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12637) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12638) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12641) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12643) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12646) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12649) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12652) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12653) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12655) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12658) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12662) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12664) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12666) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12669) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12672) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12675) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12676) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12678) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12681) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12685) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12687) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12689) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12692) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,12694) tree.c:494: }
    (<0.0>,12698) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12700) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12701) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12703) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12706) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12708) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12709) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12711) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12714) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,12717)
    (<0.0>,12721) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12723) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12725) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12726) tree.c:3175: return 0;
    (<0.0>,12728) tree.c:3176: }
    (<0.0>,12733) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12736) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12737) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12738) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12742) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12743) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12744) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12746) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12750) tree.c:3190: return 0;
    (<0.0>,12752) tree.c:3191: }
    (<0.0>,12756) tree.c:2437: if (user)
    (<0.0>,12764) fake_sched.h:43: return __running_cpu;
    (<0.0>,12768) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12770) fake_sched.h:43: return __running_cpu;
    (<0.0>,12774) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12780) fake_sched.h:43: return __running_cpu;
    (<0.0>,12784) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,12794) tree.c:624: local_irq_save(flags);
    (<0.0>,12797)
    (<0.0>,12799) fake_sched.h:43: return __running_cpu;
    (<0.0>,12803) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12805) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12809) fake_sched.h:43: return __running_cpu;
    (<0.0>,12813) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12819) fake_sched.h:43: return __running_cpu;
    (<0.0>,12823) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,12824) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,12826) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,12827) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,12828) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,12830) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,12832) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,12833) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12835) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12840) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12841) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12843) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12847) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12848) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12849) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12850) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,12852) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,12860)
    (<0.0>,12862) tree.c:634: local_irq_restore(flags);
    (<0.0>,12865)
    (<0.0>,12867) fake_sched.h:43: return __running_cpu;
    (<0.0>,12871) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12873) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12877) fake_sched.h:43: return __running_cpu;
    (<0.0>,12881) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12890) fake_sched.h:43: return __running_cpu;
    (<0.0>,12894)
    (<0.0>,12897) tree.c:580: local_irq_save(flags);
    (<0.0>,12900)
    (<0.0>,12902) fake_sched.h:43: return __running_cpu;
    (<0.0>,12906) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12908) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12912) fake_sched.h:43: return __running_cpu;
    (<0.0>,12916) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12929)
    (<0.0>,12931) fake_sched.h:43: return __running_cpu;
    (<0.0>,12935) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,12936) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,12938) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,12939) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,12940) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12946) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12947) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12952) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12953) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12954) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12955) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,12959) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,12961) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,12962) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12963) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12982)
    (<0.0>,12984)
    (<0.0>,12986) fake_sched.h:43: return __running_cpu;
    (<0.0>,12990) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,12993) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,12997) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12998) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12999) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13003) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13004) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13005) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13007) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13012) fake_sched.h:43: return __running_cpu;
    (<0.0>,13015) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13017) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13019) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13020) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,13023)
    (<0.0>,13026) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13029) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13030) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13031) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13035) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13036) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13037) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13039) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13044) fake_sched.h:43: return __running_cpu;
    (<0.0>,13047) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13049) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13051) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13052) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,13055)
    (<0.0>,13058) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13061) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13062) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13063) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13067) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13068) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13069) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13071) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13078) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13081) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13082) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13083) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13085) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13086) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13088) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13091) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13097) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13098) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13101) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13106) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13107) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13108) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13122)
    (<0.0>,13124) tree.c:583: local_irq_restore(flags);
    (<0.0>,13127)
    (<0.0>,13129) fake_sched.h:43: return __running_cpu;
    (<0.0>,13133) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13135) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13139) fake_sched.h:43: return __running_cpu;
    (<0.0>,13143) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13149) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,13152) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,13157) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,13159) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
      (<0.1>,725) fake_sync.h:241: while (!x->done)
      (<0.1>,732) fake_sched.h:43: return __running_cpu;
      (<0.1>,736)
      (<0.1>,737) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,740) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,745) tree.c:704: local_irq_save(flags);
      (<0.1>,748)
      (<0.1>,750) fake_sched.h:43: return __running_cpu;
      (<0.1>,754) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,756) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,760) fake_sched.h:43: return __running_cpu;
      (<0.1>,764) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,777)
      (<0.1>,779) fake_sched.h:43: return __running_cpu;
      (<0.1>,783) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,784) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,786) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,787) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,788) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,793) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,794) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,798) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,799) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,800) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,801) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,805) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,807) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,808) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,809) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,823)
      (<0.1>,824)
      (<0.1>,826) fake_sched.h:43: return __running_cpu;
      (<0.1>,830) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,834) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,837) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,838) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,839) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,841) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,842) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,844) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,847) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,854) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,855) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,858) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,864) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,865) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,870) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,879)
      (<0.1>,881) tree.c:707: local_irq_restore(flags);
      (<0.1>,884)
      (<0.1>,886) fake_sched.h:43: return __running_cpu;
      (<0.1>,890) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,892) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,896) fake_sched.h:43: return __running_cpu;
      (<0.1>,900) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,911)
      (<0.1>,917) litmus.c:91: y = 1;
      (<0.1>,919) fake_sched.h:43: return __running_cpu;
      (<0.1>,923)
      (<0.1>,926) tree.c:580: local_irq_save(flags);
      (<0.1>,929)
      (<0.1>,931) fake_sched.h:43: return __running_cpu;
      (<0.1>,935) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,937) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,941) fake_sched.h:43: return __running_cpu;
      (<0.1>,945) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,958)
      (<0.1>,960) fake_sched.h:43: return __running_cpu;
      (<0.1>,964) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,965) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,967) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,968) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,969) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,975) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,976) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,981) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,982) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,983) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,984) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,988) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,990) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,991) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,992) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1011)
      (<0.1>,1013)
      (<0.1>,1015) fake_sched.h:43: return __running_cpu;
      (<0.1>,1019) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1022) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,1026) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1027) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1028) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1033) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1034) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1041) fake_sched.h:43: return __running_cpu;
      (<0.1>,1044) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1046) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1048) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1049) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1052)
      (<0.1>,1055) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1058) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1059) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1060) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1065) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1066) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1073) fake_sched.h:43: return __running_cpu;
      (<0.1>,1076) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1078) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1080) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1081) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1084)
      (<0.1>,1087) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1090) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1091) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1092) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1097) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1098) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1107) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1110) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1111) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1112) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1117) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1120) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1126) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1127) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1130) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1135) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1136) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1137) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1151)
      (<0.1>,1153) tree.c:583: local_irq_restore(flags);
      (<0.1>,1156)
      (<0.1>,1158) fake_sched.h:43: return __running_cpu;
      (<0.1>,1162) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1164) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1168) fake_sched.h:43: return __running_cpu;
      (<0.1>,1172) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1178) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1181) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,5767) litmus.c:69: r_y = y;
  (<0>,5768) litmus.c:69: r_y = y;
  (<0>,5784) fake_sched.h:43: return __running_cpu;
  (<0>,5789) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
  (<0>,5795) fake_sched.h:43: return __running_cpu;
  (<0>,5800) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
  (<0>,5806) fake_sched.h:43: return __running_cpu;
  (<0>,5810) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
  (<0>,5822) fake_sched.h:43: return __running_cpu;
  (<0>,5826)
  (<0>,5829) tree.c:580: local_irq_save(flags);
  (<0>,5832)
  (<0>,5834) fake_sched.h:43: return __running_cpu;
  (<0>,5838) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5840) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5844) fake_sched.h:43: return __running_cpu;
  (<0>,5848) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5861)
  (<0>,5863) fake_sched.h:43: return __running_cpu;
  (<0>,5867) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,5868) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,5870) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,5871) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,5872) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,5878) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,5879) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,5884) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,5885) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,5886) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,5887) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,5891) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,5893) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,5894) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,5895) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,5914)
  (<0>,5916)
  (<0>,5918) fake_sched.h:43: return __running_cpu;
  (<0>,5922) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,5925) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,5929) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5930) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5931) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5935) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5936) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5937) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5939) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5944) fake_sched.h:43: return __running_cpu;
  (<0>,5947) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,5949) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,5951) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,5952) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,5955)
  (<0>,5958) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5961) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5962) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5963) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5967) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5968) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5969) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5971) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5976) fake_sched.h:43: return __running_cpu;
  (<0>,5979) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,5981) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,5983) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,5984) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,5987)
  (<0>,5990) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5993) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5994) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5995) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,5999) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6000) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6001) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6003) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6010) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6013) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6014) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6015) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6017) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6018) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6020) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6023) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6029) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6030) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6033) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6038) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6039) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6040) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6054)
  (<0>,6056) tree.c:583: local_irq_restore(flags);
  (<0>,6059)
  (<0>,6061) fake_sched.h:43: return __running_cpu;
  (<0>,6065) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6067) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6071) fake_sched.h:43: return __running_cpu;
  (<0>,6075) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6081) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6084) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6089) fake_sched.h:43: return __running_cpu;
  (<0>,6093)
  (<0>,6094) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,6097) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,6102) tree.c:704: local_irq_save(flags);
  (<0>,6105)
  (<0>,6107) fake_sched.h:43: return __running_cpu;
  (<0>,6111) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6113) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6117) fake_sched.h:43: return __running_cpu;
  (<0>,6121) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6134)
  (<0>,6136) fake_sched.h:43: return __running_cpu;
  (<0>,6140) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6141) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6143) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6144) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6145) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6150) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6151) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6155) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6156) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6157) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6158) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,6162) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6164) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6165) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,6166) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,6180)
  (<0>,6181)
  (<0>,6183) fake_sched.h:43: return __running_cpu;
  (<0>,6187) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6191) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6194) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6195) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6196) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6198) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6199) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6201) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6204) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6211) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6212) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6215) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6220) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6221) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6222) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6227) tree.c:656: if (!user && !is_idle_task(current)) {
  (<0>,6236)
  (<0>,6238) tree.c:707: local_irq_restore(flags);
  (<0>,6241)
  (<0>,6243) fake_sched.h:43: return __running_cpu;
  (<0>,6247) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6249) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6253) fake_sched.h:43: return __running_cpu;
  (<0>,6257) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6268) fake_sched.h:43: return __running_cpu;
  (<0>,6272) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
  (<0>,6276) fake_sched.h:43: return __running_cpu;
  (<0>,6280) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6285) fake_sched.h:43: return __running_cpu;
  (<0>,6289) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
  (<0>,6299) tree.c:749: local_irq_save(flags);
  (<0>,6302)
  (<0>,6304) fake_sched.h:43: return __running_cpu;
  (<0>,6308) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6310) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6315) fake_sched.h:43: return __running_cpu;
  (<0>,6319) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6320) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6322) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6323) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6324) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6326) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6328) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6329) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6331) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6336) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6337) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6339) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6343) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6344) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6345) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6346) tree.c:754: if (oldval)
  (<0>,6354)
  (<0>,6356) tree.c:759: local_irq_restore(flags);
  (<0>,6359)
  (<0>,6361) fake_sched.h:43: return __running_cpu;
  (<0>,6365) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6367) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6375)
  (<0>,6380) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,6385) fake_sched.h:43: return __running_cpu;
  (<0>,6390) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,6398) fake_sched.h:43: return __running_cpu;
  (<0>,6403) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
  (<0>,6417) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6418) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6419) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6423) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6424) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6425) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6427) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6431) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6433) fake_sched.h:43: return __running_cpu;
  (<0>,6436) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6438) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6445)
  (<0>,6446)
  (<0>,6447) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6449) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6450) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6451) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,6453) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,6455) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,6456) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,6457) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,6467)
  (<0>,6468)
  (<0>,6469) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
  (<0>,6474) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
  (<0>,6477)
  (<0>,6480) tree.c:3135: if (rcu_scheduler_fully_active &&
  (<0>,6483) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,6485) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,6488) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,6490) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,6494) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,6497)
  (<0>,6498) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6500) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6503) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6510) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6511) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6517)
  (<0>,6518)
  (<0>,6519) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,6522)
  (<0>,6523) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,6525) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,6526) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,6528) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,6534) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,6540)
  (<0>,6541) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6544)
  (<0>,6545) tree.c:453: return &rsp->node[0];
  (<0>,6549) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6550) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6552) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6556) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6557) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6559) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6562) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6563) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6564) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6568) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6571) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6574) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6577) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6578) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6581) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6583) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6586) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6589) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6592) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6593) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6595) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6598) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6602) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6604) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6606) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6609) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6612) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6615) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6616) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6618) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6621) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6625) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6627) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6629) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6632) tree.c:493: return 0; /* No grace period needed. */
  (<0>,6634) tree.c:494: }
  (<0>,6638) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6640) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6641) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6643) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6646) tree.c:3157: rdp->n_rp_gp_completed++;
  (<0>,6648) tree.c:3157: rdp->n_rp_gp_completed++;
  (<0>,6650) tree.c:3157: rdp->n_rp_gp_completed++;
  (<0>,6651) tree.c:3158: return 1;
  (<0>,6653) tree.c:3176: }
  (<0>,6657) tree.c:3189: return 1;
  (<0>,6659) tree.c:3191: }
  (<0>,6666) fake_sched.h:43: return __running_cpu;
  (<0>,6670) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
  (<0>,6674) tree.c:2437: if (user)
  (<0>,6682) fake_sched.h:43: return __running_cpu;
  (<0>,6686) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
  (<0>,6688) fake_sched.h:43: return __running_cpu;
  (<0>,6692) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6698) fake_sched.h:43: return __running_cpu;
  (<0>,6702) fake_sched.h:182: if (need_softirq[get_cpu()]) {
  (<0>,6712)
  (<0>,6715) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6716) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6717) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6721) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6722) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6723) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6725) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6729) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,6738)
  (<0>,6740) fake_sched.h:43: return __running_cpu;
  (<0>,6743) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,6745) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,6747) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,6748) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6750) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6757) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6758) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6760) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6766) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6767) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6768) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6769) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6770) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6774)
  (<0>,6775)
  (<0>,6776) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,6777) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,6784)
  (<0>,6785)
  (<0>,6786) tree.c:1584: local_irq_save(flags);
  (<0>,6789)
  (<0>,6791) fake_sched.h:43: return __running_cpu;
  (<0>,6795) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6797) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6801) fake_sched.h:43: return __running_cpu;
  (<0>,6805) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6810) tree.c:1585: rnp = rdp->mynode;
  (<0>,6812) tree.c:1585: rnp = rdp->mynode;
  (<0>,6813) tree.c:1585: rnp = rdp->mynode;
  (<0>,6814) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,6816) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,6817) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,6819) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,6822) tree.c:1588: !raw_spin_trylock(&rnp->lock)) { /* irqs already off, so later. */
  (<0>,6827)
  (<0>,6829) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
  (<0>,6830) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
  (<0>,6833) fake_sync.h:127: return 1;
  (<0>,6835) fake_sync.h:128: }
  (<0>,6841) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,6842) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,6843) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,6849)
  (<0>,6850)
  (<0>,6851)
  (<0>,6852) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,6854) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,6855) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,6857) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,6860) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,6861) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,6862) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,6870)
  (<0>,6871)
  (<0>,6872)
  (<0>,6873) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,6876) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,6879) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,6882) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,6883) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,6886) tree.c:1502: return false;
  (<0>,6888) tree.c:1527: }
  (<0>,6891) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,6892) tree.c:1552: rdp->completed = rnp->completed;
  (<0>,6894) tree.c:1552: rdp->completed = rnp->completed;
  (<0>,6895) tree.c:1552: rdp->completed = rnp->completed;
  (<0>,6897) tree.c:1552: rdp->completed = rnp->completed;
  (<0>,6901) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,6903) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,6904) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,6906) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,6909) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,6911) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,6912) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,6914) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,6917) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,6919) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,6920) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,6922) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,6923) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,6925) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,6930) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,6933) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,6934) tree.c:1573: zero_cpu_stall_ticks(rdp);
  (<0>,6937)
  (<0>,6940) tree.c:1575: return ret;
  (<0>,6944) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,6945) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,6947) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,6951)
  (<0>,6952)
  (<0>,6953) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,6954) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,6957) fake_sync.h:86: local_irq_restore(flags);
  (<0>,6960)
  (<0>,6962) fake_sched.h:43: return __running_cpu;
  (<0>,6966) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6968) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6972) fake_sched.h:43: return __running_cpu;
  (<0>,6976) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6983) tree.c:1595: if (needwake)
  (<0>,6987) tree.c:2084: if (!rdp->qs_pending)
  (<0>,6989) tree.c:2084: if (!rdp->qs_pending)
  (<0>,6994) tree.c:2558: local_irq_save(flags);
  (<0>,6997)
  (<0>,6999) fake_sched.h:43: return __running_cpu;
  (<0>,7003) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7005) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7009) fake_sched.h:43: return __running_cpu;
  (<0>,7013) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7018) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7019) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7025)
  (<0>,7026)
  (<0>,7027) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,7030)
  (<0>,7031) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7033) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7034) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7036) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7042) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,7048)
  (<0>,7049) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7052)
  (<0>,7053) tree.c:453: return &rsp->node[0];
  (<0>,7057) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7058) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7060) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7064) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7065) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7067) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7070) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7071) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7072) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7076) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7079) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7082) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7085) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7086) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7089) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7091) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7094) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7097) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7100) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7101) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7103) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7106) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7110) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7112) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7114) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7117) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7120) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7123) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7124) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7126) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7129) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7133) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7135) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7137) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7140) tree.c:493: return 0; /* No grace period needed. */
  (<0>,7142) tree.c:494: }
  (<0>,7146) tree.c:2566: local_irq_restore(flags);
  (<0>,7149)
  (<0>,7151) fake_sched.h:43: return __running_cpu;
  (<0>,7155) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7157) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7161) fake_sched.h:43: return __running_cpu;
  (<0>,7165) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7171) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7174)
  (<0>,7175) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7177) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7180) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7187) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,7190)
  (<0>,7194) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7197) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7198) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7199) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7203) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7204) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7205) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7207) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7211) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,7220)
  (<0>,7222) fake_sched.h:43: return __running_cpu;
  (<0>,7225) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7227) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7229) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7230) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7232) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7239) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7240) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7242) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7248) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7249) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7250) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7251) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7252) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7256)
  (<0>,7257)
  (<0>,7258) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7259) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7266)
  (<0>,7267)
  (<0>,7268) tree.c:1584: local_irq_save(flags);
  (<0>,7271)
  (<0>,7273) fake_sched.h:43: return __running_cpu;
  (<0>,7277) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7279) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7283) fake_sched.h:43: return __running_cpu;
  (<0>,7287) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7292) tree.c:1585: rnp = rdp->mynode;
  (<0>,7294) tree.c:1585: rnp = rdp->mynode;
  (<0>,7295) tree.c:1585: rnp = rdp->mynode;
  (<0>,7296) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7298) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7299) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7301) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7304) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7306) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7307) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7309) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7312) tree.c:1589: local_irq_restore(flags);
  (<0>,7315)
  (<0>,7317) fake_sched.h:43: return __running_cpu;
  (<0>,7321) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7323) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7327) fake_sched.h:43: return __running_cpu;
  (<0>,7331) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7338) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7340) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7345) tree.c:2558: local_irq_save(flags);
  (<0>,7348)
  (<0>,7350) fake_sched.h:43: return __running_cpu;
  (<0>,7354) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7356) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7360) fake_sched.h:43: return __running_cpu;
  (<0>,7364) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7369) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7370) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7376)
  (<0>,7377)
  (<0>,7378) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,7381)
  (<0>,7382) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7384) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7385) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7387) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7393) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,7399)
  (<0>,7400) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7403)
  (<0>,7404) tree.c:453: return &rsp->node[0];
  (<0>,7408) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7409) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7411) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7415) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7416) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7418) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7421) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7422) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7423) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7427) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7430) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7433) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7436) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7437) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7440) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7442) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7445) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7448) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7451) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7452) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7454) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7457) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7461) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7463) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7465) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7468) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7471) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7474) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7475) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7477) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7480) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7484) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7486) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7488) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7491) tree.c:493: return 0; /* No grace period needed. */
  (<0>,7493) tree.c:494: }
  (<0>,7497) tree.c:2566: local_irq_restore(flags);
  (<0>,7500)
  (<0>,7502) fake_sched.h:43: return __running_cpu;
  (<0>,7506) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7508) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7512) fake_sched.h:43: return __running_cpu;
  (<0>,7516) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7522) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7525)
  (<0>,7526) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7528) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7531) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7538) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,7541)
  (<0>,7545) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7548) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7549) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7550) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7554) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7555) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7556) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7558) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7566) fake_sched.h:43: return __running_cpu;
  (<0>,7570) fake_sched.h:184: need_softirq[get_cpu()] = 0;
  (<0>,7579) tree.c:624: local_irq_save(flags);
  (<0>,7582)
  (<0>,7584) fake_sched.h:43: return __running_cpu;
  (<0>,7588) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7590) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7594) fake_sched.h:43: return __running_cpu;
  (<0>,7598) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7604) fake_sched.h:43: return __running_cpu;
  (<0>,7608) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,7609) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,7611) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,7612) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,7613) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,7615) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,7617) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,7618) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7620) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7625) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7626) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7628) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7632) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7633) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7634) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7635) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,7637) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,7645)
  (<0>,7647) tree.c:634: local_irq_restore(flags);
  (<0>,7650)
  (<0>,7652) fake_sched.h:43: return __running_cpu;
  (<0>,7656) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7658) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7662) fake_sched.h:43: return __running_cpu;
  (<0>,7666) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7675) fake_sched.h:43: return __running_cpu;
  (<0>,7679)
  (<0>,7682) tree.c:580: local_irq_save(flags);
  (<0>,7685)
  (<0>,7687) fake_sched.h:43: return __running_cpu;
  (<0>,7691) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7693) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7697) fake_sched.h:43: return __running_cpu;
  (<0>,7701) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7714)
  (<0>,7716) fake_sched.h:43: return __running_cpu;
  (<0>,7720) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,7721) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,7723) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,7724) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,7725) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,7731) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,7732) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,7737) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,7738) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,7739) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,7740) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,7744) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,7746) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,7747) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,7748) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,7767)
  (<0>,7769)
  (<0>,7771) fake_sched.h:43: return __running_cpu;
  (<0>,7775) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,7778) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,7782) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7783) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7784) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7788) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7789) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7790) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7792) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7797) fake_sched.h:43: return __running_cpu;
  (<0>,7800) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,7802) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,7804) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,7805) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,7808)
  (<0>,7811) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7814) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7815) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7816) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7820) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7821) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7822) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7824) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7829) fake_sched.h:43: return __running_cpu;
  (<0>,7832) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,7834) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,7836) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,7837) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,7840)
  (<0>,7843) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7846) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7847) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7848) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7852) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7853) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7854) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7856) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,7863) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,7866) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,7867) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,7868) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,7870) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,7871) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,7873) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,7876) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,7882) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,7883) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,7886) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,7891) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,7892) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,7893) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,7907)
  (<0>,7909) tree.c:583: local_irq_restore(flags);
  (<0>,7912)
  (<0>,7914) fake_sched.h:43: return __running_cpu;
  (<0>,7918) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7920) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7924) fake_sched.h:43: return __running_cpu;
  (<0>,7928) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7934) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,7937) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,7942) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,7946) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,7949) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,7953) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,7954) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,7955) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
             Error: Assertion violation at (<0>,7958): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpqgg2t_y2/tmpj74l5zk6.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DFORCE_FAILURE_3 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpqgg2t_y2/tmp7z4tnqmy.ll /tmp/tmpqgg2t_y2/tmpj74l5zk6.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=3 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpqgg2t_y2/tmp7z4tnqmy.ll
Total wall-clock time: 16.19 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 163 (also 27 sleepset blocked, 0 schedulings and 221 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmphk3hiqbk/tmpqbqvmvg2.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DFORCE_FAILURE_5 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmphk3hiqbk/tmpaq23x4bc.ll /tmp/tmphk3hiqbk/tmpqbqvmvg2.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=3 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmphk3hiqbk/tmpaq23x4bc.ll
Total wall-clock time: 4.82 s
^^^ Unexpected verification success
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 100 (also 10 sleepset blocked, 0 schedulings and 53 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmprte5es9r/tmp3697nzn_.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DLIVENESS_CHECK_1 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmprte5es9r/tmpphf1mbnn.ll /tmp/tmprte5es9r/tmp3697nzn_.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmprte5es9r/tmpphf1mbnn.ll
Total wall-clock time: 2.40 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 100 (also 10 sleepset blocked, 0 schedulings and 53 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp932iyc9y/tmp6vj6rl7i.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DLIVENESS_CHECK_2 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp932iyc9y/tmpnpleratw.ll /tmp/tmp932iyc9y/tmp6vj6rl7i.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp932iyc9y/tmpnpleratw.ll
Total wall-clock time: 2.39 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 124 (also 10 sleepset blocked, 0 schedulings and 67 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpyl3lt6yx/tmpxsemeiai.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DLIVENESS_CHECK_3 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpyl3lt6yx/tmpiwzdx3a_.ll /tmp/tmpyl3lt6yx/tmpxsemeiai.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpyl3lt6yx/tmpiwzdx3a_.ll
Total wall-clock time: 2.98 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 247 (also 23 sleepset blocked, 0 schedulings and 337 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpc2qxqt9c/tmpic93hpjp.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpc2qxqt9c/tmp9jegbdiu.ll /tmp/tmpc2qxqt9c/tmpic93hpjp.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpc2qxqt9c/tmp9jegbdiu.ll
Total wall-clock time: 13.07 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 247 (also 23 sleepset blocked, 0 schedulings and 359 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpyilh0sup/tmp2gie4dor.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DFORCE_FAILURE_1 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpyilh0sup/tmpm6355msy.ll /tmp/tmpyilh0sup/tmp2gie4dor.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=3 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpyilh0sup/tmpm6355msy.ll
Total wall-clock time: 12.31 s
^^^ Unexpected verification success
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 707 (also 9 sleepset blocked, 0 schedulings and 578 branches were rejected due to the bound) 

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,75) tree_plugin.h:731: pr_info("Hierarchical RCU implementation.\n");
  (<0>,79) tree_plugin.h:76: if (rcu_fanout_exact)
  (<0>,82) tree_plugin.h:87: if (rcu_fanout_leaf != RCU_FANOUT_LEAF)
  (<0>,94) tree.c:4147: ulong d;
  (<0>,95) tree.c:4159: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,98) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,99) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,101) tree.c:4161: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,104) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,105) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,107) tree.c:4165: if (rcu_fanout_leaf == RCU_FANOUT_LEAF &&
  (<0>,120)
  (<0>,121) tree.c:4056: static void __init rcu_init_one(struct rcu_state *rsp)
  (<0>,122) tree.c:4067: int i;
  (<0>,125) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,128) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,130) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,131) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,134) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,137) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,141) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,143) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,145) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,147) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,151) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,153) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,154) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,165)
  (<0>,166) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,167) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,170) tree.c:4041: int ccur;
  (<0>,171) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,173) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,175) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,178) tree.c:4046: ccur = levelcnt[i];
  (<0>,180) tree.c:4046: ccur = levelcnt[i];
  (<0>,182) tree.c:4046: ccur = levelcnt[i];
  (<0>,183) tree.c:4046: ccur = levelcnt[i];
  (<0>,184) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,185) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,188) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,190) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,192) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,194) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,195) tree.c:4048: cprv = ccur;
  (<0>,196) tree.c:4048: cprv = ccur;
  (<0>,198) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,200) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,202) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,207) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,208) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,210) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,211) tree.c:4085: fl_mask <<= 1;
  (<0>,215) tree.c:4085: fl_mask <<= 1;
  (<0>,216) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,218) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,220) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,223) tree.c:4090: cpustride *= levelspread[i];
  (<0>,226) tree.c:4090: cpustride *= levelspread[i];
  (<0>,227) tree.c:4090: cpustride *= levelspread[i];
  (<0>,229) tree.c:4090: cpustride *= levelspread[i];
  (<0>,230) tree.c:4091: rnp = rsp->level[i];
  (<0>,232) tree.c:4091: rnp = rsp->level[i];
  (<0>,235) tree.c:4091: rnp = rsp->level[i];
  (<0>,236) tree.c:4091: rnp = rsp->level[i];
  (<0>,237) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,240) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,243) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,246) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,250)
  (<0>,251) fake_sync.h:75: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,252) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,258) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,262)
  (<0>,263) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,264) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,270) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,273) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,275) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,276) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,278) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,279) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,281) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,282) tree.c:4101: rnp->qsmask = 0;
  (<0>,284) tree.c:4101: rnp->qsmask = 0;
  (<0>,285) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,287) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,288) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,291) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,293) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,294) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,296) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,299) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,301) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,302) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,304) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,307) tree.c:4107: if (i == 0) {
  (<0>,310) tree.c:4108: rnp->grpnum = 0;
  (<0>,312) tree.c:4108: rnp->grpnum = 0;
  (<0>,313) tree.c:4109: rnp->grpmask = 0;
  (<0>,315) tree.c:4109: rnp->grpmask = 0;
  (<0>,316) tree.c:4110: rnp->parent = NULL;
  (<0>,318) tree.c:4110: rnp->parent = NULL;
  (<0>,320) tree.c:4117: rnp->level = i;
  (<0>,322) tree.c:4117: rnp->level = i;
  (<0>,324) tree.c:4117: rnp->level = i;
  (<0>,325) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,329)
  (<0>,330) fake_defs.h:358: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,331) fake_defs.h:360: list->next = list;
  (<0>,333) fake_defs.h:360: list->next = list;
  (<0>,334) fake_defs.h:361: list->prev = list;
  (<0>,335) fake_defs.h:361: list->prev = list;
  (<0>,337) fake_defs.h:361: list->prev = list;
  (<0>,339) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,342) tree_plugin.h:2431: }
  (<0>,352) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,356)
  (<0>,357) fake_sync.h:141: void spin_lock_init(raw_spinlock_t *l)
  (<0>,358) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,363) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,365) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,366) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,368) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,370) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,371) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,374) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,378) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,380) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,382) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,389) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,392) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,395) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,396) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,397) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,399) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,400) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,407)
  (<0>,408) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,409) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,411) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,412) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,415) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,417) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,418) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,421) fake_defs.h:635: if (offset & mask)
  (<0>,422) fake_defs.h:635: if (offset & mask)
  (<0>,426) fake_defs.h:636: return cpu;
  (<0>,427) fake_defs.h:636: return cpu;
  (<0>,429) fake_defs.h:639: }
  (<0>,431) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,432) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,436) tree.c:4132: while (i > rnp->grphi)
  (<0>,437) tree.c:4132: while (i > rnp->grphi)
  (<0>,439) tree.c:4132: while (i > rnp->grphi)
  (<0>,442) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,443) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,445) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,447) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,450) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,451) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,452) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,475)
  (<0>,476) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,477) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,479) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,481) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,483) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,484) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,487)
  (<0>,488) tree.c:623: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,492) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,496) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,497) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,498) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,500) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,504)
  (<0>,505) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,506) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,509) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,511) fake_sched.h:43: return __running_cpu;
  (<0>,515) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,517) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,521) fake_sched.h:43: return __running_cpu;
  (<0>,525) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,531) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,532) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,539) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,540) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,542) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,544) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,548) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,550) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,551) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,554) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,556) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,557) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,559) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,561) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,566) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,567) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,569) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,571) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,575) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,576) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,577) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,578) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,579) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,581) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,584) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,585) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,586) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,591) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,592) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,593) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,595) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,598) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,599) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,600) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,604) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,605) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,606) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,607) tree.c:3776: rdp->cpu = cpu;
  (<0>,608) tree.c:3776: rdp->cpu = cpu;
  (<0>,610) tree.c:3776: rdp->cpu = cpu;
  (<0>,611) tree.c:3777: rdp->rsp = rsp;
  (<0>,612) tree.c:3777: rdp->rsp = rsp;
  (<0>,614) tree.c:3777: rdp->rsp = rsp;
  (<0>,615) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,618) tree_plugin.h:2448: }
  (<0>,623) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,624) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,625) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,627) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,631)
  (<0>,632) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,633) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,634) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,637) fake_sync.h:93: local_irq_restore(flags);
  (<0>,640) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,642) fake_sched.h:43: return __running_cpu;
  (<0>,646) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,648) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,652) fake_sched.h:43: return __running_cpu;
  (<0>,656) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,666) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,667) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,674)
  (<0>,675)
  (<0>,676) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,678) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,679) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,682) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,684) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,685) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,688) fake_defs.h:635: if (offset & mask)
  (<0>,689) fake_defs.h:635: if (offset & mask)
  (<0>,693) fake_defs.h:636: return cpu;
  (<0>,694) fake_defs.h:636: return cpu;
  (<0>,696) fake_defs.h:639: }
  (<0>,698) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,699) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,703) tree.c:4132: while (i > rnp->grphi)
  (<0>,704) tree.c:4132: while (i > rnp->grphi)
  (<0>,706) tree.c:4132: while (i > rnp->grphi)
  (<0>,709) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,710) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,712) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,714) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,717) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,718) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,719) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,742)
  (<0>,743)
  (<0>,744) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,746) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,748) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,750) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,751) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,754)
  (<0>,755) tree.c:625: return &rsp->node[0];
  (<0>,759) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,763) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,764) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,765) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,767) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,771)
  (<0>,772)
  (<0>,773) fake_sync.h:83: local_irq_save(flags);
  (<0>,776) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,778) fake_sched.h:43: return __running_cpu;
  (<0>,782) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,784) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,788) fake_sched.h:43: return __running_cpu;
  (<0>,792) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,798) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,799) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,806) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,807) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,809) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,811) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,815) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,817) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,818) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,821) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,823) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,824) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,826) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,828) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,833) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,834) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,836) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,838) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,842) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,843) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,844) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,845) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,846) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,848) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,851) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,852) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,853) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,858) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,859) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,860) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,862) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,865) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,866) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,867) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,871) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,872) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,873) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,874) tree.c:3776: rdp->cpu = cpu;
  (<0>,875) tree.c:3776: rdp->cpu = cpu;
  (<0>,877) tree.c:3776: rdp->cpu = cpu;
  (<0>,878) tree.c:3777: rdp->rsp = rsp;
  (<0>,879) tree.c:3777: rdp->rsp = rsp;
  (<0>,881) tree.c:3777: rdp->rsp = rsp;
  (<0>,882) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,885) tree_plugin.h:2448: }
  (<0>,890) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,891) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,892) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,894) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,898)
  (<0>,899)
  (<0>,900) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,901) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,904) fake_sync.h:93: local_irq_restore(flags);
  (<0>,907) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,909) fake_sched.h:43: return __running_cpu;
  (<0>,913) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,915) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,919) fake_sched.h:43: return __running_cpu;
  (<0>,923) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,933) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,934) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,941)
  (<0>,942)
  (<0>,943) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,945) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,946) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,949) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,951) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,952) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,955) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,957) fake_defs.h:639: }
  (<0>,959) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,960) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,963) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,968)
  (<0>,969) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,970) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,971) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,972) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,974) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,979)
  (<0>,980) fake_defs.h:364: static inline void __list_add(struct list_head *new,
  (<0>,981) fake_defs.h:365: struct list_head *prev,
  (<0>,982) fake_defs.h:366: struct list_head *next)
  (<0>,983) fake_defs.h:368: next->prev = new;
  (<0>,985) fake_defs.h:368: next->prev = new;
  (<0>,986) fake_defs.h:369: new->next = next;
  (<0>,987) fake_defs.h:369: new->next = next;
  (<0>,989) fake_defs.h:369: new->next = next;
  (<0>,990) fake_defs.h:370: new->prev = prev;
  (<0>,991) fake_defs.h:370: new->prev = prev;
  (<0>,993) fake_defs.h:370: new->prev = prev;
  (<0>,994) fake_defs.h:371: prev->next = new;
  (<0>,995) fake_defs.h:371: prev->next = new;
  (<0>,997) fake_defs.h:371: prev->next = new;
  (<0>,1009)
  (<0>,1010) tree.c:4066: int cpustride = 1;
  (<0>,1011) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1014) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1017) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1019) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1020) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1023) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1026) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1027) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1030) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1032) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1034) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1036) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1037) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1040) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1042) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1043) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1054)
  (<0>,1055)
  (<0>,1056) tree.c:4036: if (rcu_fanout_exact) {
  (<0>,1059) tree.c:4044: cprv = nr_cpu_ids;
  (<0>,1060) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1062) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1064) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1067) tree.c:4046: ccur = levelcnt[i];
  (<0>,1069) tree.c:4046: ccur = levelcnt[i];
  (<0>,1071) tree.c:4046: ccur = levelcnt[i];
  (<0>,1072) tree.c:4046: ccur = levelcnt[i];
  (<0>,1073) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1074) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1077) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1079) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1081) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1083) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1084) tree.c:4048: cprv = ccur;
  (<0>,1085) tree.c:4048: cprv = ccur;
  (<0>,1087) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1089) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1091) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1096) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1097) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1099) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1100) tree.c:4085: fl_mask <<= 1;
  (<0>,1104) tree.c:4085: fl_mask <<= 1;
  (<0>,1105) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1107) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1109) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1112) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1115) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1116) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1118) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1119) tree.c:4091: rnp = rsp->level[i];
  (<0>,1121) tree.c:4091: rnp = rsp->level[i];
  (<0>,1124) tree.c:4091: rnp = rsp->level[i];
  (<0>,1125) tree.c:4091: rnp = rsp->level[i];
  (<0>,1126) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1128) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1129) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1132) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1135) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,1139)
  (<0>,1140) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1141) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1147) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1151)
  (<0>,1152) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1153) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1159) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1161) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1162) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1164) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1165) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1167) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1168) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1170) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1171) tree.c:4101: rnp->qsmask = 0;
  (<0>,1173) tree.c:4101: rnp->qsmask = 0;
  (<0>,1174) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1176) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1177) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1178) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1180) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1182) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1183) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1185) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1188) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1190) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1191) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1193) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1196) tree.c:4107: if (i == 0) {
  (<0>,1199) tree.c:4108: rnp->grpnum = 0;
  (<0>,1201) tree.c:4108: rnp->grpnum = 0;
  (<0>,1202) tree.c:4109: rnp->grpmask = 0;
  (<0>,1204) tree.c:4109: rnp->grpmask = 0;
  (<0>,1205) tree.c:4110: rnp->parent = NULL;
  (<0>,1207) tree.c:4110: rnp->parent = NULL;
  (<0>,1209) tree.c:4117: rnp->level = i;
  (<0>,1211) tree.c:4117: rnp->level = i;
  (<0>,1213) tree.c:4117: rnp->level = i;
  (<0>,1214) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1218)
  (<0>,1219) fake_defs.h:360: list->next = list;
  (<0>,1220) fake_defs.h:360: list->next = list;
  (<0>,1222) fake_defs.h:360: list->next = list;
  (<0>,1223) fake_defs.h:361: list->prev = list;
  (<0>,1224) fake_defs.h:361: list->prev = list;
  (<0>,1226) fake_defs.h:361: list->prev = list;
  (<0>,1228) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,1231) tree_plugin.h:2431: }
  (<0>,1241) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,1245)
  (<0>,1246) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1247) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1252) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1254) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1255) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1257) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1259) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1260) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1263) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1267) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1269) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1271) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1278) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1281) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1284) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1285) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1286) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1288) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1289) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1296)
  (<0>,1297)
  (<0>,1298) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1300) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1301) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1304) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1306) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1307) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1310) fake_defs.h:635: if (offset & mask)
  (<0>,1311) fake_defs.h:635: if (offset & mask)
  (<0>,1315) fake_defs.h:636: return cpu;
  (<0>,1316) fake_defs.h:636: return cpu;
  (<0>,1318) fake_defs.h:639: }
  (<0>,1320) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1321) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1325) tree.c:4132: while (i > rnp->grphi)
  (<0>,1326) tree.c:4132: while (i > rnp->grphi)
  (<0>,1328) tree.c:4132: while (i > rnp->grphi)
  (<0>,1331) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1332) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1334) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1336) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1339) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1340) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1341) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1364)
  (<0>,1365)
  (<0>,1366) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1368) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1370) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1372) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1373) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1376)
  (<0>,1377) tree.c:625: return &rsp->node[0];
  (<0>,1381) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1385) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1386) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1387) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1389) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1393)
  (<0>,1394)
  (<0>,1395) fake_sync.h:83: local_irq_save(flags);
  (<0>,1398) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1400) fake_sched.h:43: return __running_cpu;
  (<0>,1404) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1406) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1410) fake_sched.h:43: return __running_cpu;
  (<0>,1414) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1420) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1421) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1428) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1429) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1431) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1433) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1437) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1439) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1440) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1443) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1445) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1446) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1448) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1450) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1455) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1456) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1458) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1460) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1464) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1465) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1466) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1467) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1468) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1470) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1473) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1474) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1475) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1480) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1481) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1482) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1484) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1487) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1488) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1489) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1493) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1494) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1495) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1496) tree.c:3776: rdp->cpu = cpu;
  (<0>,1497) tree.c:3776: rdp->cpu = cpu;
  (<0>,1499) tree.c:3776: rdp->cpu = cpu;
  (<0>,1500) tree.c:3777: rdp->rsp = rsp;
  (<0>,1501) tree.c:3777: rdp->rsp = rsp;
  (<0>,1503) tree.c:3777: rdp->rsp = rsp;
  (<0>,1504) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1507) tree_plugin.h:2448: }
  (<0>,1512) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1513) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1514) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1516) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1520)
  (<0>,1521)
  (<0>,1522) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1523) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1526) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1529) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1531) fake_sched.h:43: return __running_cpu;
  (<0>,1535) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1541) fake_sched.h:43: return __running_cpu;
  (<0>,1545) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1555) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1556) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1563)
  (<0>,1564)
  (<0>,1565) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1567) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1568) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1571) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1573) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1574) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1577) fake_defs.h:635: if (offset & mask)
  (<0>,1578) fake_defs.h:635: if (offset & mask)
  (<0>,1582) fake_defs.h:636: return cpu;
  (<0>,1583) fake_defs.h:636: return cpu;
  (<0>,1585) fake_defs.h:639: }
  (<0>,1587) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1588) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1592) tree.c:4132: while (i > rnp->grphi)
  (<0>,1593) tree.c:4132: while (i > rnp->grphi)
  (<0>,1595) tree.c:4132: while (i > rnp->grphi)
  (<0>,1598) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1599) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1601) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1603) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1606) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1607) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1608) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1631)
  (<0>,1632)
  (<0>,1633) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1635) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1637) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1639) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1640) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1643)
  (<0>,1644) tree.c:625: return &rsp->node[0];
  (<0>,1648) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1652) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1653) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1654) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1656) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1660)
  (<0>,1661)
  (<0>,1662) fake_sync.h:83: local_irq_save(flags);
  (<0>,1665) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1667) fake_sched.h:43: return __running_cpu;
  (<0>,1671) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1673) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1677) fake_sched.h:43: return __running_cpu;
  (<0>,1681) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1687) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1688) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1695) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1696) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1698) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1700) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1704) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1706) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1707) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1710) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1712) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1713) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1715) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1717) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1722) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1723) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1725) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1727) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1731) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1732) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1733) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1734) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1735) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1737) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1740) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1741) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1742) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1747) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1748) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1749) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1751) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1754) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1755) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1756) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1760) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1761) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1762) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1763) tree.c:3776: rdp->cpu = cpu;
  (<0>,1764) tree.c:3776: rdp->cpu = cpu;
  (<0>,1766) tree.c:3776: rdp->cpu = cpu;
  (<0>,1767) tree.c:3777: rdp->rsp = rsp;
  (<0>,1768) tree.c:3777: rdp->rsp = rsp;
  (<0>,1770) tree.c:3777: rdp->rsp = rsp;
  (<0>,1771) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1774) tree_plugin.h:2448: }
  (<0>,1779) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1780) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1781) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1783) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1787)
  (<0>,1788)
  (<0>,1789) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1790) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1793) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1796) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1798) fake_sched.h:43: return __running_cpu;
  (<0>,1802) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1804) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1808) fake_sched.h:43: return __running_cpu;
  (<0>,1812) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1822) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1823) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1830)
  (<0>,1831)
  (<0>,1832) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1834) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1835) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1838) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1840) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1841) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1844) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,1846) fake_defs.h:639: }
  (<0>,1848) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1849) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1852) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1857)
  (<0>,1858)
  (<0>,1859) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1860) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1861) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1863) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1868)
  (<0>,1869)
  (<0>,1870)
  (<0>,1871) fake_defs.h:368: next->prev = new;
  (<0>,1872) fake_defs.h:368: next->prev = new;
  (<0>,1874) fake_defs.h:368: next->prev = new;
  (<0>,1875) fake_defs.h:369: new->next = next;
  (<0>,1876) fake_defs.h:369: new->next = next;
  (<0>,1878) fake_defs.h:369: new->next = next;
  (<0>,1879) fake_defs.h:370: new->prev = prev;
  (<0>,1880) fake_defs.h:370: new->prev = prev;
  (<0>,1882) fake_defs.h:370: new->prev = prev;
  (<0>,1883) fake_defs.h:371: prev->next = new;
  (<0>,1884) fake_defs.h:371: prev->next = new;
  (<0>,1886) fake_defs.h:371: prev->next = new;
  (<0>,1890) tree.c:4251: if (dump_tree)
  (<0>,1899) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1901) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1902) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1909)
  (<0>,1910)
  (<0>,1911) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1913) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1914) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1917) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1919) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1920) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1923) fake_defs.h:635: if (offset & mask)
  (<0>,1924) fake_defs.h:635: if (offset & mask)
  (<0>,1928) fake_defs.h:636: return cpu;
  (<0>,1929) fake_defs.h:636: return cpu;
  (<0>,1931) fake_defs.h:639: }
  (<0>,1933) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1934) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1937) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,1945)
  (<0>,1946) tree.c:3829: int rcutree_prepare_cpu(unsigned int cpu)
  (<0>,1947) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1948) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1952) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1953) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1954) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1956) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1960) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1961) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1987)
  (<0>,1988) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1989) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1991) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1993) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1995) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1996) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1999)
  (<0>,2000) tree.c:625: return &rsp->node[0];
  (<0>,2004) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2008) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2009) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2010) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2012) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2016)
  (<0>,2017)
  (<0>,2018) fake_sync.h:83: local_irq_save(flags);
  (<0>,2021) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2023) fake_sched.h:43: return __running_cpu;
  (<0>,2027) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2029) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2033) fake_sched.h:43: return __running_cpu;
  (<0>,2037) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2043) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2044) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2051) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2053) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2054) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2056) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2057) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2059) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2060) tree.c:3800: rdp->blimit = blimit;
  (<0>,2061) tree.c:3800: rdp->blimit = blimit;
  (<0>,2063) tree.c:3800: rdp->blimit = blimit;
  (<0>,2064) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2066) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2069) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2072)
  (<0>,2073) tree.c:1551: static void init_callback_list(struct rcu_data *rdp)
  (<0>,2076) tree_plugin.h:2469: return false;
  (<0>,2079) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2083)
  (<0>,2084) tree.c:1539: static void init_default_callback_list(struct rcu_data *rdp)
  (<0>,2086) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2087) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2089) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2092) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2094) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2096) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2099) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2101) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2103) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2105) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2108) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2110) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2112) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2115) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2117) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2119) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2121) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2124) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2126) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2128) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2131) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2133) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2135) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2137) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2140) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2142) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2144) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2147) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2149) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2151) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2153) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2160) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2162) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2164) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2165) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2167) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2170) tree_plugin.h:2902: }
  (<0>,2172) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2173) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2175) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2178) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2179) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2180) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2183) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2185) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2188) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2189) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2192)
  (<0>,2193) tree.h:721: static inline void raw_spin_unlock_rcu_node(struct rcu_node *rnp)
  (<0>,2197)
  (<0>,2198) fake_sync.h:120: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2199) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2205) tree.c:3814: rnp = rdp->mynode;
  (<0>,2207) tree.c:3814: rnp = rdp->mynode;
  (<0>,2208) tree.c:3814: rnp = rdp->mynode;
  (<0>,2209) tree.c:3815: mask = rdp->grpmask;
  (<0>,2211) tree.c:3815: mask = rdp->grpmask;
  (<0>,2212) tree.c:3815: mask = rdp->grpmask;
  (<0>,2213) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2216)
  (<0>,2217) tree.h:715: static inline void raw_spin_lock_rcu_node(struct rcu_node *rnp)
  (<0>,2221) fake_sync.h:115: preempt_disable();
  (<0>,2223) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2224) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2231) tree.c:3817: if (!rdp->beenonline)
  (<0>,2233) tree.c:3817: if (!rdp->beenonline)
  (<0>,2237) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2242) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2243) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2244) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2245) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2247) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2249) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2250) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2252) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2255) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2256) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2257) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2259) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2260) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2265) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2266) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2267) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2268) fake_defs.h:237: switch (size) {
  (<0>,2270) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2272) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2273) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2275) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2278) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2279) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2280) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2282) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2284) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2285) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2287) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2288) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2290) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2291) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2293) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2294) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2296) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2297) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2301) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2302) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2305) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2306) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2308) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2309) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2311) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2317) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2318) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2319) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2321) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2325)
  (<0>,2326)
  (<0>,2327) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2328) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2331) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2334) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2336) fake_sched.h:43: return __running_cpu;
  (<0>,2340) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2342) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2346) fake_sched.h:43: return __running_cpu;
  (<0>,2350) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2360) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2363) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2364) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2365) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2369) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2370) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2371) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2373) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2377) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2378) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2404)
  (<0>,2405)
  (<0>,2406) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2408) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2410) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2412) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2413) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2416)
  (<0>,2417) tree.c:625: return &rsp->node[0];
  (<0>,2421) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2425) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2426) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2427) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2429) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2433)
  (<0>,2434)
  (<0>,2435) fake_sync.h:83: local_irq_save(flags);
  (<0>,2438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2440) fake_sched.h:43: return __running_cpu;
  (<0>,2444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2446) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2450) fake_sched.h:43: return __running_cpu;
  (<0>,2454) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2460) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2461) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2468) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2470) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2471) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2473) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2474) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2476) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2477) tree.c:3800: rdp->blimit = blimit;
  (<0>,2478) tree.c:3800: rdp->blimit = blimit;
  (<0>,2480) tree.c:3800: rdp->blimit = blimit;
  (<0>,2481) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2483) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2486) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2489)
  (<0>,2490) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,2493) tree_plugin.h:2469: return false;
  (<0>,2496) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2500)
  (<0>,2501) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2503) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2504) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2506) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2509) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2511) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2513) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2516) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2518) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2520) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2522) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2525) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2527) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2529) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2532) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2534) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2536) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2538) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2541) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2543) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2545) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2548) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2550) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2552) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2554) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2557) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2559) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2561) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2564) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2566) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2568) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2570) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2577) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2579) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2581) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2582) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2584) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2587) tree_plugin.h:2902: }
  (<0>,2589) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2590) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2592) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2595) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2596) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2597) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2600) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2602) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2605) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2606) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2609)
  (<0>,2610) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2614)
  (<0>,2615) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2616) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2622) tree.c:3814: rnp = rdp->mynode;
  (<0>,2624) tree.c:3814: rnp = rdp->mynode;
  (<0>,2625) tree.c:3814: rnp = rdp->mynode;
  (<0>,2626) tree.c:3815: mask = rdp->grpmask;
  (<0>,2628) tree.c:3815: mask = rdp->grpmask;
  (<0>,2629) tree.c:3815: mask = rdp->grpmask;
  (<0>,2630) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2633)
  (<0>,2634) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2638) fake_sync.h:115: preempt_disable();
  (<0>,2640) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2641) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2648) tree.c:3817: if (!rdp->beenonline)
  (<0>,2650) tree.c:3817: if (!rdp->beenonline)
  (<0>,2654) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2659) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2660) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2661) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2662) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2664) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2666) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2667) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2669) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2672) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2673) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2674) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2676) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2677) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2682) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2683) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2684) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2685) fake_defs.h:237: switch (size) {
  (<0>,2687) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2689) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2690) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2692) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2695) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2696) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2697) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2699) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2701) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2702) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2704) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2705) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2707) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2708) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2710) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2711) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2713) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2714) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2718) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2719) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2722) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2723) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2725) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2726) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2728) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2734) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2735) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2736) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2738) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2742)
  (<0>,2743)
  (<0>,2744) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2745) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2748) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2751) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2753) fake_sched.h:43: return __running_cpu;
  (<0>,2757) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2759) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2763) fake_sched.h:43: return __running_cpu;
  (<0>,2767) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2777) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2780) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2781) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2782) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2786) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2787) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2788) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2790) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2794) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,2797) tree_plugin.h:1243: }
  (<0>,2799) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2802) tree_plugin.h:2461: }
  (<0>,2805) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,2823)
  (<0>,2824) tree.c:3890: void rcu_cpu_starting(unsigned int cpu)
  (<0>,2825) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2826) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2830) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2831) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2832) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2834) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2839) fake_sched.h:43: return __running_cpu;
  (<0>,2842) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2844) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2846) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2847) tree.c:3900: rnp = rdp->mynode;
  (<0>,2849) tree.c:3900: rnp = rdp->mynode;
  (<0>,2850) tree.c:3900: rnp = rdp->mynode;
  (<0>,2851) tree.c:3901: mask = rdp->grpmask;
  (<0>,2853) tree.c:3901: mask = rdp->grpmask;
  (<0>,2854) tree.c:3901: mask = rdp->grpmask;
  (<0>,2858) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2859) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2860) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2862) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2866)
  (<0>,2867)
  (<0>,2868) fake_sync.h:83: local_irq_save(flags);
  (<0>,2871) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2873) fake_sched.h:43: return __running_cpu;
  (<0>,2877) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2879) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2883) fake_sched.h:43: return __running_cpu;
  (<0>,2887) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2893) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2894) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2901) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2902) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2904) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2906) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2907) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2908) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2910) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2912) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2916) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2917) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2918) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2920) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2924)
  (<0>,2925)
  (<0>,2926) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2927) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2930) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2933) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2935) fake_sched.h:43: return __running_cpu;
  (<0>,2939) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2941) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2945) fake_sched.h:43: return __running_cpu;
  (<0>,2949) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2958) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2961) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2962) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2963) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2967) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2968) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2969) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2971) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2976) fake_sched.h:43: return __running_cpu;
  (<0>,2979) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2981) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2983) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2984) tree.c:3900: rnp = rdp->mynode;
  (<0>,2986) tree.c:3900: rnp = rdp->mynode;
  (<0>,2987) tree.c:3900: rnp = rdp->mynode;
  (<0>,2988) tree.c:3901: mask = rdp->grpmask;
  (<0>,2990) tree.c:3901: mask = rdp->grpmask;
  (<0>,2991) tree.c:3901: mask = rdp->grpmask;
  (<0>,2995) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2996) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2997) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2999) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3003)
  (<0>,3004)
  (<0>,3005) fake_sync.h:83: local_irq_save(flags);
  (<0>,3008) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3010) fake_sched.h:43: return __running_cpu;
  (<0>,3014) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3016) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3020) fake_sched.h:43: return __running_cpu;
  (<0>,3024) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3030) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3031) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3038) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3039) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3041) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3043) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3044) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3045) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3047) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3049) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3053) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3054) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3055) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3057) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3061)
  (<0>,3062)
  (<0>,3063) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3064) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3067) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3070) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3072) fake_sched.h:43: return __running_cpu;
  (<0>,3076) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3078) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3082) fake_sched.h:43: return __running_cpu;
  (<0>,3086) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3095) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3098) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3099) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3100) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3104) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3105) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3106) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3108) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3114) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3115) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3122)
  (<0>,3123)
  (<0>,3124) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3126) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3127) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3130) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3132) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3133) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3136) fake_defs.h:635: if (offset & mask)
  (<0>,3137) fake_defs.h:635: if (offset & mask)
  (<0>,3141) fake_defs.h:636: return cpu;
  (<0>,3142) fake_defs.h:636: return cpu;
  (<0>,3144) fake_defs.h:639: }
  (<0>,3146) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3147) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3150) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,3158)
  (<0>,3159) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3160) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3161) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3165) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3166) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3167) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3169) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3173) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3174) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3200)
  (<0>,3201)
  (<0>,3202) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3204) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3206) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3208) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3209) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3212)
  (<0>,3213) tree.c:625: return &rsp->node[0];
  (<0>,3217) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3221) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3222) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3223) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3225) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3229)
  (<0>,3230)
  (<0>,3231) fake_sync.h:83: local_irq_save(flags);
  (<0>,3234) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3236) fake_sched.h:43: return __running_cpu;
  (<0>,3240) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3242) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3246) fake_sched.h:43: return __running_cpu;
  (<0>,3250) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3256) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3257) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3264) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3266) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3267) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3269) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3270) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3272) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3273) tree.c:3800: rdp->blimit = blimit;
  (<0>,3274) tree.c:3800: rdp->blimit = blimit;
  (<0>,3276) tree.c:3800: rdp->blimit = blimit;
  (<0>,3277) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3279) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3282) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3285)
  (<0>,3286) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3289) tree_plugin.h:2469: return false;
  (<0>,3292) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3296)
  (<0>,3297) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3299) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3300) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3302) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3305) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3307) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3309) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3312) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3314) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3316) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3318) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3321) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3323) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3325) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3328) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3330) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3332) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3334) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3337) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3339) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3341) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3344) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3346) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3348) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3350) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3353) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3355) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3357) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3360) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3362) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3364) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3366) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3373) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3375) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3377) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3378) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3380) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3383) tree_plugin.h:2902: }
  (<0>,3385) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3386) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3388) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3391) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3392) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3393) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3396) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3398) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3401) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3402) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3405)
  (<0>,3406) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3410)
  (<0>,3411) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3412) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3418) tree.c:3814: rnp = rdp->mynode;
  (<0>,3420) tree.c:3814: rnp = rdp->mynode;
  (<0>,3421) tree.c:3814: rnp = rdp->mynode;
  (<0>,3422) tree.c:3815: mask = rdp->grpmask;
  (<0>,3424) tree.c:3815: mask = rdp->grpmask;
  (<0>,3425) tree.c:3815: mask = rdp->grpmask;
  (<0>,3426) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3429)
  (<0>,3430) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3434) fake_sync.h:115: preempt_disable();
  (<0>,3436) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3437) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3444) tree.c:3817: if (!rdp->beenonline)
  (<0>,3446) tree.c:3817: if (!rdp->beenonline)
  (<0>,3450) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3455) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3456) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3457) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3458) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3460) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3462) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3463) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3465) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3468) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3469) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3470) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3472) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3473) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3478) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3479) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3480) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3481) fake_defs.h:237: switch (size) {
  (<0>,3483) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3485) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3486) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3488) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3491) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3492) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3493) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3495) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3497) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3498) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3500) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3501) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3503) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3504) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3506) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3507) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3509) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3510) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3514) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3515) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3518) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3519) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3521) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3522) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3524) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3530) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3531) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3532) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3534) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3538)
  (<0>,3539)
  (<0>,3540) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3541) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3544) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3547) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3549) fake_sched.h:43: return __running_cpu;
  (<0>,3553) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3555) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3559) fake_sched.h:43: return __running_cpu;
  (<0>,3563) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3573) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3576) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3577) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3578) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3582) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3583) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3584) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3586) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3590) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3591) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3617)
  (<0>,3618)
  (<0>,3619) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3621) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3623) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3625) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3626) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3629)
  (<0>,3630) tree.c:625: return &rsp->node[0];
  (<0>,3634) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3638) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3639) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3640) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3642) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3646)
  (<0>,3647)
  (<0>,3648) fake_sync.h:83: local_irq_save(flags);
  (<0>,3651) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3653) fake_sched.h:43: return __running_cpu;
  (<0>,3657) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3659) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3663) fake_sched.h:43: return __running_cpu;
  (<0>,3667) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3673) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3674) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3681) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3683) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3684) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3686) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3687) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3689) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3690) tree.c:3800: rdp->blimit = blimit;
  (<0>,3691) tree.c:3800: rdp->blimit = blimit;
  (<0>,3693) tree.c:3800: rdp->blimit = blimit;
  (<0>,3694) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3696) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3699) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3702)
  (<0>,3703) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3706) tree_plugin.h:2469: return false;
  (<0>,3709) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3713)
  (<0>,3714) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3716) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3717) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3719) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3722) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3724) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3726) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3729) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3731) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3733) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3735) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3738) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3740) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3742) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3745) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3747) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3749) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3751) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3754) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3756) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3758) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3761) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3763) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3765) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3767) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3770) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3772) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3774) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3777) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3779) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3781) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3783) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3790) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3792) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3794) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3795) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3797) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3800) tree_plugin.h:2902: }
  (<0>,3802) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3803) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3805) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3808) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3809) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3810) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3813) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3815) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3818) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3819) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3822)
  (<0>,3823) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3827)
  (<0>,3828) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3829) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3835) tree.c:3814: rnp = rdp->mynode;
  (<0>,3837) tree.c:3814: rnp = rdp->mynode;
  (<0>,3838) tree.c:3814: rnp = rdp->mynode;
  (<0>,3839) tree.c:3815: mask = rdp->grpmask;
  (<0>,3841) tree.c:3815: mask = rdp->grpmask;
  (<0>,3842) tree.c:3815: mask = rdp->grpmask;
  (<0>,3843) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3846)
  (<0>,3847) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3851) fake_sync.h:115: preempt_disable();
  (<0>,3853) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3854) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3861) tree.c:3817: if (!rdp->beenonline)
  (<0>,3863) tree.c:3817: if (!rdp->beenonline)
  (<0>,3867) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3872) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3873) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3874) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3875) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3877) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3879) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3880) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3882) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3885) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3886) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3887) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3889) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3890) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3895) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3896) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3897) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3898) fake_defs.h:237: switch (size) {
  (<0>,3900) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3902) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3903) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3905) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3908) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3909) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3910) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3912) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3914) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3915) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3917) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3918) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3920) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3921) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3923) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3924) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3926) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3927) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3931) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3932) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3935) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3936) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3938) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3939) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3941) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3947) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3948) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3949) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3951) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3955)
  (<0>,3956)
  (<0>,3957) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3958) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3961) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3964) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3966) fake_sched.h:43: return __running_cpu;
  (<0>,3970) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3972) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3976) fake_sched.h:43: return __running_cpu;
  (<0>,3980) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3990) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3993) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3994) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3995) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3999) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4000) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4001) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4003) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4007) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,4010) tree_plugin.h:1243: }
  (<0>,4012) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,4015) tree_plugin.h:2461: }
  (<0>,4018) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,4036)
  (<0>,4037) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4038) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4039) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4043) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4044) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4045) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4047) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4052) fake_sched.h:43: return __running_cpu;
  (<0>,4055) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4057) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4059) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4060) tree.c:3900: rnp = rdp->mynode;
  (<0>,4062) tree.c:3900: rnp = rdp->mynode;
  (<0>,4063) tree.c:3900: rnp = rdp->mynode;
  (<0>,4064) tree.c:3901: mask = rdp->grpmask;
  (<0>,4066) tree.c:3901: mask = rdp->grpmask;
  (<0>,4067) tree.c:3901: mask = rdp->grpmask;
  (<0>,4071) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4072) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4073) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4075) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4079)
  (<0>,4080)
  (<0>,4081) fake_sync.h:83: local_irq_save(flags);
  (<0>,4084) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4086) fake_sched.h:43: return __running_cpu;
  (<0>,4090) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4092) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4096) fake_sched.h:43: return __running_cpu;
  (<0>,4100) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4106) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4107) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4114) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4115) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4117) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4119) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4120) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4121) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4123) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4125) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4129) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4130) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4131) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4133) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4137)
  (<0>,4138)
  (<0>,4139) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4140) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4143) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4146) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4148) fake_sched.h:43: return __running_cpu;
  (<0>,4152) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4154) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4158) fake_sched.h:43: return __running_cpu;
  (<0>,4162) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4171) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4175) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4180) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4181) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4182) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4184) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4189) fake_sched.h:43: return __running_cpu;
  (<0>,4192) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4194) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4196) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4197) tree.c:3900: rnp = rdp->mynode;
  (<0>,4199) tree.c:3900: rnp = rdp->mynode;
  (<0>,4200) tree.c:3900: rnp = rdp->mynode;
  (<0>,4201) tree.c:3901: mask = rdp->grpmask;
  (<0>,4203) tree.c:3901: mask = rdp->grpmask;
  (<0>,4204) tree.c:3901: mask = rdp->grpmask;
  (<0>,4208) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4209) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4210) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4212) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4216)
  (<0>,4217)
  (<0>,4218) fake_sync.h:83: local_irq_save(flags);
  (<0>,4221) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4223) fake_sched.h:43: return __running_cpu;
  (<0>,4227) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4229) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4233) fake_sched.h:43: return __running_cpu;
  (<0>,4237) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4243) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4244) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4251) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4252) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4254) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4256) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4257) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4258) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4260) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4262) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4266) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4267) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4268) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4270) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4274)
  (<0>,4275)
  (<0>,4276) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4277) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4280) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4283) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4285) fake_sched.h:43: return __running_cpu;
  (<0>,4289) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4291) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4295) fake_sched.h:43: return __running_cpu;
  (<0>,4299) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4308) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4313) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4317) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4319) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4321) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4327) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4328) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4335)
  (<0>,4336)
  (<0>,4337) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4339) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4340) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4343) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4345) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4346) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4349) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,4351) fake_defs.h:639: }
  (<0>,4353) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4354) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4358) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4360) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4363) litmus.c:123: set_cpu(i);
  (<0>,4366)
  (<0>,4367) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,4368) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4370) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4388)
  (<0>,4389) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4390) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4391) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4395) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4396) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4397) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4399) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4404) fake_sched.h:43: return __running_cpu;
  (<0>,4407) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4409) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4411) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4412) tree.c:3900: rnp = rdp->mynode;
  (<0>,4414) tree.c:3900: rnp = rdp->mynode;
  (<0>,4415) tree.c:3900: rnp = rdp->mynode;
  (<0>,4416) tree.c:3901: mask = rdp->grpmask;
  (<0>,4418) tree.c:3901: mask = rdp->grpmask;
  (<0>,4419) tree.c:3901: mask = rdp->grpmask;
  (<0>,4423) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4424) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4425) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4427) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4431)
  (<0>,4432)
  (<0>,4433) fake_sync.h:83: local_irq_save(flags);
  (<0>,4436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4438) fake_sched.h:43: return __running_cpu;
  (<0>,4442) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4448) fake_sched.h:43: return __running_cpu;
  (<0>,4452) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4458) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4459) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4466) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4467) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4469) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4471) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4472) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4473) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4475) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4477) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4481) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4482) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4483) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4485) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4489)
  (<0>,4490)
  (<0>,4491) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4492) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4495) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4498) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4500) fake_sched.h:43: return __running_cpu;
  (<0>,4504) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4506) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4510) fake_sched.h:43: return __running_cpu;
  (<0>,4514) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4523) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4526) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4527) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4528) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4532) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4533) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4534) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4541) fake_sched.h:43: return __running_cpu;
  (<0>,4544) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4546) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4548) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4549) tree.c:3900: rnp = rdp->mynode;
  (<0>,4551) tree.c:3900: rnp = rdp->mynode;
  (<0>,4552) tree.c:3900: rnp = rdp->mynode;
  (<0>,4553) tree.c:3901: mask = rdp->grpmask;
  (<0>,4555) tree.c:3901: mask = rdp->grpmask;
  (<0>,4556) tree.c:3901: mask = rdp->grpmask;
  (<0>,4560) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4561) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4562) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4564) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4568)
  (<0>,4569)
  (<0>,4570) fake_sync.h:83: local_irq_save(flags);
  (<0>,4573) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4575) fake_sched.h:43: return __running_cpu;
  (<0>,4579) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4581) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4585) fake_sched.h:43: return __running_cpu;
  (<0>,4589) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4595) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4596) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4603) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4604) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4606) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4608) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4609) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4610) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4612) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4614) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4618) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4619) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4620) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4622) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4626)
  (<0>,4627)
  (<0>,4628) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4629) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4632) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4635) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4637) fake_sched.h:43: return __running_cpu;
  (<0>,4641) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4643) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4647) fake_sched.h:43: return __running_cpu;
  (<0>,4651) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4660) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4663) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4664) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4665) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4669) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4670) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4671) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4673) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4680) tree.c:753: unsigned long flags;
  (<0>,4683) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4685) fake_sched.h:43: return __running_cpu;
  (<0>,4689) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4691) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4695) fake_sched.h:43: return __running_cpu;
  (<0>,4699) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4711) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4713) fake_sched.h:43: return __running_cpu;
  (<0>,4717) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4718) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4720) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4721) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4722) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4723) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4724) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4725) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4726) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4730) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4732) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4733) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4734) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4750)
  (<0>,4752) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4754) fake_sched.h:43: return __running_cpu;
  (<0>,4758) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4761) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4762) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4763) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4767) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4768) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4769) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4771) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4779) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4781) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4783) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4784) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4787) tree_plugin.h:2457: }
  (<0>,4790) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4793) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4794) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4795) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4799) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4800) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4801) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4803) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4808) fake_sched.h:43: return __running_cpu;
  (<0>,4811) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4813) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4815) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4816) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4819) tree_plugin.h:2457: }
  (<0>,4822) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4825) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4826) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4827) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4831) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4832) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4833) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4835) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4842) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4845) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4846) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4847) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4849) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4850) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4852) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4853) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4854) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4855) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4869) tree_plugin.h:2879: }
  (<0>,4871) tree.c:758: local_irq_restore(flags);
  (<0>,4874) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4876) fake_sched.h:43: return __running_cpu;
  (<0>,4880) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4882) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4886) fake_sched.h:43: return __running_cpu;
  (<0>,4890) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4897) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4899) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4901) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4904) litmus.c:123: set_cpu(i);
  (<0>,4907)
  (<0>,4908) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4909) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4911) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4929)
  (<0>,4930) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4931) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4932) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4936) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4937) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4938) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4940) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4948) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4950) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4952) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4953) tree.c:3900: rnp = rdp->mynode;
  (<0>,4955) tree.c:3900: rnp = rdp->mynode;
  (<0>,4956) tree.c:3900: rnp = rdp->mynode;
  (<0>,4957) tree.c:3901: mask = rdp->grpmask;
  (<0>,4959) tree.c:3901: mask = rdp->grpmask;
  (<0>,4960) tree.c:3901: mask = rdp->grpmask;
  (<0>,4964) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4965) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4966) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4968) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4972)
  (<0>,4973)
  (<0>,4974) fake_sync.h:83: local_irq_save(flags);
  (<0>,4977) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:43: return __running_cpu;
  (<0>,4983) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4985) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4989) fake_sched.h:43: return __running_cpu;
  (<0>,4993) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4999) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5000) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5007) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5008) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5010) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5012) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5013) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5014) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5016) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5018) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5022) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5023) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5024) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5026) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5030)
  (<0>,5031)
  (<0>,5032) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5033) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5036) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5039) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5041) fake_sched.h:43: return __running_cpu;
  (<0>,5045) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5047) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5051) fake_sched.h:43: return __running_cpu;
  (<0>,5055) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5064) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5067) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5068) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5069) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5073) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5074) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5075) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5077) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5082) fake_sched.h:43: return __running_cpu;
  (<0>,5085) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5087) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5089) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5090) tree.c:3900: rnp = rdp->mynode;
  (<0>,5092) tree.c:3900: rnp = rdp->mynode;
  (<0>,5093) tree.c:3900: rnp = rdp->mynode;
  (<0>,5094) tree.c:3901: mask = rdp->grpmask;
  (<0>,5096) tree.c:3901: mask = rdp->grpmask;
  (<0>,5097) tree.c:3901: mask = rdp->grpmask;
  (<0>,5101) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5102) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5103) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5105) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5109)
  (<0>,5110)
  (<0>,5111) fake_sync.h:83: local_irq_save(flags);
  (<0>,5114) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5116) fake_sched.h:43: return __running_cpu;
  (<0>,5120) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5122) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5126) fake_sched.h:43: return __running_cpu;
  (<0>,5130) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5136) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5137) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5144) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5145) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5147) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5149) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5150) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5151) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5153) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5155) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5159) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5160) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5161) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5163) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5167)
  (<0>,5168)
  (<0>,5169) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5170) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5173) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5176) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5178) fake_sched.h:43: return __running_cpu;
  (<0>,5182) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5184) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5188) fake_sched.h:43: return __running_cpu;
  (<0>,5192) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5201) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5204) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5205) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5206) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5210) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5211) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5212) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5214) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5221) tree.c:755: local_irq_save(flags);
  (<0>,5224) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5226) fake_sched.h:43: return __running_cpu;
  (<0>,5230) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5232) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5236) fake_sched.h:43: return __running_cpu;
  (<0>,5240) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5252) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5254) fake_sched.h:43: return __running_cpu;
  (<0>,5258) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5259) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5261) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5262) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5263) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5264) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5265) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5266) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5267) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,5271) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5273) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5274) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5275) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5291)
  (<0>,5293) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5295) fake_sched.h:43: return __running_cpu;
  (<0>,5299) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5302) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5303) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5304) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5308) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5309) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5310) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5312) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5317) fake_sched.h:43: return __running_cpu;
  (<0>,5320) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5322) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5324) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5325) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5328) tree_plugin.h:2457: }
  (<0>,5331) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5334) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5335) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5336) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5340) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5341) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5342) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5344) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5349) fake_sched.h:43: return __running_cpu;
  (<0>,5352) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5354) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5356) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5357) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5360) tree_plugin.h:2457: }
  (<0>,5363) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5366) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5367) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5368) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5372) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5373) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5374) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5376) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5383) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5386) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5387) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5388) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5390) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5391) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5393) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5394) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5395) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5396) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5410) tree_plugin.h:2879: }
  (<0>,5412) tree.c:758: local_irq_restore(flags);
  (<0>,5415) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5417) fake_sched.h:43: return __running_cpu;
  (<0>,5421) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5423) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5427) fake_sched.h:43: return __running_cpu;
  (<0>,5431) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5438) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5440) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5442) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5462) tree.c:3971: unsigned long flags;
  (<0>,5463) tree.c:3972: int kthread_prio_in = kthread_prio;
  (<0>,5464) tree.c:3973: struct rcu_node *rnp;
  (<0>,5467) tree.c:3983: else if (kthread_prio > 99)
  (<0>,5471) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5472) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5475) tree.c:3989: rcu_scheduler_fully_active = 1;
  (<0>,5476) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5477) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5478) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5482) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5483) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5484) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5486) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5490) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5492) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5494) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5503)
  (<0>,5504) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5505) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5506) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5507) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5508) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5511)
  (<0>,5512)
  (<0>,5519)
  (<0>,5520)
  (<0>,5527)
  (<0>,5528)
  (<0>,5535)
  (<0>,5536)
  (<0>,5543)
  (<0>,5544)
  (<0>,5551)
  (<0>,5552)
  (<0>,5559)
  (<0>,5560)
  (<0>,5567)
  (<0>,5568)
  (<0>,5575)
  (<0>,5576)
  (<0>,5583)
  (<0>,5584) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5593) fake_defs.h:868: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,5599) fake_defs.h:870: task = malloc(sizeof(*task));
  (<0>,5600) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5602) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5604) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5605) fake_defs.h:872: task->tid = t;
  (<0>,5606) fake_defs.h:872: task->tid = t;
  (<0>,5608) fake_defs.h:872: task->tid = t;
  (<0>,5609) fake_defs.h:873: return task;
  (<0>,5610) fake_defs.h:873: return task;
  (<0>,5612) fake_defs.h:876: }
  (<0>,5614) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5615) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5618)
  (<0>,5619) tree.c:625: return &rsp->node[0];
  (<0>,5623) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5627) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5628) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5629) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5631) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5635)
  (<0>,5636)
  (<0>,5637) fake_sync.h:83: local_irq_save(flags);
  (<0>,5640) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5642) fake_sched.h:43: return __running_cpu;
  (<0>,5646) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5648) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5652) fake_sched.h:43: return __running_cpu;
  (<0>,5656) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5662) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5663) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5670) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5671) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5673) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5674) tree.c:3996: if (kthread_prio) {
  (<0>,5680) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5681) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5682) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5684) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5688)
  (<0>,5689)
  (<0>,5690) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5691) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5694) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5697) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5699) fake_sched.h:43: return __running_cpu;
  (<0>,5703) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5705) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5709) fake_sched.h:43: return __running_cpu;
  (<0>,5713) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5724) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5727) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5728) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5729) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5733) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5734) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5735) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5737) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5741) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5743) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5745) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5754)
  (<0>,5755)
  (<0>,5756)
  (<0>,5757)
  (<0>,5758) fake_defs.h:865: struct task_struct *task = NULL;
  (<0>,5759) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5762)
  (<0>,5763)
  (<0>,5770)
  (<0>,5771)
  (<0>,5778)
  (<0>,5779)
  (<0>,5786)
  (<0>,5787)
  (<0>,5794)
  (<0>,5795) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5808) fake_defs.h:875: return NULL;
  (<0>,5810) fake_defs.h:876: }
  (<0>,5812) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5813) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5816)
  (<0>,5817) tree.c:625: return &rsp->node[0];
  (<0>,5821) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5825) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5826) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5827) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5829) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5833)
  (<0>,5834)
  (<0>,5835) fake_sync.h:83: local_irq_save(flags);
  (<0>,5838) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5840) fake_sched.h:43: return __running_cpu;
  (<0>,5844) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5846) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5850) fake_sched.h:43: return __running_cpu;
  (<0>,5854) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5860) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5861) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5868) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5869) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5871) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5872) tree.c:3996: if (kthread_prio) {
  (<0>,5878) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5879) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5880) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5882) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5886)
  (<0>,5887)
  (<0>,5888) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5889) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5892) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5895) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5897) fake_sched.h:43: return __running_cpu;
  (<0>,5901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5903) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5907) fake_sched.h:43: return __running_cpu;
  (<0>,5911) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5922) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5925) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5926) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5927) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5931) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5932) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5933) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5935) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5949)
  (<0>,5950) litmus.c:51: void *thread_reader(void *arg)
  (<0>,5953)
  (<0>,5954) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5955) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5958) fake_sched.h:43: return __running_cpu;
  (<0>,5962)
  (<0>,5963) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,5966) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,5971) tree.c:890: unsigned long flags;
  (<0>,5974) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5976) fake_sched.h:43: return __running_cpu;
  (<0>,5980) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5982) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5986) fake_sched.h:43: return __running_cpu;
  (<0>,5990) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6002) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6004) fake_sched.h:43: return __running_cpu;
  (<0>,6008) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6009) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6011) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6012) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6013) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6014) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6015) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6016) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6017) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,6021) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6023) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6024) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6025) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6036)
  (<0>,6037) tree.c:830: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,6039) fake_sched.h:43: return __running_cpu;
  (<0>,6043) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6047) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6050) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6051) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6052) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6054) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6055) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6057) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6058) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6059) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6060) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6070) tree_plugin.h:2883: }
  (<0>,6072) tree.c:895: local_irq_restore(flags);
  (<0>,6075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6077) fake_sched.h:43: return __running_cpu;
  (<0>,6081) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6083) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6087) fake_sched.h:43: return __running_cpu;
  (<0>,6091) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6106) litmus.c:57: r_x = x;
  (<0>,6107) litmus.c:57: r_x = x;
  (<0>,6111) fake_sched.h:43: return __running_cpu;
  (<0>,6115) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
  (<0>,6119) fake_sched.h:43: return __running_cpu;
  (<0>,6123) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6128) fake_sched.h:43: return __running_cpu;
  (<0>,6132) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
  (<0>,6143) fake_sched.h:43: return __running_cpu;
  (<0>,6147) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6148) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6150) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6151) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6152) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6154) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6156) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6157) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6158) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6159) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6160) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6161) tree.c:942: if (oldval)
  (<0>,6169) tree_plugin.h:2883: }
  (<0>,6175) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,6184) tree_plugin.h:1669: struct rcu_state *rsp;
  (<0>,6185) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6186) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6190) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6191) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6192) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6194) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6199) fake_sched.h:43: return __running_cpu;
  (<0>,6202) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6204) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6207) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6209) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6211) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6214) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6215) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6216) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6220) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6221) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6222) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6224) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6229) fake_sched.h:43: return __running_cpu;
  (<0>,6232) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6234) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6237) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6239) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6241) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6244) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6245) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6246) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6250) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6251) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6252) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6254) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6259) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,6264) fake_sched.h:43: return __running_cpu;
  (<0>,6269) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,6277) fake_sched.h:43: return __running_cpu;
  (<0>,6283) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
  (<0>,6289) fake_sched.h:43: return __running_cpu;
  (<0>,6296) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
  (<0>,6309) tree.c:3557: struct rcu_state *rsp;
  (<0>,6310) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6311) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6315) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6316) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6317) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6319) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6323) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6325) fake_sched.h:43: return __running_cpu;
  (<0>,6328) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6330) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6352)
  (<0>,6353) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6354) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6356) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6357) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6358) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6360) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6362) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6363) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6364) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6399)
  (<0>,6400) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6401) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6404) tree.c:1469: !rcu_gp_in_progress(rsp))
  (<0>,6417)
  (<0>,6418) tree.c:237: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,6423) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6424) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6425) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6426) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6428) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6430) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6431) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6433) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6436) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6437) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6438) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6439) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6444) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6445) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6446) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6447) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6449) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6451) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6452) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6454) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6457) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6458) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6459) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6467) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
  (<0>,6470) tree_plugin.h:2923: return false;
  (<0>,6473) tree.c:3503: if (rcu_scheduler_fully_active &&
  (<0>,6476) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6478) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6481) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6483) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6487) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,6490)
  (<0>,6491) tree.c:614: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,6493) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6496) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6503) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6504) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6515)
  (<0>,6516) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6517) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6530)
  (<0>,6531) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6536) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6537) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6538) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6539) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6541) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6543) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6544) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6546) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6549) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6550) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6551) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6552) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6557) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6560) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6562) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6564) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6565) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6567) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6570) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6571) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6572) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6578) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,6594)
  (<0>,6595) tree.c:633: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,6598)
  (<0>,6599) tree.c:625: return &rsp->node[0];
  (<0>,6603) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6604) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6609) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6610) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6611) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6612) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6614) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6616) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6617) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6619) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6622) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6623) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6624) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6628) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6629) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6631) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6634) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6635) tree.c:639: return READ_ONCE(*fp);
  (<0>,6639) tree.c:639: return READ_ONCE(*fp);
  (<0>,6640) tree.c:639: return READ_ONCE(*fp);
  (<0>,6641) tree.c:639: return READ_ONCE(*fp);
  (<0>,6642) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6644) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6646) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6647) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6649) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6652) tree.c:639: return READ_ONCE(*fp);
  (<0>,6653) tree.c:639: return READ_ONCE(*fp);
  (<0>,6654) tree.c:639: return READ_ONCE(*fp);
  (<0>,6658) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6661) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6664) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6667) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6668) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6671) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6673) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6676) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6679) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6682) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6683) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6685) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6688) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6692) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6694) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6696) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6699) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6702) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6705) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6706) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6708) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6711) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6715) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6717) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6719) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6722) tree.c:665: return false; /* No grace period needed. */
  (<0>,6724) tree.c:666: }
  (<0>,6727) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6732) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6733) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6734) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6735) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6737) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6739) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6740) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6742) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6745) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6746) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6747) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6748) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6750) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6753) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6758) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6759) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6760) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6761) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6763) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6765) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6766) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6768) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6771) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6772) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6773) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6774) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6776) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6779) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,6783) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,6784) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,6785) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,6786) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6788) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6789) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6790) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6791) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6794) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,6797) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,6798) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,6806) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
  (<0>,6809) tree_plugin.h:2452: return false;
  (<0>,6813) tree.c:3546: rdp->n_rp_need_nothing++;
  (<0>,6815) tree.c:3546: rdp->n_rp_need_nothing++;
  (<0>,6817) tree.c:3546: rdp->n_rp_need_nothing++;
  (<0>,6818) tree.c:3547: return 0;
  (<0>,6820) tree.c:3548: }
  (<0>,6825) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6828) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6829) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6830) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6834) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6835) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6836) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6838) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6842) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6844) fake_sched.h:43: return __running_cpu;
  (<0>,6847) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6849) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6871)
  (<0>,6872)
  (<0>,6873) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6875) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6876) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6877) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6879) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6881) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6882) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6883) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6918)
  (<0>,6919)
  (<0>,6920) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
  (<0>,6923) tree.c:1469: !rcu_gp_in_progress(rsp))
  (<0>,6936)
  (<0>,6937) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6942) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6943) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6944) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6945) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6947) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6949) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6950) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6952) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6955) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6956) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6957) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6958) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6963) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6964) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6965) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6966) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6968) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6970) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6971) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6973) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6976) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6977) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6978) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6986) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
  (<0>,6989) tree_plugin.h:2923: return false;
  (<0>,6992) tree.c:3503: if (rcu_scheduler_fully_active &&
  (<0>,6995) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6997) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,7000) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,7002) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,7006) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,7009)
  (<0>,7010) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7012) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7015) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7022) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7023) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7034)
  (<0>,7035)
  (<0>,7036) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7049)
  (<0>,7050) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7055) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7056) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7057) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7058) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7060) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7062) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7063) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7065) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7068) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7069) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7070) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7071) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7076) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7077) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7078) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7079) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7081) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7083) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7084) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7086) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7089) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7090) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7091) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7097) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,7113)
  (<0>,7114) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7117)
  (<0>,7118) tree.c:625: return &rsp->node[0];
  (<0>,7122) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7123) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7128) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7129) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7130) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7131) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7133) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7135) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7136) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7138) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7141) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7142) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7143) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7147) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7148) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7150) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7153) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7154) tree.c:639: return READ_ONCE(*fp);
  (<0>,7158) tree.c:639: return READ_ONCE(*fp);
  (<0>,7159) tree.c:639: return READ_ONCE(*fp);
  (<0>,7160) tree.c:639: return READ_ONCE(*fp);
  (<0>,7161) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7163) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7165) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7166) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7168) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7171) tree.c:639: return READ_ONCE(*fp);
  (<0>,7172) tree.c:639: return READ_ONCE(*fp);
  (<0>,7173) tree.c:639: return READ_ONCE(*fp);
  (<0>,7177) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7180) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7183) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7186) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7187) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7190) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7192) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7195) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7198) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7201) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7202) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7204) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7207) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7211) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7213) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7215) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7218) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7221) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7224) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7225) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7227) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7230) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7234) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7236) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7238) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7241) tree.c:665: return false; /* No grace period needed. */
  (<0>,7243) tree.c:666: }
  (<0>,7246) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7251) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7252) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7253) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7254) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7256) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7258) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7259) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7261) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7264) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7265) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7266) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7267) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7269) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7272) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,7277) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,7278) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,7279) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,7280) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7282) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7284) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7285) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7287) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7290) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,7291) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,7292) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,7293) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,7295) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,7298) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,7302) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,7303) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,7304) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,7305) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7307) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7308) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7309) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7310) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7313) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,7316) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,7317) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
  (<0>,7325) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
  (<0>,7328) tree_plugin.h:2452: return false;
  (<0>,7332) tree.c:3546: rdp->n_rp_need_nothing++;
  (<0>,7334) tree.c:3546: rdp->n_rp_need_nothing++;
  (<0>,7336) tree.c:3546: rdp->n_rp_need_nothing++;
  (<0>,7337) tree.c:3547: return 0;
  (<0>,7339) tree.c:3548: }
  (<0>,7344) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,7347) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,7348) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,7349) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,7353) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,7354) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,7355) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,7357) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,7361) tree.c:3562: return 0;
  (<0>,7363) tree.c:3563: }
  (<0>,7367) tree.c:2891: if (user)
  (<0>,7375) fake_sched.h:43: return __running_cpu;
  (<0>,7379) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
  (<0>,7381) fake_sched.h:43: return __running_cpu;
  (<0>,7385) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7391) fake_sched.h:43: return __running_cpu;
  (<0>,7395) fake_sched.h:183: if (need_softirq[get_cpu()]) {
  (<0>,7406) fake_sched.h:43: return __running_cpu;
  (<0>,7410) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,7411) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,7413) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,7414) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,7415) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,7417) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,7419) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,7420) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7421) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7422) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7423) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7424) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,7426) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,7434) tree_plugin.h:2879: }
      (<0.1>,1)
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:892: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,55) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,57) fake_sched.h:43: return __running_cpu;
      (<0.1>,61) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,62) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,64) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,67) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,68) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,69) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,70) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,74) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,76) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,77) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,78) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,89)
      (<0.1>,90) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,92) fake_sched.h:43: return __running_cpu;
      (<0.1>,96) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,100) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,103) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,104) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,105) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,107) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,110) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,111) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,112) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,113) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,123) tree_plugin.h:2883: }
      (<0.1>,125) tree.c:895: local_irq_restore(flags);
      (<0.1>,128) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,130) fake_sched.h:43: return __running_cpu;
      (<0.1>,134) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,140) fake_sched.h:43: return __running_cpu;
      (<0.1>,144) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,151) litmus.c:87: x = 1;
      (<0.1>,163) tree.c:3255: ret = num_online_cpus() <= 1;
      (<0.1>,165) tree.c:3257: return ret;
      (<0.1>,172) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,175) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,176) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,177) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,178) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,181) update.c:148: rcu_scheduler_active == RCU_SCHEDULER_INIT;
      (<0.1>,198)
      (<0.1>,199)
      (<0.1>,200)
      (<0.1>,201)
      (<0.1>,202) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,204) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,205) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,208) update.c:352: if (checktiny &&
      (<0.1>,211) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,213) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,218) rcupdate.h:476: }
      (<0.1>,220) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,222) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,227)
      (<0.1>,228) fake_sync.h:272: x->done = 0;
      (<0.1>,230) fake_sync.h:272: x->done = 0;
      (<0.1>,234) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,236) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,238) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,239) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,241) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,247)
      (<0.1>,248)
      (<0.1>,249) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,250) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,281)
      (<0.1>,282)
      (<0.1>,283)
      (<0.1>,284)
      (<0.1>,286)
      (<0.1>,287) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,294) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,295) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,301) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,302) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,303) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,304) tree.c:3147: if (debug_rcu_head_queue(head)) {
      (<0.1>,307) rcu.h:92: return 0;
      (<0.1>,311) tree.c:3153: head->func = func;
      (<0.1>,312) tree.c:3153: head->func = func;
      (<0.1>,315) tree.c:3153: head->func = func;
      (<0.1>,316) tree.c:3154: head->next = NULL;
      (<0.1>,318) tree.c:3154: head->next = NULL;
      (<0.1>,319) tree.c:3162: local_irq_save(flags);
      (<0.1>,322) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,324) fake_sched.h:43: return __running_cpu;
      (<0.1>,328) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,330) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,334) fake_sched.h:43: return __running_cpu;
      (<0.1>,338) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,344) fake_sched.h:43: return __running_cpu;
      (<0.1>,347) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,349) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,351) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,352) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,355) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,363) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,367) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,369) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,371) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,372) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,377) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,378) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,379) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,380) fake_defs.h:237: switch (size) {
      (<0.1>,382) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,384) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,385) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,387) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,390) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,391) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,392) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,393) tree.c:3189: if (lazy)
      (<0.1>,400) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,401) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,404) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,405) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,406) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,408) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,411) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,412) tree.c:3197: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,419) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,420) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,421) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,422) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,430)
      (<0.1>,431)
      (<0.1>,432)
      (<0.1>,433) tree.c:3086: if (!rcu_is_watching())
      (<0.1>,440) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,442) fake_sched.h:43: return __running_cpu;
      (<0.1>,448) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,449) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,450) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,455) tree.c:1060: ret = __rcu_is_watching();
      (<0.1>,457) tree.c:1062: return ret;
      (<0.1>,461) tree.c:3090: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,464) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,466) fake_sched.h:43: return __running_cpu;
      (<0.1>,470) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,480) tree.c:3205: local_irq_restore(flags);
      (<0.1>,483) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,485) fake_sched.h:43: return __running_cpu;
      (<0.1>,489) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,491) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,495) fake_sched.h:43: return __running_cpu;
      (<0.1>,499) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,508) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,510) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,512) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,513) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,516) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,518) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,519) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,522) update.c:365: if (checktiny &&
      (<0.1>,525) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,527) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,532) fake_sync.h:278: might_sleep();
      (<0.1>,536) fake_sched.h:43: return __running_cpu;
      (<0.1>,540) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,543) tree.c:755: local_irq_save(flags);
      (<0.1>,546) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,548) fake_sched.h:43: return __running_cpu;
      (<0.1>,552) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,554) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,558) fake_sched.h:43: return __running_cpu;
      (<0.1>,562) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,574) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,576) fake_sched.h:43: return __running_cpu;
      (<0.1>,580) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,581) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,583) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,584) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,585) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,586) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,587) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,588) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,589) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,593) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,595) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,596) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,597) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,613)
      (<0.1>,615) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,617) fake_sched.h:43: return __running_cpu;
      (<0.1>,621) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,624) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,625) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,626) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,630) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,631) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,632) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,639) fake_sched.h:43: return __running_cpu;
      (<0.1>,642) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,644) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,646) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,647) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,650) tree_plugin.h:2457: }
      (<0.1>,653) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,656) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,657) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,658) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,662) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,663) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,664) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,666) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,671) fake_sched.h:43: return __running_cpu;
      (<0.1>,674) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,676) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,678) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,679) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,682) tree_plugin.h:2457: }
      (<0.1>,685) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,688) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,689) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,690) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,694) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,695) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,696) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,698) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,705) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,708) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,709) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,710) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,712) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,713) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,715) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,716) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,717) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,718) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,732) tree_plugin.h:2879: }
      (<0.1>,734) tree.c:758: local_irq_restore(flags);
      (<0.1>,737) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,739) fake_sched.h:43: return __running_cpu;
      (<0.1>,743) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,745) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,749) fake_sched.h:43: return __running_cpu;
      (<0.1>,753) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,759) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,762) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,767) fake_sync.h:281: while (!x->done)
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:892: local_irq_save(flags);
    (<0.0>,35)
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,63)
    (<0.0>,65) fake_sched.h:43: return __running_cpu;
    (<0.0>,69) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,70) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,72) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,75) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,76) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,77) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,78) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,82) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,84) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,85) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,86) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,97)
    (<0.0>,98)
    (<0.0>,100) fake_sched.h:43: return __running_cpu;
    (<0.0>,104) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,111) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,112) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,113) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,115) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,116) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,118) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,119) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,120) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,121) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,131)
    (<0.0>,133) tree.c:895: local_irq_restore(flags);
    (<0.0>,136)
    (<0.0>,138) fake_sched.h:43: return __running_cpu;
    (<0.0>,142) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,144) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,148) fake_sched.h:43: return __running_cpu;
    (<0.0>,152) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,159) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,207)
    (<0.0>,208) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,211) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,214)
    (<0.0>,215) tree.c:625: return &rsp->node[0];
    (<0.0>,219) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,227) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,233) fake_sched.h:43: return __running_cpu;
    (<0.0>,237) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,241) fake_sched.h:43: return __running_cpu;
    (<0.0>,245) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,250) fake_sched.h:43: return __running_cpu;
    (<0.0>,254) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,270) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,272) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,273) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,274) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,276) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,278) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,279) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,280) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,281) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,282) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,283) tree.c:942: if (oldval)
    (<0.0>,291)
    (<0.0>,297)
    (<0.0>,306) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,307) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,308) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,312) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,313) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,314) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,316) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,321) fake_sched.h:43: return __running_cpu;
    (<0.0>,324) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,326) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,329) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,331) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,333) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,336) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,337) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,338) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,342) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,343) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,344) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,346) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,351) fake_sched.h:43: return __running_cpu;
    (<0.0>,354) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,356) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,359) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,361) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,363) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,366) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,367) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,368) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,372) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,373) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,374) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,376) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,381) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,386) fake_sched.h:43: return __running_cpu;
    (<0.0>,391) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,399) fake_sched.h:43: return __running_cpu;
    (<0.0>,405) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,411) fake_sched.h:43: return __running_cpu;
    (<0.0>,418) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,431) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,432) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,433) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,437) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,438) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,439) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,441) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,445) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,447) fake_sched.h:43: return __running_cpu;
    (<0.0>,450) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,452) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,474)
    (<0.0>,475)
    (<0.0>,476) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,478) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,479) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,480) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,482) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,484) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,485) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,486) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,521)
    (<0.0>,522)
    (<0.0>,523) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,526) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,539)
    (<0.0>,540) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,545) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,546) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,547) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,548) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,550) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,552) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,553) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,555) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,560) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,561) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,566) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,567) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,569) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,571) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,573) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,574) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,576) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,579) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,580) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,581) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,589) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,592)
    (<0.0>,595) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,598) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,600) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,603) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,605) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,609) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,612)
    (<0.0>,613) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,615) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,618) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,625) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,626) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,637)
    (<0.0>,638)
    (<0.0>,639) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,652)
    (<0.0>,653) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,658) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,659) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,660) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,661) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,663) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,665) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,666) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,668) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,671) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,672) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,673) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,674) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,679) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,680) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,681) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,682) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,684) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,686) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,692) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,693) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,694) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,700) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,716)
    (<0.0>,717) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,720)
    (<0.0>,721) tree.c:625: return &rsp->node[0];
    (<0.0>,725) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,726) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,731) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,732) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,733) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,734) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,736) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,739) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,744) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,745) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,746) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,750) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,751) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,753) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,756) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,757) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,761) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,762) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,763) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,774) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,775) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,776) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,780) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,783) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,786) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,789) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,790) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,793) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,795) tree.c:666: }
    (<0.0>,798) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,800) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,802) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,803) tree.c:3523: return 1;
    (<0.0>,805) tree.c:3548: }
    (<0.0>,809) tree.c:3561: return 1;
    (<0.0>,811) tree.c:3563: }
    (<0.0>,818) fake_sched.h:43: return __running_cpu;
    (<0.0>,822) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,826) tree.c:2891: if (user)
    (<0.0>,834) fake_sched.h:43: return __running_cpu;
    (<0.0>,838) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,840) fake_sched.h:43: return __running_cpu;
    (<0.0>,844) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,850) fake_sched.h:43: return __running_cpu;
    (<0.0>,854) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,864)
    (<0.0>,867) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,868) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,869) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,873) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,874) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,875) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,877) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,881) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,893)
    (<0.0>,895) fake_sched.h:43: return __running_cpu;
    (<0.0>,898) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,900) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,902) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,903) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,905) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,912) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,913) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,915) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,921) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,922) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,923) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,924) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,925) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,929)
    (<0.0>,930)
    (<0.0>,931) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,932) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,957)
    (<0.0>,958)
    (<0.0>,959) tree.c:1905: local_irq_save(flags);
    (<0.0>,962)
    (<0.0>,964) fake_sched.h:43: return __running_cpu;
    (<0.0>,968) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,970) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,974) fake_sched.h:43: return __running_cpu;
    (<0.0>,978) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,983) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,985) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,986) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,987) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,989) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,990) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,995) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,996) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,997) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,998) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1000) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1002) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1003) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1005) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1008) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1009) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1010) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1013) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1015) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1016) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1021) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1022) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1023) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1024) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1026) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1028) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1029) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1031) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1034) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1035) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1036) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1039) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1043) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1044) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1045) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1046) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1048) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1050) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1054) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1057) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1058) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1066) tree.c:1911: local_irq_restore(flags);
    (<0.0>,1069)
    (<0.0>,1071) fake_sched.h:43: return __running_cpu;
    (<0.0>,1075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1077) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1081) fake_sched.h:43: return __running_cpu;
    (<0.0>,1085) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1092) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1094) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1099) tree.c:3016: local_irq_save(flags);
    (<0.0>,1102)
    (<0.0>,1104) fake_sched.h:43: return __running_cpu;
    (<0.0>,1108) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1110) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1114) fake_sched.h:43: return __running_cpu;
    (<0.0>,1118) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1123) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1124) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1135)
    (<0.0>,1136)
    (<0.0>,1137) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,1150)
    (<0.0>,1151) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1156) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1157) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1158) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1161) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1163) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1164) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1166) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1169) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1170) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1171) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1172) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1177) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1178) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1179) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1180) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1182) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1184) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1185) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1187) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1190) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1191) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1192) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1198) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,1214)
    (<0.0>,1215) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1218)
    (<0.0>,1219) tree.c:625: return &rsp->node[0];
    (<0.0>,1223) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1224) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1229) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1230) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1231) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1232) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1234) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1236) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1237) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1239) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1242) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1243) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1244) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1248) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1249) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1251) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1254) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1255) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1259) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1260) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1261) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1262) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1264) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1266) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1267) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1269) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1272) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1273) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1274) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1278) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1281) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1284) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1287) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1288) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1291) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,1293) tree.c:666: }
    (<0.0>,1296) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,1299)
    (<0.0>,1300) tree.c:625: return &rsp->node[0];
    (<0.0>,1306)
    (<0.0>,1307) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,1311)
    (<0.0>,1313) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1314) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1321) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,1327)
    (<0.0>,1329) fake_sched.h:43: return __running_cpu;
    (<0.0>,1332) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1334) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1336) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1337) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1340)
    (<0.0>,1341) tree.c:625: return &rsp->node[0];
    (<0.0>,1345) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1346) tree.c:2334: bool ret = false;
    (<0.0>,1347) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1348) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1349) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1357)
    (<0.0>,1358)
    (<0.0>,1359)
    (<0.0>,1360) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1363) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1366) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1369) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1370) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1373) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1375) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1378) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1380) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1381) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1383) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1386) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1391) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1393) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1394) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1397) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1399) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1402) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1404) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1407) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1408) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1411) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1414) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1416) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1419) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1420) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1422) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1425) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1426) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1428) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1431) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1432) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1434) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1437) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1439) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1441) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1442) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1444) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1446) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1449) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1451) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1454) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1455) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1458) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1461) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1463) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1466) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1467) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1469) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1472) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1473) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1475) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1478) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1479) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1481) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1484) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1486) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1488) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1489) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1491) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1493) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1496) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1497) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1498) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1507)
    (<0.0>,1508)
    (<0.0>,1509)
    (<0.0>,1510) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1513) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1516) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1519) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1520) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1523) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1524) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1529)
    (<0.0>,1530)
    (<0.0>,1531) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1534)
    (<0.0>,1535) tree.c:625: return &rsp->node[0];
    (<0.0>,1539) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1542) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1544) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1545) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1547) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1550) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1552) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1554) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1556) tree.c:1585: }
    (<0.0>,1558) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1559) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1561) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1564) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1566) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1569) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1570) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1573) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1576) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1580) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1582) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1584) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1587) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1589) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1592) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1593) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1596) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1599) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1603) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1605) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1607) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1610) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1612) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1616) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1619) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1622) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1623) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1625) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1628) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1629) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1630) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1632) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1635) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1637) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1639) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1641) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1644) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1647) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1648) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1650) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1653) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1654) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1655) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1657) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1660) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1662) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1664) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1666) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1669) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1672) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1673) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1675) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1678) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1679) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1680) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1682) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1685) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1687) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1689) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1691) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1694) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1695) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1714)
    (<0.0>,1715)
    (<0.0>,1716)
    (<0.0>,1717) tree.c:1613: bool ret = false;
    (<0.0>,1718) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1720) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1723)
    (<0.0>,1724) tree.c:625: return &rsp->node[0];
    (<0.0>,1728) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1729) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1731) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1732) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1737)
    (<0.0>,1738)
    (<0.0>,1739) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1742)
    (<0.0>,1743) tree.c:625: return &rsp->node[0];
    (<0.0>,1747) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1750) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1752) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1753) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1755) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1758) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1760) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1762) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1764) tree.c:1585: }
    (<0.0>,1766) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1767) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1768) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1769) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1775)
    (<0.0>,1776)
    (<0.0>,1777)
    (<0.0>,1778)
    (<0.0>,1782) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1784) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1787) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1790) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1792) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1793) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1795) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1798) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1803) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1804) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1805) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1806) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1808) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1810) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1811) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1813) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1816) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1817) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1818) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1819) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1824) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1825) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1826) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1834) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1837) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1838) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1839) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1842) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1843) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1846) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1848) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1849) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1854)
    (<0.0>,1855)
    (<0.0>,1856) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1859)
    (<0.0>,1860) tree.c:625: return &rsp->node[0];
    (<0.0>,1864) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1867) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1869) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1870) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1872) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1875) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1877) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1879) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1881) tree.c:1585: }
    (<0.0>,1883) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1884) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1886) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1889) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1890) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1892) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1895) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1899) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1900) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1902) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1905) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1908) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1910) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1912) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1915) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1916) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1918) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1921) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1926) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1928) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1930) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1933) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1934) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1936) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1939) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1944) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1946) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1948) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1951) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1953) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1956) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1959) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1961) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1964) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1966) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1967) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1969) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1970) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1972) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1975) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1976) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1977) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1983)
    (<0.0>,1984)
    (<0.0>,1985)
    (<0.0>,1986)
    (<0.0>,1990) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1992) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1993) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1994) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2005)
    (<0.0>,2006)
    (<0.0>,2007)
    (<0.0>,2008) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2010) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2013) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2014) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2025)
    (<0.0>,2026)
    (<0.0>,2027) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2040)
    (<0.0>,2041) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2046) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2047) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2048) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2053) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2054) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2056) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2059) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2060) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2061) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2062) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2067) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2068) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2069) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2070) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2072) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2074) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2075) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2077) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2080) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2081) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2082) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2088) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2104)
    (<0.0>,2105) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2108)
    (<0.0>,2109) tree.c:625: return &rsp->node[0];
    (<0.0>,2113) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2114) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2119) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2120) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2121) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2122) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2124) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2126) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2127) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2129) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2132) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2133) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2134) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2138) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2139) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2141) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2144) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2145) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2149) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2150) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2151) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2152) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2154) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2156) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2157) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2162) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2163) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2164) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2168) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2170) tree.c:666: }
    (<0.0>,2175) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2180) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2181) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2182) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2183) fake_defs.h:237: switch (size) {
    (<0.0>,2185) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2187) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2188) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2190) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2193) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2194) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2195) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2198) tree.c:2318: return true;
    (<0.0>,2200) tree.c:2319: }
    (<0.0>,2203) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2206) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2207) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2211) tree.c:1689: if (c_out != NULL)
    (<0.0>,2214) tree.c:1691: return ret;
    (<0.0>,2218) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,2219) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2222) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2223) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2229) tree.c:1798: return ret;
    (<0.0>,2231) tree.c:1798: return ret;
    (<0.0>,2233) tree.c:1799: }
    (<0.0>,2235) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,2237) tree.c:1843: }
    (<0.0>,2241) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,2242) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2243) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2244) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2255)
    (<0.0>,2256)
    (<0.0>,2257)
    (<0.0>,2258) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2260) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2263) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2264) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2275)
    (<0.0>,2276)
    (<0.0>,2277) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2290)
    (<0.0>,2291) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2296) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2297) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2298) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2299) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2301) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2303) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2304) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2306) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2309) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2310) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2311) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2312) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2317) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2318) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2320) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2322) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2324) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2325) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2327) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2330) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2331) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2332) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2338) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2354)
    (<0.0>,2355) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2358)
    (<0.0>,2359) tree.c:625: return &rsp->node[0];
    (<0.0>,2363) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2364) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2369) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2370) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2371) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2372) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2374) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2376) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2377) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2379) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2382) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2383) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2384) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2388) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2389) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2391) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2394) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2395) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2399) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2400) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2401) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2407) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2409) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2412) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2413) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2414) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2418) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2420) tree.c:666: }
    (<0.0>,2425) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2430) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2431) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2432) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2433) fake_defs.h:237: switch (size) {
    (<0.0>,2435) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2437) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2438) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2440) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2443) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2444) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2445) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2448) tree.c:2318: return true;
    (<0.0>,2450) tree.c:2319: }
    (<0.0>,2454) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2455) tree.c:2346: return ret;
    (<0.0>,2459) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,2463) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2464) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2465) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2468)
    (<0.0>,2469) tree.c:625: return &rsp->node[0];
    (<0.0>,2474) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2478)
    (<0.0>,2479)
    (<0.0>,2480) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2481) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2484) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,2487)
    (<0.0>,2489) fake_sched.h:43: return __running_cpu;
    (<0.0>,2493) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2495) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2499) fake_sched.h:43: return __running_cpu;
    (<0.0>,2503) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2511) tree.c:3021: if (needwake)
    (<0.0>,2514) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,2522)
    (<0.0>,2523) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2524) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2526) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2533) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2536)
    (<0.0>,2537) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2539) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2542) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2549) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2552)
    (<0.0>,2556) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2559) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2560) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2561) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2565) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2566) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2567) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2569) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2573) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,2585)
    (<0.0>,2587) fake_sched.h:43: return __running_cpu;
    (<0.0>,2590) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2592) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2594) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2595) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2597) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2604) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2605) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2607) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2613) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2614) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2615) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2616) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2617) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2621)
    (<0.0>,2622)
    (<0.0>,2623) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2624) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2649)
    (<0.0>,2650)
    (<0.0>,2651) tree.c:1905: local_irq_save(flags);
    (<0.0>,2654)
    (<0.0>,2656) fake_sched.h:43: return __running_cpu;
    (<0.0>,2660) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2662) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2666) fake_sched.h:43: return __running_cpu;
    (<0.0>,2670) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2675) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2677) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2678) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2679) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2681) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2682) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2687) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2688) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2689) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2690) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2695) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2697) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2700) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2701) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2702) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2705) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2707) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2708) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2713) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2714) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2715) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2716) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2718) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2720) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2721) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2723) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2726) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2727) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2728) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2731) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2735) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2736) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2737) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2740) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2742) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2743) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2746) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2749) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2750) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2758) tree.c:1911: local_irq_restore(flags);
    (<0.0>,2761)
    (<0.0>,2763) fake_sched.h:43: return __running_cpu;
    (<0.0>,2767) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2769) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2773) fake_sched.h:43: return __running_cpu;
    (<0.0>,2777) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2784) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2786) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2791) tree.c:3016: local_irq_save(flags);
    (<0.0>,2794)
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2802) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2806) fake_sched.h:43: return __running_cpu;
    (<0.0>,2810) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2815) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2816) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2827)
    (<0.0>,2828)
    (<0.0>,2829) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2842)
    (<0.0>,2843) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2849) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2850) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2855) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2858) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2863) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2864) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2870) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2871) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2872) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2877) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2879) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2882) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2883) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2890) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2906)
    (<0.0>,2907) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2910)
    (<0.0>,2911) tree.c:625: return &rsp->node[0];
    (<0.0>,2915) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2916) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2921) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2922) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2923) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2924) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2926) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2929) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2934) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2935) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2936) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2940) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2941) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2943) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2946) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2947) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2951) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2952) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2953) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2954) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2956) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2959) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2964) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2965) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2966) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2970) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2973) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2976) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2979) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2980) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2983) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2985) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2988) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2991) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2994) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2995) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2997) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3000) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3004) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3006) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3008) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3011) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3014) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3017) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3018) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3020) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3023) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3027) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3029) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3031) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3034) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,3036) tree.c:666: }
    (<0.0>,3039) tree.c:3024: local_irq_restore(flags);
    (<0.0>,3042)
    (<0.0>,3044) fake_sched.h:43: return __running_cpu;
    (<0.0>,3048) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3050) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3054) fake_sched.h:43: return __running_cpu;
    (<0.0>,3058) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3064) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,3067)
    (<0.0>,3068) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3070) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3073) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3080) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3083)
    (<0.0>,3087) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3090) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3091) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3092) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3096) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3097) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3098) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3100) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3108) fake_sched.h:43: return __running_cpu;
    (<0.0>,3112) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,3122) fake_sched.h:43: return __running_cpu;
    (<0.0>,3126) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3127) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3129) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3130) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3131) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3133) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3135) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3136) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3137) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3138) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3139) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3140) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3142) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3150)
    (<0.0>,3156) fake_sched.h:43: return __running_cpu;
    (<0.0>,3160)
    (<0.0>,3163) tree.c:755: local_irq_save(flags);
    (<0.0>,3166)
    (<0.0>,3168) fake_sched.h:43: return __running_cpu;
    (<0.0>,3172) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3174) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3178) fake_sched.h:43: return __running_cpu;
    (<0.0>,3182) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3194)
    (<0.0>,3196) fake_sched.h:43: return __running_cpu;
    (<0.0>,3200) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3201) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3203) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3204) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3205) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3206) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3207) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3208) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3209) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3213) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3215) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3216) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3217) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3233)
    (<0.0>,3235)
    (<0.0>,3237) fake_sched.h:43: return __running_cpu;
    (<0.0>,3241) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3244) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3245) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3246) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3250) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3251) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3252) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3254) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3259) fake_sched.h:43: return __running_cpu;
    (<0.0>,3262) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3264) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3266) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3267) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3270)
    (<0.0>,3273) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3276) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3277) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3278) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3282) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3283) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3284) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3286) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3291) fake_sched.h:43: return __running_cpu;
    (<0.0>,3294) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3296) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3298) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3299) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3302)
    (<0.0>,3305) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3308) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3309) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3310) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3314) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3315) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3316) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3318) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3325) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3328) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3329) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3330) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3332) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3333) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3335) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3336) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3337) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3338) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3352)
    (<0.0>,3354) tree.c:758: local_irq_restore(flags);
    (<0.0>,3357)
    (<0.0>,3359) fake_sched.h:43: return __running_cpu;
    (<0.0>,3363) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3365) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3369) fake_sched.h:43: return __running_cpu;
    (<0.0>,3373) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3379) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3382) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3387) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3392) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3393) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3394) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3395) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3397) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3399) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3400) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3405) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3406) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3407) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3414) fake_sched.h:43: return __running_cpu;
    (<0.0>,3418)
    (<0.0>,3419) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3422) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3427) tree.c:892: local_irq_save(flags);
    (<0.0>,3430)
    (<0.0>,3432) fake_sched.h:43: return __running_cpu;
    (<0.0>,3436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3442) fake_sched.h:43: return __running_cpu;
    (<0.0>,3446) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3458)
    (<0.0>,3460) fake_sched.h:43: return __running_cpu;
    (<0.0>,3464) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3465) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3467) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3468) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3469) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3470) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3471) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3472) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3473) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3477) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3479) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3480) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3481) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3492)
    (<0.0>,3493)
    (<0.0>,3495) fake_sched.h:43: return __running_cpu;
    (<0.0>,3499) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3503) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3506) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3507) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3508) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3510) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3511) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3513) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3514) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3515) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3516) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3526)
    (<0.0>,3528) tree.c:895: local_irq_restore(flags);
    (<0.0>,3531)
    (<0.0>,3533) fake_sched.h:43: return __running_cpu;
    (<0.0>,3537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3539) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3543) fake_sched.h:43: return __running_cpu;
    (<0.0>,3547) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3554) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3556) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3557) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,3602)
    (<0.0>,3603) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3606)
    (<0.0>,3607) tree.c:625: return &rsp->node[0];
    (<0.0>,3611) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3613) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3614) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3615) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3620) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3621) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3622) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3623) fake_defs.h:237: switch (size) {
    (<0.0>,3625) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3627) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3628) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3630) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3633) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3634) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3635) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3636) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,3639)
    (<0.0>,3640) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,3644)
    (<0.0>,3647) fake_sched.h:43: return __running_cpu;
    (<0.0>,3651) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,3655) fake_sched.h:43: return __running_cpu;
    (<0.0>,3659) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3664) fake_sched.h:43: return __running_cpu;
    (<0.0>,3668) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3671) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3672) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3679) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3684) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3685) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3686) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3691) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3697) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3698) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3699) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3710)
    (<0.0>,3716)
    (<0.0>,3721) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3726) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3727) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3728) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3729) fake_defs.h:237: switch (size) {
    (<0.0>,3731) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3733) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3734) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3736) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3739) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3740) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3741) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3742) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3755)
    (<0.0>,3756) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3761) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3762) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3763) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3774) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3775) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3776) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3777) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3782) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3783) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3784) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3785) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3787) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3789) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3790) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3792) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3795) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3796) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3797) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3805) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3806) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3819)
    (<0.0>,3820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3825) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3826) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3827) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3830) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3833) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3835) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3838) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3839) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3840) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3841) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3846) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3847) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3849) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3854) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3859) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3860) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3868) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3869) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3870) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3873) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,3888)
    (<0.0>,3889) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3890) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3891) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3892) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3894) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3915) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3916) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3917) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3918) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3920) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3922) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3923) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3925) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3928) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3929) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3930) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3931) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3932) update.c:466: if (till_stall_check < 3) {
    (<0.0>,3935) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,3939) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,3944) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,3946) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3947) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3949) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3950) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3955) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3956) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3957) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3958) fake_defs.h:237: switch (size) {
    (<0.0>,3960) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3962) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3963) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3965) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3968) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3969) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3970) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3971) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3972) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3975) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3977) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3978) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3983) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3984) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3985) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3986) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3988) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3990) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3991) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3993) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3996) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3997) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3998) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3999) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4001) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4005) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4007) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4009) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4010) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4012) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4013) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4014) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4018) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4021)
    (<0.0>,4022) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4026)
    (<0.0>,4027) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4028) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4033) fake_sched.h:43: return __running_cpu;
    (<0.0>,4037) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4039) fake_sched.h:43: return __running_cpu;
    (<0.0>,4043) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4050) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4053) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4056) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4057) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4059) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4060) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4062) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4067) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4068) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4072)
    (<0.0>,4073)
    (<0.0>,4074) tree.c:1922: if (delay > 0 &&
    (<0.0>,4078) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4081)
    (<0.0>,4082) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4086)
    (<0.0>,4089) fake_sched.h:43: return __running_cpu;
    (<0.0>,4093) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4097) fake_sched.h:43: return __running_cpu;
    (<0.0>,4101) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4106) fake_sched.h:43: return __running_cpu;
    (<0.0>,4110) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4113) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4114) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4121) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4123) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4124) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4126) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4129) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4131) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4132) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4133) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4135) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4136) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4138) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4139) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4143) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4145) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4151) tree.c:1983: if (!oldmask) /* First online CPU for this rcu_node. */
    (<0.0>,4154) tree.c:1984: rcu_init_new_rnp(rnp);
    (<0.0>,4159)
    (<0.0>,4160) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4161) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4163) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4165) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4166) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4167) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4169) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4170) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4171) tree.c:3752: if (rnp == NULL)
    (<0.0>,4177) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4179) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4182) tree.c:2007: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4185)
    (<0.0>,4186) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4190)
    (<0.0>,4191) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4192) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4197) fake_sched.h:43: return __running_cpu;
    (<0.0>,4201) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4203) fake_sched.h:43: return __running_cpu;
    (<0.0>,4207) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4215) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4217) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4219) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4220) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4222) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4227) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4230) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4232) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4233) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4235) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4240) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4241) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4245)
    (<0.0>,4246)
    (<0.0>,4247) tree.c:1922: if (delay > 0 &&
    (<0.0>,4251) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4254)
    (<0.0>,4255) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4259)
    (<0.0>,4262) fake_sched.h:43: return __running_cpu;
    (<0.0>,4266) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4270) fake_sched.h:43: return __running_cpu;
    (<0.0>,4274) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4279) fake_sched.h:43: return __running_cpu;
    (<0.0>,4283) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4286) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4287) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4295) fake_sched.h:43: return __running_cpu;
    (<0.0>,4298) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4300) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4302) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4303) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,4309)
    (<0.0>,4310) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4312) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4317) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4318) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4320) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4324) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4325) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4326) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4328) tree.c:2028: rnp->qsmask = 0;
    (<0.0>,4330) tree.c:2028: rnp->qsmask = 0;
    (<0.0>,4332) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4334) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4335) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4336) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4341) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4342) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4343) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4344) fake_defs.h:237: switch (size) {
    (<0.0>,4346) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4348) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4349) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4351) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4354) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4355) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4356) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4357) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4359) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4360) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4362) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4367) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4368) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4370) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4371) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4373) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4377) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4378) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4379) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4382) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4383) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4385) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4388) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4389) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4390) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4412)
    (<0.0>,4413)
    (<0.0>,4414)
    (<0.0>,4415) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4417) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4418) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4420) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4423) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4427) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4428) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4429) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4430) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4432) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4433) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4434) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4435) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4438) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4441) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4442) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4450) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4451) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4452) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4461)
    (<0.0>,4462)
    (<0.0>,4463)
    (<0.0>,4464) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4467) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4470) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4473) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4474) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4477) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4478) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4483)
    (<0.0>,4484)
    (<0.0>,4485) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4488)
    (<0.0>,4489) tree.c:625: return &rsp->node[0];
    (<0.0>,4493) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4496) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4498) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4499) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4501) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4504) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4506) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4508) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4510) tree.c:1585: }
    (<0.0>,4512) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4513) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4515) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4518) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4520) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4523) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4524) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4527) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4530) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4534) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4536) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4538) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4541) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4543) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4546) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4547) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4550) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4553) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4556) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4558) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4561) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4562) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4567) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4569) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4573) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4576) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4579) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4580) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4582) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4585) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4586) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4587) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4589) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4592) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4594) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4596) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4598) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4601) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4604) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4605) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4607) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4610) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4611) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4612) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4614) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4617) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4619) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4621) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4623) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4626) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4627) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4646)
    (<0.0>,4647)
    (<0.0>,4648)
    (<0.0>,4649) tree.c:1613: bool ret = false;
    (<0.0>,4650) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4652) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4655)
    (<0.0>,4656) tree.c:625: return &rsp->node[0];
    (<0.0>,4660) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4661) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4663) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4664) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4669)
    (<0.0>,4670)
    (<0.0>,4671) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4674)
    (<0.0>,4675) tree.c:625: return &rsp->node[0];
    (<0.0>,4679) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4682) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4684) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4685) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4687) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4690) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4692) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4694) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4696) tree.c:1585: }
    (<0.0>,4698) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4699) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4700) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4701) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4707)
    (<0.0>,4708)
    (<0.0>,4709)
    (<0.0>,4710)
    (<0.0>,4714) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4716) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4719) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4722) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4724) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4725) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4727) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4730) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4732) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4735) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4737) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4738) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4739) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4740) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4746)
    (<0.0>,4747)
    (<0.0>,4748)
    (<0.0>,4749)
    (<0.0>,4754) tree.c:1689: if (c_out != NULL)
    (<0.0>,4757) tree.c:1691: return ret;
    (<0.0>,4761) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4762) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4765) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4766) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4772) tree.c:1798: return ret;
    (<0.0>,4774) tree.c:1798: return ret;
    (<0.0>,4776) tree.c:1799: }
    (<0.0>,4779) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4781) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4783) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4784) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4786) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4789) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4791) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4792) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4794) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4797) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4799) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4800) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4802) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4808) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4809) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4812) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4816) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4818) fake_sched.h:43: return __running_cpu;
    (<0.0>,4822) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4823) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4825) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4826) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4828) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4831) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4832) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,4835)
    (<0.0>,4836) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4838) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4839) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4841) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4851)
    (<0.0>,4856) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4860) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4861) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4862) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4863) fake_defs.h:237: switch (size) {
    (<0.0>,4865) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4866) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4867) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4868) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4871) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4874) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4875) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4878) tree.c:1896: return ret;
    (<0.0>,4882) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,4885)
    (<0.0>,4889) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4892)
    (<0.0>,4893) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4897)
    (<0.0>,4898) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4899) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4904) fake_sched.h:43: return __running_cpu;
    (<0.0>,4908) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4910) fake_sched.h:43: return __running_cpu;
    (<0.0>,4914) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4929) fake_sched.h:43: return __running_cpu;
    (<0.0>,4935) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,4943) fake_sched.h:43: return __running_cpu;
    (<0.0>,4947) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,4960) fake_sched.h:43: return __running_cpu;
    (<0.0>,4964)
    (<0.0>,4967) tree.c:755: local_irq_save(flags);
    (<0.0>,4970)
    (<0.0>,4972) fake_sched.h:43: return __running_cpu;
    (<0.0>,4976) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4978) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4982) fake_sched.h:43: return __running_cpu;
    (<0.0>,4986) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4998)
    (<0.0>,5000) fake_sched.h:43: return __running_cpu;
    (<0.0>,5004) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5005) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5007) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5008) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5009) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5010) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5011) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5012) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5013) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5017) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5019) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5020) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5021) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5037)
    (<0.0>,5039)
    (<0.0>,5041) fake_sched.h:43: return __running_cpu;
    (<0.0>,5045) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5048) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5049) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5050) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5054) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5055) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5056) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5058) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5063) fake_sched.h:43: return __running_cpu;
    (<0.0>,5066) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5068) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5070) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5071) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5074)
    (<0.0>,5077) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5080) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5081) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5082) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5086) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5087) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5088) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5090) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5095) fake_sched.h:43: return __running_cpu;
    (<0.0>,5098) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5100) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5102) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5103) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5106)
    (<0.0>,5109) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5112) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5113) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5114) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5118) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5119) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5120) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5122) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5129) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5132) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5133) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5134) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5136) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5137) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5139) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5140) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5141) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5142) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5156)
    (<0.0>,5158) tree.c:758: local_irq_restore(flags);
    (<0.0>,5161)
    (<0.0>,5163) fake_sched.h:43: return __running_cpu;
    (<0.0>,5167) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5169) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5173) fake_sched.h:43: return __running_cpu;
    (<0.0>,5177) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5183) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5186) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5191) fake_sched.h:43: return __running_cpu;
    (<0.0>,5195)
    (<0.0>,5196) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5199) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5204) tree.c:892: local_irq_save(flags);
    (<0.0>,5207)
    (<0.0>,5209) fake_sched.h:43: return __running_cpu;
    (<0.0>,5213) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5215) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5219) fake_sched.h:43: return __running_cpu;
    (<0.0>,5223) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5235)
    (<0.0>,5237) fake_sched.h:43: return __running_cpu;
    (<0.0>,5241) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5242) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5244) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5245) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5246) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5247) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5248) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5249) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5250) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,5254) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5256) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5257) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5258) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5269)
    (<0.0>,5270)
    (<0.0>,5272) fake_sched.h:43: return __running_cpu;
    (<0.0>,5276) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5280) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5283) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5284) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5285) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5287) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5288) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5290) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5291) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5292) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5293) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5303)
    (<0.0>,5305) tree.c:895: local_irq_restore(flags);
    (<0.0>,5308)
    (<0.0>,5310) fake_sched.h:43: return __running_cpu;
    (<0.0>,5314) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5316) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5320) fake_sched.h:43: return __running_cpu;
    (<0.0>,5324) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5338) fake_sched.h:43: return __running_cpu;
    (<0.0>,5342) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,5351) fake_sched.h:43: return __running_cpu;
    (<0.0>,5358) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,5367) fake_sched.h:43: return __running_cpu;
    (<0.0>,5371) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5373) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5379) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5380) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5381) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5386) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5387) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5388) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5389) fake_defs.h:237: switch (size) {
    (<0.0>,5391) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5393) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5394) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5396) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5399) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5400) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5401) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5403) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5405) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5407) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5408) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5410) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5415) tree.c:2046: return true;
    (<0.0>,5417) tree.c:2047: }
    (<0.0>,5421) tree.c:2214: first_gp_fqs = true;
    (<0.0>,5422) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5423) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5424) tree.c:2216: if (j > HZ) {
    (<0.0>,5427) tree.c:2220: ret = 0;
    (<0.0>,5429) tree.c:2222: if (!ret) {
    (<0.0>,5432) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5433) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5435) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5437) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5439) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5440) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5443) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5444) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5449) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5450) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5451) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5452) fake_defs.h:237: switch (size) {
    (<0.0>,5454) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5456) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5457) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5459) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5462) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5463) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5464) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5468) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5470) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5474) fake_sched.h:43: return __running_cpu;
    (<0.0>,5478) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5482) fake_sched.h:43: return __running_cpu;
    (<0.0>,5486) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5491) fake_sched.h:43: return __running_cpu;
    (<0.0>,5495) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5506) fake_sched.h:43: return __running_cpu;
    (<0.0>,5510) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5511) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5513) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5514) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5515) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5517) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5519) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5520) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5521) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5522) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5523) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5524) tree.c:942: if (oldval)
    (<0.0>,5532)
    (<0.0>,5538)
    (<0.0>,5547) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5548) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5549) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5553) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5554) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5555) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5557) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5562) fake_sched.h:43: return __running_cpu;
    (<0.0>,5565) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5567) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5570) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5572) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5574) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5577) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5578) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5579) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5583) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5584) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5585) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5587) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5592) fake_sched.h:43: return __running_cpu;
    (<0.0>,5595) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5597) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5600) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5602) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5604) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5607) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5608) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5609) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5613) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5614) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5615) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5617) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5622) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,5627) fake_sched.h:43: return __running_cpu;
    (<0.0>,5632) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,5640) fake_sched.h:43: return __running_cpu;
    (<0.0>,5646) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,5660) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5661) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5662) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5666) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5667) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5668) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5670) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5674) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5676) fake_sched.h:43: return __running_cpu;
    (<0.0>,5679) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5681) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5703)
    (<0.0>,5704)
    (<0.0>,5705) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5707) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5708) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5709) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5711) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5713) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5714) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5715) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5750)
    (<0.0>,5751)
    (<0.0>,5752) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,5755) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,5768)
    (<0.0>,5769) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5774) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5775) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5776) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5777) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5779) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5781) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5782) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5784) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5787) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5788) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5789) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5790) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5795) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5796) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5797) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5798) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5800) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5802) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5803) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5805) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5808) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5809) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5810) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5816) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,5841)
    (<0.0>,5842) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,5847) tree.c:1472: j = jiffies;
    (<0.0>,5848) tree.c:1472: j = jiffies;
    (<0.0>,5849) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5854) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5855) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5856) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5857) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5859) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5861) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5862) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5864) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5867) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5868) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5869) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5870) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5872) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5877) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5878) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5879) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5880) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5882) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5884) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5885) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5887) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5890) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5891) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5892) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5893) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5895) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5900) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5901) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5902) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5903) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5905) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5907) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5908) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5910) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5913) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5914) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5915) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5916) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5918) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5923) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5924) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5925) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5926) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5930) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5933) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5936) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5937) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5938) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5939) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5940) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5941) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5945) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5946) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5952) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,5955)
    (<0.0>,5958) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,5961) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5963) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5966) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,5968) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,5972) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,5975)
    (<0.0>,5976) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5978) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5981) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5988) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,5989) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6000)
    (<0.0>,6001)
    (<0.0>,6002) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,6015)
    (<0.0>,6016) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6021) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6022) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6023) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6024) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6026) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6028) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6029) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6031) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6034) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6035) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6036) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6037) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6042) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6043) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6044) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6045) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6047) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6050) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6052) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6055) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6056) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6057) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6063) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,6065) tree.c:666: }
    (<0.0>,6068) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6073) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6074) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6075) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6076) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6078) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6080) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6081) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6083) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6086) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6087) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6088) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6089) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6091) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6094) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6099) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6100) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6101) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6102) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6104) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6106) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6107) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6109) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6112) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6113) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6114) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6115) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6117) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6120) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6124) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6125) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6126) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6127) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6129) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6130) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6131) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6132) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6135) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6138) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6139) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6147) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,6150)
    (<0.0>,6154) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6156) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6158) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6159) tree.c:3547: return 0;
    (<0.0>,6161) tree.c:3548: }
    (<0.0>,6166) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6169) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6170) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6171) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6175) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6176) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6177) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6179) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6183) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,6185) fake_sched.h:43: return __running_cpu;
    (<0.0>,6188) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,6190) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,6212)
    (<0.0>,6213)
    (<0.0>,6214) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6216) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6217) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6218) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,6220) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,6222) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,6223) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,6224) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,6259)
    (<0.0>,6260)
    (<0.0>,6261) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,6264) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,6277)
    (<0.0>,6278) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6283) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6284) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6285) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6286) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6288) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6290) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6291) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6293) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6296) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6297) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6298) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6299) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6304) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6305) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6306) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6307) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6309) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6311) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6312) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6314) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6317) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6318) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6327) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,6330)
    (<0.0>,6333) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,6336) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,6338) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,6341) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6343) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6347) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,6350)
    (<0.0>,6351) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6353) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6356) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6363) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6364) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6375)
    (<0.0>,6376)
    (<0.0>,6377) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,6390)
    (<0.0>,6391) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6396) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6397) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6398) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6399) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6401) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6403) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6409) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6410) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6411) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6412) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6417) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6418) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6419) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6420) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6422) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6424) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6425) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6427) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6430) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6431) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6432) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6438) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,6454)
    (<0.0>,6455) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6458)
    (<0.0>,6459) tree.c:625: return &rsp->node[0];
    (<0.0>,6463) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6464) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6469) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6470) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6471) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6472) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6474) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6476) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6477) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6479) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6482) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6483) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6484) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6488) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6489) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6491) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6494) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6495) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6499) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6500) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6501) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6502) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6504) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6506) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6507) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6509) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6512) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6513) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6514) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6518) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6521) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6524) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,6527) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,6528) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,6531) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6533) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6536) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6539) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6542) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6543) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6545) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6548) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6552) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6554) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6556) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6559) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6562) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6565) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6566) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6568) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6571) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6575) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6577) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6579) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6582) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,6584) tree.c:666: }
    (<0.0>,6587) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6592) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6593) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6594) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6595) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6597) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6599) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6600) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6602) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6605) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6606) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6607) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6608) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6610) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6613) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6618) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6619) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6620) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6621) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6623) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6625) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6626) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6628) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6631) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6632) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6633) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6634) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6636) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6639) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6643) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6644) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6645) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6646) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6648) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6649) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6650) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6651) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6654) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6657) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6658) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6666) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,6669)
    (<0.0>,6673) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6675) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6677) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6678) tree.c:3547: return 0;
    (<0.0>,6680) tree.c:3548: }
    (<0.0>,6685) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6688) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6689) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6690) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6694) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6695) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6696) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6698) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6702) tree.c:3562: return 0;
    (<0.0>,6704) tree.c:3563: }
    (<0.0>,6708) tree.c:2891: if (user)
    (<0.0>,6716) fake_sched.h:43: return __running_cpu;
    (<0.0>,6720) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6722) fake_sched.h:43: return __running_cpu;
    (<0.0>,6726) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6732) fake_sched.h:43: return __running_cpu;
    (<0.0>,6736) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,6747) fake_sched.h:43: return __running_cpu;
    (<0.0>,6751) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,6752) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,6754) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,6755) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,6756) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,6758) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,6760) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,6761) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6762) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6763) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6764) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6765) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,6767) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,6775)
    (<0.0>,6781) fake_sched.h:43: return __running_cpu;
    (<0.0>,6785)
    (<0.0>,6788) tree.c:755: local_irq_save(flags);
    (<0.0>,6791)
    (<0.0>,6793) fake_sched.h:43: return __running_cpu;
    (<0.0>,6797) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6799) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6803) fake_sched.h:43: return __running_cpu;
    (<0.0>,6807) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6819)
    (<0.0>,6821) fake_sched.h:43: return __running_cpu;
    (<0.0>,6825) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,6826) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,6828) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,6829) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,6830) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6831) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6832) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6833) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6834) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,6838) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,6840) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,6841) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6842) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6858)
    (<0.0>,6860)
    (<0.0>,6862) fake_sched.h:43: return __running_cpu;
    (<0.0>,6866) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,6869) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6870) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6871) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6875) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6876) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6877) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6879) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6884) fake_sched.h:43: return __running_cpu;
    (<0.0>,6887) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6889) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6891) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6892) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6895)
    (<0.0>,6898) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6901) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6902) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6903) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6907) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6908) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6909) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6911) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6916) fake_sched.h:43: return __running_cpu;
    (<0.0>,6919) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6921) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6923) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6924) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6927)
    (<0.0>,6930) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6933) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6934) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6935) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6939) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6940) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6941) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6943) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6950) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6953) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6954) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6955) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6957) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6958) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6960) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6961) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6962) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6963) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6977)
    (<0.0>,6979) tree.c:758: local_irq_restore(flags);
    (<0.0>,6982)
    (<0.0>,6984) fake_sched.h:43: return __running_cpu;
    (<0.0>,6988) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6990) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6994) fake_sched.h:43: return __running_cpu;
    (<0.0>,6998) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7004) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,7007) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,7012) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7028)
    (<0.0>,7029)
    (<0.0>,7030) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7033)
    (<0.0>,7034) tree.c:625: return &rsp->node[0];
    (<0.0>,7038) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7039) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7044) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7045) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7046) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7047) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7052) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7054) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7057) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7058) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7059) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7061) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7062) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7063) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7064) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7068) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7073) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7074) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7075) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7076) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7078) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7080) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7081) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7083) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7086) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7087) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7088) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7091) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7094)
    (<0.0>,7098) tree.c:2064: return true;
    (<0.0>,7100) tree.c:2067: }
    (<0.0>,7105) fake_sched.h:43: return __running_cpu;
    (<0.0>,7109)
    (<0.0>,7110) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,7113) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,7118) tree.c:892: local_irq_save(flags);
    (<0.0>,7121)
    (<0.0>,7123) fake_sched.h:43: return __running_cpu;
    (<0.0>,7127) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7129) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7133) fake_sched.h:43: return __running_cpu;
    (<0.0>,7137) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7149)
    (<0.0>,7151) fake_sched.h:43: return __running_cpu;
    (<0.0>,7155) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7156) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7158) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7159) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7160) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7161) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7162) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7163) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7164) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,7168) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,7170) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,7171) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,7172) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,7183)
    (<0.0>,7184)
    (<0.0>,7186) fake_sched.h:43: return __running_cpu;
    (<0.0>,7190) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7194) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7197) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7198) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7199) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7201) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7202) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7204) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7205) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7206) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7207) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7217)
    (<0.0>,7219) tree.c:895: local_irq_restore(flags);
    (<0.0>,7222)
    (<0.0>,7224) fake_sched.h:43: return __running_cpu;
    (<0.0>,7228) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7230) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7234) fake_sched.h:43: return __running_cpu;
    (<0.0>,7238) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7245) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7246) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7247) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7248) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,7250) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,7251) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7256) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7257) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7258) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7259) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7261) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7263) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7264) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7266) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7269) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7270) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7271) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7274) tree.c:2237: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7277)
    (<0.0>,7282) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,7284) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,7285) tree.c:2280: rcu_gp_cleanup(rsp);
    (<0.0>,7325)
    (<0.0>,7326) tree.c:2109: bool needgp = false;
    (<0.0>,7327) tree.c:2110: int nocb = 0;
    (<0.0>,7328) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7331)
    (<0.0>,7332) tree.c:625: return &rsp->node[0];
    (<0.0>,7336) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7338) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7339) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7340) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7345) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7346) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7347) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7348) fake_defs.h:237: switch (size) {
    (<0.0>,7350) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7352) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7353) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7355) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7358) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7359) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7360) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7361) tree.c:2116: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,7364)
    (<0.0>,7365) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,7369)
    (<0.0>,7372) fake_sched.h:43: return __running_cpu;
    (<0.0>,7376) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,7380) fake_sched.h:43: return __running_cpu;
    (<0.0>,7384) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7389) fake_sched.h:43: return __running_cpu;
    (<0.0>,7393) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,7396) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7397) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7404) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7405) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7407) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7409) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7410) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,7411) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,7413) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,7416) tree.c:2129: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,7419)
    (<0.0>,7420) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,7424)
    (<0.0>,7425) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7426) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7431) fake_sched.h:43: return __running_cpu;
    (<0.0>,7435) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,7437) fake_sched.h:43: return __running_cpu;
    (<0.0>,7441) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7448) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7451) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7453) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7454) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7456) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7461) tree.c:2141: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,7464)
    (<0.0>,7465) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,7469)
    (<0.0>,7472) fake_sched.h:43: return __running_cpu;
    (<0.0>,7476) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,7480) fake_sched.h:43: return __running_cpu;
    (<0.0>,7484) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7489) fake_sched.h:43: return __running_cpu;
    (<0.0>,7493) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,7496) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7497) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7504) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7507)
    (<0.0>,7513) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7514) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7517)
    (<0.0>,7522) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7523) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7524) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7525) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7527) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7532) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7533) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7535) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7539) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7540) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7541) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7543) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7545) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7546) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7547) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7552) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7553) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7554) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7555) fake_defs.h:237: switch (size) {
    (<0.0>,7557) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7559) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7560) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7562) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7565) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7566) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7567) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7569) fake_sched.h:43: return __running_cpu;
    (<0.0>,7572) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7574) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7576) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7577) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,7578) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,7580) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,7583) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,7584) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,7585) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,7607)
    (<0.0>,7608)
    (<0.0>,7609)
    (<0.0>,7610) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,7612) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,7613) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,7615) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,7618) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,7619) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,7620) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,7628)
    (<0.0>,7629)
    (<0.0>,7630)
    (<0.0>,7631) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7634) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7637) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7640) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7641) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7644) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,7646) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,7649) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7651) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7652) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7654) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7657) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7661) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,7663) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,7666) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,7667) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,7670) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,7672) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,7674) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,7676) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,7679) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7681) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7682) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7684) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7687) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7692) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7694) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7695) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7698) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7701) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7702) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7704) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7707) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7709) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7711) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7713) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7714) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7717) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,7719) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,7722) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7724) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7727) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7728) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7731) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7735) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,7736) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,7737) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,7746)
    (<0.0>,7747)
    (<0.0>,7748)
    (<0.0>,7749) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7752) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7755) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7758) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7759) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7762) tree.c:1750: return false;
    (<0.0>,7764) tree.c:1799: }
    (<0.0>,7766) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,7768) tree.c:1843: }
    (<0.0>,7771) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,7772) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,7774) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,7775) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,7777) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,7781) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7783) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7784) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7786) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7789) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7793) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7794) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7795) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7796) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7798) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7799) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7800) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7801) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7804) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7807) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7808) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7816) tree.c:1896: return ret;
    (<0.0>,7820) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,7824) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,7826) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,7827) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,7834)
    (<0.0>,7835)
    (<0.0>,7836) tree.c:1702: int c = rnp->completed;
    (<0.0>,7838) tree.c:1702: int c = rnp->completed;
    (<0.0>,7840) tree.c:1702: int c = rnp->completed;
    (<0.0>,7842) fake_sched.h:43: return __running_cpu;
    (<0.0>,7845) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7847) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7849) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7850) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,7853) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,7856) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,7857) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,7861) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,7864) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,7865) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,7866) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,7867) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,7868) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,7870) tree.c:1709: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,7878)
    (<0.0>,7879)
    (<0.0>,7880)
    (<0.0>,7881)
    (<0.0>,7885) tree.c:1710: return needmore;
    (<0.0>,7887) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,7889) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,7890) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,7893)
    (<0.0>,7895) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,7896) tree.c:2151: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,7899)
    (<0.0>,7900) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,7904)
    (<0.0>,7905) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7906) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7911) fake_sched.h:43: return __running_cpu;
    (<0.0>,7915) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,7917) fake_sched.h:43: return __running_cpu;
    (<0.0>,7921) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7928) tree.c:2152: rcu_nocb_gp_cleanup(sq);
    (<0.0>,7931)
    (<0.0>,7941) fake_sched.h:43: return __running_cpu;
    (<0.0>,7947) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,7955) fake_sched.h:43: return __running_cpu;
    (<0.0>,7959) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,7972) fake_sched.h:43: return __running_cpu;
    (<0.0>,7976)
    (<0.0>,7979) tree.c:755: local_irq_save(flags);
    (<0.0>,7982)
    (<0.0>,7984) fake_sched.h:43: return __running_cpu;
    (<0.0>,7988) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7990) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7994) fake_sched.h:43: return __running_cpu;
    (<0.0>,7998) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8010)
    (<0.0>,8012) fake_sched.h:43: return __running_cpu;
    (<0.0>,8016) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8017) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8019) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8020) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8021) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8022) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8023) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8024) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8025) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,8029) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,8031) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,8032) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8033) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8049)
    (<0.0>,8051)
    (<0.0>,8053) fake_sched.h:43: return __running_cpu;
    (<0.0>,8057) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8060) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8061) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8062) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8066) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8067) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8068) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8070) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8075) fake_sched.h:43: return __running_cpu;
    (<0.0>,8078) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8080) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8082) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8083) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8086)
    (<0.0>,8089) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8092) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8093) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8094) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8098) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8099) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8100) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8102) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8107) fake_sched.h:43: return __running_cpu;
    (<0.0>,8110) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8112) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8114) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8115) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8118)
    (<0.0>,8121) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8124) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8125) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8126) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8130) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8131) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8132) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8134) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8141) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8144) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8145) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8146) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8148) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8149) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8151) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8152) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8153) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8154) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8168)
    (<0.0>,8170) tree.c:758: local_irq_restore(flags);
    (<0.0>,8173)
    (<0.0>,8175) fake_sched.h:43: return __running_cpu;
    (<0.0>,8179) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8181) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8185) fake_sched.h:43: return __running_cpu;
    (<0.0>,8189) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8195) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8198) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8203) fake_sched.h:43: return __running_cpu;
    (<0.0>,8207)
    (<0.0>,8208) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8211) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8216) tree.c:892: local_irq_save(flags);
    (<0.0>,8219)
    (<0.0>,8221) fake_sched.h:43: return __running_cpu;
    (<0.0>,8225) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8227) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8231) fake_sched.h:43: return __running_cpu;
    (<0.0>,8235) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8247)
    (<0.0>,8249) fake_sched.h:43: return __running_cpu;
    (<0.0>,8253) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8254) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8256) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8257) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8258) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8259) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8260) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8261) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8262) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,8266) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8268) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8269) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8270) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8281)
    (<0.0>,8282)
    (<0.0>,8284) fake_sched.h:43: return __running_cpu;
    (<0.0>,8288) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8292) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8295) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8296) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8297) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8299) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8300) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8302) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8303) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8304) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8305) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8315)
    (<0.0>,8317) tree.c:895: local_irq_restore(flags);
    (<0.0>,8320)
    (<0.0>,8322) fake_sched.h:43: return __running_cpu;
    (<0.0>,8326) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8328) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8332) fake_sched.h:43: return __running_cpu;
    (<0.0>,8336) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8350) fake_sched.h:43: return __running_cpu;
    (<0.0>,8354) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,8363) fake_sched.h:43: return __running_cpu;
    (<0.0>,8370) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,8379) fake_sched.h:43: return __running_cpu;
    (<0.0>,8383) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,8385) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,8391) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8392) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8393) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8398) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8399) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8400) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8401) fake_defs.h:237: switch (size) {
    (<0.0>,8403) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8405) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8406) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8408) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8411) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8412) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8413) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8414) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,8415) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,8419)
    (<0.0>,8420)
    (<0.0>,8421) tree.c:1922: if (delay > 0 &&
    (<0.0>,8426) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8428) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8430) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8431) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8433) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8438) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,8441)
    (<0.0>,8442) tree.c:625: return &rsp->node[0];
    (<0.0>,8446) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,8447) tree.c:2158: raw_spin_lock_irq_rcu_node(rnp); /* Order GP before ->completed update. */
    (<0.0>,8450)
    (<0.0>,8451) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8455)
    (<0.0>,8458) fake_sched.h:43: return __running_cpu;
    (<0.0>,8462) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8466) fake_sched.h:43: return __running_cpu;
    (<0.0>,8470) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8475) fake_sched.h:43: return __running_cpu;
    (<0.0>,8479) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8482) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8483) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8490) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,8491) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,8495)
    (<0.0>,8496)
    (<0.0>,8499) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8501) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8502) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8503) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8508) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8509) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8510) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8511) fake_defs.h:237: switch (size) {
    (<0.0>,8513) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8515) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8516) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8518) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8521) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8522) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8523) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8526) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,8528) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,8530) fake_sched.h:43: return __running_cpu;
    (<0.0>,8533) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8535) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8537) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8538) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,8539) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,8540) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,8548)
    (<0.0>,8549)
    (<0.0>,8550)
    (<0.0>,8551) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8554) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8557) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8560) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8561) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8564) tree.c:1818: return false;
    (<0.0>,8566) tree.c:1843: }
    (<0.0>,8569) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,8573) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,8574) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8577) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8578) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8589)
    (<0.0>,8590)
    (<0.0>,8591) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,8604)
    (<0.0>,8605) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8610) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8611) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8612) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8613) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8615) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8617) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8618) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8620) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8623) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8624) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8625) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8626) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8631) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8632) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8633) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8634) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8636) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8638) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8639) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8641) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8644) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8645) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8646) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8652) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,8668)
    (<0.0>,8669) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8672)
    (<0.0>,8673) tree.c:625: return &rsp->node[0];
    (<0.0>,8677) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8678) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8683) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8684) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8685) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8686) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8688) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8690) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8691) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8693) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8696) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8697) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8698) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8702) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8703) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8705) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8708) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8709) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8713) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8714) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8715) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8716) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8718) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8720) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8721) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8723) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8726) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8727) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8728) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8732) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,8734) tree.c:666: }
    (<0.0>,8739) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8744) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8745) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8746) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8747) fake_defs.h:237: switch (size) {
    (<0.0>,8749) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,8751) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,8752) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,8754) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,8757) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8758) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8759) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8763) tree.c:2174: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,8766)
    (<0.0>,8767) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8771)
    (<0.0>,8772) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8773) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8778) fake_sched.h:43: return __running_cpu;
    (<0.0>,8782) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,8784) fake_sched.h:43: return __running_cpu;
    (<0.0>,8788) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8796) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,8798) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,8803) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,8805) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,8809) fake_sched.h:43: return __running_cpu;
    (<0.0>,8813) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8817) fake_sched.h:43: return __running_cpu;
    (<0.0>,8821) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8826) fake_sched.h:43: return __running_cpu;
    (<0.0>,8830) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8841) fake_sched.h:43: return __running_cpu;
    (<0.0>,8845) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8846) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,8848) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,8849) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,8850) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,8852) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,8854) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,8855) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8856) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8857) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8858) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8859) tree.c:942: if (oldval)
    (<0.0>,8867)
    (<0.0>,8873)
    (<0.0>,8882) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8883) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8884) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8888) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8889) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8890) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8892) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8897) fake_sched.h:43: return __running_cpu;
    (<0.0>,8900) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8902) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8905) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8907) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8909) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8912) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8913) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8914) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8918) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8919) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8920) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8922) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8927) fake_sched.h:43: return __running_cpu;
    (<0.0>,8930) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8932) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8935) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8937) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8939) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8942) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8943) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8944) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8948) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8949) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8950) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8952) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8957) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,8962) fake_sched.h:43: return __running_cpu;
    (<0.0>,8967) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,8975) fake_sched.h:43: return __running_cpu;
    (<0.0>,8981) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,8995) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,8996) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,8997) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9001) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9002) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9003) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9005) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9009) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9011) fake_sched.h:43: return __running_cpu;
    (<0.0>,9014) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9016) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9038)
    (<0.0>,9039)
    (<0.0>,9040) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9042) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9043) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9044) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9046) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9048) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9049) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,9050) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,9085)
    (<0.0>,9086)
    (<0.0>,9087) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,9090) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,9103)
    (<0.0>,9104) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9109) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9110) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9111) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9112) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9114) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9116) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9117) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9119) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9122) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9123) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9124) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9125) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9130) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9131) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9132) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9133) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9135) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9137) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9138) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9140) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9143) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9144) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9145) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9153) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,9156)
    (<0.0>,9159) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,9162) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9164) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9167) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,9169) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,9173) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,9176)
    (<0.0>,9177) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9179) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9182) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9185) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,9188) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,9195) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,9197) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,9199) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,9200) tree.c:3517: return 1;
    (<0.0>,9202) tree.c:3548: }
    (<0.0>,9206) tree.c:3561: return 1;
    (<0.0>,9208) tree.c:3563: }
    (<0.0>,9215) fake_sched.h:43: return __running_cpu;
    (<0.0>,9219) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,9223) tree.c:2891: if (user)
    (<0.0>,9231) fake_sched.h:43: return __running_cpu;
    (<0.0>,9235) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9237) fake_sched.h:43: return __running_cpu;
    (<0.0>,9241) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9247) fake_sched.h:43: return __running_cpu;
    (<0.0>,9251) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,9261)
    (<0.0>,9264) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9265) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9266) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9270) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9271) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9272) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9274) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9278) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,9290)
    (<0.0>,9292) fake_sched.h:43: return __running_cpu;
    (<0.0>,9295) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,9297) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,9299) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,9300) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9302) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9309) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9310) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9312) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9318) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9319) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9320) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9321) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,9322) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,9326)
    (<0.0>,9327)
    (<0.0>,9328) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,9329) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,9354)
    (<0.0>,9355)
    (<0.0>,9356) tree.c:1905: local_irq_save(flags);
    (<0.0>,9359)
    (<0.0>,9361) fake_sched.h:43: return __running_cpu;
    (<0.0>,9365) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9367) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9371) fake_sched.h:43: return __running_cpu;
    (<0.0>,9375) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9380) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,9382) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,9383) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,9384) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9386) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9387) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9392) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9393) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9394) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9395) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9397) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9399) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9400) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9405) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9406) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9407) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9410) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9412) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9413) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9418) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9419) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9420) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9421) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9423) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9425) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9426) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9428) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9431) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9432) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9433) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9436) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9440) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9441) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9442) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9443) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9445) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9446) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9447) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9448) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9451) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9454) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9455) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9463) tree.c:1911: local_irq_restore(flags);
    (<0.0>,9466)
    (<0.0>,9468) fake_sched.h:43: return __running_cpu;
    (<0.0>,9472) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9474) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9478) fake_sched.h:43: return __running_cpu;
    (<0.0>,9482) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9489) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,9491) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,9496) tree.c:3016: local_irq_save(flags);
    (<0.0>,9499)
    (<0.0>,9501) fake_sched.h:43: return __running_cpu;
    (<0.0>,9505) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9507) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9511) fake_sched.h:43: return __running_cpu;
    (<0.0>,9515) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9520) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9521) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9532)
    (<0.0>,9533)
    (<0.0>,9534) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,9547)
    (<0.0>,9548) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9553) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9554) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9555) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9556) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9558) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9560) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9561) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9563) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9566) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9567) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9569) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9574) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9575) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9576) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9577) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9579) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9581) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9582) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9584) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9587) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9588) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9589) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9595) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,9611)
    (<0.0>,9612) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9615)
    (<0.0>,9616) tree.c:625: return &rsp->node[0];
    (<0.0>,9620) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9621) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9626) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9627) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9628) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9629) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9631) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9633) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9634) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9636) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9639) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9640) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9641) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9645) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9646) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9648) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9651) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9652) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9656) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9657) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9658) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9659) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9661) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9663) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9664) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9666) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9669) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9670) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9671) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9675) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,9677) tree.c:666: }
    (<0.0>,9680) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,9683)
    (<0.0>,9684) tree.c:625: return &rsp->node[0];
    (<0.0>,9690)
    (<0.0>,9691) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,9695)
    (<0.0>,9697) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,9698) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,9705) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,9711)
    (<0.0>,9713) fake_sched.h:43: return __running_cpu;
    (<0.0>,9716) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9718) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9720) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9721) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9724)
    (<0.0>,9725) tree.c:625: return &rsp->node[0];
    (<0.0>,9729) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9730) tree.c:2334: bool ret = false;
    (<0.0>,9731) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,9732) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,9733) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,9741)
    (<0.0>,9742)
    (<0.0>,9743)
    (<0.0>,9744) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9747) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9750) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9753) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9754) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9757) tree.c:1818: return false;
    (<0.0>,9759) tree.c:1843: }
    (<0.0>,9762) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,9766) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,9767) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,9768) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,9769) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,9780)
    (<0.0>,9781)
    (<0.0>,9782)
    (<0.0>,9783) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9785) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9788) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9789) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9800)
    (<0.0>,9801)
    (<0.0>,9802) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,9815)
    (<0.0>,9816) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9821) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9822) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9823) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9824) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9826) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9834) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9835) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9836) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9837) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9842) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9843) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9844) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9845) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9847) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9849) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9850) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9852) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9855) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9856) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9857) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9863) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,9879)
    (<0.0>,9880) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9883)
    (<0.0>,9884) tree.c:625: return &rsp->node[0];
    (<0.0>,9888) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9889) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9894) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9895) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9896) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9897) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9899) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9901) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9902) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9904) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9907) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9908) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9909) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9913) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9914) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9916) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9919) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9920) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9924) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9925) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9926) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9927) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9929) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9932) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9934) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9937) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9938) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9939) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9943) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,9945) tree.c:666: }
    (<0.0>,9950) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9955) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9956) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9957) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9958) fake_defs.h:237: switch (size) {
    (<0.0>,9960) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9962) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9963) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9965) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9968) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9969) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9970) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9973) tree.c:2318: return true;
    (<0.0>,9975) tree.c:2319: }
    (<0.0>,9979) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,9980) tree.c:2346: return ret;
    (<0.0>,9984) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,9988) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,9989) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,9990) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,9993)
    (<0.0>,9994) tree.c:625: return &rsp->node[0];
    (<0.0>,9999) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,10003)
    (<0.0>,10004)
    (<0.0>,10005) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10006) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10009) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,10012)
    (<0.0>,10014) fake_sched.h:43: return __running_cpu;
    (<0.0>,10018) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10020) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10024) fake_sched.h:43: return __running_cpu;
    (<0.0>,10028) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10036) tree.c:3021: if (needwake)
    (<0.0>,10039) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,10047)
    (<0.0>,10048) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,10049) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,10051) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,10058) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,10061)
    (<0.0>,10062) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10064) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10067) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10070) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10073) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10080) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,10081) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,10090)
    (<0.0>,10091)
    (<0.0>,10094) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10095) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10096) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10097) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10099) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10101) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10102) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10104) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10107) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10108) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10109) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10118) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,10120) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,10129) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,10130) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,10152)
    (<0.0>,10153)
    (<0.0>,10154) tree.c:2767: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,10157)
    (<0.0>,10158) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10160) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10163) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10166) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10169) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10176) tree.c:2779: local_irq_save(flags);
    (<0.0>,10179)
    (<0.0>,10181) fake_sched.h:43: return __running_cpu;
    (<0.0>,10185) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10187) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10191) fake_sched.h:43: return __running_cpu;
    (<0.0>,10195) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10200) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10201) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10202) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10203) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10204) tree.c:2781: bl = rdp->blimit;
    (<0.0>,10206) tree.c:2781: bl = rdp->blimit;
    (<0.0>,10207) tree.c:2781: bl = rdp->blimit;
    (<0.0>,10210) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,10212) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,10213) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,10214) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10217) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10218) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10219) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10221) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10222) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,10225) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,10226) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,10227) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10230) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10231) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10232) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10234) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10237) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10239) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10242) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10243) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10246) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10249) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10251) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10253) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10256) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10259) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10261) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10263) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10266) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10268) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10271) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10272) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10275) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10278) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10280) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10282) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10285) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10288) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10290) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10292) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10295) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10297) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10300) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10301) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10304) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10307) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10309) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10311) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10314) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10317) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10319) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10321) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10324) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10326) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10329) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10330) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10333) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10336) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10338) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10340) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10343) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10346) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10348) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10350) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10353) tree.c:2790: local_irq_restore(flags);
    (<0.0>,10356)
    (<0.0>,10358) fake_sched.h:43: return __running_cpu;
    (<0.0>,10362) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10364) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10368) fake_sched.h:43: return __running_cpu;
    (<0.0>,10372) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10377) tree.c:2793: count = count_lazy = 0;
    (<0.0>,10378) tree.c:2793: count = count_lazy = 0;
    (<0.0>,10380) tree.c:2794: while (list) {
    (<0.0>,10383) tree.c:2795: next = list->next;
    (<0.0>,10385) tree.c:2795: next = list->next;
    (<0.0>,10386) tree.c:2795: next = list->next;
    (<0.0>,10389) tree.c:2797: debug_rcu_head_unqueue(list);
    (<0.0>,10392)
    (<0.0>,10394) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,10396) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,10397) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,10403)
    (<0.0>,10404)
    (<0.0>,10405) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,10408) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,10410) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,10413) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,10416) rcu.h:118: head->func(head);
    (<0.0>,10419) rcu.h:118: head->func(head);
    (<0.0>,10420) rcu.h:118: head->func(head);
    (<0.0>,10426)
    (<0.0>,10427) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10428) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10429) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10433) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10434) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10435) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10436) update.c:341: complete(&rcu->completion);
    (<0.0>,10440)
    (<0.0>,10441) fake_sync.h:288: x->done++;
    (<0.0>,10443) fake_sync.h:288: x->done++;
    (<0.0>,10445) fake_sync.h:288: x->done++;
    (<0.0>,10450) rcu.h:120: return false;
    (<0.0>,10452) rcu.h:122: }
    (<0.0>,10455) tree.c:2800: list = next;
    (<0.0>,10456) tree.c:2800: list = next;
    (<0.0>,10457) tree.c:2802: if (++count >= bl &&
    (<0.0>,10459) tree.c:2802: if (++count >= bl &&
    (<0.0>,10460) tree.c:2802: if (++count >= bl &&
    (<0.0>,10464) tree.c:2794: while (list) {
    (<0.0>,10467) tree.c:2808: local_irq_save(flags);
    (<0.0>,10470)
    (<0.0>,10472) fake_sched.h:43: return __running_cpu;
    (<0.0>,10476) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10478) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10482) fake_sched.h:43: return __running_cpu;
    (<0.0>,10486) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10493) tree.c:2814: if (list != NULL) {
    (<0.0>,10497) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,10498) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,10500) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,10502) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,10504) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10506) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10507) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10509) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10510) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10515) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10516) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10517) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10518) fake_defs.h:237: switch (size) {
    (<0.0>,10520) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,10522) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,10523) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,10525) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,10528) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10529) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10530) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10531) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,10532) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,10534) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,10536) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,10537) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,10539) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,10542) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,10544) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,10547) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,10549) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,10552) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,10554) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,10555) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,10557) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,10558) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,10563) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10565) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10568) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10570) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10577) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10578) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10580) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10583) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10585) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10591) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10592) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10593) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10594) tree.c:2840: local_irq_restore(flags);
    (<0.0>,10597)
    (<0.0>,10599) fake_sched.h:43: return __running_cpu;
    (<0.0>,10603) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10605) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10609) fake_sched.h:43: return __running_cpu;
    (<0.0>,10613) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10618) tree.c:2843: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,10621)
    (<0.0>,10622) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10624) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10627) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10638) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10641)
    (<0.0>,10645) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10648) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10649) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10650) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10654) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10655) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10656) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10658) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10662) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,10674)
    (<0.0>,10676) fake_sched.h:43: return __running_cpu;
    (<0.0>,10679) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10681) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10683) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10684) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10686) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10693) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10694) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10696) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10702) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10703) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10704) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10705) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10706) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10710)
    (<0.0>,10711)
    (<0.0>,10712) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,10713) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,10738)
    (<0.0>,10739)
    (<0.0>,10740) tree.c:1905: local_irq_save(flags);
    (<0.0>,10743)
    (<0.0>,10745) fake_sched.h:43: return __running_cpu;
    (<0.0>,10749) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10751) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10755) fake_sched.h:43: return __running_cpu;
    (<0.0>,10759) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10764) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10766) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10767) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10768) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10770) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10771) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10776) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10777) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10778) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10779) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10781) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10783) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10784) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10786) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10789) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10790) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10791) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10794) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10796) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10797) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10802) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10803) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10804) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10805) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10807) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10809) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10810) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10812) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10815) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10816) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10817) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10820) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10824) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10825) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10826) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10830) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10835) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10838) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10839) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10847) tree.c:1911: local_irq_restore(flags);
    (<0.0>,10850)
    (<0.0>,10852) fake_sched.h:43: return __running_cpu;
    (<0.0>,10856) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10858) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10862) fake_sched.h:43: return __running_cpu;
    (<0.0>,10866) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10873) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,10875) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,10880) tree.c:3016: local_irq_save(flags);
    (<0.0>,10883)
    (<0.0>,10885) fake_sched.h:43: return __running_cpu;
    (<0.0>,10889) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10891) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10895) fake_sched.h:43: return __running_cpu;
    (<0.0>,10899) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10904) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10905) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10916)
    (<0.0>,10917)
    (<0.0>,10918) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,10931)
    (<0.0>,10932) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10937) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10938) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10939) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10940) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10944) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10945) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10947) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10950) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10951) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10952) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10953) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10958) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10959) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10960) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10963) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10965) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10966) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10968) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10971) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10972) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10973) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10979) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,10995)
    (<0.0>,10996) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10999)
    (<0.0>,11000) tree.c:625: return &rsp->node[0];
    (<0.0>,11004) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11005) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11010) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11011) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11012) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11013) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11015) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11017) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11018) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11020) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11023) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11024) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11025) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11029) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11030) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11032) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11035) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11036) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11040) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11041) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11042) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11043) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11045) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11047) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11048) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11050) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11053) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11054) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11055) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11059) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11062) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11065) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11068) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11069) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11072) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11074) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11077) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11080) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11083) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11084) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11086) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11089) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11093) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11095) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11097) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11100) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11103) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11106) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11107) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11109) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11112) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11116) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11118) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11120) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11123) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,11125) tree.c:666: }
    (<0.0>,11128) tree.c:3024: local_irq_restore(flags);
    (<0.0>,11131)
    (<0.0>,11133) fake_sched.h:43: return __running_cpu;
    (<0.0>,11137) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11139) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11143) fake_sched.h:43: return __running_cpu;
    (<0.0>,11147) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11153) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11156)
    (<0.0>,11157) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11159) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11162) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11169) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11172)
    (<0.0>,11176) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11179) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11180) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11181) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11185) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11186) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11187) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11189) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11197) fake_sched.h:43: return __running_cpu;
    (<0.0>,11201) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,11211) fake_sched.h:43: return __running_cpu;
    (<0.0>,11215) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11216) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,11218) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,11219) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,11220) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,11222) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,11224) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,11225) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11226) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11227) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11228) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11229) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,11231) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,11239)
    (<0.0>,11245) fake_sched.h:43: return __running_cpu;
    (<0.0>,11249)
    (<0.0>,11252) tree.c:755: local_irq_save(flags);
    (<0.0>,11255)
    (<0.0>,11257) fake_sched.h:43: return __running_cpu;
    (<0.0>,11261) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11263) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11267) fake_sched.h:43: return __running_cpu;
    (<0.0>,11271) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11283)
    (<0.0>,11285) fake_sched.h:43: return __running_cpu;
    (<0.0>,11289) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11290) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,11292) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,11293) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,11294) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11295) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11296) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11297) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11298) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,11302) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,11304) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,11305) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11306) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11322)
    (<0.0>,11324)
    (<0.0>,11326) fake_sched.h:43: return __running_cpu;
    (<0.0>,11330) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11333) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11334) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11335) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11339) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11340) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11341) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11343) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11348) fake_sched.h:43: return __running_cpu;
    (<0.0>,11351) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11353) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11355) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11356) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11359)
    (<0.0>,11362) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11365) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11366) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11367) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11371) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11372) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11373) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11375) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11380) fake_sched.h:43: return __running_cpu;
    (<0.0>,11383) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11385) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11387) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11388) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11391)
    (<0.0>,11394) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11397) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11398) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11399) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11403) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11404) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11405) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11407) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11414) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11417) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11418) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11419) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11421) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11422) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11424) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11425) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11426) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11427) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11441)
    (<0.0>,11443) tree.c:758: local_irq_restore(flags);
    (<0.0>,11446)
    (<0.0>,11448) fake_sched.h:43: return __running_cpu;
    (<0.0>,11452) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11454) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11458) fake_sched.h:43: return __running_cpu;
    (<0.0>,11462) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11468) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11471) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11476) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11481) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11482) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11483) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11484) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11486) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11488) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11489) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11491) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11494) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11495) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11496) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11503) fake_sched.h:43: return __running_cpu;
    (<0.0>,11507)
    (<0.0>,11508) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,11511) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,11516) tree.c:892: local_irq_save(flags);
    (<0.0>,11519)
    (<0.0>,11521) fake_sched.h:43: return __running_cpu;
    (<0.0>,11525) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11527) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11531) fake_sched.h:43: return __running_cpu;
    (<0.0>,11535) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11547)
    (<0.0>,11549) fake_sched.h:43: return __running_cpu;
    (<0.0>,11553) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11554) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,11556) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,11557) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,11558) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,11559) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,11560) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,11561) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,11562) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,11566) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,11568) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,11569) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,11570) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,11581)
    (<0.0>,11582)
    (<0.0>,11584) fake_sched.h:43: return __running_cpu;
    (<0.0>,11588) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11592) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11595) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11596) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11597) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11599) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11600) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11602) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11603) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11604) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11605) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11615)
    (<0.0>,11617) tree.c:895: local_irq_restore(flags);
    (<0.0>,11620)
    (<0.0>,11622) fake_sched.h:43: return __running_cpu;
    (<0.0>,11626) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11628) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11632) fake_sched.h:43: return __running_cpu;
    (<0.0>,11636) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11643) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,11645) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,11646) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,11691)
    (<0.0>,11692) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11695)
    (<0.0>,11696) tree.c:625: return &rsp->node[0];
    (<0.0>,11700) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11702) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11703) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11704) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11709) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11710) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11711) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11712) fake_defs.h:237: switch (size) {
    (<0.0>,11714) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11716) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11717) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11719) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11722) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11723) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11724) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11725) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,11728)
    (<0.0>,11729) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,11733)
    (<0.0>,11736) fake_sched.h:43: return __running_cpu;
    (<0.0>,11740) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,11744) fake_sched.h:43: return __running_cpu;
    (<0.0>,11748) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11753) fake_sched.h:43: return __running_cpu;
    (<0.0>,11757) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,11760) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,11761) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,11768) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11773) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11774) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11775) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11776) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11778) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11780) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11781) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11783) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11786) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11787) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11788) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11799)
    (<0.0>,11805)
    (<0.0>,11810) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11815) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11816) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11817) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11818) fake_defs.h:237: switch (size) {
    (<0.0>,11820) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11822) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11823) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11825) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11828) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11829) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11830) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11831) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11844)
    (<0.0>,11845) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11850) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11851) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11852) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11855) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11857) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11858) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11860) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11863) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11864) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11865) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11866) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11871) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11872) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11873) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11878) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11879) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11881) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11885) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11886) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11894) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11895) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11908)
    (<0.0>,11909) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11914) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11915) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11916) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11917) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11919) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11921) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11922) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11924) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11927) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11928) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11929) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11930) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11935) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11936) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11937) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11938) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11940) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11943) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11945) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11948) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11949) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11950) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11957) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11958) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11959) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11962) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,11977)
    (<0.0>,11978) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,11979) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,11980) tree.c:1240: rsp->gp_start = j;
    (<0.0>,11981) tree.c:1240: rsp->gp_start = j;
    (<0.0>,11983) tree.c:1240: rsp->gp_start = j;
    (<0.0>,12004) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12005) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12006) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12007) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12009) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12011) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12012) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12014) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12017) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12018) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12019) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12020) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12021) update.c:466: if (till_stall_check < 3) {
    (<0.0>,12024) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,12028) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,12033) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,12035) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12036) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12038) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12039) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12044) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12045) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12046) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12047) fake_defs.h:237: switch (size) {
    (<0.0>,12049) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12051) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12052) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12054) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12057) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12058) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12059) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12060) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12061) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12064) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12066) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12067) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12072) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12073) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12074) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12075) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12077) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12079) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12080) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12082) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12085) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12086) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12087) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12088) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12090) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12094) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12096) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12098) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12099) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12101) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12102) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12103) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12107) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,12110)
    (<0.0>,12111) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12115)
    (<0.0>,12116) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12117) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12122) fake_sched.h:43: return __running_cpu;
    (<0.0>,12126) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12128) fake_sched.h:43: return __running_cpu;
    (<0.0>,12132) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12139) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12142) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12145) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12146) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12148) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12149) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12151) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12156) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,12157) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,12161)
    (<0.0>,12162)
    (<0.0>,12163) tree.c:1922: if (delay > 0 &&
    (<0.0>,12167) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,12170)
    (<0.0>,12171) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12175)
    (<0.0>,12178) fake_sched.h:43: return __running_cpu;
    (<0.0>,12182) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12186) fake_sched.h:43: return __running_cpu;
    (<0.0>,12190) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12195) fake_sched.h:43: return __running_cpu;
    (<0.0>,12199) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12202) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12203) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12210) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12212) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12213) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12215) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12218) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,12220) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,12223) tree.c:1973: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,12226)
    (<0.0>,12227) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12231)
    (<0.0>,12232) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12233) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12238) fake_sched.h:43: return __running_cpu;
    (<0.0>,12242) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12244) fake_sched.h:43: return __running_cpu;
    (<0.0>,12248) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12256) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12258) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12260) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12261) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12263) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12268) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12271) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12273) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12274) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12276) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12281) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,12282) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,12286)
    (<0.0>,12287)
    (<0.0>,12288) tree.c:1922: if (delay > 0 &&
    (<0.0>,12292) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,12295)
    (<0.0>,12296) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12300)
    (<0.0>,12303) fake_sched.h:43: return __running_cpu;
    (<0.0>,12307) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12311) fake_sched.h:43: return __running_cpu;
    (<0.0>,12315) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12320) fake_sched.h:43: return __running_cpu;
    (<0.0>,12324) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12327) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12328) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12336) fake_sched.h:43: return __running_cpu;
    (<0.0>,12339) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12341) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12343) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12344) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,12350)
    (<0.0>,12351) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12353) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12358) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12359) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12361) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12365) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12366) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12367) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12369) tree.c:2028: rnp->qsmask = 0;
    (<0.0>,12371) tree.c:2028: rnp->qsmask = 0;
    (<0.0>,12373) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12375) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12376) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12377) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12382) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12383) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12384) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12385) fake_defs.h:237: switch (size) {
    (<0.0>,12387) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12389) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12390) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12392) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12395) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12396) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12397) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12398) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12400) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12401) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12403) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12408) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12409) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12411) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12412) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12414) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12418) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12419) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12420) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12423) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,12424) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,12426) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,12429) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,12430) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,12431) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,12453)
    (<0.0>,12454)
    (<0.0>,12455)
    (<0.0>,12456) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,12458) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,12459) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,12461) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,12464) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12468) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12469) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12470) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12471) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12473) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12474) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12475) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12476) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12479) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12482) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12483) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12491) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,12492) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,12493) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,12502)
    (<0.0>,12503)
    (<0.0>,12504)
    (<0.0>,12505) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,12508) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,12511) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,12514) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,12515) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,12518) tree.c:1750: return false;
    (<0.0>,12520) tree.c:1799: }
    (<0.0>,12523) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,12525) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12527) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12528) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12530) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12533) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,12535) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,12536) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,12538) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,12541) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,12543) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,12544) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,12546) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,12552) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,12553) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,12556) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,12560) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,12562) fake_sched.h:43: return __running_cpu;
    (<0.0>,12566) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,12567) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,12569) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,12570) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,12572) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,12575) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,12576) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,12579)
    (<0.0>,12580) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,12582) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,12583) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,12585) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,12595)
    (<0.0>,12600) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12604) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12605) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12606) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12607) fake_defs.h:237: switch (size) {
    (<0.0>,12609) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,12610) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,12611) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,12612) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,12615) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12618) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12619) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12622) tree.c:1896: return ret;
    (<0.0>,12626) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,12629)
    (<0.0>,12633) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,12636)
    (<0.0>,12637) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12641)
    (<0.0>,12642) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12643) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12648) fake_sched.h:43: return __running_cpu;
    (<0.0>,12652) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12654) fake_sched.h:43: return __running_cpu;
    (<0.0>,12658) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12673) fake_sched.h:43: return __running_cpu;
    (<0.0>,12679) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,12687) fake_sched.h:43: return __running_cpu;
    (<0.0>,12691) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,12704) fake_sched.h:43: return __running_cpu;
    (<0.0>,12708)
    (<0.0>,12711) tree.c:755: local_irq_save(flags);
    (<0.0>,12714)
    (<0.0>,12716) fake_sched.h:43: return __running_cpu;
    (<0.0>,12720) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12722) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12726) fake_sched.h:43: return __running_cpu;
    (<0.0>,12730) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12742)
    (<0.0>,12744) fake_sched.h:43: return __running_cpu;
    (<0.0>,12748) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12749) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12751) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12752) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12753) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12754) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12755) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12756) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12757) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,12761) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,12763) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,12764) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12765) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12781)
    (<0.0>,12783)
    (<0.0>,12785) fake_sched.h:43: return __running_cpu;
    (<0.0>,12789) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12792) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12793) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12794) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12798) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12799) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12800) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12802) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12807) fake_sched.h:43: return __running_cpu;
    (<0.0>,12810) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12812) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12814) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12815) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12818)
    (<0.0>,12821) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12824) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12825) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12826) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12830) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12831) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12832) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12834) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12839) fake_sched.h:43: return __running_cpu;
    (<0.0>,12842) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12844) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12846) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12847) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12850)
    (<0.0>,12853) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12856) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12857) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12858) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12862) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12863) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12864) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12866) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12873) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12876) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12877) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12878) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12880) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12881) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12883) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12884) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12885) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12886) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12900)
    (<0.0>,12902) tree.c:758: local_irq_restore(flags);
    (<0.0>,12905)
    (<0.0>,12907) fake_sched.h:43: return __running_cpu;
    (<0.0>,12911) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12913) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12917) fake_sched.h:43: return __running_cpu;
    (<0.0>,12921) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12927) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12930) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12935) fake_sched.h:43: return __running_cpu;
    (<0.0>,12939)
    (<0.0>,12940) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12943) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12948) tree.c:892: local_irq_save(flags);
    (<0.0>,12951)
    (<0.0>,12953) fake_sched.h:43: return __running_cpu;
    (<0.0>,12957) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12959) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12963) fake_sched.h:43: return __running_cpu;
    (<0.0>,12967) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12979)
    (<0.0>,12981) fake_sched.h:43: return __running_cpu;
    (<0.0>,12985) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12986) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12988) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12989) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12990) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12991) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12992) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12993) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12994) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,12998) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,13000) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,13001) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,13002) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,13013)
    (<0.0>,13014)
    (<0.0>,13016) fake_sched.h:43: return __running_cpu;
    (<0.0>,13020) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13024) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13027) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13028) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13029) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13031) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13032) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13034) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13035) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13036) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13037) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13047)
    (<0.0>,13049) tree.c:895: local_irq_restore(flags);
    (<0.0>,13052)
    (<0.0>,13054) fake_sched.h:43: return __running_cpu;
    (<0.0>,13058) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13060) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13064) fake_sched.h:43: return __running_cpu;
    (<0.0>,13068) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13082) fake_sched.h:43: return __running_cpu;
    (<0.0>,13086) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,13095) fake_sched.h:43: return __running_cpu;
    (<0.0>,13102) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,13111) fake_sched.h:43: return __running_cpu;
    (<0.0>,13115) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,13117) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,13123) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13124) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13125) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13130) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13131) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13132) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13133) fake_defs.h:237: switch (size) {
    (<0.0>,13135) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13137) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13138) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13140) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13143) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13144) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13145) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13147) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13149) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13151) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13152) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13154) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13159) tree.c:2046: return true;
    (<0.0>,13161) tree.c:2047: }
    (<0.0>,13165) tree.c:2214: first_gp_fqs = true;
    (<0.0>,13166) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,13167) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,13168) tree.c:2216: if (j > HZ) {
    (<0.0>,13171) tree.c:2220: ret = 0;
    (<0.0>,13173) tree.c:2222: if (!ret) {
    (<0.0>,13176) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13177) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13179) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13181) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13183) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13184) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13187) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13188) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13193) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13194) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13195) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13196) fake_defs.h:237: switch (size) {
    (<0.0>,13198) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13200) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13201) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13203) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13206) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13207) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13208) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13212) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,13214) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,13218) fake_sched.h:43: return __running_cpu;
    (<0.0>,13222) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,13226) fake_sched.h:43: return __running_cpu;
    (<0.0>,13230) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13235) fake_sched.h:43: return __running_cpu;
    (<0.0>,13239) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,13250) fake_sched.h:43: return __running_cpu;
    (<0.0>,13254) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13255) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,13257) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,13258) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,13259) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,13261) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,13263) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,13264) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13265) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13266) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13267) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13268) tree.c:942: if (oldval)
    (<0.0>,13276)
    (<0.0>,13282)
    (<0.0>,13291) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13292) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13293) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13297) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13298) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13299) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13301) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13306) fake_sched.h:43: return __running_cpu;
    (<0.0>,13309) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13311) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13314) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13316) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13318) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13321) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13322) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13323) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13327) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13328) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13329) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13331) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13336) fake_sched.h:43: return __running_cpu;
    (<0.0>,13339) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13341) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13344) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13346) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13348) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13351) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13352) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13353) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13357) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13358) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13359) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13361) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13366) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,13371) fake_sched.h:43: return __running_cpu;
    (<0.0>,13376) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,13384) fake_sched.h:43: return __running_cpu;
    (<0.0>,13390) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,13404) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13405) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13406) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13410) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13411) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13412) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13414) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13418) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13420) fake_sched.h:43: return __running_cpu;
    (<0.0>,13423) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13425) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13447)
    (<0.0>,13448)
    (<0.0>,13449) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13451) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13452) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13453) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13455) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13457) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13458) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,13459) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,13494)
    (<0.0>,13495)
    (<0.0>,13496) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,13499) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,13512)
    (<0.0>,13513) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13518) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13519) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13520) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13521) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13523) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13525) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13526) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13528) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13531) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13532) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13533) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13534) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13539) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13540) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13541) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13542) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13544) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13546) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13547) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13549) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13552) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13553) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13554) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13560) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,13585)
    (<0.0>,13586) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,13591) tree.c:1472: j = jiffies;
    (<0.0>,13592) tree.c:1472: j = jiffies;
    (<0.0>,13593) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13598) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13599) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13600) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13601) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13603) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13605) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13606) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13608) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13611) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13612) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13613) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13614) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13616) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13621) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13622) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13623) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13624) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13626) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13628) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13629) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13631) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13634) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13635) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13636) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13637) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13639) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13644) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13645) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13646) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13647) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13649) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13651) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13652) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13654) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13657) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13658) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13659) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13660) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13662) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13667) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13668) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13669) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13670) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13672) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13674) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13675) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13677) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13680) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13681) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13682) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13683) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13684) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,13685) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,13689) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,13690) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,13696) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,13699)
    (<0.0>,13702) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,13705) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,13707) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,13710) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,13712) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,13716) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,13719)
    (<0.0>,13720) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,13722) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,13725) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,13732) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,13733) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,13744)
    (<0.0>,13745)
    (<0.0>,13746) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,13759)
    (<0.0>,13760) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13765) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13766) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13767) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13770) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13772) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13773) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13775) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13778) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13779) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13780) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13781) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13786) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13787) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13788) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13789) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13791) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13793) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13794) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13796) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13799) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13800) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13801) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13807) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,13809) tree.c:666: }
    (<0.0>,13812) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13817) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13818) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13819) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13820) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13822) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13824) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13825) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13830) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13831) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13832) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13833) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13835) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13838) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13843) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13844) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13845) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13846) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13848) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13850) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13856) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13857) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13858) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13859) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13861) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13864) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13868) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13869) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13870) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13871) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13873) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13875) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13879) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13882) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13883) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13891) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,13894)
    (<0.0>,13898) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,13900) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,13902) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,13903) tree.c:3547: return 0;
    (<0.0>,13905) tree.c:3548: }
    (<0.0>,13910) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13913) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13914) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13915) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13919) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13920) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13921) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13923) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13927) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13929) fake_sched.h:43: return __running_cpu;
    (<0.0>,13932) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13934) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13956)
    (<0.0>,13957)
    (<0.0>,13958) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13960) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13961) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13962) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13964) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13966) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13967) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,13968) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,14003)
    (<0.0>,14004)
    (<0.0>,14005) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,14008) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,14021)
    (<0.0>,14022) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14027) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14028) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14029) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14030) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14032) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14034) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14035) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14037) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14040) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14041) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14042) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14043) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14048) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14049) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14050) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14053) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14055) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14056) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14058) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14061) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14062) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14063) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14071) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,14074)
    (<0.0>,14077) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,14080) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14082) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14085) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14087) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14091) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,14094)
    (<0.0>,14095) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,14097) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,14100) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,14107) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,14108) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,14119)
    (<0.0>,14120)
    (<0.0>,14121) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,14134)
    (<0.0>,14135) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14140) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14141) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14142) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14143) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14145) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14147) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14148) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14150) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14153) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14154) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14155) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14156) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14161) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14162) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14163) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14164) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14166) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14168) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14169) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14171) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14174) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14175) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14176) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14182) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,14198)
    (<0.0>,14199) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,14202)
    (<0.0>,14203) tree.c:625: return &rsp->node[0];
    (<0.0>,14207) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,14208) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14213) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14214) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14215) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14216) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14218) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14220) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14221) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14223) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14226) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14227) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14228) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14232) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14233) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,14235) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,14238) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,14239) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14243) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14244) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14245) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14246) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14248) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14250) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14251) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14253) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14256) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14257) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14258) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14262) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,14265) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,14268) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,14271) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,14272) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,14275) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14277) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14280) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14283) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14286) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14287) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14289) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14292) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14296) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14298) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14300) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14303) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14306) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14309) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14310) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14312) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14315) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14319) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14321) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14323) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14326) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,14328) tree.c:666: }
    (<0.0>,14331) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14336) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14337) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14338) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14339) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14341) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14343) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14344) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14346) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14349) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14350) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14351) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14352) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14354) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14357) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14362) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14363) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14364) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14365) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14367) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14369) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14370) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14372) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14375) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14376) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14377) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14378) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14380) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14383) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14387) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14388) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14389) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14390) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14392) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14393) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14394) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14395) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14398) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14401) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14402) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14410) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,14413)
    (<0.0>,14417) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,14419) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,14421) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,14422) tree.c:3547: return 0;
    (<0.0>,14424) tree.c:3548: }
    (<0.0>,14429) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14432) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14433) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14434) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14438) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14439) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14440) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14442) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14446) tree.c:3562: return 0;
    (<0.0>,14448) tree.c:3563: }
    (<0.0>,14452) tree.c:2891: if (user)
    (<0.0>,14460) fake_sched.h:43: return __running_cpu;
    (<0.0>,14464) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,14466) fake_sched.h:43: return __running_cpu;
    (<0.0>,14470) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14476) fake_sched.h:43: return __running_cpu;
    (<0.0>,14480) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,14491) fake_sched.h:43: return __running_cpu;
    (<0.0>,14495) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14496) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,14498) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,14499) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,14500) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,14502) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,14504) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,14505) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14506) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14507) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14508) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14509) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,14511) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,14519)
    (<0.0>,14525) fake_sched.h:43: return __running_cpu;
    (<0.0>,14529)
    (<0.0>,14532) tree.c:755: local_irq_save(flags);
    (<0.0>,14535)
    (<0.0>,14537) fake_sched.h:43: return __running_cpu;
    (<0.0>,14541) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14543) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14547) fake_sched.h:43: return __running_cpu;
    (<0.0>,14551) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14563)
    (<0.0>,14565) fake_sched.h:43: return __running_cpu;
    (<0.0>,14569) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14570) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,14572) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,14573) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,14574) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14575) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14576) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14577) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14578) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,14582) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,14584) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,14585) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,14586) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,14602)
    (<0.0>,14604)
    (<0.0>,14606) fake_sched.h:43: return __running_cpu;
    (<0.0>,14610) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14613) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14614) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14615) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14619) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14620) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14621) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14623) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14628) fake_sched.h:43: return __running_cpu;
    (<0.0>,14631) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14633) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14635) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14636) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,14639)
    (<0.0>,14642) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14645) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14646) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14647) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14651) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14652) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14653) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14655) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14660) fake_sched.h:43: return __running_cpu;
    (<0.0>,14663) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14665) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14667) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14668) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,14671)
    (<0.0>,14674) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14677) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14678) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14679) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14683) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14684) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14685) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14687) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14694) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14697) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14698) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14699) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14701) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14702) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14704) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14705) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14706) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14707) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14721)
    (<0.0>,14723) tree.c:758: local_irq_restore(flags);
    (<0.0>,14726)
    (<0.0>,14728) fake_sched.h:43: return __running_cpu;
    (<0.0>,14732) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14734) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14738) fake_sched.h:43: return __running_cpu;
    (<0.0>,14742) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14748) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,14751) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,14756) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,14772)
    (<0.0>,14773)
    (<0.0>,14774) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,14777)
    (<0.0>,14778) tree.c:625: return &rsp->node[0];
    (<0.0>,14782) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,14783) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14788) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14789) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14790) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14791) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14793) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14795) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14796) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14798) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14801) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14802) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14803) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14805) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14806) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14807) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,14808) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,14812) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14817) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14818) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14819) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14820) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14822) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14824) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14825) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14830) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14831) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14832) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14835) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14838)
    (<0.0>,14842) tree.c:2064: return true;
    (<0.0>,14844) tree.c:2067: }
    (<0.0>,14849) fake_sched.h:43: return __running_cpu;
    (<0.0>,14853)
    (<0.0>,14854) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,14857) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,14862) tree.c:892: local_irq_save(flags);
    (<0.0>,14865)
    (<0.0>,14867) fake_sched.h:43: return __running_cpu;
    (<0.0>,14871) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14873) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14877) fake_sched.h:43: return __running_cpu;
    (<0.0>,14881) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14893)
    (<0.0>,14895) fake_sched.h:43: return __running_cpu;
    (<0.0>,14899) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14900) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14902) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14903) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14904) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14905) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14906) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14907) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14908) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,14912) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,14914) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,14915) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,14916) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,14927)
    (<0.0>,14928)
    (<0.0>,14930) fake_sched.h:43: return __running_cpu;
    (<0.0>,14934) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14938) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14941) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14942) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14943) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14945) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14946) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14948) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14949) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14950) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14951) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14961)
    (<0.0>,14963) tree.c:895: local_irq_restore(flags);
    (<0.0>,14966)
    (<0.0>,14968) fake_sched.h:43: return __running_cpu;
    (<0.0>,14972) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14974) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14978) fake_sched.h:43: return __running_cpu;
    (<0.0>,14982) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14989) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,14990) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,14991) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,14992) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,14994) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,14995) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15000) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15001) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15002) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15003) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15005) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15007) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15008) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15010) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15013) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15014) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15015) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15018) tree.c:2237: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15021)
    (<0.0>,15026) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,15028) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,15029) tree.c:2280: rcu_gp_cleanup(rsp);
    (<0.0>,15069)
    (<0.0>,15070) tree.c:2109: bool needgp = false;
    (<0.0>,15071) tree.c:2110: int nocb = 0;
    (<0.0>,15072) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15075)
    (<0.0>,15076) tree.c:625: return &rsp->node[0];
    (<0.0>,15080) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15082) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15083) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15084) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15089) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15090) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15091) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15092) fake_defs.h:237: switch (size) {
    (<0.0>,15094) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15096) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15097) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15099) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15102) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15103) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15104) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15105) tree.c:2116: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,15108)
    (<0.0>,15109) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,15113)
    (<0.0>,15116) fake_sched.h:43: return __running_cpu;
    (<0.0>,15120) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,15124) fake_sched.h:43: return __running_cpu;
    (<0.0>,15128) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15133) fake_sched.h:43: return __running_cpu;
    (<0.0>,15137) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,15140) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,15141) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,15148) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,15149) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,15151) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,15153) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,15154) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,15155) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,15157) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,15160) tree.c:2129: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,15163)
    (<0.0>,15164) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,15168)
    (<0.0>,15169) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,15170) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,15175) fake_sched.h:43: return __running_cpu;
    (<0.0>,15179) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,15181) fake_sched.h:43: return __running_cpu;
    (<0.0>,15185) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15192) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,15195) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,15197) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,15198) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,15200) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,15205) tree.c:2141: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,15208)
    (<0.0>,15209) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,15213)
    (<0.0>,15216) fake_sched.h:43: return __running_cpu;
    (<0.0>,15220) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,15224) fake_sched.h:43: return __running_cpu;
    (<0.0>,15228) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15233) fake_sched.h:43: return __running_cpu;
    (<0.0>,15237) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,15240) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,15241) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,15248) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15251)
    (<0.0>,15257) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15258) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15261)
    (<0.0>,15266) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15267) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15268) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15269) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15271) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15276) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15277) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15279) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15283) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15284) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15285) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15287) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15289) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15290) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15291) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15296) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15297) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15298) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15299) fake_defs.h:237: switch (size) {
    (<0.0>,15301) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15303) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15304) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15306) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15309) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15310) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15311) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15313) fake_sched.h:43: return __running_cpu;
    (<0.0>,15316) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15318) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15320) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15321) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,15322) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,15324) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,15327) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,15328) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,15329) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,15351)
    (<0.0>,15352)
    (<0.0>,15353)
    (<0.0>,15354) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,15356) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,15357) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,15359) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,15362) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,15363) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,15364) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,15372)
    (<0.0>,15373)
    (<0.0>,15374)
    (<0.0>,15375) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15378) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15381) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15384) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15385) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15388) tree.c:1818: return false;
    (<0.0>,15390) tree.c:1843: }
    (<0.0>,15393) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,15394) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,15396) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,15397) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,15399) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,15403) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15405) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15406) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15408) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15411) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15415) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15416) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15417) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15418) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15420) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15421) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15422) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15423) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15426) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15429) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15430) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15438) tree.c:1896: return ret;
    (<0.0>,15442) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,15446) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,15448) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,15449) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,15456)
    (<0.0>,15457)
    (<0.0>,15458) tree.c:1702: int c = rnp->completed;
    (<0.0>,15460) tree.c:1702: int c = rnp->completed;
    (<0.0>,15462) tree.c:1702: int c = rnp->completed;
    (<0.0>,15464) fake_sched.h:43: return __running_cpu;
    (<0.0>,15467) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15469) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15471) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15472) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,15475) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,15478) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,15479) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,15483) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,15486) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,15487) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,15488) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,15489) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,15490) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,15492) tree.c:1709: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,15500)
    (<0.0>,15501)
    (<0.0>,15502)
    (<0.0>,15503)
    (<0.0>,15507) tree.c:1710: return needmore;
    (<0.0>,15509) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,15511) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,15512) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,15515)
    (<0.0>,15517) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,15518) tree.c:2151: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,15521)
    (<0.0>,15522) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,15526)
    (<0.0>,15527) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,15528) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,15533) fake_sched.h:43: return __running_cpu;
    (<0.0>,15537) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,15539) fake_sched.h:43: return __running_cpu;
    (<0.0>,15543) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15550) tree.c:2152: rcu_nocb_gp_cleanup(sq);
    (<0.0>,15553)
    (<0.0>,15563) fake_sched.h:43: return __running_cpu;
    (<0.0>,15569) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,15577) fake_sched.h:43: return __running_cpu;
    (<0.0>,15581) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,15594) fake_sched.h:43: return __running_cpu;
    (<0.0>,15598)
    (<0.0>,15601) tree.c:755: local_irq_save(flags);
    (<0.0>,15604)
    (<0.0>,15606) fake_sched.h:43: return __running_cpu;
    (<0.0>,15610) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15612) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15616) fake_sched.h:43: return __running_cpu;
    (<0.0>,15620) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15632)
    (<0.0>,15634) fake_sched.h:43: return __running_cpu;
    (<0.0>,15638) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15639) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15641) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15642) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15643) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15644) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15645) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15646) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15647) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,15651) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,15653) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,15654) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,15655) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,15671)
    (<0.0>,15673)
    (<0.0>,15675) fake_sched.h:43: return __running_cpu;
    (<0.0>,15679) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15682) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15683) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15684) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15688) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15689) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15690) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15692) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15697) fake_sched.h:43: return __running_cpu;
    (<0.0>,15700) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15702) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15704) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15705) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15708)
    (<0.0>,15711) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15714) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15715) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15716) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15720) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15721) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15722) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15724) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15729) fake_sched.h:43: return __running_cpu;
    (<0.0>,15732) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15734) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15736) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15737) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15740)
    (<0.0>,15743) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15746) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15747) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15748) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15752) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15753) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15754) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15756) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15763) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15766) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15767) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15768) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15770) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15771) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15773) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15774) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15775) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15776) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15790)
    (<0.0>,15792) tree.c:758: local_irq_restore(flags);
    (<0.0>,15795)
    (<0.0>,15797) fake_sched.h:43: return __running_cpu;
    (<0.0>,15801) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15803) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15807) fake_sched.h:43: return __running_cpu;
    (<0.0>,15811) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15817) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,15820) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,15825) fake_sched.h:43: return __running_cpu;
    (<0.0>,15829)
    (<0.0>,15830) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,15833) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,15838) tree.c:892: local_irq_save(flags);
    (<0.0>,15841)
    (<0.0>,15843) fake_sched.h:43: return __running_cpu;
    (<0.0>,15847) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15849) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15853) fake_sched.h:43: return __running_cpu;
    (<0.0>,15857) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15869)
    (<0.0>,15871) fake_sched.h:43: return __running_cpu;
    (<0.0>,15875) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15876) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,15878) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,15879) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,15880) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,15881) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,15882) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,15883) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,15884) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,15888) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,15890) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,15891) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,15892) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,15903)
    (<0.0>,15904)
    (<0.0>,15906) fake_sched.h:43: return __running_cpu;
    (<0.0>,15910) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15914) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15917) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15918) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15919) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15921) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15922) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15924) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15925) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15926) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15927) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15937)
    (<0.0>,15939) tree.c:895: local_irq_restore(flags);
    (<0.0>,15942)
    (<0.0>,15944) fake_sched.h:43: return __running_cpu;
    (<0.0>,15948) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15950) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15954) fake_sched.h:43: return __running_cpu;
    (<0.0>,15958) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15972) fake_sched.h:43: return __running_cpu;
    (<0.0>,15976) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,15985) fake_sched.h:43: return __running_cpu;
    (<0.0>,15992) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,16001) fake_sched.h:43: return __running_cpu;
    (<0.0>,16005) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,16007) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,16013) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16014) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16015) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16020) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16021) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16022) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16023) fake_defs.h:237: switch (size) {
    (<0.0>,16025) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16027) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16028) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16030) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16033) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16034) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16035) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16036) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,16037) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,16041)
    (<0.0>,16042)
    (<0.0>,16043) tree.c:1922: if (delay > 0 &&
    (<0.0>,16048) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,16050) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,16052) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,16053) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,16055) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,16060) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,16063)
    (<0.0>,16064) tree.c:625: return &rsp->node[0];
    (<0.0>,16068) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,16069) tree.c:2158: raw_spin_lock_irq_rcu_node(rnp); /* Order GP before ->completed update. */
    (<0.0>,16072)
    (<0.0>,16073) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,16077)
    (<0.0>,16080) fake_sched.h:43: return __running_cpu;
    (<0.0>,16084) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,16088) fake_sched.h:43: return __running_cpu;
    (<0.0>,16092) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,16097) fake_sched.h:43: return __running_cpu;
    (<0.0>,16101) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,16104) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,16105) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,16112) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,16113) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,16117)
    (<0.0>,16118)
    (<0.0>,16121) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16123) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16124) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16125) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16130) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16131) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16132) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16133) fake_defs.h:237: switch (size) {
    (<0.0>,16135) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16137) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16138) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16140) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16143) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16144) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16145) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16148) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,16150) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,16152) fake_sched.h:43: return __running_cpu;
    (<0.0>,16155) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16157) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16159) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16160) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,16161) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,16162) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,16170)
    (<0.0>,16171)
    (<0.0>,16172)
    (<0.0>,16173) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,16176) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,16179) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,16182) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,16183) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,16186) tree.c:1818: return false;
    (<0.0>,16188) tree.c:1843: }
    (<0.0>,16191) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,16195) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,16196) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,16199) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,16200) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,16211)
    (<0.0>,16212)
    (<0.0>,16213) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,16226)
    (<0.0>,16227) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16232) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16233) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16234) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16235) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16237) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16239) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16240) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16242) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16245) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16246) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16247) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16248) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16253) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16254) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16255) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16256) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16258) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16260) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16261) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16263) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16266) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16267) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16268) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16274) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,16290)
    (<0.0>,16291) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16294)
    (<0.0>,16295) tree.c:625: return &rsp->node[0];
    (<0.0>,16299) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16300) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16305) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16306) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16307) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16308) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16310) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16312) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16313) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16315) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16318) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16319) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16320) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16324) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16325) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16327) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16330) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16331) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16335) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16336) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16337) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16338) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16340) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16342) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16343) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16345) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16348) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16349) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16350) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16354) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,16357) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,16360) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,16363) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,16364) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,16367) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16369) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16372) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16375) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16378) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16379) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16381) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16384) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16388) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16390) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16392) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16395) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16398) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16401) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16402) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16404) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16407) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16411) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16413) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16415) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16418) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,16420) tree.c:666: }
    (<0.0>,16423) tree.c:2174: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,16426)
    (<0.0>,16427) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,16431)
    (<0.0>,16432) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,16433) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,16438) fake_sched.h:43: return __running_cpu;
    (<0.0>,16442) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,16444) fake_sched.h:43: return __running_cpu;
    (<0.0>,16448) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,16456) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,16458) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,16463) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,16465) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,16469) fake_sched.h:43: return __running_cpu;
    (<0.0>,16473) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,16477) fake_sched.h:43: return __running_cpu;
    (<0.0>,16481) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,16486) fake_sched.h:43: return __running_cpu;
    (<0.0>,16490) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,16501) fake_sched.h:43: return __running_cpu;
    (<0.0>,16505) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,16506) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,16508) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,16509) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,16510) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,16512) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,16514) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,16515) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16516) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16517) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16518) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16519) tree.c:942: if (oldval)
    (<0.0>,16527)
    (<0.0>,16533)
    (<0.0>,16542) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16543) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16544) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16548) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16549) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16550) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16552) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16557) fake_sched.h:43: return __running_cpu;
    (<0.0>,16560) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16562) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16565) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16567) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16569) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16572) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16573) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16574) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16578) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16579) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16580) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16582) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16587) fake_sched.h:43: return __running_cpu;
    (<0.0>,16590) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16592) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16595) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16597) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16599) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16602) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16603) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16604) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16608) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16609) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16610) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16612) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16617) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,16622) fake_sched.h:43: return __running_cpu;
    (<0.0>,16627) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,16635) fake_sched.h:43: return __running_cpu;
    (<0.0>,16641) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,16655) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16656) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16657) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16661) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16662) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16663) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16665) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16669) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,16671) fake_sched.h:43: return __running_cpu;
    (<0.0>,16674) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,16676) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,16698)
    (<0.0>,16699)
    (<0.0>,16700) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,16702) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,16703) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,16704) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,16706) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,16708) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,16709) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,16710) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,16745)
    (<0.0>,16746)
    (<0.0>,16747) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,16750) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,16763)
    (<0.0>,16764) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16769) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16770) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16771) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16772) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16774) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16776) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16777) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16779) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16782) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16783) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16784) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16785) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16790) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16791) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16792) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16793) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16795) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16797) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16798) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16800) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16803) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16804) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16805) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16813) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,16816)
    (<0.0>,16819) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,16822) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,16824) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,16827) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,16829) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,16833) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,16836)
    (<0.0>,16837) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16839) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16842) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16849) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,16850) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,16861)
    (<0.0>,16862)
    (<0.0>,16863) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,16876)
    (<0.0>,16877) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16882) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16883) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16885) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16887) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16889) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16890) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16892) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16895) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16896) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16897) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16898) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16903) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16904) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16905) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16906) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16908) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16910) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16911) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16913) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16916) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16917) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16918) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16924) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,16940)
    (<0.0>,16941) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16944)
    (<0.0>,16945) tree.c:625: return &rsp->node[0];
    (<0.0>,16949) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16950) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16955) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16956) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16957) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16960) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16962) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16963) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16965) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16968) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16969) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16970) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16974) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16975) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16977) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16980) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16981) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16985) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16986) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16987) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16988) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16990) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16992) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16993) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16995) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16998) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16999) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17000) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17004) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,17007) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,17010) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17013) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17014) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17017) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17019) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17022) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17025) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17028) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17029) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17031) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17034) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17038) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17040) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17042) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17045) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17048) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17051) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17052) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17054) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17057) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17061) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17063) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17065) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17068) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,17070) tree.c:666: }
    (<0.0>,17073) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17078) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17079) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17080) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17081) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17083) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17085) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17086) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17088) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17091) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17092) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17093) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17094) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17096) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17099) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17104) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17105) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17106) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17107) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17109) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17111) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17112) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17114) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17117) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17118) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17119) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17120) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17122) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17125) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17129) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17130) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17131) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17132) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17134) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17135) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17136) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17137) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17140) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17143) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17144) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17152) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,17155)
    (<0.0>,17159) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17161) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17163) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17164) tree.c:3547: return 0;
    (<0.0>,17166) tree.c:3548: }
    (<0.0>,17171) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17174) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17175) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17176) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17180) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17181) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17182) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17184) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17188) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,17190) fake_sched.h:43: return __running_cpu;
    (<0.0>,17193) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,17195) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,17217)
    (<0.0>,17218)
    (<0.0>,17219) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,17221) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,17222) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,17223) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,17225) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,17227) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,17228) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,17229) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,17264)
    (<0.0>,17265)
    (<0.0>,17266) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,17269) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,17282)
    (<0.0>,17283) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17288) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17289) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17290) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17291) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17293) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17295) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17296) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17298) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17301) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17302) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17303) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17304) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17309) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17310) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17311) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17312) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17314) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17316) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17317) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17319) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17322) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17323) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17324) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17332) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,17335)
    (<0.0>,17338) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,17341) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,17343) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,17346) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,17348) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,17352) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,17355)
    (<0.0>,17356) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,17358) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,17361) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,17368) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,17369) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,17380)
    (<0.0>,17381)
    (<0.0>,17382) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,17395)
    (<0.0>,17396) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17401) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17402) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17403) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17408) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17409) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17411) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17414) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17415) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17416) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17417) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17422) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17423) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17424) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17425) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17427) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17429) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17430) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17432) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17435) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17436) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17437) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17443) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,17459)
    (<0.0>,17460) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,17463)
    (<0.0>,17464) tree.c:625: return &rsp->node[0];
    (<0.0>,17468) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,17469) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17474) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17475) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17476) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17477) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17479) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17481) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17482) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17484) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17487) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17488) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17489) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17493) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17494) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,17496) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,17499) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,17500) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17504) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17505) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17506) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17507) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17509) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17511) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17512) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17514) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17517) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17518) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17519) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17523) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,17526) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,17529) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17532) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17533) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17536) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17538) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17541) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17544) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17547) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17548) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17550) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17553) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17557) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17559) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17561) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17564) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17567) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17570) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17571) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17573) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17576) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17580) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17582) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17584) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17587) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,17589) tree.c:666: }
    (<0.0>,17592) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17597) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17598) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17599) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17600) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17602) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17604) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17605) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17607) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17610) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17611) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17612) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17613) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17615) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17618) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17623) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17624) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17625) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17626) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17628) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17630) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17631) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17633) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17636) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17637) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17638) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17639) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17641) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17644) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17648) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17649) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17650) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17651) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17653) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17654) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17655) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17656) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17659) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17662) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17663) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17671) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,17674)
    (<0.0>,17678) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17680) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17682) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17683) tree.c:3547: return 0;
    (<0.0>,17685) tree.c:3548: }
    (<0.0>,17690) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17693) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17694) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17695) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17699) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17700) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17701) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17703) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17707) tree.c:3562: return 0;
    (<0.0>,17709) tree.c:3563: }
    (<0.0>,17713) tree.c:2891: if (user)
    (<0.0>,17721) fake_sched.h:43: return __running_cpu;
    (<0.0>,17725) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,17727) fake_sched.h:43: return __running_cpu;
    (<0.0>,17731) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,17737) fake_sched.h:43: return __running_cpu;
    (<0.0>,17741) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,17752) fake_sched.h:43: return __running_cpu;
    (<0.0>,17756) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,17757) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,17759) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,17760) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,17761) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,17763) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,17765) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,17766) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17767) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17768) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17769) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17770) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,17772) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,17780)
    (<0.0>,17786) fake_sched.h:43: return __running_cpu;
    (<0.0>,17790)
    (<0.0>,17793) tree.c:755: local_irq_save(flags);
    (<0.0>,17796)
    (<0.0>,17798) fake_sched.h:43: return __running_cpu;
    (<0.0>,17802) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,17804) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,17808) fake_sched.h:43: return __running_cpu;
    (<0.0>,17812) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,17824)
    (<0.0>,17826) fake_sched.h:43: return __running_cpu;
    (<0.0>,17830) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,17831) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,17833) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,17834) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,17835) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17836) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17837) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17838) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17839) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,17843) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,17845) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,17846) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,17847) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,17863)
    (<0.0>,17865)
    (<0.0>,17867) fake_sched.h:43: return __running_cpu;
    (<0.0>,17871) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,17874) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17875) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17876) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17880) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17881) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17882) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17884) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17889) fake_sched.h:43: return __running_cpu;
    (<0.0>,17892) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17894) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17896) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17897) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,17900)
    (<0.0>,17903) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17906) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17907) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17908) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17912) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17913) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17914) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17916) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17921) fake_sched.h:43: return __running_cpu;
    (<0.0>,17924) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17926) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17928) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17929) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,17932)
    (<0.0>,17935) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17938) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17939) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17940) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17944) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17945) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17946) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17948) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17955) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17958) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17959) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17960) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17962) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17963) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17965) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17966) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17967) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17968) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17982)
    (<0.0>,17984) tree.c:758: local_irq_restore(flags);
    (<0.0>,17987)
    (<0.0>,17989) fake_sched.h:43: return __running_cpu;
    (<0.0>,17993) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,17995) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,17999) fake_sched.h:43: return __running_cpu;
    (<0.0>,18003) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,18009) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,18012) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,18017) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18022) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18023) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18024) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18025) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18027) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18029) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18030) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18032) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18035) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18036) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18037) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18044) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18049) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18050) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18051) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18052) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18054) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18056) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18057) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18059) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18062) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18063) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18064) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18071) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18076) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18077) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18078) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18079) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18081) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18083) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18084) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18086) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18089) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18090) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18091) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18098) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18103) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18104) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18105) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18106) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18108) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18110) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18111) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18113) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18116) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18117) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18118) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18125) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18130) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18131) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18132) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18133) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18135) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18137) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18138) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18140) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18143) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18144) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18145) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
      (<0.1>,769) fake_sync.h:281: while (!x->done)
      (<0.1>,776) fake_sched.h:43: return __running_cpu;
      (<0.1>,780)
      (<0.1>,781) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,784) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,789) tree.c:892: local_irq_save(flags);
      (<0.1>,792)
      (<0.1>,794) fake_sched.h:43: return __running_cpu;
      (<0.1>,798) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,804) fake_sched.h:43: return __running_cpu;
      (<0.1>,808) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,820)
      (<0.1>,822) fake_sched.h:43: return __running_cpu;
      (<0.1>,826) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,827) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,829) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,830) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,831) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,832) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,833) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,834) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,835) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,839) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,841) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,842) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,843) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,854)
      (<0.1>,855)
      (<0.1>,857) fake_sched.h:43: return __running_cpu;
      (<0.1>,861) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,865) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,868) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,869) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,870) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,872) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,873) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,875) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,876) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,877) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,878) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,888)
      (<0.1>,890) tree.c:895: local_irq_restore(flags);
      (<0.1>,893)
      (<0.1>,895) fake_sched.h:43: return __running_cpu;
      (<0.1>,899) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,905) fake_sched.h:43: return __running_cpu;
      (<0.1>,909) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,917) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,919) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,924)
      (<0.1>,927) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,929) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,931) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,932) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,940) litmus.c:91: y = 1;
      (<0.1>,942) fake_sched.h:43: return __running_cpu;
      (<0.1>,946)
      (<0.1>,949) tree.c:755: local_irq_save(flags);
      (<0.1>,952)
      (<0.1>,954) fake_sched.h:43: return __running_cpu;
      (<0.1>,958) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,960) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,964) fake_sched.h:43: return __running_cpu;
      (<0.1>,968) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,980)
      (<0.1>,982) fake_sched.h:43: return __running_cpu;
      (<0.1>,986) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,987) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,989) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,990) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,991) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,992) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,993) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,994) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,995) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,999) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1001) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1002) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1003) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1019)
      (<0.1>,1021)
      (<0.1>,1023) fake_sched.h:43: return __running_cpu;
      (<0.1>,1027) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,1030) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1031) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1037) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1038) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1040) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1045) fake_sched.h:43: return __running_cpu;
      (<0.1>,1048) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1050) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1052) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1053) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1056)
      (<0.1>,1059) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1062) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1063) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1069) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1070) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1072) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1077) fake_sched.h:43: return __running_cpu;
      (<0.1>,1080) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1082) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1084) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1085) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1088)
      (<0.1>,1091) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1094) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1095) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1101) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1102) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1104) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1111) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1116) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1118) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1119) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1121) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1122) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1123) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1124) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1138)
      (<0.1>,1140) tree.c:758: local_irq_restore(flags);
      (<0.1>,1143)
      (<0.1>,1145) fake_sched.h:43: return __running_cpu;
      (<0.1>,1149) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1151) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1155) fake_sched.h:43: return __running_cpu;
      (<0.1>,1159) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1165) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1168) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,7439) litmus.c:69: r_y = y;
  (<0>,7440) litmus.c:69: r_y = y;
  (<0>,7456) fake_sched.h:43: return __running_cpu;
  (<0>,7462) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
  (<0>,7468) fake_sched.h:43: return __running_cpu;
  (<0>,7475) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
  (<0>,7477) fake_sched.h:43: return __running_cpu;
  (<0>,7484) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
  (<0>,7492) fake_sched.h:43: return __running_cpu;
  (<0>,7496) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
  (<0>,7509) fake_sched.h:43: return __running_cpu;
  (<0>,7513)
  (<0>,7516) tree.c:755: local_irq_save(flags);
  (<0>,7519)
  (<0>,7521) fake_sched.h:43: return __running_cpu;
  (<0>,7525) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7527) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7531) fake_sched.h:43: return __running_cpu;
  (<0>,7535) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7547)
  (<0>,7549) fake_sched.h:43: return __running_cpu;
  (<0>,7553) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,7554) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,7556) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,7557) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,7558) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7559) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7560) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7561) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7562) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,7566) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,7568) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,7569) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,7570) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,7586)
  (<0>,7588)
  (<0>,7590) fake_sched.h:43: return __running_cpu;
  (<0>,7594) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,7597) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7598) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7599) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7603) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7604) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7605) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7607) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7612) fake_sched.h:43: return __running_cpu;
  (<0>,7615) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,7617) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,7619) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,7620) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,7623)
  (<0>,7626) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7629) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7630) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7631) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7635) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7636) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7637) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7639) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7644) fake_sched.h:43: return __running_cpu;
  (<0>,7647) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,7649) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,7651) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,7652) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,7655)
  (<0>,7658) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7661) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7662) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7663) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7667) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7668) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7669) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7671) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,7678) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,7681) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,7682) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,7683) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,7685) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,7686) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,7688) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7689) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7690) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7691) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7705)
  (<0>,7707) tree.c:758: local_irq_restore(flags);
  (<0>,7710)
  (<0>,7712) fake_sched.h:43: return __running_cpu;
  (<0>,7716) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7718) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7722) fake_sched.h:43: return __running_cpu;
  (<0>,7726) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7732) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,7735) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,7740) fake_sched.h:43: return __running_cpu;
  (<0>,7744)
  (<0>,7745) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,7748) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,7753) tree.c:892: local_irq_save(flags);
  (<0>,7756)
  (<0>,7758) fake_sched.h:43: return __running_cpu;
  (<0>,7762) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7764) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7768) fake_sched.h:43: return __running_cpu;
  (<0>,7772) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7784)
  (<0>,7786) fake_sched.h:43: return __running_cpu;
  (<0>,7790) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,7791) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,7793) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,7794) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,7795) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,7796) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,7797) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,7798) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,7799) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,7803) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,7805) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,7806) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,7807) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,7818)
  (<0>,7819)
  (<0>,7821) fake_sched.h:43: return __running_cpu;
  (<0>,7825) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,7829) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,7832) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,7833) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,7834) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,7836) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,7837) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,7839) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7840) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7841) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7842) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7852)
  (<0>,7854) tree.c:895: local_irq_restore(flags);
  (<0>,7857)
  (<0>,7859) fake_sched.h:43: return __running_cpu;
  (<0>,7863) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7865) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7869) fake_sched.h:43: return __running_cpu;
  (<0>,7873) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7884) fake_sched.h:43: return __running_cpu;
  (<0>,7888) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
  (<0>,7892) fake_sched.h:43: return __running_cpu;
  (<0>,7896) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7901) fake_sched.h:43: return __running_cpu;
  (<0>,7905) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
  (<0>,7916) fake_sched.h:43: return __running_cpu;
  (<0>,7920) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,7921) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,7923) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,7924) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,7925) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,7927) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,7929) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,7930) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7931) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7932) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7933) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,7934) tree.c:942: if (oldval)
  (<0>,7942)
  (<0>,7948)
  (<0>,7957) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7958) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7959) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7963) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7964) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7965) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7967) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7972) fake_sched.h:43: return __running_cpu;
  (<0>,7975) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,7977) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,7980) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,7982) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,7984) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7987) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7988) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7989) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7993) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7994) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7995) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,7997) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8002) fake_sched.h:43: return __running_cpu;
  (<0>,8005) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8007) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8010) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8012) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8014) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8017) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8018) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8019) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8023) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8024) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8025) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8027) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8032) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,8037) fake_sched.h:43: return __running_cpu;
  (<0>,8042) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,8050) fake_sched.h:43: return __running_cpu;
  (<0>,8056) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
  (<0>,8070) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8071) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8072) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8076) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8077) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8078) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8080) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8084) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8086) fake_sched.h:43: return __running_cpu;
  (<0>,8089) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8091) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8113)
  (<0>,8114)
  (<0>,8115) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8117) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8118) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8119) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8121) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8123) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8124) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,8125) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,8160)
  (<0>,8161)
  (<0>,8162) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
  (<0>,8165) tree.c:1469: !rcu_gp_in_progress(rsp))
  (<0>,8178)
  (<0>,8179) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8184) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8185) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8186) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8187) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8189) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8191) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8192) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8194) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8197) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8198) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8199) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8200) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8205) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8206) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8207) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8208) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8210) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8212) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8213) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8215) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8218) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8219) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8220) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8228) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
  (<0>,8231)
  (<0>,8234) tree.c:3503: if (rcu_scheduler_fully_active &&
  (<0>,8237) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8239) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8242) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,8244) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,8248) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,8251)
  (<0>,8252) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,8254) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,8257) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,8264) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,8265) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,8276)
  (<0>,8277)
  (<0>,8278) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,8291)
  (<0>,8292) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8297) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8298) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8299) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8300) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8302) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8304) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8305) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8307) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8310) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8311) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8312) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8313) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8318) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8320) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8321) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8323) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8325) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8326) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8328) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8331) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8332) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8333) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8339) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,8355)
  (<0>,8356) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,8359)
  (<0>,8360) tree.c:625: return &rsp->node[0];
  (<0>,8364) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,8365) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,8370) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,8371) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,8372) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,8373) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8375) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8377) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8378) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8380) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8383) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,8384) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,8385) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,8389) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,8390) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,8392) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,8395) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,8396) tree.c:639: return READ_ONCE(*fp);
  (<0>,8400) tree.c:639: return READ_ONCE(*fp);
  (<0>,8401) tree.c:639: return READ_ONCE(*fp);
  (<0>,8402) tree.c:639: return READ_ONCE(*fp);
  (<0>,8403) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8405) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8407) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8408) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8410) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8413) tree.c:639: return READ_ONCE(*fp);
  (<0>,8414) tree.c:639: return READ_ONCE(*fp);
  (<0>,8415) tree.c:639: return READ_ONCE(*fp);
  (<0>,8419) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,8422) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,8425) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,8428) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,8429) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,8432) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8434) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8437) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8440) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8443) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8444) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8446) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8449) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8453) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8455) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8457) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8460) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8463) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8466) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8467) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8469) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8472) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8476) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8478) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8480) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8483) tree.c:665: return false; /* No grace period needed. */
  (<0>,8485) tree.c:666: }
  (<0>,8488) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,8493) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,8494) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,8495) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,8496) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8498) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8500) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8501) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8503) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8506) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,8507) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,8508) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,8509) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,8511) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,8514) tree.c:3528: rdp->n_rp_gp_completed++;
  (<0>,8516) tree.c:3528: rdp->n_rp_gp_completed++;
  (<0>,8518) tree.c:3528: rdp->n_rp_gp_completed++;
  (<0>,8519) tree.c:3529: return 1;
  (<0>,8521) tree.c:3548: }
  (<0>,8525) tree.c:3561: return 1;
  (<0>,8527) tree.c:3563: }
  (<0>,8534) fake_sched.h:43: return __running_cpu;
  (<0>,8538) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
  (<0>,8542) tree.c:2891: if (user)
  (<0>,8550) fake_sched.h:43: return __running_cpu;
  (<0>,8554) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
  (<0>,8556) fake_sched.h:43: return __running_cpu;
  (<0>,8560) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8566) fake_sched.h:43: return __running_cpu;
  (<0>,8570) fake_sched.h:183: if (need_softirq[get_cpu()]) {
  (<0>,8580)
  (<0>,8583) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8584) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8585) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8589) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8590) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8591) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8593) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8597) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,8609)
  (<0>,8611) fake_sched.h:43: return __running_cpu;
  (<0>,8614) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,8616) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,8618) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,8619) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,8621) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,8628) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,8629) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,8631) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,8637) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,8638) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,8639) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,8640) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,8641) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,8645)
  (<0>,8646)
  (<0>,8647) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,8648) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,8673)
  (<0>,8674)
  (<0>,8675) tree.c:1905: local_irq_save(flags);
  (<0>,8678)
  (<0>,8680) fake_sched.h:43: return __running_cpu;
  (<0>,8684) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8686) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8690) fake_sched.h:43: return __running_cpu;
  (<0>,8694) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8699) tree.c:1906: rnp = rdp->mynode;
  (<0>,8701) tree.c:1906: rnp = rdp->mynode;
  (<0>,8702) tree.c:1906: rnp = rdp->mynode;
  (<0>,8703) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,8705) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,8706) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,8711) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,8712) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,8713) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,8714) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8716) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8718) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8719) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8721) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8724) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,8725) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,8726) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,8729) tree.c:1910: !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */
  (<0>,8733)
  (<0>,8734) tree.h:752: bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,8739)
  (<0>,8741) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
  (<0>,8742) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
  (<0>,8745) fake_sync.h:134: return 1;
  (<0>,8747) fake_sync.h:135: }
  (<0>,8751) tree.h:752: bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,8752) tree.h:754: if (locked)
  (<0>,8758) tree.h:756: return locked;
  (<0>,8762) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,8763) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,8764) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,8786)
  (<0>,8787)
  (<0>,8788)
  (<0>,8789) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,8791) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,8792) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,8794) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,8797) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,8798) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,8799) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,8807)
  (<0>,8808)
  (<0>,8809)
  (<0>,8810) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,8813) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,8816) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,8819) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,8820) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,8823) tree.c:1818: return false;
  (<0>,8825) tree.c:1843: }
  (<0>,8828) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,8829) tree.c:1870: rdp->completed = rnp->completed;
  (<0>,8831) tree.c:1870: rdp->completed = rnp->completed;
  (<0>,8832) tree.c:1870: rdp->completed = rnp->completed;
  (<0>,8834) tree.c:1870: rdp->completed = rnp->completed;
  (<0>,8838) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,8840) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,8841) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,8843) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,8846) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,8848) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,8849) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,8851) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,8854) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,8856) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,8857) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,8859) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,8865) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,8866) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,8869) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,8873) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,8875) fake_sched.h:43: return __running_cpu;
  (<0>,8879) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,8880) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,8882) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,8883) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,8885) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,8888) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,8889) tree.c:1893: zero_cpu_stall_ticks(rdp);
  (<0>,8892)
  (<0>,8893) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,8895) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,8896) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,8898) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,8908)
  (<0>,8913) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,8917) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,8918) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,8919) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,8920) fake_defs.h:237: switch (size) {
  (<0>,8922) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,8923) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,8924) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,8925) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,8928) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,8931) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,8932) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,8935) tree.c:1896: return ret;
  (<0>,8939) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,8943) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,8944) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,8945) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,8947) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,8951)
  (<0>,8952)
  (<0>,8953) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,8954) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,8957) fake_sync.h:93: local_irq_restore(flags);
  (<0>,8960)
  (<0>,8962) fake_sched.h:43: return __running_cpu;
  (<0>,8966) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8968) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8972) fake_sched.h:43: return __running_cpu;
  (<0>,8976) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8984) tree.c:1916: if (needwake)
  (<0>,8988) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,8990) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,8995) tree.c:3016: local_irq_save(flags);
  (<0>,8998)
  (<0>,9000) fake_sched.h:43: return __running_cpu;
  (<0>,9004) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9006) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9010) fake_sched.h:43: return __running_cpu;
  (<0>,9014) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9019) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9020) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9031)
  (<0>,9032)
  (<0>,9033) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,9046)
  (<0>,9047) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9052) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9053) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9054) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9055) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9057) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9059) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9060) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9062) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9065) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9066) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9067) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9068) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9073) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9074) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9075) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9076) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9078) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9080) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9081) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9083) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9086) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9087) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9088) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9094) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,9110)
  (<0>,9111) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,9114)
  (<0>,9115) tree.c:625: return &rsp->node[0];
  (<0>,9119) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,9120) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9125) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9126) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9127) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9128) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9130) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9132) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9133) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9135) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9138) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9139) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9140) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9144) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9145) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9147) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9150) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9151) tree.c:639: return READ_ONCE(*fp);
  (<0>,9155) tree.c:639: return READ_ONCE(*fp);
  (<0>,9156) tree.c:639: return READ_ONCE(*fp);
  (<0>,9157) tree.c:639: return READ_ONCE(*fp);
  (<0>,9158) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9160) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9162) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9163) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9165) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9168) tree.c:639: return READ_ONCE(*fp);
  (<0>,9169) tree.c:639: return READ_ONCE(*fp);
  (<0>,9170) tree.c:639: return READ_ONCE(*fp);
  (<0>,9174) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,9177) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,9180) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9183) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9184) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9187) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9189) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9192) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9195) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9198) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9199) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9201) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9204) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9208) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9210) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9212) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9215) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9218) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9221) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9222) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9224) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9227) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9231) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9233) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9235) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9238) tree.c:665: return false; /* No grace period needed. */
  (<0>,9240) tree.c:666: }
  (<0>,9243) tree.c:3024: local_irq_restore(flags);
  (<0>,9246)
  (<0>,9248) fake_sched.h:43: return __running_cpu;
  (<0>,9252) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9254) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9258) fake_sched.h:43: return __running_cpu;
  (<0>,9262) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9268) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,9271)
  (<0>,9272) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9274) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9277) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9284) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,9287)
  (<0>,9291) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9294) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9295) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9296) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9300) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9301) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9302) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9304) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9308) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,9320)
  (<0>,9322) fake_sched.h:43: return __running_cpu;
  (<0>,9325) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9327) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9329) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9330) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9332) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9339) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9340) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9342) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9348) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9349) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9350) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9351) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,9352) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,9356)
  (<0>,9357)
  (<0>,9358) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,9359) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,9384)
  (<0>,9385)
  (<0>,9386) tree.c:1905: local_irq_save(flags);
  (<0>,9389)
  (<0>,9391) fake_sched.h:43: return __running_cpu;
  (<0>,9395) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9397) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9401) fake_sched.h:43: return __running_cpu;
  (<0>,9405) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9410) tree.c:1906: rnp = rdp->mynode;
  (<0>,9412) tree.c:1906: rnp = rdp->mynode;
  (<0>,9413) tree.c:1906: rnp = rdp->mynode;
  (<0>,9414) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9416) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9417) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9422) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9423) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9424) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9425) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9427) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9429) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9430) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9432) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9435) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9436) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9437) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9440) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9442) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9443) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9448) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9449) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9450) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9451) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9453) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9455) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9456) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9458) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9461) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9462) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9463) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9466) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9470) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9471) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9472) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9473) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9475) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9476) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9477) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9478) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9481) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9484) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9485) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9493) tree.c:1911: local_irq_restore(flags);
  (<0>,9496)
  (<0>,9498) fake_sched.h:43: return __running_cpu;
  (<0>,9502) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9504) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9508) fake_sched.h:43: return __running_cpu;
  (<0>,9512) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9519) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,9521) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,9526) tree.c:3016: local_irq_save(flags);
  (<0>,9529)
  (<0>,9531) fake_sched.h:43: return __running_cpu;
  (<0>,9535) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9537) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9541) fake_sched.h:43: return __running_cpu;
  (<0>,9545) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9550) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9551) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9562)
  (<0>,9563)
  (<0>,9564) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,9577)
  (<0>,9578) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9583) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9584) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9585) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9586) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9588) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9590) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9591) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9593) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9596) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9597) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9598) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9599) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9604) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9605) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9606) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9607) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9609) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9611) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9612) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9614) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9617) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9618) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9619) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9625) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,9641)
  (<0>,9642) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,9645)
  (<0>,9646) tree.c:625: return &rsp->node[0];
  (<0>,9650) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,9651) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9656) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9657) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9658) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9659) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9661) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9663) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9664) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9666) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9669) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9670) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9671) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9675) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9676) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9678) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9681) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9682) tree.c:639: return READ_ONCE(*fp);
  (<0>,9686) tree.c:639: return READ_ONCE(*fp);
  (<0>,9687) tree.c:639: return READ_ONCE(*fp);
  (<0>,9688) tree.c:639: return READ_ONCE(*fp);
  (<0>,9689) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9691) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9693) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9694) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9696) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9699) tree.c:639: return READ_ONCE(*fp);
  (<0>,9700) tree.c:639: return READ_ONCE(*fp);
  (<0>,9701) tree.c:639: return READ_ONCE(*fp);
  (<0>,9705) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,9708) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,9711) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9714) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9715) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9718) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9720) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9723) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9726) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9729) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9730) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9732) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9735) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9739) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9741) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9743) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9746) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9749) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9752) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9753) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9755) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9758) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9762) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9764) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9766) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9769) tree.c:665: return false; /* No grace period needed. */
  (<0>,9771) tree.c:666: }
  (<0>,9774) tree.c:3024: local_irq_restore(flags);
  (<0>,9777)
  (<0>,9779) fake_sched.h:43: return __running_cpu;
  (<0>,9783) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9785) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9789) fake_sched.h:43: return __running_cpu;
  (<0>,9793) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9799) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,9802)
  (<0>,9803) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9805) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9808) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9815) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,9818)
  (<0>,9822) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9825) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9826) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9827) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9831) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9832) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9833) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9835) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9843) fake_sched.h:43: return __running_cpu;
  (<0>,9847) fake_sched.h:185: need_softirq[get_cpu()] = 0;
  (<0>,9857) fake_sched.h:43: return __running_cpu;
  (<0>,9861) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,9862) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,9864) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,9865) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,9866) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,9868) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,9870) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,9871) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,9872) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,9873) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,9874) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,9875) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,9877) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,9885)
  (<0>,9891) fake_sched.h:43: return __running_cpu;
  (<0>,9895)
  (<0>,9898) tree.c:755: local_irq_save(flags);
  (<0>,9901)
  (<0>,9903) fake_sched.h:43: return __running_cpu;
  (<0>,9907) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9909) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9913) fake_sched.h:43: return __running_cpu;
  (<0>,9917) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9929)
  (<0>,9931) fake_sched.h:43: return __running_cpu;
  (<0>,9935) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,9936) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,9938) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,9939) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,9940) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,9941) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,9942) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,9943) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,9944) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,9948) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,9950) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,9951) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,9952) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,9968)
  (<0>,9970)
  (<0>,9972) fake_sched.h:43: return __running_cpu;
  (<0>,9976) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,9979) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,9980) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,9981) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,9985) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,9986) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,9987) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,9989) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,9994) fake_sched.h:43: return __running_cpu;
  (<0>,9997) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,9999) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10001) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10002) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,10005)
  (<0>,10008) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10011) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10012) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10013) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10017) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10018) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10019) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10021) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10026) fake_sched.h:43: return __running_cpu;
  (<0>,10029) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10031) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10033) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10034) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,10037)
  (<0>,10040) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10043) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10044) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10045) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10049) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10050) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10051) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10053) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10060) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10063) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10064) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10065) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10067) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10068) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10070) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10071) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10072) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10073) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10087)
  (<0>,10089) tree.c:758: local_irq_restore(flags);
  (<0>,10092)
  (<0>,10094) fake_sched.h:43: return __running_cpu;
  (<0>,10098) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10100) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10104) fake_sched.h:43: return __running_cpu;
  (<0>,10108) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,10114) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,10117) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,10122) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,10126) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10129) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10133) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10134) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10135) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
              Error: Assertion violation at (<0>,10138): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpdfm01nq1/tmpr5clet2i.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DFORCE_FAILURE_3 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpdfm01nq1/tmpt3xglynq.ll /tmp/tmpdfm01nq1/tmpr5clet2i.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=3 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpdfm01nq1/tmpt3xglynq.ll
Total wall-clock time: 37.19 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 161 (also 27 sleepset blocked, 0 schedulings and 201 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpz8st5_op/tmp7t8gr_42.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DFORCE_FAILURE_5 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpz8st5_op/tmpmzdkxfv9.ll /tmp/tmpz8st5_op/tmp7t8gr_42.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=3 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpz8st5_op/tmpmzdkxfv9.ll
Total wall-clock time: 7.81 s
^^^ Unexpected verification success
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 78 (also 8 sleepset blocked, 0 schedulings and 42 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpf1krxq4e/tmpr0vlnqcl.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DLIVENESS_CHECK_1 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpf1krxq4e/tmpbz_pvq15.ll /tmp/tmpf1krxq4e/tmpr0vlnqcl.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpf1krxq4e/tmpbz_pvq15.ll
Total wall-clock time: 3.17 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 112 (also 8 sleepset blocked, 0 schedulings and 50 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmplx_t0ud1/tmp7wtf2aqm.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DLIVENESS_CHECK_2 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmplx_t0ud1/tmpnaiimvw6.ll /tmp/tmplx_t0ud1/tmp7wtf2aqm.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmplx_t0ud1/tmpnaiimvw6.ll
Total wall-clock time: 4.36 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 96 (also 8 sleepset blocked, 0 schedulings and 50 branches were rejected due to the bound) 
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpy7dj08_s/tmp_p3j5oic.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DLIVENESS_CHECK_3 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpy7dj08_s/tmpsl9pxgma.ll /tmp/tmpy7dj08_s/tmp_p3j5oic.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=3 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpy7dj08_s/tmpsl9pxgma.ll
Total wall-clock time: 3.77 s
--------------------------------------------------------------------
---  UNEXPECTED VERIFICATION RESULTS
--------------------------------------------------------------------
