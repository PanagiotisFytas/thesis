--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 18174 (also 1224 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp7k28f2pn/tmp2vsje87a.ll -S -emit-llvm -g -I v3.0 -std=gnu99 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp7k28f2pn/tmpsx_rslke.ll /tmp/tmp7k28f2pn/tmp2vsje87a.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp7k28f2pn/tmpsx_rslke.ll
Total wall-clock time: 316.78 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 134 (also 12 sleepset blocked)

 Error detected:
  (<0>,1)
  (<0>,6)
  (<0>,7) litmus_v3.c:109: pthread_t tu, th;
  (<0>,11) rcutree_plugin.h:909: printk(KERN_INFO "Hierarchical RCU implementation.\n");
  (<0>,24)
  (<0>,25) rcutree.c:2030: static void __init rcu_init_one(struct rcu_state *rsp,
  (<0>,26) rcutree.c:2031: struct rcu_data __percpu *rda)
  (<0>,27) rcutree.c:2038: int i;
  (<0>,29) rcutree.c:2046: for (i = 1; i < NUM_RCU_LVLS; i++)
  (<0>,32) rcutree.c:2048: rcu_init_levelspread(rsp);
  (<0>,38)
  (<0>,39) rcutree.c:2012: static void __init rcu_init_levelspread(struct rcu_state *rsp)
  (<0>,40) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,42) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,45) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,47) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,50) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,51) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,52) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,53) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,56) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,59) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,61) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,64) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,65) rcutree.c:2022: cprv = ccur;
  (<0>,66) rcutree.c:2022: cprv = ccur;
  (<0>,68) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,70) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,72) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,76) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,78) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,81) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,83) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,86) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,88) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,90) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,91) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,93) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,96) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,97) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,98) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,100) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,101) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,103) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,106) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,109) rcutree.c:2056: raw_spin_lock_init(&rnp->lock);
  (<0>,113)
  (<0>,114) fake_sync.h:72: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,115) fake_sync.h:74: if (pthread_mutex_init(l, NULL))
  (<0>,121) rcutree.c:2059: rnp->gpnum = 0;
  (<0>,123) rcutree.c:2059: rnp->gpnum = 0;
  (<0>,124) rcutree.c:2060: rnp->qsmask = 0;
  (<0>,126) rcutree.c:2060: rnp->qsmask = 0;
  (<0>,127) rcutree.c:2061: rnp->qsmaskinit = 0;
  (<0>,129) rcutree.c:2061: rnp->qsmaskinit = 0;
  (<0>,130) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,131) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,133) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,135) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,136) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,138) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,141) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,143) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,144) rcutree.c:2064: if (rnp->grphi >= NR_CPUS)
  (<0>,146) rcutree.c:2064: if (rnp->grphi >= NR_CPUS)
  (<0>,149) rcutree.c:2066: if (i == 0) {
  (<0>,152) rcutree.c:2067: rnp->grpnum = 0;
  (<0>,154) rcutree.c:2067: rnp->grpnum = 0;
  (<0>,155) rcutree.c:2068: rnp->grpmask = 0;
  (<0>,157) rcutree.c:2068: rnp->grpmask = 0;
  (<0>,158) rcutree.c:2069: rnp->parent = NULL;
  (<0>,160) rcutree.c:2069: rnp->parent = NULL;
  (<0>,162) rcutree.c:2076: rnp->level = i;
  (<0>,164) rcutree.c:2076: rnp->level = i;
  (<0>,166) rcutree.c:2076: rnp->level = i;
  (<0>,167) rcutree.c:2077: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,171)
  (<0>,172) fake_defs.h:196: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,173) fake_defs.h:198: list->next = list;
  (<0>,175) fake_defs.h:198: list->next = list;
  (<0>,176) fake_defs.h:199: list->prev = list;
  (<0>,177) fake_defs.h:199: list->prev = list;
  (<0>,179) fake_defs.h:199: list->prev = list;
  (<0>,182) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,184) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,185) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,187) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,189) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,190) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,192) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,195) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,199) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,201) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,203) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,206) rcutree.c:2081: rsp->rda = rda;
  (<0>,207) rcutree.c:2081: rsp->rda = rda;
  (<0>,209) rcutree.c:2081: rsp->rda = rda;
  (<0>,210) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,213) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,214) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,215) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,217) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,221) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,222) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,224) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,227) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,228) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,230) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,232) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,235) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,236) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,237) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,245)
  (<0>,246) rcutree.c:1854: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,247) rcutree.c:1854: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,249) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,251) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,253) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,254) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,257)
  (<0>,258) rcutree.c:295: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,262) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,263) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,265) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,269)
  (<0>,270) fake_sync.h:78: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,271) fake_sync.h:78: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,274) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,276) fake_sched.h:43: return __running_cpu;
  (<0>,280) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,282) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,286) fake_sched.h:43: return __running_cpu;
  (<0>,290) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,296) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,297) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,301) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,302) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,304) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,306) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,310) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,312) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,313) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,315) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,316) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,318) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,321) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,323) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,325) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,328) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,330) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,332) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,334) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,337) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,339) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,341) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,344) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,346) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,348) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,350) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,353) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,355) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,357) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,360) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,362) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,364) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,366) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,369) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,371) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,373) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,376) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,378) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,380) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,382) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,385) rcutree.c:1867: rdp->qlen = 0;
  (<0>,387) rcutree.c:1867: rdp->qlen = 0;
  (<0>,388) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,391) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,393) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,394) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,395) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,397) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,398) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,400) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,404)
  (<0>,405) fake_sync.h:86: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,406) fake_sync.h:86: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,407) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,410) fake_sync.h:90: local_irq_restore(flags);
  (<0>,413) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,415) fake_sched.h:43: return __running_cpu;
  (<0>,419) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,421) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,425) fake_sched.h:43: return __running_cpu;
  (<0>,429) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,438) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,440) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,442) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,446) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,447) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,449) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,452) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,453) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,455) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,457) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,460) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,461) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,462) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,470)
  (<0>,471)
  (<0>,472) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,474) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,476) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,478) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,479) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,482)
  (<0>,483) rcutree.c:297: return &rsp->node[0];
  (<0>,487) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,488) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,490) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,494)
  (<0>,495)
  (<0>,496) fake_sync.h:80: local_irq_save(flags);
  (<0>,499) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,501) fake_sched.h:43: return __running_cpu;
  (<0>,505) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,507) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,511) fake_sched.h:43: return __running_cpu;
  (<0>,515) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,521) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,522) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,526) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,527) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,529) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,531) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,535) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,537) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,538) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,540) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,541) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,543) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,546) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,548) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,550) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,553) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,555) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,557) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,559) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,562) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,564) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,566) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,569) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,571) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,573) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,575) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,578) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,580) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,582) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,585) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,587) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,589) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,591) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,594) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,596) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,598) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,601) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,603) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,605) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,607) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,610) rcutree.c:1867: rdp->qlen = 0;
  (<0>,612) rcutree.c:1867: rdp->qlen = 0;
  (<0>,613) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,616) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,618) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,619) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,620) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,622) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,623) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,625) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,629)
  (<0>,630)
  (<0>,631) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,632) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,635) fake_sync.h:90: local_irq_restore(flags);
  (<0>,638) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,640) fake_sched.h:43: return __running_cpu;
  (<0>,644) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,646) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,650) fake_sched.h:43: return __running_cpu;
  (<0>,654) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,663) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,665) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,667) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,678)
  (<0>,679)
  (<0>,680) rcutree.c:2037: int cpustride = 1;
  (<0>,681) rcutree.c:2046: for (i = 1; i < NUM_RCU_LVLS; i++)
  (<0>,683) rcutree.c:2046: for (i = 1; i < NUM_RCU_LVLS; i++)
  (<0>,686) rcutree.c:2048: rcu_init_levelspread(rsp);
  (<0>,692)
  (<0>,693) rcutree.c:2018: cprv = NR_CPUS;
  (<0>,694) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,696) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,699) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,701) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,704) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,705) rcutree.c:2020: ccur = rsp->levelcnt[i];
  (<0>,706) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,707) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,710) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,713) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,715) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,718) rcutree.c:2021: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,719) rcutree.c:2022: cprv = ccur;
  (<0>,720) rcutree.c:2022: cprv = ccur;
  (<0>,722) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,724) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,726) rcutree.c:2019: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,730) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,732) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,735) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,737) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,740) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,742) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,744) rcutree.c:2053: cpustride *= rsp->levelspread[i];
  (<0>,745) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,747) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,750) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,751) rcutree.c:2054: rnp = rsp->level[i];
  (<0>,752) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,754) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,755) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,757) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,760) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,763) rcutree.c:2056: raw_spin_lock_init(&rnp->lock);
  (<0>,767)
  (<0>,768) fake_sync.h:74: if (pthread_mutex_init(l, NULL))
  (<0>,769) fake_sync.h:74: if (pthread_mutex_init(l, NULL))
  (<0>,775) rcutree.c:2059: rnp->gpnum = 0;
  (<0>,777) rcutree.c:2059: rnp->gpnum = 0;
  (<0>,778) rcutree.c:2060: rnp->qsmask = 0;
  (<0>,780) rcutree.c:2060: rnp->qsmask = 0;
  (<0>,781) rcutree.c:2061: rnp->qsmaskinit = 0;
  (<0>,783) rcutree.c:2061: rnp->qsmaskinit = 0;
  (<0>,784) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,785) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,787) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,789) rcutree.c:2062: rnp->grplo = j * cpustride;
  (<0>,790) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,792) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,795) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,797) rcutree.c:2063: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,798) rcutree.c:2064: if (rnp->grphi >= NR_CPUS)
  (<0>,800) rcutree.c:2064: if (rnp->grphi >= NR_CPUS)
  (<0>,803) rcutree.c:2066: if (i == 0) {
  (<0>,806) rcutree.c:2067: rnp->grpnum = 0;
  (<0>,808) rcutree.c:2067: rnp->grpnum = 0;
  (<0>,809) rcutree.c:2068: rnp->grpmask = 0;
  (<0>,811) rcutree.c:2068: rnp->grpmask = 0;
  (<0>,812) rcutree.c:2069: rnp->parent = NULL;
  (<0>,814) rcutree.c:2069: rnp->parent = NULL;
  (<0>,816) rcutree.c:2076: rnp->level = i;
  (<0>,818) rcutree.c:2076: rnp->level = i;
  (<0>,820) rcutree.c:2076: rnp->level = i;
  (<0>,821) rcutree.c:2077: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,825)
  (<0>,826) fake_defs.h:198: list->next = list;
  (<0>,827) fake_defs.h:198: list->next = list;
  (<0>,829) fake_defs.h:198: list->next = list;
  (<0>,830) fake_defs.h:199: list->prev = list;
  (<0>,831) fake_defs.h:199: list->prev = list;
  (<0>,833) fake_defs.h:199: list->prev = list;
  (<0>,836) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,838) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,839) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,841) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,843) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,844) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,846) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,849) rcutree.c:2055: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,853) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,855) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,857) rcutree.c:2052: for (i = NUM_RCU_LVLS - 1; i >= 0; i--) {
  (<0>,860) rcutree.c:2081: rsp->rda = rda;
  (<0>,861) rcutree.c:2081: rsp->rda = rda;
  (<0>,863) rcutree.c:2081: rsp->rda = rda;
  (<0>,864) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,867) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,868) rcutree.c:2082: rnp = rsp->level[NUM_RCU_LVLS - 1];
  (<0>,869) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,871) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,875) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,876) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,878) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,881) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,882) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,884) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,886) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,889) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,890) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,891) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,899)
  (<0>,900)
  (<0>,901) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,903) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,905) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,907) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,908) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,911)
  (<0>,912) rcutree.c:297: return &rsp->node[0];
  (<0>,916) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,917) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,919) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,923)
  (<0>,924)
  (<0>,925) fake_sync.h:80: local_irq_save(flags);
  (<0>,928) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,930) fake_sched.h:43: return __running_cpu;
  (<0>,934) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,936) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,940) fake_sched.h:43: return __running_cpu;
  (<0>,944) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,950) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,951) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,955) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,956) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,958) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,960) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,964) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,966) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,967) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,969) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,970) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,972) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,975) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,977) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,979) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,982) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,984) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,986) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,988) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,991) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,993) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,995) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,998) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1000) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1002) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1004) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1007) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1009) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1011) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1014) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1016) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1018) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1020) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1023) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1025) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1027) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1030) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1032) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1034) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1036) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1039) rcutree.c:1867: rdp->qlen = 0;
  (<0>,1041) rcutree.c:1867: rdp->qlen = 0;
  (<0>,1042) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1045) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1047) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1048) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1049) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1051) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1052) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1054) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1058)
  (<0>,1059)
  (<0>,1060) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1061) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1064) fake_sync.h:90: local_irq_restore(flags);
  (<0>,1067) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1069) fake_sched.h:43: return __running_cpu;
  (<0>,1073) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1075) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1079) fake_sched.h:43: return __running_cpu;
  (<0>,1083) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1092) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1094) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1096) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1100) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,1101) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,1103) rcutree.c:2084: while (i > rnp->grphi)
  (<0>,1106) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,1107) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,1109) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,1111) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,1114) rcutree.c:2086: rsp->rda[i].mynode = rnp;
  (<0>,1115) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1116) rcutree.c:2087: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1124)
  (<0>,1125)
  (<0>,1126) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1128) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1130) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1132) rcutree.c:1858: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1133) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1136)
  (<0>,1137) rcutree.c:297: return &rsp->node[0];
  (<0>,1141) rcutree.c:1859: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1142) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1144) rcutree.c:1862: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1148)
  (<0>,1149)
  (<0>,1150) fake_sync.h:80: local_irq_save(flags);
  (<0>,1153) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1155) fake_sched.h:43: return __running_cpu;
  (<0>,1159) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1161) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1165) fake_sched.h:43: return __running_cpu;
  (<0>,1169) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1175) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1176) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1180) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1181) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1183) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1185) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1189) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1191) rcutree.c:1863: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1192) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,1194) rcutree.c:1864: rdp->nxtlist = NULL;
  (<0>,1195) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1197) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1200) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1202) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1204) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1207) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1209) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1211) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1213) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1216) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1218) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1220) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1223) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1225) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1227) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1229) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1232) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1234) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1236) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1239) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1241) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1243) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1245) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1248) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1250) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1252) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1255) rcutree.c:1866: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1257) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1259) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1261) rcutree.c:1865: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1264) rcutree.c:1867: rdp->qlen = 0;
  (<0>,1266) rcutree.c:1867: rdp->qlen = 0;
  (<0>,1267) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1270) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1272) rcutree.c:1869: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1273) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1274) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1276) rcutree.c:1871: rdp->cpu = cpu;
  (<0>,1277) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1279) rcutree.c:1872: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1283)
  (<0>,1284)
  (<0>,1285) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1286) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1289) fake_sync.h:90: local_irq_restore(flags);
  (<0>,1292) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1294) fake_sched.h:43: return __running_cpu;
  (<0>,1298) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1300) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1304) fake_sched.h:43: return __running_cpu;
  (<0>,1308) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1317) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1319) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1321) rcutree.c:2083: for_each_possible_cpu(i) {
  (<0>,1331) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,1333) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,1336) rcutree.c:2108: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,1346)
  (<0>,1347) rcutree.c:1938: static int __cpuinit rcu_cpu_notify(struct notifier_block *self,
  (<0>,1348) rcutree.c:1939: unsigned long action, void *hcpu)
  (<0>,1349) rcutree.c:1939: unsigned long action, void *hcpu)
  (<0>,1351) rcutree.c:1941: long cpu = (long)hcpu;
  (<0>,1352) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1353) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1355) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1357) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1358) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1360) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1361) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1362) rcutree.c:1945: switch (action) {
  (<0>,1364) rcutree.c:1948: rcu_prepare_cpu(cpu);
  (<0>,1368)
  (<0>,1369) rcutree.c:1928: static void __cpuinit rcu_prepare_cpu(int cpu)
  (<0>,1378)
  (<0>,1379) rcutree.c:1882: rcu_init_percpu_data(int cpu, struct rcu_state *rsp, int preemptible)
  (<0>,1380) rcutree.c:1882: rcu_init_percpu_data(int cpu, struct rcu_state *rsp, int preemptible)
  (<0>,1381) rcutree.c:1882: rcu_init_percpu_data(int cpu, struct rcu_state *rsp, int preemptible)
  (<0>,1383) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1385) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1387) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1388) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1391)
  (<0>,1392) rcutree.c:297: return &rsp->node[0];
  (<0>,1396) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1397) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1399) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1403)
  (<0>,1404)
  (<0>,1405) fake_sync.h:80: local_irq_save(flags);
  (<0>,1408) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1410) fake_sched.h:43: return __running_cpu;
  (<0>,1414) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1416) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1420) fake_sched.h:43: return __running_cpu;
  (<0>,1424) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1430) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1431) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1435) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1437) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1438) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1440) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1441) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1443) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1444) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1446) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1449) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1450) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1452) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1453) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1455) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1456) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1458) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1459) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1461) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1463) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1464) rcutree.c:1898: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,1468)
  (<0>,1469) fake_sync.h:117: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,1470) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1475) rcutree.c:1906: raw_spin_lock(&rsp->onofflock);		/* irqs already disabled. */
  (<0>,1479) fake_sync.h:112: preempt_disable();
  (<0>,1481) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1482) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1486) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1488) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1489) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1490) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1492) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1493) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1495) rcutree.c:1913: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,1499) fake_sync.h:112: preempt_disable();
  (<0>,1501) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1502) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1506) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1507) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1509) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1511) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1512) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1514) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1515) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1516) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1517) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1519) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1522) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1524) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1525) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1527) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1528) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1530) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1531) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1533) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1534) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1536) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1538) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1540) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1542) rcutree.c:1921: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,1546)
  (<0>,1547) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1548) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1553) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1555) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1556) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1558) rcutree.c:1923: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,1562) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,1564) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,1568)
  (<0>,1569)
  (<0>,1570) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1571) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1574) fake_sync.h:90: local_irq_restore(flags);
  (<0>,1577) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1579) fake_sched.h:43: return __running_cpu;
  (<0>,1583) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1585) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1589) fake_sched.h:43: return __running_cpu;
  (<0>,1593) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1601) rcutree.c:1931: rcu_init_percpu_data(cpu, &rcu_bh_state, 0);
  (<0>,1610)
  (<0>,1611)
  (<0>,1612)
  (<0>,1613) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1615) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1617) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1619) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1620) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1623)
  (<0>,1624) rcutree.c:297: return &rsp->node[0];
  (<0>,1628) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1629) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1631) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1635)
  (<0>,1636)
  (<0>,1637) fake_sync.h:80: local_irq_save(flags);
  (<0>,1640) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1642) fake_sched.h:43: return __running_cpu;
  (<0>,1646) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1648) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1652) fake_sched.h:43: return __running_cpu;
  (<0>,1656) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1662) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1663) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1667) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1669) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1670) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1672) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1673) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1675) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1676) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1678) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1681) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1682) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1684) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1685) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1687) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1688) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1690) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1691) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1693) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1695) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1696) rcutree.c:1898: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,1700)
  (<0>,1701) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1702) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1707) rcutree.c:1906: raw_spin_lock(&rsp->onofflock);		/* irqs already disabled. */
  (<0>,1711) fake_sync.h:112: preempt_disable();
  (<0>,1713) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1714) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1718) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1720) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1721) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,1722) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1724) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1725) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,1727) rcutree.c:1913: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,1731) fake_sync.h:112: preempt_disable();
  (<0>,1733) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1734) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,1738) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1739) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1741) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1743) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,1744) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1746) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1747) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,1748) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1749) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1751) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,1754) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1756) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1757) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1759) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,1760) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1762) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1763) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1765) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,1766) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1768) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1770) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1772) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,1774) rcutree.c:1921: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,1778)
  (<0>,1779) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1780) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1785) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1787) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1788) rcutree.c:1922: rnp = rnp->parent;
  (<0>,1790) rcutree.c:1923: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,1794) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,1796) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,1800)
  (<0>,1801)
  (<0>,1802) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1803) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,1806) fake_sync.h:90: local_irq_restore(flags);
  (<0>,1809) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1811) fake_sched.h:43: return __running_cpu;
  (<0>,1815) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1817) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1821) fake_sched.h:43: return __running_cpu;
  (<0>,1825) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1833) rcutree.c:1932: rcu_preempt_init_percpu_data(cpu);
  (<0>,1836) rcutree_plugin.h:1090: }
  (<0>,1839) rcutree.c:1949: rcu_prepare_kthreads(cpu);
  (<0>,1843) rcutree_plugin.h:1763: }
  (<0>,1848) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,1850) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,1852) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,1855) rcutree.c:2108: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,1865)
  (<0>,1866)
  (<0>,1867)
  (<0>,1868) rcutree.c:1941: long cpu = (long)hcpu;
  (<0>,1870) rcutree.c:1941: long cpu = (long)hcpu;
  (<0>,1871) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1872) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1874) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1876) rcutree.c:1942: struct rcu_data *rdp = &rcu_state->rda[cpu];
  (<0>,1877) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1879) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1880) rcutree.c:1943: struct rcu_node *rnp = rdp->mynode;
  (<0>,1881) rcutree.c:1945: switch (action) {
  (<0>,1883) rcutree.c:1948: rcu_prepare_cpu(cpu);
  (<0>,1887)
  (<0>,1888) rcutree.c:1930: rcu_init_percpu_data(cpu, &rcu_sched_state, 0);
  (<0>,1897)
  (<0>,1898)
  (<0>,1899)
  (<0>,1900) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1902) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1904) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1906) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1907) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1910)
  (<0>,1911) rcutree.c:297: return &rsp->node[0];
  (<0>,1915) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1916) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1918) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1922)
  (<0>,1923)
  (<0>,1924) fake_sync.h:80: local_irq_save(flags);
  (<0>,1927) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1929) fake_sched.h:43: return __running_cpu;
  (<0>,1933) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1935) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1939) fake_sched.h:43: return __running_cpu;
  (<0>,1943) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1949) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1950) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,1954) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1956) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,1957) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1959) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,1960) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1962) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,1963) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1965) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1968) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,1969) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1971) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,1972) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1974) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1975) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1977) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,1978) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1980) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1982) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,1983) rcutree.c:1898: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,1987)
  (<0>,1988) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1989) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,1994) rcutree.c:1906: raw_spin_lock(&rsp->onofflock);		/* irqs already disabled. */
  (<0>,1998) fake_sync.h:112: preempt_disable();
  (<0>,2000) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2001) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2005) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2007) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2008) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2009) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2011) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2012) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2014) rcutree.c:1913: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2018) fake_sync.h:112: preempt_disable();
  (<0>,2020) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2021) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2025) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2026) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2028) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2030) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2031) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2033) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2034) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2035) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2036) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2038) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2041) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2043) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2044) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2046) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2047) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2049) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2050) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2052) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2053) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2055) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2057) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2059) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2061) rcutree.c:1921: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2065)
  (<0>,2066) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2067) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2072) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2074) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2075) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2077) rcutree.c:1923: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2081) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,2083) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,2087)
  (<0>,2088)
  (<0>,2089) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,2090) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,2093) fake_sync.h:90: local_irq_restore(flags);
  (<0>,2096) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2098) fake_sched.h:43: return __running_cpu;
  (<0>,2102) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2104) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2108) fake_sched.h:43: return __running_cpu;
  (<0>,2112) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2120) rcutree.c:1931: rcu_init_percpu_data(cpu, &rcu_bh_state, 0);
  (<0>,2129)
  (<0>,2130)
  (<0>,2131)
  (<0>,2132) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2134) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2136) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2138) rcutree.c:1886: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2139) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2142)
  (<0>,2143) rcutree.c:297: return &rsp->node[0];
  (<0>,2147) rcutree.c:1887: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2148) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2150) rcutree.c:1890: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2154)
  (<0>,2155)
  (<0>,2156) fake_sync.h:80: local_irq_save(flags);
  (<0>,2159) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2161) fake_sched.h:43: return __running_cpu;
  (<0>,2165) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2167) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2171) fake_sched.h:43: return __running_cpu;
  (<0>,2175) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2181) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,2182) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,2186) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,2188) rcutree.c:1891: rdp->passed_quiesc = 0;  /* We could be racing with new GP, */
  (<0>,2189) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,2191) rcutree.c:1892: rdp->qs_pending = 1;	 /*  so set up to respond to current GP. */
  (<0>,2192) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2194) rcutree.c:1893: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2195) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,2197) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,2200) rcutree.c:1894: rdp->preemptible = preemptible;
  (<0>,2201) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,2203) rcutree.c:1895: rdp->qlen_last_fqs_check = 0;
  (<0>,2204) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2206) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2207) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2209) rcutree.c:1896: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2210) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,2212) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,2214) rcutree.c:1897: rdp->blimit = blimit;
  (<0>,2215) rcutree.c:1898: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2219)
  (<0>,2220) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2221) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2226) rcutree.c:1906: raw_spin_lock(&rsp->onofflock);		/* irqs already disabled. */
  (<0>,2230) fake_sync.h:112: preempt_disable();
  (<0>,2232) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2233) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2237) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2239) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2240) rcutree.c:1909: rnp = rdp->mynode;
  (<0>,2241) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2243) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2244) rcutree.c:1910: mask = rdp->grpmask;
  (<0>,2246) rcutree.c:1913: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2250) fake_sync.h:112: preempt_disable();
  (<0>,2252) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2253) fake_sync.h:113: if (pthread_mutex_lock(l))
  (<0>,2257) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2258) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2260) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2262) rcutree.c:1914: rnp->qsmaskinit |= mask;
  (<0>,2263) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2265) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2266) rcutree.c:1915: mask = rnp->grpmask;
  (<0>,2267) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2268) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2270) rcutree.c:1916: if (rnp == rdp->mynode) {
  (<0>,2273) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2275) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2276) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2278) rcutree.c:1917: rdp->gpnum = rnp->completed; /* if GP in progress... */
  (<0>,2279) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2281) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2282) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2284) rcutree.c:1918: rdp->completed = rnp->completed;
  (<0>,2285) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2287) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2289) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2291) rcutree.c:1919: rdp->passed_quiesc_completed = rnp->completed - 1;
  (<0>,2293) rcutree.c:1921: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2297)
  (<0>,2298) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2299) fake_sync.h:119: if (pthread_mutex_unlock(l))
  (<0>,2304) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2306) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2307) rcutree.c:1922: rnp = rnp->parent;
  (<0>,2309) rcutree.c:1923: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2313) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,2315) rcutree.c:1925: raw_spin_unlock_irqrestore(&rsp->onofflock, flags);
  (<0>,2319)
  (<0>,2320)
  (<0>,2321) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,2322) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,2325) fake_sync.h:90: local_irq_restore(flags);
  (<0>,2328) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2330) fake_sched.h:43: return __running_cpu;
  (<0>,2334) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2336) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2340) fake_sched.h:43: return __running_cpu;
  (<0>,2344) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2352) rcutree.c:1932: rcu_preempt_init_percpu_data(cpu);
  (<0>,2355) rcutree_plugin.h:1090: }
  (<0>,2358) rcutree.c:1949: rcu_prepare_kthreads(cpu);
  (<0>,2362) rcutree_plugin.h:1763: }
  (<0>,2367) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,2369) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,2371) rcutree.c:2107: for_each_online_cpu(cpu)
  (<0>,2379) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2381) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2384) litmus_v3.c:114: set_cpu(i);
  (<0>,2387)
  (<0>,2388) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,2389) fake_sched.h:56: __running_cpu = cpu;
  (<0>,2399) rcutree.c:351: unsigned long flags;
  (<0>,2402) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2404) fake_sched.h:43: return __running_cpu;
  (<0>,2408) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2410) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2414) fake_sched.h:43: return __running_cpu;
  (<0>,2418) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2424) fake_sched.h:43: return __running_cpu;
  (<0>,2428) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,2429) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2431) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2433) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2437) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2440) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2441) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2442) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2444) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2445) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2447) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2450) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2456) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2457) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2460) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2465) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2466) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2467) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2468) rcutree.c:365: local_irq_restore(flags);
  (<0>,2471) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2473) fake_sched.h:43: return __running_cpu;
  (<0>,2477) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2479) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2483) fake_sched.h:43: return __running_cpu;
  (<0>,2487) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2494) fake_sched.h:43: return __running_cpu;
  (<0>,2498) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
  (<0>,2508) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2510) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2512) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2515) litmus_v3.c:114: set_cpu(i);
  (<0>,2518)
  (<0>,2519) fake_sched.h:56: __running_cpu = cpu;
  (<0>,2520) fake_sched.h:56: __running_cpu = cpu;
  (<0>,2530) rcutree.c:354: local_irq_save(flags);
  (<0>,2533) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2535) fake_sched.h:43: return __running_cpu;
  (<0>,2539) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2541) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2545) fake_sched.h:43: return __running_cpu;
  (<0>,2549) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2555) fake_sched.h:43: return __running_cpu;
  (<0>,2559) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,2560) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2562) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2564) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,2568) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2571) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2572) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2573) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2575) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2576) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,2578) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2581) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2587) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2588) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2591) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2596) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2597) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2598) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,2599) rcutree.c:365: local_irq_restore(flags);
  (<0>,2602) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2604) fake_sched.h:43: return __running_cpu;
  (<0>,2608) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2610) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2614) fake_sched.h:43: return __running_cpu;
  (<0>,2618) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2625) fake_sched.h:43: return __running_cpu;
  (<0>,2629) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
  (<0>,2639) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2641) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2643) litmus_v3.c:113: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,2654)
  (<0>,2655) litmus_v3.c:46: void *thread_reader(void *arg)
  (<0>,2658)
  (<0>,2659) fake_sched.h:56: __running_cpu = cpu;
  (<0>,2660) fake_sched.h:56: __running_cpu = cpu;
  (<0>,2663) fake_sched.h:43: return __running_cpu;
  (<0>,2667)
  (<0>,2668) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,2671) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,2682) rcutree.c:383: unsigned long flags;
  (<0>,2685) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2687) fake_sched.h:43: return __running_cpu;
  (<0>,2691) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2693) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2697) fake_sched.h:43: return __running_cpu;
  (<0>,2701) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2707) fake_sched.h:43: return __running_cpu;
  (<0>,2711) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,2712) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2714) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2716) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2720) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2723) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2724) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2725) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2727) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2728) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,2730) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2733) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2740) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2741) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2744) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2749) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2750) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2751) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,2752) rcutree.c:397: local_irq_restore(flags);
  (<0>,2755) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2757) fake_sched.h:43: return __running_cpu;
  (<0>,2761) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2763) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2767) fake_sched.h:43: return __running_cpu;
  (<0>,2771) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2786) litmus_v3.c:52: r_x = x;
  (<0>,2787) litmus_v3.c:52: r_x = x;
  (<0>,2791) fake_sched.h:43: return __running_cpu;
  (<0>,2795) fake_sched.h:153: if (!local_irq_depth[get_cpu()]) {
  (<0>,2799) fake_sched.h:43: return __running_cpu;
  (<0>,2803) fake_sched.h:154: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2808) fake_sched.h:43: return __running_cpu;
  (<0>,2812) fake_sched.h:157: local_irq_depth[get_cpu()] = 1;
  (<0>,2824) rcutree.c:386: local_irq_save(flags);
  (<0>,2827) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2829) fake_sched.h:43: return __running_cpu;
  (<0>,2833) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2835) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2840) fake_sched.h:43: return __running_cpu;
  (<0>,2844) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,2845) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2847) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2849) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,2852) rcutree.c:389: local_irq_restore(flags);
  (<0>,2855) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2857) fake_sched.h:43: return __running_cpu;
  (<0>,2861) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2863) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2872) fake_sched.h:43: return __running_cpu;
  (<0>,2877)
  (<0>,2878) rcutree.c:1290: void rcu_check_callbacks(int cpu, int user)
  (<0>,2879) rcutree.c:1290: void rcu_check_callbacks(int cpu, int user)
  (<0>,2882) rcutree.c:1320: rcu_bh_qs(cpu);
  (<0>,2886)
  (<0>,2887) rcutree.c:172: void rcu_bh_qs(int cpu)
  (<0>,2890) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
  (<0>,2891) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,2893) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,2895) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,2897) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,2899) rcutree.c:178: rdp->passed_quiesc = 1;
  (<0>,2901) rcutree.c:178: rdp->passed_quiesc = 1;
  (<0>,2904) rcutree.c:1322: rcu_preempt_check_callbacks(cpu);
  (<0>,2907) rcutree_plugin.h:1024: }
  (<0>,2909) rcutree.c:1323: if (rcu_pending(cpu))
  (<0>,2912)
  (<0>,2913) rcutree.c:1755: static int rcu_pending(int cpu)
  (<0>,2921)
  (<0>,2922) rcutree.c:1687: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,2923) rcutree.c:1687: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,2925) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,2926) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,2927) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,2929) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,2931) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,2932) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,2933) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,2940)
  (<0>,2941) rcutree.c:613: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,2942) rcutree.c:613: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,2945) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,2946) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,2947) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,2949) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,2950) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,2951) rcutree.c:623: rnp = rdp->mynode;
  (<0>,2953) rcutree.c:623: rnp = rdp->mynode;
  (<0>,2954) rcutree.c:623: rnp = rdp->mynode;
  (<0>,2955) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,2957) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,2958) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,2960) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,2964) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
  (<0>,2967)
  (<0>,2968) rcutree.c:150: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,2970) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,2971) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,2973) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,2981) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,2983) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,2986) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,2988) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,2991) rcutree.c:1704: rdp->n_rp_qs_pending++;
  (<0>,2993) rcutree.c:1704: rdp->n_rp_qs_pending++;
  (<0>,2995) rcutree.c:1704: rdp->n_rp_qs_pending++;
  (<0>,2996) rcutree.c:1705: if (!rdp->preemptible &&
  (<0>,2998) rcutree.c:1705: if (!rdp->preemptible &&
  (<0>,3001) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
  (<0>,3003) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
  (<0>,3005) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
  (<0>,3013) rcutree.c:1715: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,3016)
  (<0>,3017) rcutree.c:278: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,3019) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3022) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3028) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3029) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3033)
  (<0>,3034) rcutree.c:287: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3035) rcutree.c:287: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3038) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3039) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3046) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,3048) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,3049) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,3051) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,3054) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,3056) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,3057) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,3059) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,3062) rcutree.c:1739: if (rcu_gp_in_progress(rsp) &&
  (<0>,3065)
  (<0>,3066) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3068) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3069) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3071) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3077) rcutree.c:1746: rdp->n_rp_need_nothing++;
  (<0>,3079) rcutree.c:1746: rdp->n_rp_need_nothing++;
  (<0>,3081) rcutree.c:1746: rdp->n_rp_need_nothing++;
  (<0>,3082) rcutree.c:1747: return 0;
  (<0>,3084) rcutree.c:1748: }
  (<0>,3088) rcutree.c:1758: __rcu_pending(&rcu_bh_state, &rcu_bh_data[cpu]) ||
  (<0>,3096)
  (<0>,3097)
  (<0>,3098) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,3100) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,3101) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,3102) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,3104) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,3106) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,3107) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,3108) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,3115)
  (<0>,3116)
  (<0>,3117) rcutree.c:619: if (rcu_cpu_stall_suppress)
  (<0>,3120) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,3121) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,3122) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,3124) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,3125) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,3126) rcutree.c:623: rnp = rdp->mynode;
  (<0>,3128) rcutree.c:623: rnp = rdp->mynode;
  (<0>,3129) rcutree.c:623: rnp = rdp->mynode;
  (<0>,3130) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,3132) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,3133) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,3135) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,3139) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
  (<0>,3142)
  (<0>,3143) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3145) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3146) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3148) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,3156) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,3158) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,3161) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,3163) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,3166) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,3168) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,3171) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,3173) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,3176) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,3178) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,3180) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,3181) rcutree.c:1711: return 1;
  (<0>,3183) rcutree.c:1748: }
  (<0>,3194) fake_sched.h:43: return __running_cpu;
  (<0>,3198) rcutree.c:1526: raise_softirq(RCU_SOFTIRQ);
  (<0>,3205) fake_sched.h:43: return __running_cpu;
  (<0>,3209) fake_sched.h:162: local_irq_depth[get_cpu()] = 0;
  (<0>,3211) fake_sched.h:43: return __running_cpu;
  (<0>,3215) fake_sched.h:163: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3221) fake_sched.h:43: return __running_cpu;
  (<0>,3225) fake_sched.h:192: if (need_softirq[get_cpu()]) {
  (<0>,3230) rcutree.c:1499: &rcu_sched_data[get_cpu()]);
  (<0>,3232) fake_sched.h:43: return __running_cpu;
  (<0>,3243)
  (<0>,3244) rcutree.c:1460: __rcu_process_callbacks(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3245) rcutree.c:1460: __rcu_process_callbacks(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3247) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3254) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3255) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3257) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3263) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3264) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3265) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3266) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3268) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3269) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3273) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,3274) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,3280)
  (<0>,3281) rcutree.c:781: rcu_process_gp_end(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3282) rcutree.c:781: rcu_process_gp_end(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3285) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3287) fake_sched.h:43: return __running_cpu;
  (<0>,3291) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3293) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3297) fake_sched.h:43: return __running_cpu;
  (<0>,3301) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3306) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3308) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3309) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3310) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3312) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3313) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3315) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3318) rcutree.c:790: local_irq_restore(flags);
  (<0>,3321) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3323) fake_sched.h:43: return __running_cpu;
  (<0>,3327) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3329) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3333) fake_sched.h:43: return __running_cpu;
  (<0>,3337) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3344) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,3345) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,3349)
  (<0>,3350) rcutree.c:1071: rcu_check_quiescent_state(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3351) rcutree.c:1071: rcu_check_quiescent_state(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3352) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,3358)
  (<0>,3359) rcutree.c:721: check_for_new_grace_period(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3360) rcutree.c:721: check_for_new_grace_period(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3361) rcutree.c:726: local_irq_save(flags);
  (<0>,3364) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3366) fake_sched.h:43: return __running_cpu;
  (<0>,3370) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3372) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3376) fake_sched.h:43: return __running_cpu;
  (<0>,3380) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3385) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3387) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3388) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3390) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3393) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,3394) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,3400)
  (<0>,3401) rcutree.c:699: static void note_new_gpnum(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3402) rcutree.c:699: static void note_new_gpnum(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,3405) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3407) fake_sched.h:43: return __running_cpu;
  (<0>,3411) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3413) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3417) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3419) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3420) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3421) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3423) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3424) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
    (<0.0>,1)
    (<0.0>,2)
    (<0.0>,3) litmus_v3.c:75: set_cpu(cpu0);
    (<0.0>,6)
    (<0.0>,7) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,8) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,11) fake_sched.h:43: return __running_cpu;
    (<0.0>,15)
    (<0.0>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,30) rcutree.c:386: local_irq_save(flags);
    (<0.0>,33) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,35) fake_sched.h:43: return __running_cpu;
    (<0.0>,39) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,41) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,45) fake_sched.h:43: return __running_cpu;
    (<0.0>,49) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,55) fake_sched.h:43: return __running_cpu;
    (<0.0>,59) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,60) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,62) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,64) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,68) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,71) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,72) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,73) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,75) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,76) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,78) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,81) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,88) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,89) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,92) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,97) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,98) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,99) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,100) rcutree.c:397: local_irq_restore(flags);
    (<0.0>,103) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,105) fake_sched.h:43: return __running_cpu;
    (<0.0>,109) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,111) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,115) fake_sched.h:43: return __running_cpu;
    (<0.0>,119) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,127) litmus_v3.c:78: x = 1;
    (<0.0>,138) rcupdate.h:178: }
    (<0.0>,143)
    (<0.0>,144) fake_sync.h:260: x->done = 0;
    (<0.0>,146) fake_sync.h:260: x->done = 0;
    (<0.0>,154)
    (<0.0>,155)
    (<0.0>,156) rcutree.c:1601: __call_rcu(head, func, &rcu_sched_state);
    (<0.0>,157) rcutree.c:1601: __call_rcu(head, func, &rcu_sched_state);
    (<0.0>,166)
    (<0.0>,167)
    (<0.0>,168)
    (<0.0>,169) rcutree.c:1536: debug_rcu_head_queue(head);
    (<0.0>,172) rcupdate.h:808: }
    (<0.0>,174) rcutree.c:1537: head->func = func;
    (<0.0>,175) rcutree.c:1537: head->func = func;
    (<0.0>,178) rcutree.c:1537: head->func = func;
    (<0.0>,179) rcutree.c:1538: head->next = NULL;
    (<0.0>,181) rcutree.c:1538: head->next = NULL;
    (<0.0>,183) rcutree.c:1548: local_irq_save(flags);
    (<0.0>,186) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,188) fake_sched.h:43: return __running_cpu;
    (<0.0>,192) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,194) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,198) fake_sched.h:43: return __running_cpu;
    (<0.0>,202) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,208) fake_sched.h:43: return __running_cpu;
    (<0.0>,211) rcutree.c:1549: rdp = &rsp->rda[get_cpu()];
    (<0.0>,213) rcutree.c:1549: rdp = &rsp->rda[get_cpu()];
    (<0.0>,215) rcutree.c:1549: rdp = &rsp->rda[get_cpu()];
    (<0.0>,216) rcutree.c:1552: *rdp->nxttail[RCU_NEXT_TAIL] = head;
    (<0.0>,217) rcutree.c:1552: *rdp->nxttail[RCU_NEXT_TAIL] = head;
    (<0.0>,220) rcutree.c:1552: *rdp->nxttail[RCU_NEXT_TAIL] = head;
    (<0.0>,221) rcutree.c:1552: *rdp->nxttail[RCU_NEXT_TAIL] = head;
    (<0.0>,222) rcutree.c:1553: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
    (<0.0>,224) rcutree.c:1553: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
    (<0.0>,227) rcutree.c:1553: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
    (<0.0>,228) rcutree.c:1554: rdp->qlen++;
    (<0.0>,230) rcutree.c:1554: rdp->qlen++;
    (<0.0>,232) rcutree.c:1554: rdp->qlen++;
    (<0.0>,233) rcutree.c:1557: if (irqs_disabled_flags(flags)) {
    (<0.0>,236) fake_sched.h:169: return !!local_irq_depth[get_cpu()];
    (<0.0>,238) fake_sched.h:43: return __running_cpu;
    (<0.0>,242) fake_sched.h:169: return !!local_irq_depth[get_cpu()];
    (<0.0>,250) rcutree.c:1558: local_irq_restore(flags);
    (<0.0>,253) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,255) fake_sched.h:43: return __running_cpu;
    (<0.0>,259) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,261) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,280) fake_sync.h:266: might_sleep();
    (<0.0>,284) fake_sched.h:43: return __running_cpu;
    (<0.0>,288) fake_sched.h:96: rcu_enter_nohz();
    (<0.0>,297) rcutree.c:354: local_irq_save(flags);
    (<0.0>,300) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,302) fake_sched.h:43: return __running_cpu;
    (<0.0>,306) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,308) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,312) fake_sched.h:43: return __running_cpu;
    (<0.0>,316) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,322) fake_sched.h:43: return __running_cpu;
    (<0.0>,326) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,327) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,329) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,331) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,335) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,338) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,339) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,340) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,342) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,343) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,345) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,348) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,354) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,355) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,358) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,363) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,364) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,365) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,366) rcutree.c:365: local_irq_restore(flags);
    (<0.0>,369) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,371) fake_sched.h:43: return __running_cpu;
    (<0.0>,375) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,377) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,381) fake_sched.h:43: return __running_cpu;
    (<0.0>,385) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,392) fake_sched.h:43: return __running_cpu;
    (<0.0>,396) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
    (<0.0>,405) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,408) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,413) fake_sync.h:269: while (!x->done)
      (<0.1>,1)
      (<0.1>,2)
      (<0.1>,3) litmus_v3.c:91: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,30) rcutree.c:386: local_irq_save(flags);
      (<0.1>,33) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:43: return __running_cpu;
      (<0.1>,39) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,41) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,45) fake_sched.h:43: return __running_cpu;
      (<0.1>,49) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,55) fake_sched.h:43: return __running_cpu;
      (<0.1>,59) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,60) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,62) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,64) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,68) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,71) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,72) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,73) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,75) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,76) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,78) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,81) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,88) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,89) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,92) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,97) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,98) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,99) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,100) rcutree.c:397: local_irq_restore(flags);
      (<0.1>,103) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,105) fake_sched.h:43: return __running_cpu;
      (<0.1>,109) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,111) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,115) fake_sched.h:43: return __running_cpu;
      (<0.1>,119) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,130) fake_sched.h:43: return __running_cpu;
      (<0.1>,134) fake_sched.h:153: if (!local_irq_depth[get_cpu()]) {
      (<0.1>,138) fake_sched.h:43: return __running_cpu;
      (<0.1>,142) fake_sched.h:154: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,147) fake_sched.h:43: return __running_cpu;
      (<0.1>,151) fake_sched.h:157: local_irq_depth[get_cpu()] = 1;
      (<0.1>,163) rcutree.c:386: local_irq_save(flags);
      (<0.1>,166) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,168) fake_sched.h:43: return __running_cpu;
      (<0.1>,172) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,174) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,179) fake_sched.h:43: return __running_cpu;
      (<0.1>,183) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,184) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,186) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,188) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,191) rcutree.c:389: local_irq_restore(flags);
      (<0.1>,194) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,196) fake_sched.h:43: return __running_cpu;
      (<0.1>,200) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,202) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,211) fake_sched.h:43: return __running_cpu;
      (<0.1>,216)
      (<0.1>,217)
      (<0.1>,218) rcutree.c:1292: if (user ||
      (<0.1>,221) rcutree.c:1320: rcu_bh_qs(cpu);
      (<0.1>,225)
      (<0.1>,226) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,229) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,230) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,232) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,234) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,236) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,238) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,240) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,243) rcutree.c:1322: rcu_preempt_check_callbacks(cpu);
      (<0.1>,246) rcutree_plugin.h:1024: }
      (<0.1>,248) rcutree.c:1323: if (rcu_pending(cpu))
      (<0.1>,251)
      (<0.1>,252) rcutree.c:1757: return __rcu_pending(&rcu_sched_state, &rcu_sched_data[cpu]) ||
      (<0.1>,260)
      (<0.1>,261)
      (<0.1>,262) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,264) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,265) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,266) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,268) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,270) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,271) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,272) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,279)
      (<0.1>,280)
      (<0.1>,281) rcutree.c:619: if (rcu_cpu_stall_suppress)
      (<0.1>,284) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,285) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,286) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,288) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,289) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,290) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,292) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,293) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,294) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,296) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,297) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,299) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,303) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
      (<0.1>,306)
      (<0.1>,307) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,309) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,310) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,312) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,320) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,322) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,325) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,327) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,330) rcutree.c:1704: rdp->n_rp_qs_pending++;
      (<0.1>,332) rcutree.c:1704: rdp->n_rp_qs_pending++;
      (<0.1>,334) rcutree.c:1704: rdp->n_rp_qs_pending++;
      (<0.1>,335) rcutree.c:1705: if (!rdp->preemptible &&
      (<0.1>,337) rcutree.c:1705: if (!rdp->preemptible &&
      (<0.1>,340) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
      (<0.1>,342) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
      (<0.1>,344) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
      (<0.1>,352) rcutree.c:1715: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
      (<0.1>,355)
      (<0.1>,356) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,358) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,361) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,367) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,368) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,372)
      (<0.1>,373)
      (<0.1>,374) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,377) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,378) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,381) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,384)
      (<0.1>,385) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,387) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,388) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,390) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,401) rcutree.c:1722: rdp->n_rp_cpu_needs_gp++;
      (<0.1>,403) rcutree.c:1722: rdp->n_rp_cpu_needs_gp++;
      (<0.1>,405) rcutree.c:1722: rdp->n_rp_cpu_needs_gp++;
      (<0.1>,406) rcutree.c:1723: return 1;
      (<0.1>,408) rcutree.c:1748: }
      (<0.1>,419) fake_sched.h:43: return __running_cpu;
      (<0.1>,423) rcutree.c:1526: raise_softirq(RCU_SOFTIRQ);
      (<0.1>,430) fake_sched.h:43: return __running_cpu;
      (<0.1>,434) fake_sched.h:162: local_irq_depth[get_cpu()] = 0;
      (<0.1>,436) fake_sched.h:43: return __running_cpu;
      (<0.1>,440) fake_sched.h:163: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,446) fake_sched.h:43: return __running_cpu;
      (<0.1>,450) fake_sched.h:192: if (need_softirq[get_cpu()]) {
      (<0.1>,455) rcutree.c:1499: &rcu_sched_data[get_cpu()]);
      (<0.1>,457) fake_sched.h:43: return __running_cpu;
      (<0.1>,468)
      (<0.1>,469)
      (<0.1>,470) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,472) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,479) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,480) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,482) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,488) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,489) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,490) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,491) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,493) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,494) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,498) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,499) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,505)
      (<0.1>,506)
      (<0.1>,507) rcutree.c:786: local_irq_save(flags);
      (<0.1>,510) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,512) fake_sched.h:43: return __running_cpu;
      (<0.1>,516) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,518) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,522) fake_sched.h:43: return __running_cpu;
      (<0.1>,526) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,531) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,533) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,534) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,535) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,537) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,538) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,540) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,543) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,546) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,548) fake_sched.h:43: return __running_cpu;
      (<0.1>,552) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,554) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,558) fake_sched.h:43: return __running_cpu;
      (<0.1>,562) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,569) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,570) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,574)
      (<0.1>,575)
      (<0.1>,576) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,577) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,583)
      (<0.1>,584)
      (<0.1>,585) rcutree.c:724: int ret = 0;
      (<0.1>,586) rcutree.c:726: local_irq_save(flags);
      (<0.1>,589) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,591) fake_sched.h:43: return __running_cpu;
      (<0.1>,595) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,597) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,601) fake_sched.h:43: return __running_cpu;
      (<0.1>,605) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,610) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,612) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,613) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,615) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,618) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,619) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,625)
      (<0.1>,626)
      (<0.1>,627) rcutree.c:704: local_irq_save(flags);
      (<0.1>,630) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,632) fake_sched.h:43: return __running_cpu;
      (<0.1>,636) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,638) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,642) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,644) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,645) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,646) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,648) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,649) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,651) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,654) rcutree.c:708: local_irq_restore(flags);
      (<0.1>,657) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,659) fake_sched.h:43: return __running_cpu;
      (<0.1>,663) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,665) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,671) rcutree.c:729: ret = 1;
      (<0.1>,673) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,676) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,678) fake_sched.h:43: return __running_cpu;
      (<0.1>,682) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,684) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,688) fake_sched.h:43: return __running_cpu;
      (<0.1>,692) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,697) rcutree.c:732: return ret;
      (<0.1>,703) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,704) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,708)
      (<0.1>,709)
      (<0.1>,710) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,713) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,714) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,717) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,720)
      (<0.1>,721) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,723) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,724) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,726) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,737) rcutree.c:1484: raw_spin_lock_irqsave(&rcu_get_root(rsp)->lock, flags);
      (<0.1>,740)
      (<0.1>,741) rcutree.c:297: return &rsp->node[0];
      (<0.1>,746) rcutree.c:1484: raw_spin_lock_irqsave(&rcu_get_root(rsp)->lock, flags);
      (<0.1>,750)
      (<0.1>,751)
      (<0.1>,752) fake_sync.h:80: local_irq_save(flags);
      (<0.1>,755) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,757) fake_sched.h:43: return __running_cpu;
      (<0.1>,761) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,763) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,767) fake_sched.h:43: return __running_cpu;
      (<0.1>,771) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,777) fake_sync.h:82: if (pthread_mutex_lock(l))
      (<0.1>,778) fake_sync.h:82: if (pthread_mutex_lock(l))
      (<0.1>,782) rcutree.c:1485: rcu_start_gp(rsp, flags);  /* releases above lock */
      (<0.1>,783) rcutree.c:1485: rcu_start_gp(rsp, flags);  /* releases above lock */
      (<0.1>,792)
      (<0.1>,793) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,795) fake_sched.h:43: return __running_cpu;
      (<0.1>,798) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,800) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,802) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,803) rcutree.c:837: struct rcu_node *rnp = rcu_get_root(rsp);
      (<0.1>,806)
      (<0.1>,807) rcutree.c:297: return &rsp->node[0];
      (<0.1>,811) rcutree.c:837: struct rcu_node *rnp = rcu_get_root(rsp);
      (<0.1>,812) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,813) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,817)
      (<0.1>,818)
      (<0.1>,819) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,822) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,823) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,826) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,829)
      (<0.1>,830) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,832) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,833) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,835) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,846) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,848) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,852) rcutree.c:863: rsp->gpnum++;
      (<0.1>,854) rcutree.c:863: rsp->gpnum++;
      (<0.1>,856) rcutree.c:863: rsp->gpnum++;
      (<0.1>,857) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,859) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,865) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,866) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,868) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,873) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,874) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,875) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,876) rcutree.c:865: rsp->signaled = RCU_GP_INIT; /* Hold off force_quiescent_state. */
      (<0.1>,878) rcutree.c:865: rsp->signaled = RCU_GP_INIT; /* Hold off force_quiescent_state. */
      (<0.1>,879) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,881) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,883) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,884) rcutree.c:867: record_gp_stall_check_time(rsp);
      (<0.1>,887)
      (<0.1>,888) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,889) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,891) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,892) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,894) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,896) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,898) rcutree.c:871: rcu_preempt_check_blocked_tasks(rnp);
      (<0.1>,904)
      (<0.1>,905) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,907) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,912) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,913) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,915) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,919) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,920) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,921) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,923) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,925) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,926) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,928) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,929) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,931) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,932) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,934) rcutree.c:877: rnp->gpnum = rsp->gpnum;
  (<0>,3426) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3429) rcutree.c:707: !raw_spin_trylock(&rnp->lock)) { /* irqs already off, so later. */
  (<0>,3434) fake_sync.h:126: preempt_disable();
  (<0>,3436) fake_sync.h:127: if (pthread_mutex_trylock(l)) {
      (<0.1>,935) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,937) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,938) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,940) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,941) rcutree.c:879: rsp->signaled = RCU_SIGNAL_INIT; /* force_quiescent_state OK. */
      (<0.1>,943) rcutree.c:879: rsp->signaled = RCU_SIGNAL_INIT; /* force_quiescent_state OK. */
      (<0.1>,944) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,945) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,946) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,951)
      (<0.1>,952)
      (<0.1>,953)
      (<0.1>,954) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,955) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,956) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,961)
      (<0.1>,962)
      (<0.1>,963)
      (<0.1>,964) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,966) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,967) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,969) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,972) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,975) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,976) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,979) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,980) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,983) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,984) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,987) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,988) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,991) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,992) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,995) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,996) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,998) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,999) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,1001) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,1002) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,1004) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,1005) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,1007) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,1011) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,1013) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,1014) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,1016) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,1022) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1025) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1026) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1029) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1030) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1033) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1034) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1037) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,1038) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,1039) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,1040) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,1045)
      (<0.1>,1046)
      (<0.1>,1047)
      (<0.1>,1048) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,1050) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,1051) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,1053) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,1056) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,1058) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,1059) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,1061) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,1062) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,1064) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,1065) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,1067) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,1071) rcutree.c:688: rdp->qs_pending = 1;
      (<0.1>,1073) rcutree.c:688: rdp->qs_pending = 1;
      (<0.1>,1074) rcutree.c:690: rdp->passed_quiesc = 0;
      (<0.1>,1076) rcutree.c:690: rdp->passed_quiesc = 0;
      (<0.1>,1081) rcutree.c:881: rcu_preempt_boost_start_gp(rnp);
      (<0.1>,1084) rcutree_plugin.h:1736: }
      (<0.1>,1086) rcutree.c:882: raw_spin_unlock_irqrestore(&rnp->lock, flags);
      (<0.1>,1088) rcutree.c:882: raw_spin_unlock_irqrestore(&rnp->lock, flags);
      (<0.1>,1092)
      (<0.1>,1093)
      (<0.1>,1094) fake_sync.h:88: if (pthread_mutex_unlock(l))
      (<0.1>,1095) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,3437) fake_sync.h:127: if (pthread_mutex_trylock(l)) {
  (<0>,3440) fake_sync.h:131: return 1;
  (<0>,3442) fake_sync.h:132: }
  (<0>,3446) rcutree.c:711: __note_new_gpnum(rsp, rnp, rdp);
  (<0>,3447) rcutree.c:711: __note_new_gpnum(rsp, rnp, rdp);
  (<0>,3448) rcutree.c:711: __note_new_gpnum(rsp, rnp, rdp);
  (<0>,3453)
  (<0>,3454)
  (<0>,3455)
  (<0>,3456) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
  (<0>,3458) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
  (<0>,3459) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
  (<0>,3461) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
  (<0>,3464) rcutree.c:683: rdp->gpnum = rnp->gpnum;
  (<0>,3466) rcutree.c:683: rdp->gpnum = rnp->gpnum;
  (<0>,3467) rcutree.c:683: rdp->gpnum = rnp->gpnum;
  (<0>,3469) rcutree.c:683: rdp->gpnum = rnp->gpnum;
  (<0>,3470) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
  (<0>,3472) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
  (<0>,3473) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
  (<0>,3475) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
  (<0>,3479) rcutree.c:688: rdp->qs_pending = 1;
  (<0>,3481) rcutree.c:688: rdp->qs_pending = 1;
  (<0>,3482) rcutree.c:690: rdp->passed_quiesc = 0;
  (<0>,3484) rcutree.c:690: rdp->passed_quiesc = 0;
  (<0>,3488) rcutree.c:712: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,3490) rcutree.c:712: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,3494)
  (<0>,3495)
  (<0>,3496) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,3497) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,3500) fake_sync.h:90: local_irq_restore(flags);
  (<0>,3503) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3505) fake_sched.h:43: return __running_cpu;
  (<0>,3509) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3511) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3519) rcutree.c:729: ret = 1;
  (<0>,3521) rcutree.c:731: local_irq_restore(flags);
  (<0>,3524) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3526) fake_sched.h:43: return __running_cpu;
  (<0>,3530) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3532) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3536) fake_sched.h:43: return __running_cpu;
  (<0>,3540) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3545) rcutree.c:732: return ret;
  (<0>,3551) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3552) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3556)
  (<0>,3557)
  (<0>,3558) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3561) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3562) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3569) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,3572)
  (<0>,3573) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3575) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3578) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3586) fake_sched.h:43: return __running_cpu;
  (<0>,3597)
  (<0>,3598)
  (<0>,3599) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3601) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3608) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3609) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3611) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3617) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3618) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3619) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,3620) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3622) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3623) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,3627) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,3628) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,3634)
  (<0>,3635)
  (<0>,3636) rcutree.c:786: local_irq_save(flags);
  (<0>,3639) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3641) fake_sched.h:43: return __running_cpu;
  (<0>,3645) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3647) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3651) fake_sched.h:43: return __running_cpu;
  (<0>,3655) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3660) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3662) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3663) rcutree.c:787: rnp = rdp->mynode;
  (<0>,3664) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3666) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3667) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3669) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,3672) rcutree.c:790: local_irq_restore(flags);
  (<0>,3675) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3677) fake_sched.h:43: return __running_cpu;
  (<0>,3681) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3683) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3687) fake_sched.h:43: return __running_cpu;
  (<0>,3691) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3698) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,3699) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,3703)
  (<0>,3704)
  (<0>,3705) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,3706) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,3712)
  (<0>,3713)
  (<0>,3714) rcutree.c:724: int ret = 0;
  (<0>,3715) rcutree.c:726: local_irq_save(flags);
  (<0>,3718) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3720) fake_sched.h:43: return __running_cpu;
  (<0>,3724) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3726) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3730) fake_sched.h:43: return __running_cpu;
  (<0>,3734) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3739) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3741) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3742) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3744) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,3747) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,3748) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,3754)
  (<0>,3755)
  (<0>,3756) rcutree.c:704: local_irq_save(flags);
  (<0>,3759) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3761) fake_sched.h:43: return __running_cpu;
  (<0>,3765) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3767) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3771) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3773) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3774) rcutree.c:705: rnp = rdp->mynode;
  (<0>,3775) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3777) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3778) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3780) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,3783) rcutree.c:708: local_irq_restore(flags);
  (<0>,3786) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3788) fake_sched.h:43: return __running_cpu;
  (<0>,3792) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3794) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3800) rcutree.c:729: ret = 1;
  (<0>,3802) rcutree.c:731: local_irq_restore(flags);
  (<0>,3805) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3807) fake_sched.h:43: return __running_cpu;
  (<0>,3811) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3813) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3817) fake_sched.h:43: return __running_cpu;
  (<0>,3821) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3826) rcutree.c:732: return ret;
  (<0>,3832) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3833) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,3837)
  (<0>,3838)
  (<0>,3839) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3842) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3843) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,3850) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,3853)
  (<0>,3854) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3856) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3859) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,3872) fake_sched.h:43: return __running_cpu;
  (<0>,3876) fake_sched.h:194: need_softirq[get_cpu()] = 0;
  (<0>,3887) rcutree.c:354: local_irq_save(flags);
  (<0>,3890) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3892) fake_sched.h:43: return __running_cpu;
  (<0>,3896) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3898) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3902) fake_sched.h:43: return __running_cpu;
  (<0>,3906) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3912) fake_sched.h:43: return __running_cpu;
  (<0>,3916) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3917) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,3919) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,3921) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,3924) rcutree.c:357: local_irq_restore(flags);
  (<0>,3927) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3929) fake_sched.h:43: return __running_cpu;
  (<0>,3933) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3935) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3939) fake_sched.h:43: return __running_cpu;
  (<0>,3943) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3955) fake_sched.h:43: return __running_cpu;
  (<0>,3959)
  (<0>,3960) rcutree.c:187: rcu_sched_qs(cpu);
  (<0>,3964)
  (<0>,3965) rcutree.c:165: struct rcu_data *rdp = &rcu_sched_data[cpu];
  (<0>,3968) rcutree.c:165: struct rcu_data *rdp = &rcu_sched_data[cpu];
  (<0>,3969) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,3971) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,3973) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,3975) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,3977) rcutree.c:169: rdp->passed_quiesc = 1;
  (<0>,3979) rcutree.c:169: rdp->passed_quiesc = 1;
  (<0>,3981) rcutree.c:188: rcu_preempt_note_context_switch(cpu);
  (<0>,3984) rcutree_plugin.h:938: }
  (<0>,3988) fake_sched.h:43: return __running_cpu;
  (<0>,3992) fake_sched.h:96: rcu_enter_nohz();
  (<0>,4001) rcutree.c:354: local_irq_save(flags);
  (<0>,4004) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4006) fake_sched.h:43: return __running_cpu;
  (<0>,4010) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4012) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4016) fake_sched.h:43: return __running_cpu;
  (<0>,4020) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4026) fake_sched.h:43: return __running_cpu;
  (<0>,4030) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4031) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,4033) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,4035) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,4039) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4042) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4043) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4044) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4046) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4047) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,4049) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4052) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4058) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4059) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4062) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4067) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4068) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4069) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4070) rcutree.c:365: local_irq_restore(flags);
  (<0>,4073) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4075) fake_sched.h:43: return __running_cpu;
  (<0>,4079) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4081) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4085) fake_sched.h:43: return __running_cpu;
  (<0>,4089) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4096) fake_sched.h:43: return __running_cpu;
  (<0>,4100) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
  (<0>,4109) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,4112) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,4117) fake_sched.h:43: return __running_cpu;
  (<0>,4121)
  (<0>,4122) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,4125) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,4136) rcutree.c:386: local_irq_save(flags);
  (<0>,4139) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4141) fake_sched.h:43: return __running_cpu;
  (<0>,4145) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4147) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4151) fake_sched.h:43: return __running_cpu;
  (<0>,4155) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4161) fake_sched.h:43: return __running_cpu;
  (<0>,4165) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4166) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4168) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4170) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4174) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4177) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4178) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4179) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4181) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4182) rcutree.c:393: atomic_inc(&rdtp->dynticks);
  (<0>,4184) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4187) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4194) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4195) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4198) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4203) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4204) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4205) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4206) rcutree.c:397: local_irq_restore(flags);
  (<0>,4209) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4211) fake_sched.h:43: return __running_cpu;
  (<0>,4215) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4217) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4221) fake_sched.h:43: return __running_cpu;
  (<0>,4225) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4237) fake_sched.h:43: return __running_cpu;
  (<0>,4241) fake_sched.h:153: if (!local_irq_depth[get_cpu()]) {
  (<0>,4245) fake_sched.h:43: return __running_cpu;
  (<0>,4249) fake_sched.h:154: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4254) fake_sched.h:43: return __running_cpu;
  (<0>,4258) fake_sched.h:157: local_irq_depth[get_cpu()] = 1;
  (<0>,4270) rcutree.c:386: local_irq_save(flags);
  (<0>,4273) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4275) fake_sched.h:43: return __running_cpu;
  (<0>,4279) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4281) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4286) fake_sched.h:43: return __running_cpu;
  (<0>,4290) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4291) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4293) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4295) rcutree.c:388: if (rdtp->dynticks_nesting++) {
  (<0>,4298) rcutree.c:389: local_irq_restore(flags);
  (<0>,4301) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4303) fake_sched.h:43: return __running_cpu;
  (<0>,4307) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4309) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4318) fake_sched.h:43: return __running_cpu;
  (<0>,4323)
  (<0>,4324)
  (<0>,4325) rcutree.c:1292: if (user ||
  (<0>,4328) rcutree.c:1320: rcu_bh_qs(cpu);
  (<0>,4332)
  (<0>,4333) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
  (<0>,4336) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
  (<0>,4337) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,4339) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,4341) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,4343) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
  (<0>,4345) rcutree.c:178: rdp->passed_quiesc = 1;
  (<0>,4347) rcutree.c:178: rdp->passed_quiesc = 1;
  (<0>,4350) rcutree.c:1322: rcu_preempt_check_callbacks(cpu);
  (<0>,4353) rcutree_plugin.h:1024: }
  (<0>,4355) rcutree.c:1323: if (rcu_pending(cpu))
  (<0>,4358)
  (<0>,4359) rcutree.c:1757: return __rcu_pending(&rcu_sched_state, &rcu_sched_data[cpu]) ||
  (<0>,4367)
  (<0>,4368)
  (<0>,4369) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,4371) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,4372) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
  (<0>,4373) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,4375) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,4377) rcutree.c:1691: rdp->n_rcu_pending++;
  (<0>,4378) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,4379) rcutree.c:1694: check_cpu_stall(rsp, rdp);
  (<0>,4386)
  (<0>,4387)
  (<0>,4388) rcutree.c:619: if (rcu_cpu_stall_suppress)
  (<0>,4391) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,4392) rcutree.c:621: j = ACCESS_ONCE(jiffies);
  (<0>,4393) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,4395) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,4396) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
  (<0>,4397) rcutree.c:623: rnp = rdp->mynode;
  (<0>,4399) rcutree.c:623: rnp = rdp->mynode;
  (<0>,4400) rcutree.c:623: rnp = rdp->mynode;
  (<0>,4401) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4403) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4404) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4406) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4410) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4411) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
  (<0>,4415) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
  (<0>,4418)
  (<0>,4419) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4421) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4422) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4424) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,4430) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
  (<0>,4431) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
  (<0>,4438) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,4440) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,4443) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,4445) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
  (<0>,4448) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,4450) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,4453) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,4455) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
  (<0>,4458) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,4460) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,4462) rcutree.c:1710: rdp->n_rp_report_qs++;
  (<0>,4463) rcutree.c:1711: return 1;
  (<0>,4465) rcutree.c:1748: }
  (<0>,4476) fake_sched.h:43: return __running_cpu;
  (<0>,4480) rcutree.c:1526: raise_softirq(RCU_SOFTIRQ);
  (<0>,4487) fake_sched.h:43: return __running_cpu;
  (<0>,4491) fake_sched.h:162: local_irq_depth[get_cpu()] = 0;
  (<0>,4493) fake_sched.h:43: return __running_cpu;
  (<0>,4497) fake_sched.h:163: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4503) fake_sched.h:43: return __running_cpu;
  (<0>,4507) fake_sched.h:192: if (need_softirq[get_cpu()]) {
  (<0>,4512) rcutree.c:1499: &rcu_sched_data[get_cpu()]);
  (<0>,4514) fake_sched.h:43: return __running_cpu;
  (<0>,4525)
  (<0>,4526)
  (<0>,4527) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4529) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4536) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4537) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4539) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4545) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4546) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4547) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,4548) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,4550) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,4551) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,4555) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,4556) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,4562)
  (<0>,4563)
  (<0>,4564) rcutree.c:786: local_irq_save(flags);
  (<0>,4567) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4569) fake_sched.h:43: return __running_cpu;
  (<0>,4573) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4575) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4579) fake_sched.h:43: return __running_cpu;
  (<0>,4583) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4588) rcutree.c:787: rnp = rdp->mynode;
  (<0>,4590) rcutree.c:787: rnp = rdp->mynode;
  (<0>,4591) rcutree.c:787: rnp = rdp->mynode;
  (<0>,4592) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,4594) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,4595) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,4597) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,4600) rcutree.c:789: !raw_spin_trylock(&rnp->lock)) { /* irqs already off, so later. */
  (<0>,4605) fake_sync.h:126: preempt_disable();
  (<0>,4607) fake_sync.h:127: if (pthread_mutex_trylock(l)) {
  (<0>,4608) fake_sync.h:127: if (pthread_mutex_trylock(l)) {
  (<0>,4611) fake_sync.h:131: return 1;
  (<0>,4613) fake_sync.h:132: }
  (<0>,4617) rcutree.c:793: __rcu_process_gp_end(rsp, rnp, rdp);
  (<0>,4618) rcutree.c:793: __rcu_process_gp_end(rsp, rnp, rdp);
  (<0>,4619) rcutree.c:793: __rcu_process_gp_end(rsp, rnp, rdp);
  (<0>,4624)
  (<0>,4625)
  (<0>,4626)
  (<0>,4627) rcutree.c:745: if (rdp->completed != rnp->completed) {
  (<0>,4629) rcutree.c:745: if (rdp->completed != rnp->completed) {
  (<0>,4630) rcutree.c:745: if (rdp->completed != rnp->completed) {
  (<0>,4632) rcutree.c:745: if (rdp->completed != rnp->completed) {
  (<0>,4635) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
  (<0>,4638) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
  (<0>,4639) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
  (<0>,4642) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
  (<0>,4643) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
  (<0>,4646) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
  (<0>,4647) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
  (<0>,4650) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
  (<0>,4651) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4654) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4655) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4658) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4659) rcutree.c:753: rdp->completed = rnp->completed;
  (<0>,4661) rcutree.c:753: rdp->completed = rnp->completed;
  (<0>,4662) rcutree.c:753: rdp->completed = rnp->completed;
  (<0>,4664) rcutree.c:753: rdp->completed = rnp->completed;
  (<0>,4665) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
  (<0>,4667) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
  (<0>,4668) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
  (<0>,4670) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
  (<0>,4674) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
  (<0>,4676) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
  (<0>,4677) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
  (<0>,4679) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
  (<0>,4685) rcutree.c:794: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4687) rcutree.c:794: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4691)
  (<0>,4692)
  (<0>,4693) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,4694) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,4697) fake_sync.h:90: local_irq_restore(flags);
  (<0>,4700) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4702) fake_sched.h:43: return __running_cpu;
  (<0>,4706) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4708) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4712) fake_sched.h:43: return __running_cpu;
  (<0>,4716) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4725) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,4726) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,4730)
  (<0>,4731)
  (<0>,4732) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,4733) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,4739)
  (<0>,4740)
  (<0>,4741) rcutree.c:724: int ret = 0;
  (<0>,4742) rcutree.c:726: local_irq_save(flags);
  (<0>,4745) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4747) fake_sched.h:43: return __running_cpu;
  (<0>,4751) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4753) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4757) fake_sched.h:43: return __running_cpu;
  (<0>,4761) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4766) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,4768) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,4769) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,4771) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,4774) rcutree.c:731: local_irq_restore(flags);
  (<0>,4777) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4779) fake_sched.h:43: return __running_cpu;
  (<0>,4783) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4785) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4789) fake_sched.h:43: return __running_cpu;
  (<0>,4793) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4798) rcutree.c:732: return ret;
  (<0>,4802) rcutree.c:1081: if (!rdp->qs_pending)
  (<0>,4804) rcutree.c:1081: if (!rdp->qs_pending)
  (<0>,4807) rcutree.c:1088: if (!rdp->passed_quiesc)
  (<0>,4809) rcutree.c:1088: if (!rdp->passed_quiesc)
  (<0>,4812) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4814) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4815) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4816) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4817) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4819) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
  (<0>,4828)
  (<0>,4829)
  (<0>,4830)
  (<0>,4831)
  (<0>,4832) rcutree.c:1032: rnp = rdp->mynode;
  (<0>,4834) rcutree.c:1032: rnp = rdp->mynode;
  (<0>,4835) rcutree.c:1032: rnp = rdp->mynode;
  (<0>,4836) rcutree.c:1033: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4838) rcutree.c:1033: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4842)
  (<0>,4843)
  (<0>,4844) fake_sync.h:80: local_irq_save(flags);
  (<0>,4847) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4849) fake_sched.h:43: return __running_cpu;
  (<0>,4853) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4855) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4859) fake_sched.h:43: return __running_cpu;
  (<0>,4863) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4869) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,4870) fake_sync.h:82: if (pthread_mutex_lock(l))
  (<0>,4874) rcutree.c:1034: if (lastcomp != rnp->completed) {
  (<0>,4875) rcutree.c:1034: if (lastcomp != rnp->completed) {
  (<0>,4877) rcutree.c:1034: if (lastcomp != rnp->completed) {
  (<0>,4880) rcutree.c:1048: mask = rdp->grpmask;
  (<0>,4882) rcutree.c:1048: mask = rdp->grpmask;
  (<0>,4883) rcutree.c:1048: mask = rdp->grpmask;
  (<0>,4884) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
  (<0>,4886) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
  (<0>,4887) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
  (<0>,4891) rcutree.c:1052: rdp->qs_pending = 0;
  (<0>,4893) rcutree.c:1052: rdp->qs_pending = 0;
  (<0>,4894) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4897) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4898) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4901) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
  (<0>,4902) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,4903) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,4904) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,4905) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,4915)
  (<0>,4916)
  (<0>,4917)
  (<0>,4918) rcutree.c:978: for (;;) {
  (<0>,4920) rcutree.c:979: if (!(rnp->qsmask & mask)) {
  (<0>,4922) rcutree.c:979: if (!(rnp->qsmask & mask)) {
  (<0>,4923) rcutree.c:979: if (!(rnp->qsmask & mask)) {
  (<0>,4927) rcutree.c:985: rnp->qsmask &= ~mask;
  (<0>,4929) rcutree.c:985: rnp->qsmask &= ~mask;
  (<0>,4931) rcutree.c:985: rnp->qsmask &= ~mask;
  (<0>,4933) rcutree.c:985: rnp->qsmask &= ~mask;
  (<0>,4934) rcutree.c:987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,4936) rcutree.c:987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,4939) rcutree.c:990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4941) rcutree.c:990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4945)
  (<0>,4946)
  (<0>,4947) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,4948) fake_sync.h:88: if (pthread_mutex_unlock(l))
  (<0>,4951) fake_sync.h:90: local_irq_restore(flags);
  (<0>,4954) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4956) fake_sched.h:43: return __running_cpu;
  (<0>,4960) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4962) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4966) fake_sched.h:43: return __running_cpu;
  (<0>,4970) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4983) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,4984) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,4988)
  (<0>,4989)
  (<0>,4990) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,4993) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,4994) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5001) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,5004)
  (<0>,5005) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5007) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5010) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5018) fake_sched.h:43: return __running_cpu;
  (<0>,5029)
  (<0>,5030)
  (<0>,5031) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5033) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5040) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5041) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5043) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5049) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5050) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5051) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5052) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,5054) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,5055) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
  (<0>,5059) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,5060) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
  (<0>,5066)
  (<0>,5067)
  (<0>,5068) rcutree.c:786: local_irq_save(flags);
  (<0>,5071) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5073) fake_sched.h:43: return __running_cpu;
  (<0>,5077) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5079) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5083) fake_sched.h:43: return __running_cpu;
  (<0>,5087) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5092) rcutree.c:787: rnp = rdp->mynode;
  (<0>,5094) rcutree.c:787: rnp = rdp->mynode;
  (<0>,5095) rcutree.c:787: rnp = rdp->mynode;
  (<0>,5096) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,5098) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,5099) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,5101) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
  (<0>,5104) rcutree.c:790: local_irq_restore(flags);
  (<0>,5107) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5109) fake_sched.h:43: return __running_cpu;
  (<0>,5113) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5115) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5119) fake_sched.h:43: return __running_cpu;
  (<0>,5123) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5130) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5131) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5135)
  (<0>,5136)
  (<0>,5137) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,5138) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
  (<0>,5144)
  (<0>,5145)
  (<0>,5146) rcutree.c:724: int ret = 0;
  (<0>,5147) rcutree.c:726: local_irq_save(flags);
  (<0>,5150) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5152) fake_sched.h:43: return __running_cpu;
  (<0>,5156) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5158) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5162) fake_sched.h:43: return __running_cpu;
  (<0>,5166) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5171) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,5173) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,5174) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,5176) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
  (<0>,5179) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,5180) rcutree.c:728: note_new_gpnum(rsp, rdp);
  (<0>,5186)
  (<0>,5187)
  (<0>,5188) rcutree.c:704: local_irq_save(flags);
  (<0>,5191) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5193) fake_sched.h:43: return __running_cpu;
  (<0>,5197) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5199) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5203) rcutree.c:705: rnp = rdp->mynode;
  (<0>,5205) rcutree.c:705: rnp = rdp->mynode;
  (<0>,5206) rcutree.c:705: rnp = rdp->mynode;
  (<0>,5207) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,5209) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,5210) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,5212) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
  (<0>,5215) rcutree.c:708: local_irq_restore(flags);
  (<0>,5218) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5220) fake_sched.h:43: return __running_cpu;
  (<0>,5224) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5226) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5232) rcutree.c:729: ret = 1;
  (<0>,5234) rcutree.c:731: local_irq_restore(flags);
  (<0>,5237) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5239) fake_sched.h:43: return __running_cpu;
  (<0>,5243) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5245) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5249) fake_sched.h:43: return __running_cpu;
  (<0>,5253) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5258) rcutree.c:732: return ret;
  (<0>,5264) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5265) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5269)
  (<0>,5270)
  (<0>,5271) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5274) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5275) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
  (<0>,5282) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,5285)
  (<0>,5286) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5288) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5291) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
  (<0>,5304) fake_sched.h:43: return __running_cpu;
  (<0>,5308) fake_sched.h:194: need_softirq[get_cpu()] = 0;
  (<0>,5319) rcutree.c:354: local_irq_save(flags);
  (<0>,5322) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5324) fake_sched.h:43: return __running_cpu;
  (<0>,5328) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5330) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5334) fake_sched.h:43: return __running_cpu;
  (<0>,5338) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5344) fake_sched.h:43: return __running_cpu;
  (<0>,5348) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,5349) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5351) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5353) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5356) rcutree.c:357: local_irq_restore(flags);
  (<0>,5359) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5361) fake_sched.h:43: return __running_cpu;
  (<0>,5365) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5367) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5371) fake_sched.h:43: return __running_cpu;
  (<0>,5375) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1096) fake_sync.h:88: if (pthread_mutex_unlock(l))
      (<0.1>,1098) fake_sync.h:90: local_irq_restore(flags);
      (<0.1>,1101)
      (<0.1>,1103) fake_sched.h:43: return __running_cpu;
      (<0.1>,1107) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1109) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1113) fake_sched.h:43: return __running_cpu;
      (<0.1>,1117) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1127) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,1130)
      (<0.1>,1131) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1133) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1136) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1144) fake_sched.h:43: return __running_cpu;
      (<0.1>,1155)
      (<0.1>,1156)
      (<0.1>,1157) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1159) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1166) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1167) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1169) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1175) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1176) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1177) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,1178) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,1180) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,1181) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,1185) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,1186) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,1192)
      (<0.1>,1193)
      (<0.1>,1194) rcutree.c:786: local_irq_save(flags);
      (<0.1>,1197)
      (<0.1>,1199) fake_sched.h:43: return __running_cpu;
      (<0.1>,1203) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1205) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1209) fake_sched.h:43: return __running_cpu;
      (<0.1>,1213) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1218) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,1220) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,1221) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,1222) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,1224) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,1225) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,1227) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,1230) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,1233)
      (<0.1>,1235) fake_sched.h:43: return __running_cpu;
      (<0.1>,1239) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1241) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1245) fake_sched.h:43: return __running_cpu;
      (<0.1>,1249) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1256) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,1257) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,1261)
      (<0.1>,1262)
      (<0.1>,1263) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,1264) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,1270)
      (<0.1>,1271)
      (<0.1>,1272) rcutree.c:724: int ret = 0;
      (<0.1>,1273) rcutree.c:726: local_irq_save(flags);
      (<0.1>,1276)
      (<0.1>,1278) fake_sched.h:43: return __running_cpu;
      (<0.1>,1282) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1284) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1288) fake_sched.h:43: return __running_cpu;
      (<0.1>,1292) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1297) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,1299) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,1300) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,1302) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,1305) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,1306) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,1312)
      (<0.1>,1313)
      (<0.1>,1314) rcutree.c:704: local_irq_save(flags);
      (<0.1>,1317)
      (<0.1>,1319) fake_sched.h:43: return __running_cpu;
      (<0.1>,1323) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1325) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1329) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,1331) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,1332) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,1333) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,1335) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,1336) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,1338) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,1341) rcutree.c:708: local_irq_restore(flags);
      (<0.1>,1344)
      (<0.1>,1346) fake_sched.h:43: return __running_cpu;
      (<0.1>,1350) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1352) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1358) rcutree.c:729: ret = 1;
      (<0.1>,1360) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,1363)
      (<0.1>,1365) fake_sched.h:43: return __running_cpu;
      (<0.1>,1369) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1371) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1375) fake_sched.h:43: return __running_cpu;
      (<0.1>,1379) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1384) rcutree.c:732: return ret;
      (<0.1>,1390) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,1391) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,1395)
      (<0.1>,1396)
      (<0.1>,1397) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,1400) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,1401) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,1408) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,1411)
      (<0.1>,1412) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1414) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1417) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,1430) fake_sched.h:43: return __running_cpu;
      (<0.1>,1434) fake_sched.h:194: need_softirq[get_cpu()] = 0;
      (<0.1>,1445) rcutree.c:354: local_irq_save(flags);
      (<0.1>,1448)
      (<0.1>,1450) fake_sched.h:43: return __running_cpu;
      (<0.1>,1454) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1456) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1460) fake_sched.h:43: return __running_cpu;
      (<0.1>,1464) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1470) fake_sched.h:43: return __running_cpu;
      (<0.1>,1474) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1475) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1477) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1479) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1482) rcutree.c:357: local_irq_restore(flags);
      (<0.1>,1485)
      (<0.1>,1487) fake_sched.h:43: return __running_cpu;
      (<0.1>,1491) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1493) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1497) fake_sched.h:43: return __running_cpu;
      (<0.1>,1501) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1513) fake_sched.h:43: return __running_cpu;
      (<0.1>,1517)
      (<0.1>,1518) rcutree.c:187: rcu_sched_qs(cpu);
      (<0.1>,1522)
      (<0.1>,1523) rcutree.c:165: struct rcu_data *rdp = &rcu_sched_data[cpu];
      (<0.1>,1526) rcutree.c:165: struct rcu_data *rdp = &rcu_sched_data[cpu];
      (<0.1>,1527) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1529) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1531) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1533) rcutree.c:167: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1535) rcutree.c:169: rdp->passed_quiesc = 1;
      (<0.1>,1537) rcutree.c:169: rdp->passed_quiesc = 1;
      (<0.1>,1539) rcutree.c:188: rcu_preempt_note_context_switch(cpu);
      (<0.1>,1542)
      (<0.1>,1546) fake_sched.h:43: return __running_cpu;
      (<0.1>,1550)
      (<0.1>,1559) rcutree.c:354: local_irq_save(flags);
      (<0.1>,1562)
      (<0.1>,1564) fake_sched.h:43: return __running_cpu;
      (<0.1>,1568) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1570) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1574) fake_sched.h:43: return __running_cpu;
      (<0.1>,1578) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1584) fake_sched.h:43: return __running_cpu;
      (<0.1>,1588) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1589) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1591) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1593) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,1597) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1600) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1601) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1602) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1604) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1605) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,1607) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1610) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1616) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1617) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1620) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1625) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1626) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1627) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1628) rcutree.c:365: local_irq_restore(flags);
      (<0.1>,1631)
      (<0.1>,1633) fake_sched.h:43: return __running_cpu;
      (<0.1>,1637) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1639) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1643) fake_sched.h:43: return __running_cpu;
      (<0.1>,1647) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1654) fake_sched.h:43: return __running_cpu;
      (<0.1>,1658) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
      (<0.1>,1667) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1670) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1675) fake_sched.h:43: return __running_cpu;
      (<0.1>,1679)
      (<0.1>,1680) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,1683) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,1694) rcutree.c:386: local_irq_save(flags);
      (<0.1>,1697)
      (<0.1>,1699) fake_sched.h:43: return __running_cpu;
      (<0.1>,1703) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1705) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1709) fake_sched.h:43: return __running_cpu;
      (<0.1>,1713) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1719) fake_sched.h:43: return __running_cpu;
      (<0.1>,1723) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1724) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1726) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1728) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1732) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1735) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1736) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1737) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1739) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1740) rcutree.c:393: atomic_inc(&rdtp->dynticks);
      (<0.1>,1742) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1745) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1752) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1753) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1756) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1761) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1762) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1763) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,1764) rcutree.c:397: local_irq_restore(flags);
      (<0.1>,1767)
      (<0.1>,1769) fake_sched.h:43: return __running_cpu;
      (<0.1>,1773) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1775) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1779) fake_sched.h:43: return __running_cpu;
      (<0.1>,1783) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1795) fake_sched.h:43: return __running_cpu;
      (<0.1>,1799) fake_sched.h:153: if (!local_irq_depth[get_cpu()]) {
      (<0.1>,1803) fake_sched.h:43: return __running_cpu;
      (<0.1>,1807) fake_sched.h:154: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,1812) fake_sched.h:43: return __running_cpu;
      (<0.1>,1816) fake_sched.h:157: local_irq_depth[get_cpu()] = 1;
      (<0.1>,1828) rcutree.c:386: local_irq_save(flags);
      (<0.1>,1831)
      (<0.1>,1833) fake_sched.h:43: return __running_cpu;
      (<0.1>,1837) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1839) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,1844) fake_sched.h:43: return __running_cpu;
      (<0.1>,1848) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1849) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1851) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1853) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,1856) rcutree.c:389: local_irq_restore(flags);
      (<0.1>,1859)
      (<0.1>,1861) fake_sched.h:43: return __running_cpu;
      (<0.1>,1865) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1867) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1876) fake_sched.h:43: return __running_cpu;
      (<0.1>,1881)
      (<0.1>,1882)
      (<0.1>,1883) rcutree.c:1292: if (user ||
      (<0.1>,1886) rcutree.c:1320: rcu_bh_qs(cpu);
      (<0.1>,1890)
      (<0.1>,1891) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,1894) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,1895) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1897) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1899) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1901) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,1903) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,1905) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,1908) rcutree.c:1322: rcu_preempt_check_callbacks(cpu);
      (<0.1>,1911)
      (<0.1>,1913) rcutree.c:1323: if (rcu_pending(cpu))
      (<0.1>,1916)
      (<0.1>,1917) rcutree.c:1757: return __rcu_pending(&rcu_sched_state, &rcu_sched_data[cpu]) ||
      (<0.1>,1925)
      (<0.1>,1926)
      (<0.1>,1927) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,1929) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,1930) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,1931) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,1933) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,1935) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,1936) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,1937) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,1944)
      (<0.1>,1945)
      (<0.1>,1946) rcutree.c:619: if (rcu_cpu_stall_suppress)
      (<0.1>,1949) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,1950) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,1951) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,1953) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,1954) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,1955) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,1957) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,1958) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,1959) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1961) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1962) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1964) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1968) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1969) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,1973) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
      (<0.1>,1976)
      (<0.1>,1977) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,1979) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,1980) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,1982) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,1988) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
      (<0.1>,1989) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
      (<0.1>,1996) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,1998) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,2001) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,2003) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,2006) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,2008) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,2011) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,2013) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,2016) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,2018) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,2020) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,2021) rcutree.c:1711: return 1;
      (<0.1>,2023) rcutree.c:1748: }
      (<0.1>,2034) fake_sched.h:43: return __running_cpu;
      (<0.1>,2038) rcutree.c:1526: raise_softirq(RCU_SOFTIRQ);
      (<0.1>,2045) fake_sched.h:43: return __running_cpu;
      (<0.1>,2049) fake_sched.h:162: local_irq_depth[get_cpu()] = 0;
      (<0.1>,2051) fake_sched.h:43: return __running_cpu;
      (<0.1>,2055) fake_sched.h:163: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,2061) fake_sched.h:43: return __running_cpu;
      (<0.1>,2065) fake_sched.h:192: if (need_softirq[get_cpu()]) {
      (<0.1>,2070)
      (<0.1>,2072) fake_sched.h:43: return __running_cpu;
      (<0.1>,2083)
      (<0.1>,2084)
      (<0.1>,2085) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2087) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2094) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2095) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2097) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2103) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2104) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2105) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,2106) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,2108) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,2109) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,2113) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,2114) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,2120)
      (<0.1>,2121)
      (<0.1>,2122) rcutree.c:786: local_irq_save(flags);
      (<0.1>,2125)
      (<0.1>,2127) fake_sched.h:43: return __running_cpu;
      (<0.1>,2131) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2133) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2137) fake_sched.h:43: return __running_cpu;
      (<0.1>,2141) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,2146) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,2148) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,2149) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,2150) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2152) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2153) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2155) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,2158) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,2161)
      (<0.1>,2163) fake_sched.h:43: return __running_cpu;
      (<0.1>,2167) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2169) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2173) fake_sched.h:43: return __running_cpu;
      (<0.1>,2177) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,2184) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,2185) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,2189)
      (<0.1>,2190)
      (<0.1>,2191) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,2192) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,2198)
      (<0.1>,2199)
      (<0.1>,2200) rcutree.c:724: int ret = 0;
      (<0.1>,2201) rcutree.c:726: local_irq_save(flags);
      (<0.1>,2204)
      (<0.1>,2206) fake_sched.h:43: return __running_cpu;
      (<0.1>,2210) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2212) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2216) fake_sched.h:43: return __running_cpu;
      (<0.1>,2220) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,2225) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2227) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2228) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2230) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,2233) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,2236)
      (<0.1>,2238) fake_sched.h:43: return __running_cpu;
      (<0.1>,2242) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2244) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2248) fake_sched.h:43: return __running_cpu;
      (<0.1>,2252) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,2257) rcutree.c:732: return ret;
      (<0.1>,2261) rcutree.c:1081: if (!rdp->qs_pending)
      (<0.1>,2263) rcutree.c:1081: if (!rdp->qs_pending)
      (<0.1>,2266) rcutree.c:1088: if (!rdp->passed_quiesc)
      (<0.1>,2268) rcutree.c:1088: if (!rdp->passed_quiesc)
      (<0.1>,2271) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2273) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2274) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2275) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2276) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2278) rcutree.c:1095: rcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);
      (<0.1>,2287)
      (<0.1>,2288)
      (<0.1>,2289)
      (<0.1>,2290)
      (<0.1>,2291) rcutree.c:1032: rnp = rdp->mynode;
      (<0.1>,2293) rcutree.c:1032: rnp = rdp->mynode;
      (<0.1>,2294) rcutree.c:1032: rnp = rdp->mynode;
      (<0.1>,2295) rcutree.c:1033: raw_spin_lock_irqsave(&rnp->lock, flags);
      (<0.1>,2297) rcutree.c:1033: raw_spin_lock_irqsave(&rnp->lock, flags);
      (<0.1>,2301)
      (<0.1>,2302)
      (<0.1>,2303) fake_sync.h:80: local_irq_save(flags);
      (<0.1>,2306)
      (<0.1>,2308) fake_sched.h:43: return __running_cpu;
      (<0.1>,2312) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2314) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2318) fake_sched.h:43: return __running_cpu;
      (<0.1>,2322) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,2328) fake_sync.h:82: if (pthread_mutex_lock(l))
      (<0.1>,2329) fake_sync.h:82: if (pthread_mutex_lock(l))
      (<0.1>,2333) rcutree.c:1034: if (lastcomp != rnp->completed) {
      (<0.1>,2334) rcutree.c:1034: if (lastcomp != rnp->completed) {
      (<0.1>,2336) rcutree.c:1034: if (lastcomp != rnp->completed) {
      (<0.1>,2339) rcutree.c:1048: mask = rdp->grpmask;
      (<0.1>,2341) rcutree.c:1048: mask = rdp->grpmask;
      (<0.1>,2342) rcutree.c:1048: mask = rdp->grpmask;
      (<0.1>,2343) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
      (<0.1>,2345) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
      (<0.1>,2346) rcutree.c:1049: if ((rnp->qsmask & mask) == 0) {
      (<0.1>,2350) rcutree.c:1052: rdp->qs_pending = 0;
      (<0.1>,2352) rcutree.c:1052: rdp->qs_pending = 0;
      (<0.1>,2353) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2356) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2357) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2360) rcutree.c:1058: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2361) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
      (<0.1>,2362) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
      (<0.1>,2363) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
      (<0.1>,2364) rcutree.c:1060: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
      (<0.1>,2374)
      (<0.1>,2375)
      (<0.1>,2376)
      (<0.1>,2377)
      (<0.1>,2379) rcutree.c:979: if (!(rnp->qsmask & mask)) {
      (<0.1>,2381) rcutree.c:979: if (!(rnp->qsmask & mask)) {
      (<0.1>,2382) rcutree.c:979: if (!(rnp->qsmask & mask)) {
      (<0.1>,2386) rcutree.c:985: rnp->qsmask &= ~mask;
      (<0.1>,2388) rcutree.c:985: rnp->qsmask &= ~mask;
      (<0.1>,2390) rcutree.c:985: rnp->qsmask &= ~mask;
      (<0.1>,2392) rcutree.c:985: rnp->qsmask &= ~mask;
      (<0.1>,2393) rcutree.c:987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
      (<0.1>,2395) rcutree.c:987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
      (<0.1>,2398) rcutree.c:987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
      (<0.1>,2401)
      (<0.1>,2405) rcutree.c:994: mask = rnp->grpmask;
      (<0.1>,2407) rcutree.c:994: mask = rnp->grpmask;
      (<0.1>,2408) rcutree.c:994: mask = rnp->grpmask;
      (<0.1>,2409) rcutree.c:995: if (rnp->parent == NULL) {
      (<0.1>,2411) rcutree.c:995: if (rnp->parent == NULL) {
      (<0.1>,2415) rcutree.c:1013: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
      (<0.1>,2416) rcutree.c:1013: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
      (<0.1>,2424)
      (<0.1>,2425)
      (<0.1>,2426) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
      (<0.1>,2429)
      (<0.1>,2430) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2432) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2433) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2435) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2444) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
      (<0.1>,2445) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
      (<0.1>,2448)
      (<0.1>,2449) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2451) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2452) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2454) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2461) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
      (<0.1>,2462) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
      (<0.1>,2463) rcutree.c:944: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
      (<0.1>,2465) rcutree.c:951: gp_duration = jiffies - rsp->gp_start;
      (<0.1>,2466) rcutree.c:951: gp_duration = jiffies - rsp->gp_start;
      (<0.1>,2468) rcutree.c:951: gp_duration = jiffies - rsp->gp_start;
      (<0.1>,2470) rcutree.c:951: gp_duration = jiffies - rsp->gp_start;
      (<0.1>,2471) rcutree.c:952: if (gp_duration > rsp->gp_max)
      (<0.1>,2472) rcutree.c:952: if (gp_duration > rsp->gp_max)
      (<0.1>,2474) rcutree.c:952: if (gp_duration > rsp->gp_max)
      (<0.1>,2477) rcutree.c:954: rsp->completed = rsp->gpnum;
      (<0.1>,2479) rcutree.c:954: rsp->completed = rsp->gpnum;
      (<0.1>,2480) rcutree.c:954: rsp->completed = rsp->gpnum;
      (<0.1>,2482) rcutree.c:954: rsp->completed = rsp->gpnum;
      (<0.1>,2483) rcutree.c:955: rsp->signaled = RCU_GP_IDLE;
      (<0.1>,2485) rcutree.c:955: rsp->signaled = RCU_GP_IDLE;
      (<0.1>,2486) rcutree.c:956: rcu_start_gp(rsp, flags);  /* releases root node's rnp->lock. */
      (<0.1>,2487) rcutree.c:956: rcu_start_gp(rsp, flags);  /* releases root node's rnp->lock. */
      (<0.1>,2496)
      (<0.1>,2497)
      (<0.1>,2499) fake_sched.h:43: return __running_cpu;
      (<0.1>,2502) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,2504) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,2506) rcutree.c:836: struct rcu_data *rdp = &rsp->rda[get_cpu()];
      (<0.1>,2507) rcutree.c:837: struct rcu_node *rnp = rcu_get_root(rsp);
      (<0.1>,2510)
      (<0.1>,2511) rcutree.c:297: return &rsp->node[0];
      (<0.1>,2515) rcutree.c:837: struct rcu_node *rnp = rcu_get_root(rsp);
      (<0.1>,2516) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,2517) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,2521)
      (<0.1>,2522)
      (<0.1>,2523) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2526) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2527) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2530) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2533)
      (<0.1>,2534) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2536) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2537) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2539) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,2550) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,2552) rcutree.c:839: if (!cpu_needs_another_gp(rsp, rdp) || rsp->fqs_active) {
      (<0.1>,2556) rcutree.c:863: rsp->gpnum++;
      (<0.1>,2558) rcutree.c:863: rsp->gpnum++;
      (<0.1>,2560) rcutree.c:863: rsp->gpnum++;
      (<0.1>,2561) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,2563) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,2569) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,2570) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,2572) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,2577) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,2578) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,2579) rcutree.c:864: WARN_ON_ONCE(rsp->signaled == RCU_GP_INIT);
      (<0.1>,2580) rcutree.c:865: rsp->signaled = RCU_GP_INIT; /* Hold off force_quiescent_state. */
      (<0.1>,2582) rcutree.c:865: rsp->signaled = RCU_GP_INIT; /* Hold off force_quiescent_state. */
      (<0.1>,2583) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,2585) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,2587) rcutree.c:866: rsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;
      (<0.1>,2588) rcutree.c:867: record_gp_stall_check_time(rsp);
      (<0.1>,2591)
      (<0.1>,2592) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,2593) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,2595) rcutree.c:534: rsp->gp_start = jiffies;
      (<0.1>,2596) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,2598) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,2600) rcutree.c:535: rsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;
      (<0.1>,2602) rcutree.c:871: rcu_preempt_check_blocked_tasks(rnp);
      (<0.1>,2608)
      (<0.1>,2609) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,2611) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,2616) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,2617) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,2619) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,2623) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,2624) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,2625) rcutree_plugin.h:990: WARN_ON_ONCE(rnp->qsmask);
      (<0.1>,2627) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,2629) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,2630) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,2632) rcutree.c:875: rnp->qsmask = rnp->qsmaskinit;
      (<0.1>,2633) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,2635) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,2636) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,2638) rcutree.c:877: rnp->gpnum = rsp->gpnum;
      (<0.1>,2639) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,2641) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,2642) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,2644) rcutree.c:878: rnp->completed = rsp->completed;
      (<0.1>,2645) rcutree.c:879: rsp->signaled = RCU_SIGNAL_INIT; /* force_quiescent_state OK. */
      (<0.1>,2647) rcutree.c:879: rsp->signaled = RCU_SIGNAL_INIT; /* force_quiescent_state OK. */
      (<0.1>,2648) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,2649) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,2650) rcutree.c:880: rcu_start_gp_per_cpu(rsp, rnp, rdp);
      (<0.1>,2655)
      (<0.1>,2656)
      (<0.1>,2657)
      (<0.1>,2658) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,2659) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,2660) rcutree.c:806: __rcu_process_gp_end(rsp, rnp, rdp);
      (<0.1>,2665)
      (<0.1>,2666)
      (<0.1>,2667)
      (<0.1>,2668) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,2670) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,2671) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,2673) rcutree.c:745: if (rdp->completed != rnp->completed) {
      (<0.1>,2676) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,2679) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,2680) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,2683) rcutree.c:748: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];
      (<0.1>,2684) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,2687) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,2688) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,2691) rcutree.c:749: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];
      (<0.1>,2692) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2695) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2696) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2699) rcutree.c:750: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2700) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,2702) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,2703) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,2705) rcutree.c:753: rdp->completed = rnp->completed;
      (<0.1>,2706) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,2708) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,2709) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,2711) rcutree.c:763: if (ULONG_CMP_LT(rdp->gpnum, rdp->completed))
      (<0.1>,2715) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,2717) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,2718) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,2720) rcutree.c:770: if ((rnp->qsmask & rdp->grpmask) == 0)
      (<0.1>,2726) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2729) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2730) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2733) rcutree.c:819: rdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2734) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2737) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2738) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2741) rcutree.c:820: rdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];
      (<0.1>,2742) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,2743) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,2744) rcutree.c:823: __note_new_gpnum(rsp, rnp, rdp);
      (<0.1>,2749)
      (<0.1>,2750)
      (<0.1>,2751)
      (<0.1>,2752) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,2754) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,2755) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,2757) rcutree.c:677: if (rdp->gpnum != rnp->gpnum) {
      (<0.1>,2760) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,2762) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,2763) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,2765) rcutree.c:683: rdp->gpnum = rnp->gpnum;
      (<0.1>,2766) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,2768) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,2769) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,2771) rcutree.c:684: if (rnp->qsmask & rdp->grpmask) {
      (<0.1>,2775) rcutree.c:688: rdp->qs_pending = 1;
      (<0.1>,2777) rcutree.c:688: rdp->qs_pending = 1;
      (<0.1>,2778) rcutree.c:690: rdp->passed_quiesc = 0;
      (<0.1>,2780) rcutree.c:690: rdp->passed_quiesc = 0;
      (<0.1>,2785) rcutree.c:881: rcu_preempt_boost_start_gp(rnp);
      (<0.1>,2788)
      (<0.1>,2790) rcutree.c:882: raw_spin_unlock_irqrestore(&rnp->lock, flags);
      (<0.1>,2792) rcutree.c:882: raw_spin_unlock_irqrestore(&rnp->lock, flags);
      (<0.1>,2796)
      (<0.1>,2797)
      (<0.1>,2798) fake_sync.h:88: if (pthread_mutex_unlock(l))
      (<0.1>,2799) fake_sync.h:88: if (pthread_mutex_unlock(l))
      (<0.1>,2802) fake_sync.h:90: local_irq_restore(flags);
      (<0.1>,2805)
      (<0.1>,2807) fake_sched.h:43: return __running_cpu;
      (<0.1>,2811) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2813) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,2817) fake_sched.h:43: return __running_cpu;
      (<0.1>,2821) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,2837) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,2838) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,2842)
      (<0.1>,2843)
      (<0.1>,2844) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2847) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2848) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,2855) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,2858)
      (<0.1>,2859) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2861) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2864) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2870) rcutree.c:1490: invoke_rcu_callbacks(rsp, rdp);
      (<0.1>,2871) rcutree.c:1490: invoke_rcu_callbacks(rsp, rdp);
      (<0.1>,2875)
      (<0.1>,2876)
      (<0.1>,2877) rcutree.c:1515: if (unlikely(!ACCESS_ONCE(rcu_scheduler_fully_active)))
      (<0.1>,2886) rcutree.c:1517: if (likely(!rsp->boost)) {
      (<0.1>,2888) rcutree.c:1517: if (likely(!rsp->boost)) {
      (<0.1>,2897) rcutree.c:1518: rcu_do_batch(rsp, rdp);
      (<0.1>,2898) rcutree.c:1518: rcu_do_batch(rsp, rdp);
      (<0.1>,2907)
      (<0.1>,2908)
      (<0.1>,2909) rcutree.c:1219: if (!cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,2912)
      (<0.1>,2913) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2915) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2918) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2924) rcutree.c:1226: local_irq_save(flags);
      (<0.1>,2927)
      (<0.1>,2929) fake_sched.h:43: return __running_cpu;
      (<0.1>,2933) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2935) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,2939) fake_sched.h:43: return __running_cpu;
      (<0.1>,2943) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,2948) rcutree.c:1227: list = rdp->nxtlist;
      (<0.1>,2950) rcutree.c:1227: list = rdp->nxtlist;
      (<0.1>,2951) rcutree.c:1227: list = rdp->nxtlist;
      (<0.1>,2952) rcutree.c:1228: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2955) rcutree.c:1228: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2956) rcutree.c:1228: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2957) rcutree.c:1228: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2959) rcutree.c:1228: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2960) rcutree.c:1229: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
      (<0.1>,2963) rcutree.c:1229: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
      (<0.1>,2964) rcutree.c:1229: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
      (<0.1>,2965) rcutree.c:1230: tail = rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2968) rcutree.c:1230: tail = rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2969) rcutree.c:1230: tail = rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,2970) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,2972) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,2975) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,2977) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,2980) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,2981) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,2984) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,2987) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,2989) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,2991) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,2994) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,2997) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,2999) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,3001) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,3004) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3006) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3009) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3010) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3013) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3016) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3018) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3020) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3023) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3026) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,3028) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,3030) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,3033) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3035) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3038) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3039) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3042) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3045) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3047) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3049) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3052) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3055) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,3057) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,3059) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,3062) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3064) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3067) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3068) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3071) rcutree.c:1232: if (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])
      (<0.1>,3074) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3076) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3078) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3081) rcutree.c:1233: rdp->nxttail[count] = &rdp->nxtlist;
      (<0.1>,3084) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,3086) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,3088) rcutree.c:1231: for (count = RCU_NEXT_SIZE - 1; count >= 0; count--)
      (<0.1>,3091) rcutree.c:1234: local_irq_restore(flags);
      (<0.1>,3094)
      (<0.1>,3096) fake_sched.h:43: return __running_cpu;
      (<0.1>,3100) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3102) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3106) fake_sched.h:43: return __running_cpu;
      (<0.1>,3110) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,3115) rcutree.c:1237: count = 0;
      (<0.1>,3117) rcutree.c:1238: while (list) {
      (<0.1>,3120) rcutree.c:1239: next = list->next;
      (<0.1>,3122) rcutree.c:1239: next = list->next;
      (<0.1>,3123) rcutree.c:1239: next = list->next;
      (<0.1>,3126) rcutree.c:1241: debug_rcu_head_unqueue(list);
      (<0.1>,3129)
      (<0.1>,3131) rcutree.c:1242: __rcu_reclaim(list);
      (<0.1>,3136)
      (<0.1>,3137) rcupdate.h:837: unsigned long offset = (unsigned long)head->func;
      (<0.1>,3140) rcupdate.h:837: unsigned long offset = (unsigned long)head->func;
      (<0.1>,3142) rcupdate.h:837: unsigned long offset = (unsigned long)head->func;
      (<0.1>,3143) rcupdate.h:839: if (__is_kfree_rcu_offset(offset))
      (<0.1>,3144) rcupdate.h:839: if (__is_kfree_rcu_offset(offset))
      (<0.1>,3145) rcupdate.h:817: return offset < 4096;
      (<0.1>,3148) rcupdate.h:842: head->func(head);
      (<0.1>,3151) rcupdate.h:842: head->func(head);
      (<0.1>,3152) rcupdate.h:842: head->func(head);
      (<0.1>,3158)
      (<0.1>,3159) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,3160) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,3161) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,3165) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,3166) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,3167) rcupdate.c:105: rcu = container_of(head, struct rcu_synchronize, head);
      (<0.1>,3168) rcupdate.c:106: complete(&rcu->completion);
      (<0.1>,3172)
      (<0.1>,3173) fake_sync.h:276: x->done++;
      (<0.1>,3175) fake_sync.h:276: x->done++;
      (<0.1>,3177) fake_sync.h:276: x->done++;
    (<0.0>,415) fake_sync.h:269: while (!x->done)
    (<0.0>,422) fake_sched.h:43: return __running_cpu;
    (<0.0>,426)
    (<0.0>,427) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,3178) fake_sync.h:277: }
      (<0.1>,3182) rcutree.c:1243: list = next;
      (<0.1>,3183) rcutree.c:1243: list = next;
      (<0.1>,3184) rcutree.c:1244: if (++count >= rdp->blimit)
      (<0.1>,3186) rcutree.c:1244: if (++count >= rdp->blimit)
      (<0.1>,3188) rcutree.c:1244: if (++count >= rdp->blimit)
      (<0.1>,3190) rcutree.c:1244: if (++count >= rdp->blimit)
      (<0.1>,3194) rcutree.c:1238: while (list) {
      (<0.1>,3197) rcutree.c:1248: local_irq_save(flags);
      (<0.1>,3200)
      (<0.1>,3202) fake_sched.h:43: return __running_cpu;
      (<0.1>,3206) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3208) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3212) fake_sched.h:43: return __running_cpu;
      (<0.1>,3216) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,3221) rcutree.c:1251: rdp->qlen -= count;
      (<0.1>,3223) rcutree.c:1251: rdp->qlen -= count;
      (<0.1>,3225) rcutree.c:1251: rdp->qlen -= count;
      (<0.1>,3227) rcutree.c:1251: rdp->qlen -= count;
      (<0.1>,3228) rcutree.c:1252: rdp->n_cbs_invoked += count;
      (<0.1>,3230) rcutree.c:1252: rdp->n_cbs_invoked += count;
      (<0.1>,3232) rcutree.c:1252: rdp->n_cbs_invoked += count;
      (<0.1>,3234) rcutree.c:1252: rdp->n_cbs_invoked += count;
      (<0.1>,3235) rcutree.c:1253: if (list != NULL) {
      (<0.1>,3238) rcutree.c:1264: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
      (<0.1>,3240) rcutree.c:1264: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
      (<0.1>,3243) rcutree.c:1268: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
      (<0.1>,3245) rcutree.c:1268: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
      (<0.1>,3248) rcutree.c:1268: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
      (<0.1>,3250) rcutree.c:1268: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
      (<0.1>,3253) rcutree.c:1271: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
      (<0.1>,3255) rcutree.c:1271: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
      (<0.1>,3256) rcutree.c:1271: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
      (<0.1>,3258) rcutree.c:1271: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
      (<0.1>,3259) rcutree.c:1271: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
      (<0.1>,3265) rcutree.c:1274: local_irq_restore(flags);
      (<0.1>,3268)
      (<0.1>,3270) fake_sched.h:43: return __running_cpu;
      (<0.1>,3274) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3276) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3280) fake_sched.h:43: return __running_cpu;
      (<0.1>,3284) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,3289) rcutree.c:1277: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,3292)
      (<0.1>,3293) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3295) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3298) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3310) fake_sched.h:43: return __running_cpu;
      (<0.1>,3321)
      (<0.1>,3322)
      (<0.1>,3323) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3325) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3332) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3333) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3335) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3341) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3342) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3343) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,3344) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,3346) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,3347) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,3351) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,3352) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,3358)
      (<0.1>,3359)
      (<0.1>,3360) rcutree.c:786: local_irq_save(flags);
      (<0.1>,3363)
      (<0.1>,3365) fake_sched.h:43: return __running_cpu;
      (<0.1>,3369) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3371) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3375) fake_sched.h:43: return __running_cpu;
      (<0.1>,3379) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,3384) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,3386) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,3387) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,3388) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,3390) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,3391) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,3393) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,3396) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,3399)
      (<0.1>,3401) fake_sched.h:43: return __running_cpu;
      (<0.1>,3405) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3407) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3411) fake_sched.h:43: return __running_cpu;
      (<0.1>,3415) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,3422) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,3423) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,3427)
      (<0.1>,3428)
      (<0.1>,3429) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,3430) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,3436)
      (<0.1>,3437)
      (<0.1>,3438) rcutree.c:724: int ret = 0;
      (<0.1>,3439) rcutree.c:726: local_irq_save(flags);
      (<0.1>,3442)
      (<0.1>,3444) fake_sched.h:43: return __running_cpu;
      (<0.1>,3448) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3450) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3454) fake_sched.h:43: return __running_cpu;
      (<0.1>,3458) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,3463) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,3465) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,3466) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,3468) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,3471) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,3472) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,3478)
      (<0.1>,3479)
      (<0.1>,3480) rcutree.c:704: local_irq_save(flags);
      (<0.1>,3483)
      (<0.1>,3485) fake_sched.h:43: return __running_cpu;
      (<0.1>,3489) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3491) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3495) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,3497) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,3498) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,3499) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,3501) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,3502) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,3504) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,3507) rcutree.c:708: local_irq_restore(flags);
      (<0.1>,3510)
      (<0.1>,3512) fake_sched.h:43: return __running_cpu;
      (<0.1>,3516) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3518) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3524) rcutree.c:729: ret = 1;
      (<0.1>,3526) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,3529)
      (<0.1>,3531) fake_sched.h:43: return __running_cpu;
      (<0.1>,3535) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3537) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3541) fake_sched.h:43: return __running_cpu;
      (<0.1>,3545) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,3550) rcutree.c:732: return ret;
      (<0.1>,3556) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,3557) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,3561)
      (<0.1>,3562)
      (<0.1>,3563) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3566) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3567) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3574) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,3577)
      (<0.1>,3578) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3580) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3583) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3596) fake_sched.h:43: return __running_cpu;
      (<0.1>,3600) fake_sched.h:194: need_softirq[get_cpu()] = 0;
      (<0.1>,3611) rcutree.c:354: local_irq_save(flags);
      (<0.1>,3614)
      (<0.1>,3616) fake_sched.h:43: return __running_cpu;
      (<0.1>,3620) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3622) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3626) fake_sched.h:43: return __running_cpu;
      (<0.1>,3630) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,3636) fake_sched.h:43: return __running_cpu;
      (<0.1>,3640) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,3641) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,3643) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,3645) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,3648) rcutree.c:357: local_irq_restore(flags);
      (<0.1>,3651)
      (<0.1>,3653) fake_sched.h:43: return __running_cpu;
      (<0.1>,3657) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3659) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3663) fake_sched.h:43: return __running_cpu;
      (<0.1>,3667) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,3680) fake_sched.h:43: return __running_cpu;
      (<0.1>,3684) fake_sched.h:153: if (!local_irq_depth[get_cpu()]) {
      (<0.1>,3688) fake_sched.h:43: return __running_cpu;
      (<0.1>,3692) fake_sched.h:154: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,3697) fake_sched.h:43: return __running_cpu;
      (<0.1>,3701) fake_sched.h:157: local_irq_depth[get_cpu()] = 1;
      (<0.1>,3713) rcutree.c:386: local_irq_save(flags);
      (<0.1>,3716)
      (<0.1>,3718) fake_sched.h:43: return __running_cpu;
      (<0.1>,3722) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3724) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,3729) fake_sched.h:43: return __running_cpu;
      (<0.1>,3733) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,3734) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,3736) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,3738) rcutree.c:388: if (rdtp->dynticks_nesting++) {
      (<0.1>,3741) rcutree.c:389: local_irq_restore(flags);
      (<0.1>,3744)
      (<0.1>,3746) fake_sched.h:43: return __running_cpu;
      (<0.1>,3750) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3752) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,3761) fake_sched.h:43: return __running_cpu;
      (<0.1>,3766)
      (<0.1>,3767)
      (<0.1>,3768) rcutree.c:1292: if (user ||
      (<0.1>,3771) rcutree.c:1320: rcu_bh_qs(cpu);
      (<0.1>,3775)
      (<0.1>,3776) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,3779) rcutree.c:174: struct rcu_data *rdp = &rcu_bh_data[cpu];
      (<0.1>,3780) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,3782) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,3784) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,3786) rcutree.c:176: rdp->passed_quiesc_completed = rdp->gpnum - 1;
      (<0.1>,3788) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,3790) rcutree.c:178: rdp->passed_quiesc = 1;
      (<0.1>,3793) rcutree.c:1322: rcu_preempt_check_callbacks(cpu);
      (<0.1>,3796)
      (<0.1>,3798) rcutree.c:1323: if (rcu_pending(cpu))
      (<0.1>,3801)
      (<0.1>,3802) rcutree.c:1757: return __rcu_pending(&rcu_sched_state, &rcu_sched_data[cpu]) ||
      (<0.1>,3810)
      (<0.1>,3811)
      (<0.1>,3812) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,3814) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,3815) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,3816) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,3818) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,3820) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,3821) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,3822) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,3829)
      (<0.1>,3830)
      (<0.1>,3831) rcutree.c:619: if (rcu_cpu_stall_suppress)
      (<0.1>,3834) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,3835) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,3836) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,3838) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,3839) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,3840) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,3842) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,3843) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,3844) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3846) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3847) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3849) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3853) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3854) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,3858) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
      (<0.1>,3861)
      (<0.1>,3862) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3864) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3865) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3867) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3873) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
      (<0.1>,3874) rcutree.c:630: ULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {
      (<0.1>,3881) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,3883) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,3886) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,3888) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,3891) rcutree.c:1704: rdp->n_rp_qs_pending++;
      (<0.1>,3893) rcutree.c:1704: rdp->n_rp_qs_pending++;
      (<0.1>,3895) rcutree.c:1704: rdp->n_rp_qs_pending++;
      (<0.1>,3896) rcutree.c:1705: if (!rdp->preemptible &&
      (<0.1>,3898) rcutree.c:1705: if (!rdp->preemptible &&
      (<0.1>,3901) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
      (<0.1>,3903) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
      (<0.1>,3905) rcutree.c:1706: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,
      (<0.1>,3910) rcutree.c:1715: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
      (<0.1>,3913)
      (<0.1>,3914) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3916) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3919) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,3925) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,3926) rcutree.c:1721: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,3930)
      (<0.1>,3931)
      (<0.1>,3932) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3935) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3936) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,3943) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
      (<0.1>,3945) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
      (<0.1>,3946) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
      (<0.1>,3948) rcutree.c:1727: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
      (<0.1>,3951) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,3953) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,3954) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,3956) rcutree.c:1733: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,3959) rcutree.c:1739: if (rcu_gp_in_progress(rsp) &&
      (<0.1>,3962)
      (<0.1>,3963) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3965) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3966) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3968) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,3974) rcutree.c:1740: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies)) {
      (<0.1>,3976) rcutree.c:1740: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies)) {
      (<0.1>,3977) rcutree.c:1740: ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies)) {
      (<0.1>,3981) rcutree.c:1746: rdp->n_rp_need_nothing++;
      (<0.1>,3983) rcutree.c:1746: rdp->n_rp_need_nothing++;
      (<0.1>,3985) rcutree.c:1746: rdp->n_rp_need_nothing++;
      (<0.1>,3986) rcutree.c:1747: return 0;
      (<0.1>,3988) rcutree.c:1748: }
      (<0.1>,3992) rcutree.c:1758: __rcu_pending(&rcu_bh_state, &rcu_bh_data[cpu]) ||
      (<0.1>,4000)
      (<0.1>,4001)
      (<0.1>,4002) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,4004) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,4005) rcutree.c:1689: struct rcu_node *rnp = rdp->mynode;
      (<0.1>,4006) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,4008) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,4010) rcutree.c:1691: rdp->n_rcu_pending++;
      (<0.1>,4011) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,4012) rcutree.c:1694: check_cpu_stall(rsp, rdp);
      (<0.1>,4019)
      (<0.1>,4020)
      (<0.1>,4021) rcutree.c:619: if (rcu_cpu_stall_suppress)
      (<0.1>,4024) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,4025) rcutree.c:621: j = ACCESS_ONCE(jiffies);
      (<0.1>,4026) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,4028) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,4029) rcutree.c:622: js = ACCESS_ONCE(rsp->jiffies_stall);
      (<0.1>,4030) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,4032) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,4033) rcutree.c:623: rnp = rdp->mynode;
      (<0.1>,4034) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,4036) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,4037) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,4039) rcutree.c:624: if ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {
      (<0.1>,4043) rcutree.c:629: } else if (rcu_gp_in_progress(rsp) &&
      (<0.1>,4046)
      (<0.1>,4047) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,4049) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,4050) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,4052) rcutree.c:152: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
      (<0.1>,4060) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,4062) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,4065) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,4067) rcutree.c:1697: if (rdp->qs_pending && !rdp->passed_quiesc) {
      (<0.1>,4070) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,4072) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,4075) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,4077) rcutree.c:1709: } else if (rdp->qs_pending && rdp->passed_quiesc) {
      (<0.1>,4080) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,4082) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,4084) rcutree.c:1710: rdp->n_rp_report_qs++;
      (<0.1>,4085) rcutree.c:1711: return 1;
      (<0.1>,4087) rcutree.c:1748: }
      (<0.1>,4098) fake_sched.h:43: return __running_cpu;
      (<0.1>,4102) rcutree.c:1526: raise_softirq(RCU_SOFTIRQ);
      (<0.1>,4109) fake_sched.h:43: return __running_cpu;
      (<0.1>,4113) fake_sched.h:162: local_irq_depth[get_cpu()] = 0;
      (<0.1>,4115) fake_sched.h:43: return __running_cpu;
      (<0.1>,4119) fake_sched.h:163: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4125) fake_sched.h:43: return __running_cpu;
      (<0.1>,4129) fake_sched.h:192: if (need_softirq[get_cpu()]) {
      (<0.1>,4134)
      (<0.1>,4136) fake_sched.h:43: return __running_cpu;
      (<0.1>,4147)
      (<0.1>,4148)
      (<0.1>,4149) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4151) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4158) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4159) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4161) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4167) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4168) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4169) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4170) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,4172) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,4173) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,4177) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,4178) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,4184)
      (<0.1>,4185)
      (<0.1>,4186) rcutree.c:786: local_irq_save(flags);
      (<0.1>,4189)
      (<0.1>,4191) fake_sched.h:43: return __running_cpu;
      (<0.1>,4195) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4197) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4201) fake_sched.h:43: return __running_cpu;
      (<0.1>,4205) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4210) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,4212) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,4213) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,4214) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4216) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4217) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4219) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4222) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,4225)
      (<0.1>,4227) fake_sched.h:43: return __running_cpu;
      (<0.1>,4231) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4233) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4237) fake_sched.h:43: return __running_cpu;
      (<0.1>,4241) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4248) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,4249) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,4253)
      (<0.1>,4254)
      (<0.1>,4255) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,4256) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,4262)
      (<0.1>,4263)
      (<0.1>,4264) rcutree.c:724: int ret = 0;
      (<0.1>,4265) rcutree.c:726: local_irq_save(flags);
      (<0.1>,4268)
      (<0.1>,4270) fake_sched.h:43: return __running_cpu;
      (<0.1>,4274) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4276) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4280) fake_sched.h:43: return __running_cpu;
      (<0.1>,4284) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4289) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4291) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4292) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4294) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4297) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,4300)
      (<0.1>,4302) fake_sched.h:43: return __running_cpu;
      (<0.1>,4306) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4308) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4312) fake_sched.h:43: return __running_cpu;
      (<0.1>,4316) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4321) rcutree.c:732: return ret;
      (<0.1>,4325) rcutree.c:1081: if (!rdp->qs_pending)
      (<0.1>,4327) rcutree.c:1081: if (!rdp->qs_pending)
      (<0.1>,4330) rcutree.c:1088: if (!rdp->passed_quiesc)
      (<0.1>,4332) rcutree.c:1088: if (!rdp->passed_quiesc)
      (<0.1>,4337) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,4338) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,4342)
      (<0.1>,4343)
      (<0.1>,4344) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,4347) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,4348) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,4355) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,4358)
      (<0.1>,4359) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4361) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4364) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4372) fake_sched.h:43: return __running_cpu;
      (<0.1>,4383)
      (<0.1>,4384)
      (<0.1>,4385) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4387) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4394) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4395) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4397) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4403) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4404) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4405) rcutree.c:1464: WARN_ON_ONCE(rdp->beenonline == 0);
      (<0.1>,4406) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,4408) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,4409) rcutree.c:1470: if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))
      (<0.1>,4413) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,4414) rcutree.c:1477: rcu_process_gp_end(rsp, rdp);
      (<0.1>,4420)
      (<0.1>,4421)
      (<0.1>,4422) rcutree.c:786: local_irq_save(flags);
      (<0.1>,4425)
      (<0.1>,4427) fake_sched.h:43: return __running_cpu;
      (<0.1>,4431) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4433) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4437) fake_sched.h:43: return __running_cpu;
      (<0.1>,4441) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4446) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,4448) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,4449) rcutree.c:787: rnp = rdp->mynode;
      (<0.1>,4450) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4452) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4453) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4455) rcutree.c:788: if (rdp->completed == ACCESS_ONCE(rnp->completed) || /* outside lock. */
      (<0.1>,4458) rcutree.c:790: local_irq_restore(flags);
      (<0.1>,4461)
      (<0.1>,4463) fake_sched.h:43: return __running_cpu;
      (<0.1>,4467) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4469) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4473) fake_sched.h:43: return __running_cpu;
      (<0.1>,4477) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4484) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,4485) rcutree.c:1480: rcu_check_quiescent_state(rsp, rdp);
      (<0.1>,4489)
      (<0.1>,4490)
      (<0.1>,4491) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,4492) rcutree.c:1074: if (check_for_new_grace_period(rsp, rdp))
      (<0.1>,4498)
      (<0.1>,4499)
      (<0.1>,4500) rcutree.c:724: int ret = 0;
      (<0.1>,4501) rcutree.c:726: local_irq_save(flags);
      (<0.1>,4504)
      (<0.1>,4506) fake_sched.h:43: return __running_cpu;
      (<0.1>,4510) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4512) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4516) fake_sched.h:43: return __running_cpu;
      (<0.1>,4520) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4525) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4527) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4528) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4530) rcutree.c:727: if (rdp->gpnum != rsp->gpnum) {
      (<0.1>,4533) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,4534) rcutree.c:728: note_new_gpnum(rsp, rdp);
      (<0.1>,4540)
      (<0.1>,4541)
      (<0.1>,4542) rcutree.c:704: local_irq_save(flags);
      (<0.1>,4545)
      (<0.1>,4547) fake_sched.h:43: return __running_cpu;
      (<0.1>,4551) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4553) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4557) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,4559) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,4560) rcutree.c:705: rnp = rdp->mynode;
      (<0.1>,4561) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,4563) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,4564) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,4566) rcutree.c:706: if (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) || /* outside lock. */
      (<0.1>,4569) rcutree.c:708: local_irq_restore(flags);
      (<0.1>,4572)
      (<0.1>,4574) fake_sched.h:43: return __running_cpu;
      (<0.1>,4578) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4580) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4586) rcutree.c:729: ret = 1;
      (<0.1>,4588) rcutree.c:731: local_irq_restore(flags);
      (<0.1>,4591)
      (<0.1>,4593) fake_sched.h:43: return __running_cpu;
      (<0.1>,4597) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4599) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4603) fake_sched.h:43: return __running_cpu;
      (<0.1>,4607) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4612) rcutree.c:732: return ret;
      (<0.1>,4618) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,4619) rcutree.c:1483: if (cpu_needs_another_gp(rsp, rdp)) {
      (<0.1>,4623)
      (<0.1>,4624)
      (<0.1>,4625) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,4628) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,4629) rcutree.c:289: return *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);
      (<0.1>,4636) rcutree.c:1489: if (cpu_has_callbacks_ready_to_invoke(rdp))
      (<0.1>,4639)
      (<0.1>,4640) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4642) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4645) rcutree.c:280: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];
      (<0.1>,4658) fake_sched.h:43: return __running_cpu;
      (<0.1>,4662) fake_sched.h:194: need_softirq[get_cpu()] = 0;
      (<0.1>,4673) rcutree.c:354: local_irq_save(flags);
      (<0.1>,4676)
      (<0.1>,4678) fake_sched.h:43: return __running_cpu;
      (<0.1>,4682) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4684) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4688) fake_sched.h:43: return __running_cpu;
      (<0.1>,4692) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4698) fake_sched.h:43: return __running_cpu;
      (<0.1>,4702) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,4703) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4705) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4707) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4710) rcutree.c:357: local_irq_restore(flags);
      (<0.1>,4713)
      (<0.1>,4715) fake_sched.h:43: return __running_cpu;
      (<0.1>,4719) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4721) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4725) fake_sched.h:43: return __running_cpu;
      (<0.1>,4729) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4740) fake_sched.h:43: return __running_cpu;
      (<0.1>,4744)
      (<0.1>,4753) rcutree.c:354: local_irq_save(flags);
      (<0.1>,4756)
      (<0.1>,4758) fake_sched.h:43: return __running_cpu;
      (<0.1>,4762) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4764) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,4768) fake_sched.h:43: return __running_cpu;
      (<0.1>,4772) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,4778) fake_sched.h:43: return __running_cpu;
      (<0.1>,4782) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,4783) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4785) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4787) rcutree.c:356: if (--rdtp->dynticks_nesting) {
      (<0.1>,4791) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4794) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4795) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4796) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4798) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4799) rcutree.c:362: atomic_inc(&rdtp->dynticks);
      (<0.1>,4801) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4804) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4810) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4811) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4814) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4819) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4820) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4821) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,4822) rcutree.c:365: local_irq_restore(flags);
      (<0.1>,4825)
      (<0.1>,4827) fake_sched.h:43: return __running_cpu;
      (<0.1>,4831) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4833) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,4837) fake_sched.h:43: return __running_cpu;
      (<0.1>,4841) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,4848) fake_sched.h:43: return __running_cpu;
      (<0.1>,4852) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
      (<0.1>,4861) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,4864) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,430) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,441) rcutree.c:386: local_irq_save(flags);
    (<0.0>,444)
    (<0.0>,446) fake_sched.h:43: return __running_cpu;
    (<0.0>,450) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,452) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,456) fake_sched.h:43: return __running_cpu;
    (<0.0>,460) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,466) fake_sched.h:43: return __running_cpu;
    (<0.0>,470) rcutree.c:387: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,471) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,473) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,475) rcutree.c:388: if (rdtp->dynticks_nesting++) {
    (<0.0>,479) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,482) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,483) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,484) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,486) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,487) rcutree.c:393: atomic_inc(&rdtp->dynticks);
    (<0.0>,489) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,492) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,499) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,500) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,503) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,508) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,509) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,510) rcutree.c:396: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,511) rcutree.c:397: local_irq_restore(flags);
    (<0.0>,514)
    (<0.0>,516) fake_sched.h:43: return __running_cpu;
    (<0.0>,520) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,522) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,526) fake_sched.h:43: return __running_cpu;
    (<0.0>,530) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,542)
    (<0.0>,547) litmus_v3.c:83: y = 1;
  (<0>,5385) litmus_v3.c:62: r_y = y;
  (<0>,5386) litmus_v3.c:62: r_y = y;
  (<0>,5395) fake_sched.h:43: return __running_cpu;
  (<0>,5399)
  (<0>,5408) rcutree.c:354: local_irq_save(flags);
  (<0>,5411)
  (<0>,5413) fake_sched.h:43: return __running_cpu;
  (<0>,5417) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5419) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5423) fake_sched.h:43: return __running_cpu;
  (<0>,5427) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5433) fake_sched.h:43: return __running_cpu;
  (<0>,5437) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,5438) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5440) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5442) rcutree.c:356: if (--rdtp->dynticks_nesting) {
  (<0>,5446) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5449) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5450) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5451) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5453) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5454) rcutree.c:362: atomic_inc(&rdtp->dynticks);
  (<0>,5456) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5459) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5465) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5466) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5469) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5474) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5475) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5476) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,5477) rcutree.c:365: local_irq_restore(flags);
  (<0>,5480)
  (<0>,5482) fake_sched.h:43: return __running_cpu;
  (<0>,5486) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5488) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5492) fake_sched.h:43: return __running_cpu;
  (<0>,5496) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5503) fake_sched.h:43: return __running_cpu;
  (<0>,5507) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
  (<0>,5516) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,5519) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,5524) litmus_v3.c:124: if (pthread_join(tu, NULL))
    (<0.0>,548) litmus_v3.c:85: fake_release_cpu(get_cpu());
    (<0.0>,549) fake_sched.h:43: return __running_cpu;
    (<0.0>,553)
    (<0.0>,562) rcutree.c:354: local_irq_save(flags);
    (<0.0>,565)
    (<0.0>,567) fake_sched.h:43: return __running_cpu;
    (<0.0>,571) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,573) fake_sched.h:137: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,577) fake_sched.h:43: return __running_cpu;
    (<0.0>,581) fake_sched.h:138: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,587) fake_sched.h:43: return __running_cpu;
    (<0.0>,591) rcutree.c:355: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,592) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,594) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,596) rcutree.c:356: if (--rdtp->dynticks_nesting) {
    (<0.0>,600) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,603) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,604) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,605) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,607) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,608) rcutree.c:362: atomic_inc(&rdtp->dynticks);
    (<0.0>,610) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,613) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,619) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,620) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,623) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,628) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,629) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,630) rcutree.c:364: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,631) rcutree.c:365: local_irq_restore(flags);
    (<0.0>,634)
    (<0.0>,636) fake_sched.h:43: return __running_cpu;
    (<0.0>,640) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,642) fake_sched.h:145: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,646) fake_sched.h:43: return __running_cpu;
    (<0.0>,650) fake_sched.h:146: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,657) fake_sched.h:43: return __running_cpu;
    (<0.0>,661) fake_sched.h:174: return !!local_irq_depth[get_cpu()];
    (<0.0>,670) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,673) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,5525) litmus_v3.c:124: if (pthread_join(tu, NULL))
  (<0>,5528) litmus_v3.c:126: if (pthread_join(th, NULL))
      (<0.1>,4865) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,5529) litmus_v3.c:126: if (pthread_join(th, NULL))
  (<0>,5532) litmus_v3.c:129: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,5535) litmus_v3.c:129: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,5539) litmus_v3.c:129: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,5540) litmus_v3.c:129: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,5541) litmus_v3.c:129: BUG_ON(r_x == 0 && r_y == 1);
             Error: Assertion violation at (<0>,5544): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmplonuo38v/tmpxjg6s752.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DFORCE_FAILURE_1 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmplonuo38v/tmp54fpmdr9.ll /tmp/tmplonuo38v/tmpxjg6s752.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=-1 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmplonuo38v/tmp54fpmdr9.ll
Total wall-clock time: 2.41 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 2354 (also 18 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp4a0671ok/tmpjfyw982x.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DFORCE_FAILURE_3 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp4a0671ok/tmpm7xg40ho.ll /tmp/tmp4a0671ok/tmpjfyw982x.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp4a0671ok/tmpm7xg40ho.ll
Total wall-clock time: 33.90 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 4870 (also 18 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp73f0n1t1/tmpw5ukvomu.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DFORCE_FAILURE_5 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp73f0n1t1/tmpuv33hc4a.ll /tmp/tmp73f0n1t1/tmpw5ukvomu.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp73f0n1t1/tmpuv33hc4a.ll
Total wall-clock time: 71.29 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 2006 (also 18 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp9nkkqzaf/tmpissit3_l.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DLIVENESS_CHECK_1 -DASSERT_0 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp9nkkqzaf/tmpbazw75vx.ll /tmp/tmp9nkkqzaf/tmpissit3_l.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp9nkkqzaf/tmpbazw75vx.ll
Total wall-clock time: 28.94 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 3866 (also 22 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpiszyrs8_/tmpipupbs24.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DLIVENESS_CHECK_2 -DASSERT_0 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpiszyrs8_/tmp8mf6knq6.ll /tmp/tmpiszyrs8_/tmpipupbs24.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpiszyrs8_/tmp8mf6knq6.ll
Total wall-clock time: 56.71 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.0 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 2166 (also 18 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp79h7d64p/tmps_fduenl.ll -S -emit-llvm -g -I v3.0 -std=gnu99 -DLIVENESS_CHECK_3 -DASSERT_0 litmus_v3.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp79h7d64p/tmpwhcl9nq7.ll /tmp/tmp79h7d64p/tmps_fduenl.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp79h7d64p/tmpwhcl9nq7.ll
Total wall-clock time: 30.68 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 24740 (also 20 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpakgz83ph/tmpzf2zr0hc.ll -S -emit-llvm -g -I v3.19 -std=gnu99 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpakgz83ph/tmpj_0izwor.ll /tmp/tmpakgz83ph/tmpzf2zr0hc.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpakgz83ph/tmpj_0izwor.ll
Total wall-clock time: 907.70 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 35 (also 6 sleepset blocked)

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,73) tree_plugin.h:931: pr_info("Hierarchical RCU implementation.\n");
  (<0>,77) tree_plugin.h:91: if (rcu_fanout_leaf != CONFIG_RCU_FANOUT_LEAF)
  (<0>,89) tree.c:3718: ulong d;
  (<0>,90) tree.c:3722: int rcu_capacity[MAX_RCU_LVLS + 1];
  (<0>,91) tree.c:3732: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,94) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,95) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,97) tree.c:3734: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,100) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,101) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,103) tree.c:3738: if (rcu_fanout_leaf == CONFIG_RCU_FANOUT_LEAF &&
  (<0>,115)
  (<0>,116) tree.c:3628: static void __init rcu_init_one(struct rcu_state *rsp,
  (<0>,117) tree.c:3629: struct rcu_data __percpu *rda)
  (<0>,118) tree.c:3643: int i;
  (<0>,121) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,123) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,124) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,127) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,130) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,133) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,136) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,140) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,142) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,143) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,146) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,149) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,152) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,158)
  (<0>,159) tree.c:3610: static void __init rcu_init_levelspread(struct rcu_state *rsp)
  (<0>,160) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,162) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,164) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,169) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,172) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,173) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,174) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,175) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,183) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,186) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,187) tree.c:3620: cprv = ccur;
  (<0>,188) tree.c:3620: cprv = ccur;
  (<0>,190) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,192) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,194) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,198) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,199) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,201) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,202) tree.c:3661: fl_mask <<= 1;
  (<0>,206) tree.c:3661: fl_mask <<= 1;
  (<0>,207) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,209) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,211) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,219) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,221) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,223) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,224) tree.c:3667: rnp = rsp->level[i];
  (<0>,226) tree.c:3667: rnp = rsp->level[i];
  (<0>,229) tree.c:3667: rnp = rsp->level[i];
  (<0>,230) tree.c:3667: rnp = rsp->level[i];
  (<0>,231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,233) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,236) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,242) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,246)
  (<0>,247) fake_sync.h:68: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,248) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,254) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,258)
  (<0>,259) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,260) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,268) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,269) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,271) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,274) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,275) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,277) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,278) tree.c:3677: rnp->qsmask = 0;
  (<0>,280) tree.c:3677: rnp->qsmask = 0;
  (<0>,281) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,283) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,285) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,287) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,295) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,297) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,298) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,300) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,303) tree.c:3683: if (i == 0) {
  (<0>,306) tree.c:3684: rnp->grpnum = 0;
  (<0>,308) tree.c:3684: rnp->grpnum = 0;
  (<0>,309) tree.c:3685: rnp->grpmask = 0;
  (<0>,311) tree.c:3685: rnp->grpmask = 0;
  (<0>,312) tree.c:3686: rnp->parent = NULL;
  (<0>,314) tree.c:3686: rnp->parent = NULL;
  (<0>,316) tree.c:3693: rnp->level = i;
  (<0>,318) tree.c:3693: rnp->level = i;
  (<0>,320) tree.c:3693: rnp->level = i;
  (<0>,321) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,325)
  (<0>,326) fake_defs.h:273: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,327) fake_defs.h:275: list->next = list;
  (<0>,329) fake_defs.h:275: list->next = list;
  (<0>,330) fake_defs.h:276: list->prev = list;
  (<0>,331) fake_defs.h:276: list->prev = list;
  (<0>,333) fake_defs.h:276: list->prev = list;
  (<0>,335) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,338) tree_plugin.h:2694: }
  (<0>,341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,348) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,351) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,354) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,358) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,360) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,362) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,365) tree.c:3699: rsp->rda = rda;
  (<0>,366) tree.c:3699: rsp->rda = rda;
  (<0>,368) tree.c:3699: rsp->rda = rda;
  (<0>,371) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,374) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,377) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,378) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,379) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,381) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,382) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,389)
  (<0>,390) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,391) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,393) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,394) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,397) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,399) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,400) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,403) fake_defs.h:536: if (offset & mask)
  (<0>,404) fake_defs.h:536: if (offset & mask)
  (<0>,408) fake_defs.h:537: return cpu;
  (<0>,409) fake_defs.h:537: return cpu;
  (<0>,411) fake_defs.h:540: }
  (<0>,413) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,414) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,418) tree.c:3703: while (i > rnp->grphi)
  (<0>,419) tree.c:3703: while (i > rnp->grphi)
  (<0>,421) tree.c:3703: while (i > rnp->grphi)
  (<0>,424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,425) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,429) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,432) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,433) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,434) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,447)
  (<0>,448) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,449) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,451) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,453) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,455) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,456) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,459)
  (<0>,460) tree.c:451: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,464) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,465) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,467) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,471)
  (<0>,472) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,473) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,476) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,478) fake_sched.h:43: return __running_cpu;
  (<0>,482) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,484) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,488) fake_sched.h:43: return __running_cpu;
  (<0>,492) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,498) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,499) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,504) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,506) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,508) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,512) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,514) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,515) tree.c:3412: init_callback_list(rdp);
  (<0>,519)
  (<0>,520) tree.c:1223: static void init_callback_list(struct rcu_data *rdp)
  (<0>,523) tree_plugin.h:2732: return false;
  (<0>,526) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,528) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,529) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,531) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,534) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,538) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,541) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,543) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,545) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,547) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,550) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,554) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,557) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,559) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,561) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,563) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,566) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,570) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,573) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,575) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,577) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,579) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,582) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,586) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,589) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,591) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,593) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,595) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,599) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,601) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,602) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,604) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,608) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,610) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,611) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,613) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,621) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,623) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,629) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,630) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,631) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,634) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,642) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,645) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,652) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,653) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,654) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,655) tree.c:3418: rdp->cpu = cpu;
  (<0>,656) tree.c:3418: rdp->cpu = cpu;
  (<0>,658) tree.c:3418: rdp->cpu = cpu;
  (<0>,659) tree.c:3419: rdp->rsp = rsp;
  (<0>,660) tree.c:3419: rdp->rsp = rsp;
  (<0>,662) tree.c:3419: rdp->rsp = rsp;
  (<0>,663) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,666) tree_plugin.h:2711: }
  (<0>,668) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,670) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,674)
  (<0>,675) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,676) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,677) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,680) fake_sync.h:86: local_irq_restore(flags);
  (<0>,683) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,685) fake_sched.h:43: return __running_cpu;
  (<0>,689) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,691) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,695) fake_sched.h:43: return __running_cpu;
  (<0>,699) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,708) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,709) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,716)
  (<0>,717)
  (<0>,718) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,720) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,721) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,724) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,726) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,727) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,730) fake_defs.h:536: if (offset & mask)
  (<0>,731) fake_defs.h:536: if (offset & mask)
  (<0>,735) fake_defs.h:537: return cpu;
  (<0>,736) fake_defs.h:537: return cpu;
  (<0>,738) fake_defs.h:540: }
  (<0>,740) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,741) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,745) tree.c:3703: while (i > rnp->grphi)
  (<0>,746) tree.c:3703: while (i > rnp->grphi)
  (<0>,748) tree.c:3703: while (i > rnp->grphi)
  (<0>,751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,752) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,756) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,759) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,760) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,761) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,774)
  (<0>,775)
  (<0>,776) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,778) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,780) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,782) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,783) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,786)
  (<0>,787) tree.c:453: return &rsp->node[0];
  (<0>,791) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,792) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,794) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,798)
  (<0>,799)
  (<0>,800) fake_sync.h:76: local_irq_save(flags);
  (<0>,803) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,805) fake_sched.h:43: return __running_cpu;
  (<0>,809) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,811) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,815) fake_sched.h:43: return __running_cpu;
  (<0>,819) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,825) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,826) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,831) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,833) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,835) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,839) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,841) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,842) tree.c:3412: init_callback_list(rdp);
  (<0>,846)
  (<0>,847) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,850) tree_plugin.h:2732: return false;
  (<0>,853) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,855) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,856) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,858) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,861) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,865) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,868) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,870) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,872) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,874) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,877) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,881) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,884) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,886) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,888) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,890) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,893) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,897) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,900) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,902) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,904) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,906) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,909) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,913) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,916) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,918) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,920) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,922) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,926) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,928) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,929) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,931) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,935) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,937) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,938) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,940) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,948) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,950) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,956) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,957) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,958) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,961) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,969) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,972) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,979) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,980) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,981) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,982) tree.c:3418: rdp->cpu = cpu;
  (<0>,983) tree.c:3418: rdp->cpu = cpu;
  (<0>,985) tree.c:3418: rdp->cpu = cpu;
  (<0>,986) tree.c:3419: rdp->rsp = rsp;
  (<0>,987) tree.c:3419: rdp->rsp = rsp;
  (<0>,989) tree.c:3419: rdp->rsp = rsp;
  (<0>,990) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,993) tree_plugin.h:2711: }
  (<0>,995) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,997) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1001)
  (<0>,1002)
  (<0>,1003) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1004) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1007) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1010) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1012) fake_sched.h:43: return __running_cpu;
  (<0>,1016) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1018) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1022) fake_sched.h:43: return __running_cpu;
  (<0>,1026) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1035) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1036) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1043)
  (<0>,1044)
  (<0>,1045) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1047) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1048) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1051) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1053) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1054) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1057) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,1059) fake_defs.h:540: }
  (<0>,1061) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1062) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1065) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1070)
  (<0>,1071) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1072) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1073) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1074) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1076) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1081)
  (<0>,1082) fake_defs.h:279: static inline void __list_add(struct list_head *new,
  (<0>,1083) fake_defs.h:280: struct list_head *prev,
  (<0>,1084) fake_defs.h:281: struct list_head *next)
  (<0>,1085) fake_defs.h:283: next->prev = new;
  (<0>,1087) fake_defs.h:283: next->prev = new;
  (<0>,1088) fake_defs.h:284: new->next = next;
  (<0>,1089) fake_defs.h:284: new->next = next;
  (<0>,1091) fake_defs.h:284: new->next = next;
  (<0>,1092) fake_defs.h:285: new->prev = prev;
  (<0>,1093) fake_defs.h:285: new->prev = prev;
  (<0>,1095) fake_defs.h:285: new->prev = prev;
  (<0>,1096) fake_defs.h:286: prev->next = new;
  (<0>,1097) fake_defs.h:286: prev->next = new;
  (<0>,1099) fake_defs.h:286: prev->next = new;
  (<0>,1110)
  (<0>,1111)
  (<0>,1112) tree.c:3642: int cpustride = 1;
  (<0>,1113) tree.c:3650: if (rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1116) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1118) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1119) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1122) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1125) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1126) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1128) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1133) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1135) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1137) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1141) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1143) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1144) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1147) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,1153)
  (<0>,1154) tree.c:3616: cprv = nr_cpu_ids;
  (<0>,1155) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1157) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1159) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1162) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1164) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1168) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1169) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1170) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1173) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1176) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1182) tree.c:3620: cprv = ccur;
  (<0>,1183) tree.c:3620: cprv = ccur;
  (<0>,1185) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1187) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1189) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1193) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1194) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1196) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1197) tree.c:3661: fl_mask <<= 1;
  (<0>,1201) tree.c:3661: fl_mask <<= 1;
  (<0>,1202) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1204) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1206) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1209) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1211) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1218) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1219) tree.c:3667: rnp = rsp->level[i];
  (<0>,1221) tree.c:3667: rnp = rsp->level[i];
  (<0>,1224) tree.c:3667: rnp = rsp->level[i];
  (<0>,1225) tree.c:3667: rnp = rsp->level[i];
  (<0>,1226) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1228) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1229) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1237) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,1241)
  (<0>,1242) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1243) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1249) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1253)
  (<0>,1254) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1255) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1261) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1263) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1264) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1267) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1269) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1270) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1273) tree.c:3677: rnp->qsmask = 0;
  (<0>,1275) tree.c:3677: rnp->qsmask = 0;
  (<0>,1276) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1278) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1279) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1280) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1282) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1285) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1287) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1293) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1295) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1298) tree.c:3683: if (i == 0) {
  (<0>,1301) tree.c:3684: rnp->grpnum = 0;
  (<0>,1303) tree.c:3684: rnp->grpnum = 0;
  (<0>,1304) tree.c:3685: rnp->grpmask = 0;
  (<0>,1306) tree.c:3685: rnp->grpmask = 0;
  (<0>,1307) tree.c:3686: rnp->parent = NULL;
  (<0>,1309) tree.c:3686: rnp->parent = NULL;
  (<0>,1311) tree.c:3693: rnp->level = i;
  (<0>,1313) tree.c:3693: rnp->level = i;
  (<0>,1315) tree.c:3693: rnp->level = i;
  (<0>,1316) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1320)
  (<0>,1321) fake_defs.h:275: list->next = list;
  (<0>,1322) fake_defs.h:275: list->next = list;
  (<0>,1324) fake_defs.h:275: list->next = list;
  (<0>,1325) fake_defs.h:276: list->prev = list;
  (<0>,1326) fake_defs.h:276: list->prev = list;
  (<0>,1328) fake_defs.h:276: list->prev = list;
  (<0>,1330) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,1333) tree_plugin.h:2694: }
  (<0>,1336) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1338) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1339) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1353) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1355) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1357) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1360) tree.c:3699: rsp->rda = rda;
  (<0>,1361) tree.c:3699: rsp->rda = rda;
  (<0>,1363) tree.c:3699: rsp->rda = rda;
  (<0>,1366) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1369) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1372) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1373) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1374) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1376) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1377) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1384)
  (<0>,1385)
  (<0>,1386) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1388) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1389) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1392) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1394) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1395) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1398) fake_defs.h:536: if (offset & mask)
  (<0>,1399) fake_defs.h:536: if (offset & mask)
  (<0>,1403) fake_defs.h:537: return cpu;
  (<0>,1404) fake_defs.h:537: return cpu;
  (<0>,1406) fake_defs.h:540: }
  (<0>,1408) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1409) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1413) tree.c:3703: while (i > rnp->grphi)
  (<0>,1414) tree.c:3703: while (i > rnp->grphi)
  (<0>,1416) tree.c:3703: while (i > rnp->grphi)
  (<0>,1419) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1420) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1422) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1428) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1429) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1442)
  (<0>,1443)
  (<0>,1444) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1446) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1448) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1450) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1451) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1454)
  (<0>,1455) tree.c:453: return &rsp->node[0];
  (<0>,1459) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1460) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1462) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1466)
  (<0>,1467)
  (<0>,1468) fake_sync.h:76: local_irq_save(flags);
  (<0>,1471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1473) fake_sched.h:43: return __running_cpu;
  (<0>,1477) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1479) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1483) fake_sched.h:43: return __running_cpu;
  (<0>,1487) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1493) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1494) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1498) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1499) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1501) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1507) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1509) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1510) tree.c:3412: init_callback_list(rdp);
  (<0>,1514)
  (<0>,1515) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1518) tree_plugin.h:2732: return false;
  (<0>,1521) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1523) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1529) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1531) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1533) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1538) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1540) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1542) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1545) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1547) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1549) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1554) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1556) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1558) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1561) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1563) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1565) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1570) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1572) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1574) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1577) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1579) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1581) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1586) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1588) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1590) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1594) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1596) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1597) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1599) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1600) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1603) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1606) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1608) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1610) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1616) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1618) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1624) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1626) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1627) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1629) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1638) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1640) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1647) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1649) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1650) tree.c:3418: rdp->cpu = cpu;
  (<0>,1651) tree.c:3418: rdp->cpu = cpu;
  (<0>,1653) tree.c:3418: rdp->cpu = cpu;
  (<0>,1654) tree.c:3419: rdp->rsp = rsp;
  (<0>,1655) tree.c:3419: rdp->rsp = rsp;
  (<0>,1657) tree.c:3419: rdp->rsp = rsp;
  (<0>,1658) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1661) tree_plugin.h:2711: }
  (<0>,1663) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1665) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1669)
  (<0>,1670)
  (<0>,1671) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1672) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1675) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1678) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1680) fake_sched.h:43: return __running_cpu;
  (<0>,1684) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1686) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1690) fake_sched.h:43: return __running_cpu;
  (<0>,1694) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1703) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1704) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1711)
  (<0>,1712)
  (<0>,1713) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1715) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1716) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1719) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1721) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1722) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1725) fake_defs.h:536: if (offset & mask)
  (<0>,1726) fake_defs.h:536: if (offset & mask)
  (<0>,1730) fake_defs.h:537: return cpu;
  (<0>,1731) fake_defs.h:537: return cpu;
  (<0>,1733) fake_defs.h:540: }
  (<0>,1735) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1736) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1740) tree.c:3703: while (i > rnp->grphi)
  (<0>,1741) tree.c:3703: while (i > rnp->grphi)
  (<0>,1743) tree.c:3703: while (i > rnp->grphi)
  (<0>,1746) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1747) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1749) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1755) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1756) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1769)
  (<0>,1770)
  (<0>,1771) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1773) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1775) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1777) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1778) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1781)
  (<0>,1782) tree.c:453: return &rsp->node[0];
  (<0>,1786) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1787) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1789) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1793)
  (<0>,1794)
  (<0>,1795) fake_sync.h:76: local_irq_save(flags);
  (<0>,1798) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1800) fake_sched.h:43: return __running_cpu;
  (<0>,1804) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1806) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1810) fake_sched.h:43: return __running_cpu;
  (<0>,1814) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1820) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1821) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1825) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1826) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1828) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1834) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1836) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1837) tree.c:3412: init_callback_list(rdp);
  (<0>,1841)
  (<0>,1842) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1845) tree_plugin.h:2732: return false;
  (<0>,1848) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1850) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1851) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1853) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1856) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1858) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1860) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1865) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1867) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1869) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1872) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1874) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1876) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1881) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1883) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1885) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1888) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1890) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1892) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1897) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1899) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1901) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1904) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1906) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1908) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1913) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1915) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1917) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1921) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1923) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1924) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1926) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1927) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1930) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1933) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1935) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1937) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1943) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1945) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1951) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1953) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1954) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1956) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1965) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1967) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1974) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1976) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1977) tree.c:3418: rdp->cpu = cpu;
  (<0>,1978) tree.c:3418: rdp->cpu = cpu;
  (<0>,1980) tree.c:3418: rdp->cpu = cpu;
  (<0>,1981) tree.c:3419: rdp->rsp = rsp;
  (<0>,1982) tree.c:3419: rdp->rsp = rsp;
  (<0>,1984) tree.c:3419: rdp->rsp = rsp;
  (<0>,1985) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1988) tree_plugin.h:2711: }
  (<0>,1990) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1992) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1996)
  (<0>,1997)
  (<0>,1998) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1999) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,2002) fake_sync.h:86: local_irq_restore(flags);
  (<0>,2005) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2007) fake_sched.h:43: return __running_cpu;
  (<0>,2011) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2013) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2017) fake_sched.h:43: return __running_cpu;
  (<0>,2021) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2030) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2031) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2038)
  (<0>,2039)
  (<0>,2040) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2042) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2043) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2046) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2048) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2049) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2052) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,2054) fake_defs.h:540: }
  (<0>,2056) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2057) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2060) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,2065)
  (<0>,2066)
  (<0>,2067) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2068) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2069) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2071) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2076)
  (<0>,2077)
  (<0>,2078)
  (<0>,2079) fake_defs.h:283: next->prev = new;
  (<0>,2080) fake_defs.h:283: next->prev = new;
  (<0>,2082) fake_defs.h:283: next->prev = new;
  (<0>,2083) fake_defs.h:284: new->next = next;
  (<0>,2084) fake_defs.h:284: new->next = next;
  (<0>,2086) fake_defs.h:284: new->next = next;
  (<0>,2087) fake_defs.h:285: new->prev = prev;
  (<0>,2088) fake_defs.h:285: new->prev = prev;
  (<0>,2090) fake_defs.h:285: new->prev = prev;
  (<0>,2091) fake_defs.h:286: prev->next = new;
  (<0>,2092) fake_defs.h:286: prev->next = new;
  (<0>,2094) fake_defs.h:286: prev->next = new;
  (<0>,2106) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2108) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2109) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2116)
  (<0>,2117)
  (<0>,2118) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2120) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2121) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2124) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2126) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2127) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2130) fake_defs.h:536: if (offset & mask)
  (<0>,2131) fake_defs.h:536: if (offset & mask)
  (<0>,2135) fake_defs.h:537: return cpu;
  (<0>,2136) fake_defs.h:537: return cpu;
  (<0>,2138) fake_defs.h:540: }
  (<0>,2140) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2141) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2144) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2163)
  (<0>,2164) tree.c:3493: static int rcu_cpu_notify(struct notifier_block *self,
  (<0>,2165) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2166) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2168) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2169) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2170) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2172) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2174) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2175) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2177) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2178) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2181) tree.c:3502: switch (action) {
  (<0>,2183) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2192)
  (<0>,2193) tree.c:3482: static void rcu_prepare_cpu(int cpu)
  (<0>,2194) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2195) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2199) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2200) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2201) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2203) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2207) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2208) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2216)
  (<0>,2217) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2218) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2220) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2222) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2224) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2225) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2228)
  (<0>,2229) tree.c:453: return &rsp->node[0];
  (<0>,2233) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2234) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2238)
  (<0>,2239) fake_sync.h:172: void mutex_lock(struct mutex *l)
  (<0>,2241) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2245) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2247) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2251)
  (<0>,2252)
  (<0>,2253) fake_sync.h:76: local_irq_save(flags);
  (<0>,2256) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2258) fake_sched.h:43: return __running_cpu;
  (<0>,2262) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2264) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2268) fake_sched.h:43: return __running_cpu;
  (<0>,2272) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2278) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2279) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2283) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2285) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2286) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2288) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2289) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2291) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2292) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2294) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2295) tree.c:3446: rdp->blimit = blimit;
  (<0>,2296) tree.c:3446: rdp->blimit = blimit;
  (<0>,2298) tree.c:3446: rdp->blimit = blimit;
  (<0>,2299) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2303)
  (<0>,2304) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2307) tree_plugin.h:2732: return false;
  (<0>,2310) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2312) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2313) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2315) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2318) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2320) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2322) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2325) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2327) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2329) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2331) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2334) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2336) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2338) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2341) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2343) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2345) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2347) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2350) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2352) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2354) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2357) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2359) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2361) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2363) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2366) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2368) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2370) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2373) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2375) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2377) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2379) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2383) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2385) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2387) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2388) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2390) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2393) tree_plugin.h:3164: }
  (<0>,2395) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2397) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2400) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2403) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2405) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2408) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2409) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2413)
  (<0>,2414) fake_sync.h:113: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2415) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2420) tree.c:3455: rnp = rdp->mynode;
  (<0>,2422) tree.c:3455: rnp = rdp->mynode;
  (<0>,2423) tree.c:3455: rnp = rdp->mynode;
  (<0>,2424) tree.c:3456: mask = rdp->grpmask;
  (<0>,2426) tree.c:3456: mask = rdp->grpmask;
  (<0>,2427) tree.c:3456: mask = rdp->grpmask;
  (<0>,2429) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2433) fake_sync.h:108: preempt_disable();
  (<0>,2435) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2436) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2440) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2441) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2443) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2445) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2446) tree.c:3461: mask = rnp->grpmask;
  (<0>,2448) tree.c:3461: mask = rnp->grpmask;
  (<0>,2449) tree.c:3461: mask = rnp->grpmask;
  (<0>,2450) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2451) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2453) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2456) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2458) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2459) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2461) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2462) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2464) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2465) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2467) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2468) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2470) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2471) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2473) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2477) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2481)
  (<0>,2482) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2483) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2488) tree.c:3475: rnp = rnp->parent;
  (<0>,2490) tree.c:3475: rnp = rnp->parent;
  (<0>,2491) tree.c:3475: rnp = rnp->parent;
  (<0>,2493) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2497) tree.c:3477: local_irq_restore(flags);
  (<0>,2500) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2502) fake_sched.h:43: return __running_cpu;
  (<0>,2506) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2508) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2512) fake_sched.h:43: return __running_cpu;
  (<0>,2516) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2521) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2525)
  (<0>,2526) fake_sync.h:178: void mutex_unlock(struct mutex *l)
  (<0>,2528) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2534) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2537) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2538) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2539) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2543) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2544) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2545) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2547) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2551) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2552) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2560)
  (<0>,2561)
  (<0>,2562) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2564) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2566) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2568) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2569) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2572)
  (<0>,2573) tree.c:453: return &rsp->node[0];
  (<0>,2577) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2578) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2582)
  (<0>,2583) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2585) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2589) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2591) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2595)
  (<0>,2596)
  (<0>,2597) fake_sync.h:76: local_irq_save(flags);
  (<0>,2600) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2602) fake_sched.h:43: return __running_cpu;
  (<0>,2606) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2608) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2612) fake_sched.h:43: return __running_cpu;
  (<0>,2616) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2622) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2623) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2627) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2629) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2630) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2632) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2633) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2635) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2636) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2638) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2639) tree.c:3446: rdp->blimit = blimit;
  (<0>,2640) tree.c:3446: rdp->blimit = blimit;
  (<0>,2642) tree.c:3446: rdp->blimit = blimit;
  (<0>,2643) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2647)
  (<0>,2648) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2651) tree_plugin.h:2732: return false;
  (<0>,2654) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2656) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2657) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2659) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2662) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2664) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2666) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2669) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2671) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2673) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2675) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2678) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2680) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2682) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2685) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2687) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2689) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2691) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2694) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2696) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2698) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2701) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2703) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2705) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2707) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2710) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2712) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2714) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2717) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2719) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2721) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2723) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2727) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2729) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2731) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2732) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2734) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2737) tree_plugin.h:3164: }
  (<0>,2739) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2741) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2744) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2747) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2749) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2752) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2753) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2757)
  (<0>,2758) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2759) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2764) tree.c:3455: rnp = rdp->mynode;
  (<0>,2766) tree.c:3455: rnp = rdp->mynode;
  (<0>,2767) tree.c:3455: rnp = rdp->mynode;
  (<0>,2768) tree.c:3456: mask = rdp->grpmask;
  (<0>,2770) tree.c:3456: mask = rdp->grpmask;
  (<0>,2771) tree.c:3456: mask = rdp->grpmask;
  (<0>,2773) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2777) fake_sync.h:108: preempt_disable();
  (<0>,2779) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2780) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2784) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2785) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2787) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2789) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2790) tree.c:3461: mask = rnp->grpmask;
  (<0>,2792) tree.c:3461: mask = rnp->grpmask;
  (<0>,2793) tree.c:3461: mask = rnp->grpmask;
  (<0>,2794) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2795) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2797) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2800) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2802) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2803) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2805) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2806) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2808) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2809) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2811) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2812) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2814) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2815) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2817) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2821) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2825)
  (<0>,2826) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2827) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2832) tree.c:3475: rnp = rnp->parent;
  (<0>,2834) tree.c:3475: rnp = rnp->parent;
  (<0>,2835) tree.c:3475: rnp = rnp->parent;
  (<0>,2837) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2841) tree.c:3477: local_irq_restore(flags);
  (<0>,2844) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2846) fake_sched.h:43: return __running_cpu;
  (<0>,2850) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2852) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2856) fake_sched.h:43: return __running_cpu;
  (<0>,2860) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2865) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2869)
  (<0>,2870) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2872) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2878) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2881) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2882) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2883) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2887) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2888) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2889) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2891) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2896) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,2900) tree_plugin.h:1499: }
  (<0>,2902) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2906) tree_plugin.h:2724: }
  (<0>,2913) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2914) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2921)
  (<0>,2922)
  (<0>,2923) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2925) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2926) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2929) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2931) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2932) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2935) fake_defs.h:536: if (offset & mask)
  (<0>,2936) fake_defs.h:536: if (offset & mask)
  (<0>,2940) fake_defs.h:537: return cpu;
  (<0>,2941) fake_defs.h:537: return cpu;
  (<0>,2943) fake_defs.h:540: }
  (<0>,2945) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2946) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2949) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2968)
  (<0>,2969)
  (<0>,2970)
  (<0>,2971) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2973) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2974) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2975) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2977) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2979) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2980) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2982) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2983) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2986) tree.c:3502: switch (action) {
  (<0>,2988) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2997)
  (<0>,2998) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2999) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3000) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3004) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3005) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3006) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3008) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3012) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3013) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3021)
  (<0>,3022)
  (<0>,3023) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3025) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3027) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3029) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3030) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3033)
  (<0>,3034) tree.c:453: return &rsp->node[0];
  (<0>,3038) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3039) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3043)
  (<0>,3044) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3046) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3050) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3052) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3056)
  (<0>,3057)
  (<0>,3058) fake_sync.h:76: local_irq_save(flags);
  (<0>,3061) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3063) fake_sched.h:43: return __running_cpu;
  (<0>,3067) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3069) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3073) fake_sched.h:43: return __running_cpu;
  (<0>,3077) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3083) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3084) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3088) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3090) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3091) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3093) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3094) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3096) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3097) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3099) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3100) tree.c:3446: rdp->blimit = blimit;
  (<0>,3101) tree.c:3446: rdp->blimit = blimit;
  (<0>,3103) tree.c:3446: rdp->blimit = blimit;
  (<0>,3104) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3108)
  (<0>,3109) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3112) tree_plugin.h:2732: return false;
  (<0>,3115) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3117) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3118) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3120) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3123) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3125) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3127) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3130) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3132) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3134) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3136) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3139) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3141) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3143) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3146) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3148) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3150) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3152) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3155) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3157) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3159) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3162) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3164) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3166) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3168) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3171) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3173) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3175) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3178) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3180) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3182) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3184) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3188) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3190) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3192) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3193) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3195) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3198) tree_plugin.h:3164: }
  (<0>,3200) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3202) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3205) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3208) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3210) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3213) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3214) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3218)
  (<0>,3219) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3220) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3225) tree.c:3455: rnp = rdp->mynode;
  (<0>,3227) tree.c:3455: rnp = rdp->mynode;
  (<0>,3228) tree.c:3455: rnp = rdp->mynode;
  (<0>,3229) tree.c:3456: mask = rdp->grpmask;
  (<0>,3231) tree.c:3456: mask = rdp->grpmask;
  (<0>,3232) tree.c:3456: mask = rdp->grpmask;
  (<0>,3234) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3238) fake_sync.h:108: preempt_disable();
  (<0>,3240) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3241) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3245) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3246) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3248) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3250) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3251) tree.c:3461: mask = rnp->grpmask;
  (<0>,3253) tree.c:3461: mask = rnp->grpmask;
  (<0>,3254) tree.c:3461: mask = rnp->grpmask;
  (<0>,3255) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3256) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3258) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3261) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3263) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3264) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3266) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3267) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3269) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3270) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3272) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3273) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3275) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3276) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3278) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3282) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3286)
  (<0>,3287) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3288) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3293) tree.c:3475: rnp = rnp->parent;
  (<0>,3295) tree.c:3475: rnp = rnp->parent;
  (<0>,3296) tree.c:3475: rnp = rnp->parent;
  (<0>,3298) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3302) tree.c:3477: local_irq_restore(flags);
  (<0>,3305) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3307) fake_sched.h:43: return __running_cpu;
  (<0>,3311) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3313) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3317) fake_sched.h:43: return __running_cpu;
  (<0>,3321) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3326) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3330)
  (<0>,3331) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3333) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3339) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3342) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3343) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3344) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3348) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3349) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3350) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3352) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3356) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3357) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3365)
  (<0>,3366)
  (<0>,3367) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3369) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3371) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3373) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3374) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3377)
  (<0>,3378) tree.c:453: return &rsp->node[0];
  (<0>,3382) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3383) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3387)
  (<0>,3388) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3390) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3394) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3396) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3400)
  (<0>,3401)
  (<0>,3402) fake_sync.h:76: local_irq_save(flags);
  (<0>,3405) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3407) fake_sched.h:43: return __running_cpu;
  (<0>,3411) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3413) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3417) fake_sched.h:43: return __running_cpu;
  (<0>,3421) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3427) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3428) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3432) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3434) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3435) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3437) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3438) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3440) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3441) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3443) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3444) tree.c:3446: rdp->blimit = blimit;
  (<0>,3445) tree.c:3446: rdp->blimit = blimit;
  (<0>,3447) tree.c:3446: rdp->blimit = blimit;
  (<0>,3448) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3452)
  (<0>,3453) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3456) tree_plugin.h:2732: return false;
  (<0>,3459) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3461) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3462) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3464) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3467) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3469) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3471) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3474) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3476) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3478) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3480) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3483) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3485) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3487) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3490) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3492) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3494) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3496) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3499) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3501) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3503) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3506) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3508) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3510) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3512) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3515) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3517) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3519) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3522) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3528) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3532) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3534) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3536) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3537) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3539) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3542) tree_plugin.h:3164: }
  (<0>,3544) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3546) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3549) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3552) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3554) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3557) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3558) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3562)
  (<0>,3563) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3564) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3569) tree.c:3455: rnp = rdp->mynode;
  (<0>,3571) tree.c:3455: rnp = rdp->mynode;
  (<0>,3572) tree.c:3455: rnp = rdp->mynode;
  (<0>,3573) tree.c:3456: mask = rdp->grpmask;
  (<0>,3575) tree.c:3456: mask = rdp->grpmask;
  (<0>,3576) tree.c:3456: mask = rdp->grpmask;
  (<0>,3578) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3582) fake_sync.h:108: preempt_disable();
  (<0>,3584) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3585) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3589) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3590) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3592) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3594) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3595) tree.c:3461: mask = rnp->grpmask;
  (<0>,3597) tree.c:3461: mask = rnp->grpmask;
  (<0>,3598) tree.c:3461: mask = rnp->grpmask;
  (<0>,3599) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3600) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3602) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3605) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3607) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3608) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3610) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3611) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3613) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3614) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3616) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3617) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3619) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3620) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3622) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3626) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3630)
  (<0>,3631) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3632) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3637) tree.c:3475: rnp = rnp->parent;
  (<0>,3639) tree.c:3475: rnp = rnp->parent;
  (<0>,3640) tree.c:3475: rnp = rnp->parent;
  (<0>,3642) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3646) tree.c:3477: local_irq_restore(flags);
  (<0>,3649) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3651) fake_sched.h:43: return __running_cpu;
  (<0>,3655) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3657) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3661) fake_sched.h:43: return __running_cpu;
  (<0>,3665) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3670) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3674)
  (<0>,3675) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3677) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3683) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3686) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3687) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3688) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3692) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3693) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3694) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3696) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3701) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,3705) tree_plugin.h:1499: }
  (<0>,3707) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,3711) tree_plugin.h:2724: }
  (<0>,3718) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3719) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3726)
  (<0>,3727)
  (<0>,3728) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3730) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3731) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3734) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3736) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3737) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3740) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,3742) fake_defs.h:540: }
  (<0>,3744) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3745) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3751) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3753) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3756) litmus.c:123: set_cpu(i);
  (<0>,3759)
  (<0>,3760) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,3761) fake_sched.h:56: __running_cpu = cpu;
  (<0>,3765) tree.c:578: unsigned long flags;
  (<0>,3768) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3770) fake_sched.h:43: return __running_cpu;
  (<0>,3774) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3776) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3780) fake_sched.h:43: return __running_cpu;
  (<0>,3784) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3797) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3799) fake_sched.h:43: return __running_cpu;
  (<0>,3803) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3804) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3806) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3807) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3808) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3814) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3815) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3820) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3821) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3822) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3823) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,3827) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3829) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3830) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3831) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3850)
  (<0>,3852) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3854) fake_sched.h:43: return __running_cpu;
  (<0>,3858) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3861) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,3865) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3866) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3867) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3871) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3872) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3873) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3875) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3880) fake_sched.h:43: return __running_cpu;
  (<0>,3883) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3885) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3887) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3888) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3891) tree_plugin.h:2720: }
  (<0>,3894) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3897) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3898) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3899) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3903) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3904) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3905) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3907) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3912) fake_sched.h:43: return __running_cpu;
  (<0>,3915) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3917) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3919) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3920) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3923) tree_plugin.h:2720: }
  (<0>,3926) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3929) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3930) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3931) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3935) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3936) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3937) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3939) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3946) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3949) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3950) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3951) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3953) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3954) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3956) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3959) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3965) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3966) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3969) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3974) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3975) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3976) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3990) tree_plugin.h:3141: }
  (<0>,3992) tree.c:583: local_irq_restore(flags);
  (<0>,3995) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3997) fake_sched.h:43: return __running_cpu;
  (<0>,4001) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4003) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4007) fake_sched.h:43: return __running_cpu;
  (<0>,4011) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4018) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4020) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4022) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4025) litmus.c:123: set_cpu(i);
  (<0>,4028)
  (<0>,4029) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4030) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4034) tree.c:580: local_irq_save(flags);
  (<0>,4037) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4039) fake_sched.h:43: return __running_cpu;
  (<0>,4043) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4045) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4049) fake_sched.h:43: return __running_cpu;
  (<0>,4053) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4066) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4068) fake_sched.h:43: return __running_cpu;
  (<0>,4072) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4073) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4075) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4076) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4077) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4083) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4084) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4089) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4090) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4091) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4092) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4096) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4098) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4099) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4100) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4119)
  (<0>,4121) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4123) fake_sched.h:43: return __running_cpu;
  (<0>,4127) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4130) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,4134) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4135) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4136) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4140) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4141) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4142) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4144) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4149) fake_sched.h:43: return __running_cpu;
  (<0>,4152) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4154) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4156) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4157) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4160) tree_plugin.h:2720: }
  (<0>,4163) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4166) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4167) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4168) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4172) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4173) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4181) fake_sched.h:43: return __running_cpu;
  (<0>,4184) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4186) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4188) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4189) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4192) tree_plugin.h:2720: }
  (<0>,4195) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4198) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4199) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4200) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4204) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4205) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4206) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4208) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4215) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4218) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4219) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4220) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4222) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4223) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4225) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4228) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4234) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4235) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4238) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4243) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4244) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4245) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4259) tree_plugin.h:3141: }
  (<0>,4261) tree.c:583: local_irq_restore(flags);
  (<0>,4264) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4266) fake_sched.h:43: return __running_cpu;
  (<0>,4270) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4272) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4276) fake_sched.h:43: return __running_cpu;
  (<0>,4280) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4287) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4289) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4291) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4303) tree.c:3561: unsigned long flags;
  (<0>,4304) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4305) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4306) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4310) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4314) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4320) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4322) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4331)
  (<0>,4332) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4333) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4334) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4335) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4336) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4339)
  (<0>,4340)
  (<0>,4347)
  (<0>,4348)
  (<0>,4355)
  (<0>,4356)
  (<0>,4363)
  (<0>,4364)
  (<0>,4371)
  (<0>,4372)
  (<0>,4379)
  (<0>,4380)
  (<0>,4387)
  (<0>,4388)
  (<0>,4395)
  (<0>,4396)
  (<0>,4403)
  (<0>,4404)
  (<0>,4411)
  (<0>,4412) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4421) fake_defs.h:730: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,4427) fake_defs.h:732: task = malloc(sizeof(*task));
  (<0>,4428) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4430) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4432) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4433) fake_defs.h:734: task->tid = t;
  (<0>,4434) fake_defs.h:734: task->tid = t;
  (<0>,4436) fake_defs.h:734: task->tid = t;
  (<0>,4437) fake_defs.h:735: return task;
  (<0>,4438) fake_defs.h:735: return task;
  (<0>,4440) fake_defs.h:738: }
  (<0>,4442) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4443) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4446)
  (<0>,4447) tree.c:453: return &rsp->node[0];
  (<0>,4451) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4452) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4454) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4458)
  (<0>,4459)
  (<0>,4460) fake_sync.h:76: local_irq_save(flags);
  (<0>,4463) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4465) fake_sched.h:43: return __running_cpu;
  (<0>,4469) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4475) fake_sched.h:43: return __running_cpu;
  (<0>,4479) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4485) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4486) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4490) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4491) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4493) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4494) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4496) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4500)
  (<0>,4501)
  (<0>,4502) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4503) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4506) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4509) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4511) fake_sched.h:43: return __running_cpu;
  (<0>,4515) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4517) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4521) fake_sched.h:43: return __running_cpu;
  (<0>,4525) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4533) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4537) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4538) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4542) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4543) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4544) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4546) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4550) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4552) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4554) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4563)
  (<0>,4564)
  (<0>,4565)
  (<0>,4566)
  (<0>,4567) fake_defs.h:727: struct task_struct *task = NULL;
  (<0>,4568) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4571)
  (<0>,4572)
  (<0>,4579)
  (<0>,4580)
  (<0>,4587)
  (<0>,4588)
  (<0>,4595)
  (<0>,4596)
  (<0>,4603)
  (<0>,4604) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4617) fake_defs.h:737: return NULL;
  (<0>,4619) fake_defs.h:738: }
  (<0>,4621) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4622) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4625)
  (<0>,4626) tree.c:453: return &rsp->node[0];
  (<0>,4630) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4631) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4633) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4637)
  (<0>,4638)
  (<0>,4639) fake_sync.h:76: local_irq_save(flags);
  (<0>,4642) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4644) fake_sched.h:43: return __running_cpu;
  (<0>,4648) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4650) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4654) fake_sched.h:43: return __running_cpu;
  (<0>,4658) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4664) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4665) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4669) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4670) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4672) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4673) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4675) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4679)
  (<0>,4680)
  (<0>,4681) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4682) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4685) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4688) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4690) fake_sched.h:43: return __running_cpu;
  (<0>,4694) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4696) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4700) fake_sched.h:43: return __running_cpu;
  (<0>,4704) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4712) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4715) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4716) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4717) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4721) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4722) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4723) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4725) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4739)
  (<0>,4740) litmus.c:51: void *thread_reader(void *arg)
  (<0>,4743)
  (<0>,4744) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4745) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4748) fake_sched.h:43: return __running_cpu;
  (<0>,4752)
  (<0>,4753) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,4756) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,4761) tree.c:702: unsigned long flags;
  (<0>,4764) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4766) fake_sched.h:43: return __running_cpu;
  (<0>,4770) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4772) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4780) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4793) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4795) fake_sched.h:43: return __running_cpu;
  (<0>,4799) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4800) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4802) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4803) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4804) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4809) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4810) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4814) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4815) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4816) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4817) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,4821) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4823) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4824) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4825) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4839)
  (<0>,4840) tree.c:644: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,4842) fake_sched.h:43: return __running_cpu;
  (<0>,4846) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4850) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4853) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4854) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4855) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4857) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4858) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4860) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4870) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4871) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4874) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4879) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4880) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4881) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4886) tree.c:656: if (!user && !is_idle_task(current)) {
  (<0>,4895) tree_plugin.h:3145: }
  (<0>,4897) tree.c:707: local_irq_restore(flags);
  (<0>,4900) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4902) fake_sched.h:43: return __running_cpu;
  (<0>,4906) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4908) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4912) fake_sched.h:43: return __running_cpu;
  (<0>,4916) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4932) litmus.c:57: r_x = x;
  (<0>,4933) litmus.c:57: r_x = x;
  (<0>,4937) fake_sched.h:43: return __running_cpu;
  (<0>,4941) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4949) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4954) fake_sched.h:43: return __running_cpu;
  (<0>,4958) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
  (<0>,4968) tree.c:745: unsigned long flags;
  (<0>,4971) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4973) fake_sched.h:43: return __running_cpu;
  (<0>,4977) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4984) fake_sched.h:43: return __running_cpu;
  (<0>,4988) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4989) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4991) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4992) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4993) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4995) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4997) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4998) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5000) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5005) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5006) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5008) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5012) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5013) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5014) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5015) tree.c:754: if (oldval)
  (<0>,5023) tree_plugin.h:3145: }
  (<0>,5025) tree.c:759: local_irq_restore(flags);
  (<0>,5028) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5030) fake_sched.h:43: return __running_cpu;
  (<0>,5034) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5036) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5044) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,5049) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,5054) fake_sched.h:43: return __running_cpu;
  (<0>,5059) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,5067) fake_sched.h:43: return __running_cpu;
  (<0>,5072) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
  (<0>,5078) fake_sched.h:43: return __running_cpu;
  (<0>,5083) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
  (<0>,5096) tree.c:3185: struct rcu_state *rsp;
  (<0>,5097) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5098) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5102) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5103) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5104) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5106) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5110) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5112) fake_sched.h:43: return __running_cpu;
  (<0>,5115) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5117) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5124)
  (<0>,5125) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5126) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5128) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5129) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5130) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5132) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5134) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5135) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5136) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5146)
  (<0>,5147) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5148) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5153) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
  (<0>,5156) tree_plugin.h:3185: return 0;
  (<0>,5159) tree.c:3135: if (rcu_scheduler_fully_active &&
  (<0>,5162) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5164) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5167) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5169) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5173) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,5176)
  (<0>,5177) tree.c:442: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,5179) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5182) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5189) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5190) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5196)
  (<0>,5197) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5198) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5201)
  (<0>,5202) tree.c:176: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,5204) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5205) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5207) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5213) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5219)
  (<0>,5220) tree.c:461: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,5223)
  (<0>,5224) tree.c:453: return &rsp->node[0];
  (<0>,5228) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5229) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5231) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5235) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5236) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5238) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5241) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5242) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5243) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5247) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5250) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5253) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5256) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5257) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5260) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5262) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5265) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5268) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5271) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5272) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5274) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5277) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5281) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5283) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5285) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5288) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5291) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5294) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5295) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5297) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5300) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5304) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5306) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5308) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5311) tree.c:493: return 0; /* No grace period needed. */
  (<0>,5313) tree.c:494: }
  (<0>,5317) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5319) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5320) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5322) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5325) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,1)
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:704: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,56) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,58) fake_sched.h:43: return __running_cpu;
      (<0.1>,62) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,63) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,67) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,72) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,73) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,77) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,78) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,79) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,80) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,84) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,86) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,87) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,88) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,102)
      (<0.1>,103) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,105) fake_sched.h:43: return __running_cpu;
      (<0.1>,109) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,113) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,116) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,117) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,118) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,120) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,123) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,126) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,133) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,137) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,143) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,144) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,149) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,158) tree_plugin.h:3145: }
      (<0.1>,160) tree.c:707: local_irq_restore(flags);
      (<0.1>,163) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,165) fake_sched.h:43: return __running_cpu;
      (<0.1>,169) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,171) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,175) fake_sched.h:43: return __running_cpu;
      (<0.1>,179) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,186) litmus.c:87: x = 1;
      (<0.1>,196) tree.c:2787: ret = num_online_cpus() <= 1;
      (<0.1>,198) tree.c:2789: return ret;
      (<0.1>,202) tree.c:2841: if (rcu_expedited)
      (<0.1>,208) update.c:223: init_rcu_head_on_stack(&rcu.head);
      (<0.1>,212) rcupdate.h:401: }
      (<0.1>,217)
      (<0.1>,218) fake_sync.h:232: x->done = 0;
      (<0.1>,220) fake_sync.h:232: x->done = 0;
      (<0.1>,224) update.c:226: crf(&rcu.head, wakeme_after_rcu);
      (<0.1>,229)
      (<0.1>,230)
      (<0.1>,231) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,232) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,249)
      (<0.1>,250)
      (<0.1>,251)
      (<0.1>,252)
      (<0.1>,254)
      (<0.1>,255) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,262) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,263) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,269) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,270) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,271) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,272) tree.c:2690: if (debug_rcu_head_queue(head)) {
      (<0.1>,275) rcu.h:92: return 0;
      (<0.1>,279) tree.c:2696: head->func = func;
      (<0.1>,280) tree.c:2696: head->func = func;
      (<0.1>,282) tree.c:2696: head->func = func;
      (<0.1>,283) tree.c:2697: head->next = NULL;
      (<0.1>,285) tree.c:2697: head->next = NULL;
      (<0.1>,286) tree.c:2705: local_irq_save(flags);
      (<0.1>,289) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,291) fake_sched.h:43: return __running_cpu;
      (<0.1>,295) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,297) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,301) fake_sched.h:43: return __running_cpu;
      (<0.1>,305) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,311) fake_sched.h:43: return __running_cpu;
      (<0.1>,314) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,316) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,318) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,319) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,322) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,330) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,333) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,335) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,337) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,339) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,340) tree.c:2721: if (lazy)
      (<0.1>,347) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,348) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,351) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,352) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,353) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,355) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,358) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,359) tree.c:2729: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,366) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,367) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,368) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,369) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,377)
      (<0.1>,378)
      (<0.1>,379)
      (<0.1>,380) tree.c:2628: if (!rcu_is_watching() && cpu_online(smp_processor_id()))
      (<0.1>,386) fake_sched.h:43: return __running_cpu;
      (<0.1>,392) tree.c:815: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,397) tree.c:829: ret = __rcu_is_watching();
      (<0.1>,399) tree.c:831: return ret;
      (<0.1>,403) tree.c:2632: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,406) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,408) fake_sched.h:43: return __running_cpu;
      (<0.1>,412) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,422) tree.c:2737: local_irq_restore(flags);
      (<0.1>,425) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,427) fake_sched.h:43: return __running_cpu;
      (<0.1>,431) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,433) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,437) fake_sched.h:43: return __running_cpu;
      (<0.1>,441) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,452) fake_sync.h:238: might_sleep();
      (<0.1>,456) fake_sched.h:43: return __running_cpu;
      (<0.1>,460) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,463) tree.c:580: local_irq_save(flags);
      (<0.1>,466) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,468) fake_sched.h:43: return __running_cpu;
      (<0.1>,472) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,474) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,478) fake_sched.h:43: return __running_cpu;
      (<0.1>,482) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,495) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,497) fake_sched.h:43: return __running_cpu;
      (<0.1>,501) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,502) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,504) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,505) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,506) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,512) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,513) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,518) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,519) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,520) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,521) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,525) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,527) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,528) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,529) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,548)
      (<0.1>,550) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,552) fake_sched.h:43: return __running_cpu;
      (<0.1>,556) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,559) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,563) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,564) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,565) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,569) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,570) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,571) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,573) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,578) fake_sched.h:43: return __running_cpu;
      (<0.1>,581) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,583) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,585) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,586) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,589) tree_plugin.h:2720: }
      (<0.1>,592) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,595) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,596) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,597) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,601) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,602) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,603) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,605) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,610) fake_sched.h:43: return __running_cpu;
      (<0.1>,613) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,615) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,617) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,618) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,621) tree_plugin.h:2720: }
      (<0.1>,624) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,627) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,628) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,629) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,633) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,635) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,637) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,644) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,647) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,648) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,649) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,651) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,652) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,654) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,657) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,663) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,664) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,667) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,672) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,673) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,674) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,688) tree_plugin.h:3141: }
      (<0.1>,690) tree.c:583: local_irq_restore(flags);
      (<0.1>,693) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,695) fake_sched.h:43: return __running_cpu;
      (<0.1>,699) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,701) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,705) fake_sched.h:43: return __running_cpu;
      (<0.1>,709) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,715) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,718) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:704: local_irq_save(flags);
    (<0.0>,35) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,64) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,66) fake_sched.h:43: return __running_cpu;
    (<0.0>,70) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,71) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,75) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,80) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,81) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,85) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,86) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,87) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,88) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,92) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,94) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,95) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,96) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,110)
    (<0.0>,111) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,113) fake_sched.h:43: return __running_cpu;
    (<0.0>,117) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,124) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,125) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,126) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,128) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,129) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,131) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,141) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,145) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,150) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,151) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,152) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,157) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,166) tree_plugin.h:3145: }
    (<0.0>,168) tree.c:707: local_irq_restore(flags);
    (<0.0>,171) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,173) fake_sched.h:43: return __running_cpu;
    (<0.0>,177) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,179) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,183) fake_sched.h:43: return __running_cpu;
    (<0.0>,187) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,194) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,206)
    (<0.0>,207) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,209) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,213)
    (<0.0>,214) tree.c:453: return &rsp->node[0];
    (<0.0>,218) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,223) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,225) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) fake_sched.h:43: return __running_cpu;
    (<0.0>,233) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,237) fake_sched.h:43: return __running_cpu;
    (<0.0>,241) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,246) fake_sched.h:43: return __running_cpu;
    (<0.0>,250) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,260) tree.c:749: local_irq_save(flags);
    (<0.0>,263) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,271) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,276) fake_sched.h:43: return __running_cpu;
    (<0.0>,280) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,281) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,283) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,284) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,285) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,287) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,289) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,290) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,292) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,297) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,298) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,300) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,304) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,305) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,306) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,307) tree.c:754: if (oldval)
    (<0.0>,315) tree_plugin.h:3145: }
    (<0.0>,317) tree.c:759: local_irq_restore(flags);
    (<0.0>,320) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,322) fake_sched.h:43: return __running_cpu;
    (<0.0>,326) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,328) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,336) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,341) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,346) fake_sched.h:43: return __running_cpu;
    (<0.0>,351) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,359) fake_sched.h:43: return __running_cpu;
    (<0.0>,364) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,370) fake_sched.h:43: return __running_cpu;
    (<0.0>,375) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,388) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,389) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,390) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,394) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,395) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,396) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,398) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,402) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,404) fake_sched.h:43: return __running_cpu;
    (<0.0>,407) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,409) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,416)
    (<0.0>,417)
    (<0.0>,418) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,420) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,421) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,422) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,424) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,426) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,427) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,428) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,438)
    (<0.0>,439)
    (<0.0>,440) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,445) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,448) tree_plugin.h:3185: return 0;
    (<0.0>,451) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,454) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,456) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,459) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,461) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,465) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,468)
    (<0.0>,469) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,471) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,474) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,481) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,482) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,488)
    (<0.0>,489)
    (<0.0>,490) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,493)
    (<0.0>,494) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,496) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,497) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,499) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,505) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,511)
    (<0.0>,512) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,515)
    (<0.0>,516) tree.c:453: return &rsp->node[0];
    (<0.0>,520) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,521) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,523) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,527) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,528) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,530) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,533) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,534) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,535) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,539) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,542) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,545) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,548) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,549) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,552) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,554) tree.c:494: }
    (<0.0>,558) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,560) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,562) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,563) tree.c:3152: return 1;
    (<0.0>,565) tree.c:3176: }
    (<0.0>,569) tree.c:3189: return 1;
    (<0.0>,571) tree.c:3191: }
    (<0.0>,578) fake_sched.h:43: return __running_cpu;
    (<0.0>,582) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,586) tree.c:2437: if (user)
    (<0.0>,594) fake_sched.h:43: return __running_cpu;
    (<0.0>,598) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,600) fake_sched.h:43: return __running_cpu;
    (<0.0>,604) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,610) fake_sched.h:43: return __running_cpu;
    (<0.0>,614) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,624) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,627) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,628) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,629) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,633) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,634) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,635) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,637) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,641) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,650) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,652) fake_sched.h:43: return __running_cpu;
    (<0.0>,655) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,657) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,659) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,660) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,662) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,669) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,670) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,672) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,678) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,679) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,680) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,681) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,682) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,686)
    (<0.0>,687)
    (<0.0>,688) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,689) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,696)
    (<0.0>,697)
    (<0.0>,698) tree.c:1584: local_irq_save(flags);
    (<0.0>,701) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,703) fake_sched.h:43: return __running_cpu;
    (<0.0>,707) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,709) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,713) fake_sched.h:43: return __running_cpu;
    (<0.0>,717) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,722) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,724) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,725) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,726) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,728) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,729) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,731) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,734) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,736) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,737) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,739) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,742) tree.c:1589: local_irq_restore(flags);
    (<0.0>,745) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,747) fake_sched.h:43: return __running_cpu;
    (<0.0>,751) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,753) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,757) fake_sched.h:43: return __running_cpu;
    (<0.0>,761) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,768) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,770) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,775) tree.c:2558: local_irq_save(flags);
    (<0.0>,778) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,780) fake_sched.h:43: return __running_cpu;
    (<0.0>,784) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,786) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,790) fake_sched.h:43: return __running_cpu;
    (<0.0>,794) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,799) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,800) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,806)
    (<0.0>,807)
    (<0.0>,808) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,811)
    (<0.0>,812) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,814) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,815) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,817) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,823) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,829)
    (<0.0>,830) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,833)
    (<0.0>,834) tree.c:453: return &rsp->node[0];
    (<0.0>,838) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,839) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,841) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,845) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,846) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,848) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,851) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,852) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,853) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,857) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,860) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,863) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,866) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,867) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,870) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,872) tree.c:494: }
    (<0.0>,876) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,879)
    (<0.0>,880) tree.c:453: return &rsp->node[0];
    (<0.0>,887) fake_sync.h:108: preempt_disable();
    (<0.0>,889) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,890) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,894) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,900) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,902) fake_sched.h:43: return __running_cpu;
    (<0.0>,905) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,907) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,909) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,910) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,913)
    (<0.0>,914) tree.c:453: return &rsp->node[0];
    (<0.0>,918) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,919) tree.c:1925: bool ret = false;
    (<0.0>,920) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,921) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,922) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,930)
    (<0.0>,931)
    (<0.0>,932)
    (<0.0>,933) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,936) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,939) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,942) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,943) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,946) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,948) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,951) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,953) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,954) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,956) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,959) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,964) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,966) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,967) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,970) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,972) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,975) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,977) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,980) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,981) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,984) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,987) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,989) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,992) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,993) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,995) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,998) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,999) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1001) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1004) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1005) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1007) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1010) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1012) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1014) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1015) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1017) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1019) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1022) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1024) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1027) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1028) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1031) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1034) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1036) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1039) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1040) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1042) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1045) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1046) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1048) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1051) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1052) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1054) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1057) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1059) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1061) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1062) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1064) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1066) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1069) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1070) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1071) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1080)
    (<0.0>,1081)
    (<0.0>,1082)
    (<0.0>,1083) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1086) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1089) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1092) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1093) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1096) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1097) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1102)
    (<0.0>,1103)
    (<0.0>,1104) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1107)
    (<0.0>,1108) tree.c:453: return &rsp->node[0];
    (<0.0>,1112) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1115) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1117) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1118) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1120) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1123) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1125) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1127) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1129) tree.c:1261: }
    (<0.0>,1131) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1132) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1134) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1137) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1139) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1142) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1143) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1146) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1149) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1153) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1155) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1157) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1160) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1162) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1165) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1166) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1169) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1172) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1176) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1178) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1180) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1183) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1185) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1189) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1192) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1195) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1196) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1198) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1201) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1202) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1203) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1205) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1208) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1210) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1212) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1214) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1217) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1220) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1221) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1223) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1226) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1227) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1228) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1230) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1233) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1235) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1237) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1239) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1242) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1245) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1246) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1248) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1251) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1252) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1253) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1255) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1258) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1260) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1262) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1264) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1267) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1268) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1277)
    (<0.0>,1278)
    (<0.0>,1279)
    (<0.0>,1280) tree.c:1289: bool ret = false;
    (<0.0>,1281) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1283) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1286)
    (<0.0>,1287) tree.c:453: return &rsp->node[0];
    (<0.0>,1291) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1292) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1294) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1295) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1300)
    (<0.0>,1301)
    (<0.0>,1302) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1305)
    (<0.0>,1306) tree.c:453: return &rsp->node[0];
    (<0.0>,1310) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1313) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1315) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1316) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1318) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1321) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1323) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1325) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1327) tree.c:1261: }
    (<0.0>,1329) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1330) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1331) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1332) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1338)
    (<0.0>,1339)
    (<0.0>,1340)
    (<0.0>,1341) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1345) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1347) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1350) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1353) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1355) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1356) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1358) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1361) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1363) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1364) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1366) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1369) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1370) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1373) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1375) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1376) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1381)
    (<0.0>,1382)
    (<0.0>,1383) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1386)
    (<0.0>,1387) tree.c:453: return &rsp->node[0];
    (<0.0>,1391) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1394) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1396) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1397) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1399) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1402) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1404) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1406) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1408) tree.c:1261: }
    (<0.0>,1410) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1411) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1413) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1416) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1417) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1419) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1422) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1426) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1427) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1429) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1432) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1435) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1437) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1439) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1442) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1443) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1445) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1448) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1453) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1455) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1457) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1460) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1461) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1463) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1466) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1471) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1473) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1475) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1478) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1480) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1483) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1486) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1488) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1491) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1493) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1494) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1496) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1497) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1499) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1502) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1503) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1504) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1510)
    (<0.0>,1511)
    (<0.0>,1512)
    (<0.0>,1513) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1517) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1519) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1520) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1521) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1527)
    (<0.0>,1528)
    (<0.0>,1529)
    (<0.0>,1530) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1532) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1535) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1536) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1542)
    (<0.0>,1543)
    (<0.0>,1544) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1547)
    (<0.0>,1548) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1550) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1551) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1553) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1559) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1565)
    (<0.0>,1566) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1569)
    (<0.0>,1570) tree.c:453: return &rsp->node[0];
    (<0.0>,1574) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1575) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1577) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1581) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1582) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1584) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1587) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1588) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1589) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1593) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1595) tree.c:494: }
    (<0.0>,1599) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1601) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1604) tree.c:1909: return true;
    (<0.0>,1606) tree.c:1910: }
    (<0.0>,1609) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1612) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1613) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1617) tree.c:1372: if (c_out != NULL)
    (<0.0>,1620) tree.c:1374: return ret;
    (<0.0>,1624) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1625) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1628) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1629) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1635) tree.c:1482: return ret;
    (<0.0>,1637) tree.c:1482: return ret;
    (<0.0>,1639) tree.c:1483: }
    (<0.0>,1641) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1643) tree.c:1527: }
    (<0.0>,1647) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1648) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1649) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1650) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1656)
    (<0.0>,1657)
    (<0.0>,1658)
    (<0.0>,1659) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1661) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1664) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1665) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1671)
    (<0.0>,1672)
    (<0.0>,1673) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1676)
    (<0.0>,1677) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1679) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1680) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1682) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1688) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1694)
    (<0.0>,1695) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1698)
    (<0.0>,1699) tree.c:453: return &rsp->node[0];
    (<0.0>,1703) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1704) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1706) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1710) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1711) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1713) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1716) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1717) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1718) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1722) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1724) tree.c:494: }
    (<0.0>,1728) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1730) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1733) tree.c:1909: return true;
    (<0.0>,1735) tree.c:1910: }
    (<0.0>,1739) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1740) tree.c:1937: return ret;
    (<0.0>,1744) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,1745) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1748)
    (<0.0>,1749) tree.c:453: return &rsp->node[0];
    (<0.0>,1754) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1758)
    (<0.0>,1759)
    (<0.0>,1760) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1761) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1764) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,1767) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1769) fake_sched.h:43: return __running_cpu;
    (<0.0>,1773) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1775) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1779) fake_sched.h:43: return __running_cpu;
    (<0.0>,1783) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1790) tree.c:2563: if (needwake)
    (<0.0>,1793) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,1796)
    (<0.0>,1797) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1798) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1800) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1807) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,1810)
    (<0.0>,1811) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1813) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1816) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1823) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,1826) tree_plugin.h:2720: }
    (<0.0>,1830) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1833) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1834) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1835) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1839) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1840) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1841) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1843) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1847) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,1856) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1858) fake_sched.h:43: return __running_cpu;
    (<0.0>,1861) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1863) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1865) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1866) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1868) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1875) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1876) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1878) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1884) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1885) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1886) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1887) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1888) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1892)
    (<0.0>,1893)
    (<0.0>,1894) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1895) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1902)
    (<0.0>,1903)
    (<0.0>,1904) tree.c:1584: local_irq_save(flags);
    (<0.0>,1907) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1909) fake_sched.h:43: return __running_cpu;
    (<0.0>,1913) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1915) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1919) fake_sched.h:43: return __running_cpu;
    (<0.0>,1923) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1928) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1930) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1931) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1932) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1934) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1935) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1937) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1940) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1942) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1943) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1945) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1948) tree.c:1589: local_irq_restore(flags);
    (<0.0>,1951) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1953) fake_sched.h:43: return __running_cpu;
    (<0.0>,1957) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1959) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1963) fake_sched.h:43: return __running_cpu;
    (<0.0>,1967) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1974) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1976) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1981) tree.c:2558: local_irq_save(flags);
    (<0.0>,1984) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1986) fake_sched.h:43: return __running_cpu;
    (<0.0>,1990) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1992) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1996) fake_sched.h:43: return __running_cpu;
    (<0.0>,2000) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2005) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2006) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2012)
    (<0.0>,2013)
    (<0.0>,2014) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,2017)
    (<0.0>,2018) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2020) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2021) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2023) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2029) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,2035)
    (<0.0>,2036) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2039)
    (<0.0>,2040) tree.c:453: return &rsp->node[0];
    (<0.0>,2044) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2045) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2047) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2051) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2052) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2054) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2057) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2058) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2059) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2063) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2066) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2069) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2072) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2073) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2076) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2078) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2081) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2084) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2087) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2088) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2090) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2093) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2097) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2099) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2101) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2104) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2107) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2110) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2111) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2113) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2116) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2120) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2122) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2124) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2127) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,2129) tree.c:494: }
    (<0.0>,2133) tree.c:2566: local_irq_restore(flags);
    (<0.0>,2136) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2138) fake_sched.h:43: return __running_cpu;
    (<0.0>,2142) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2144) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2148) fake_sched.h:43: return __running_cpu;
    (<0.0>,2152) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2158) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2161)
    (<0.0>,2162) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2164) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2167) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2174) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2177) tree_plugin.h:2720: }
    (<0.0>,2181) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2184) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2185) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2186) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2190) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2191) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2192) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2194) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2202) fake_sched.h:43: return __running_cpu;
    (<0.0>,2206) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,2215) tree.c:624: local_irq_save(flags);
    (<0.0>,2218) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2220) fake_sched.h:43: return __running_cpu;
    (<0.0>,2224) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2226) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2230) fake_sched.h:43: return __running_cpu;
    (<0.0>,2234) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2240) fake_sched.h:43: return __running_cpu;
    (<0.0>,2244) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2245) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2247) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2248) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2249) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2251) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2253) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2254) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2256) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2261) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2262) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2264) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2268) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2269) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2270) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2271) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2273) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2281) tree_plugin.h:3141: }
    (<0.0>,2283) tree.c:634: local_irq_restore(flags);
    (<0.0>,2286) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2288) fake_sched.h:43: return __running_cpu;
    (<0.0>,2292) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2294) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2298) fake_sched.h:43: return __running_cpu;
    (<0.0>,2302) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2311) fake_sched.h:43: return __running_cpu;
    (<0.0>,2315) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,2318) tree.c:580: local_irq_save(flags);
    (<0.0>,2321) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2323) fake_sched.h:43: return __running_cpu;
    (<0.0>,2327) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2329) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2333) fake_sched.h:43: return __running_cpu;
    (<0.0>,2337) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2350) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2352) fake_sched.h:43: return __running_cpu;
    (<0.0>,2356) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2357) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2359) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2360) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2361) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2367) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2368) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2373) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2374) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2375) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2376) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,2380) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2382) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2383) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2384) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2403)
    (<0.0>,2405) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2407) fake_sched.h:43: return __running_cpu;
    (<0.0>,2411) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2414) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,2418) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2419) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2420) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2424) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2425) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2426) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2428) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2433) fake_sched.h:43: return __running_cpu;
    (<0.0>,2436) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2438) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2440) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2441) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2444) tree_plugin.h:2720: }
    (<0.0>,2447) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2450) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2451) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2452) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2456) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2457) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2458) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2460) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2465) fake_sched.h:43: return __running_cpu;
    (<0.0>,2468) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2470) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2472) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2473) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2476) tree_plugin.h:2720: }
    (<0.0>,2479) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2482) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2483) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2484) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2488) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2489) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2490) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2492) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2499) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2502) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2503) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2504) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2506) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2507) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2509) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2512) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2518) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2519) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2522) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2527) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2528) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2529) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2543) tree_plugin.h:3141: }
    (<0.0>,2545) tree.c:583: local_irq_restore(flags);
    (<0.0>,2548) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2550) fake_sched.h:43: return __running_cpu;
    (<0.0>,2554) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2556) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2560) fake_sched.h:43: return __running_cpu;
    (<0.0>,2564) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2570) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2573) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2578) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2580) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2589) fake_sched.h:43: return __running_cpu;
    (<0.0>,2593)
    (<0.0>,2594) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2597) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2602) tree.c:704: local_irq_save(flags);
    (<0.0>,2605) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2607) fake_sched.h:43: return __running_cpu;
    (<0.0>,2611) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2613) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2617) fake_sched.h:43: return __running_cpu;
    (<0.0>,2621) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2634) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2636) fake_sched.h:43: return __running_cpu;
    (<0.0>,2640) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2641) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2643) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2644) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2645) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2650) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2651) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2655) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2656) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2657) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2658) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,2662) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2664) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2665) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2666) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2680)
    (<0.0>,2681) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2683) fake_sched.h:43: return __running_cpu;
    (<0.0>,2687) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2691) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2694) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2695) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2696) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2698) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2699) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2701) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2704) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2711) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2712) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2715) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2720) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2721) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2722) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2727) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,2736) tree_plugin.h:3145: }
    (<0.0>,2738) tree.c:707: local_irq_restore(flags);
    (<0.0>,2741) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2743) fake_sched.h:43: return __running_cpu;
    (<0.0>,2747) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2749) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2753) fake_sched.h:43: return __running_cpu;
    (<0.0>,2757) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2764) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,2776)
    (<0.0>,2777) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2780)
    (<0.0>,2781) tree.c:453: return &rsp->node[0];
    (<0.0>,2785) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2789) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2793) fake_sync.h:92: local_irq_disable();
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2804) fake_sched.h:43: return __running_cpu;
    (<0.0>,2808) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2813) fake_sched.h:43: return __running_cpu;
    (<0.0>,2817) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,2820) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2821) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2827) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2829) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2832) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2834) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2835) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2838)
    (<0.0>,2839) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2841) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2842) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2844) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2852) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2853) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2856)
    (<0.0>,2857) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2859) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2860) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2870) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2871) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2874) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,2879)
    (<0.0>,2880) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2881) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2882) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2883) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2885) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2889) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2890) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2891) update.c:344: if (till_stall_check < 3) {
    (<0.0>,2894) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,2898) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,2903) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,2904) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2905) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2907) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2909) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2910) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2911) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2914) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2916) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2920) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2922) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2924) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2926) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2930) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,2934)
    (<0.0>,2935) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2936) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2941) fake_sched.h:43: return __running_cpu;
    (<0.0>,2945) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,2947) fake_sched.h:43: return __running_cpu;
    (<0.0>,2951) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2957) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,2961)
    (<0.0>,2962) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2964) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2970) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2973) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2975) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2976) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2978) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2983) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2987) fake_sync.h:92: local_irq_disable();
    (<0.0>,2990) fake_sched.h:43: return __running_cpu;
    (<0.0>,2994) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2998) fake_sched.h:43: return __running_cpu;
    (<0.0>,3002) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3007) fake_sched.h:43: return __running_cpu;
    (<0.0>,3011) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3014) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3015) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3022) fake_sched.h:43: return __running_cpu;
    (<0.0>,3025) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3027) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3029) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3030) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,3036)
    (<0.0>,3037) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3039) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3044) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3045) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3047) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3051) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3052) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3053) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3055) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3057) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3058) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3060) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3061) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3063) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3064) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3066) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
  (<0>,5327) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5328) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5330) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5333) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5335) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5337) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5338) tree.c:3164: return 1;
  (<0>,5340) tree.c:3176: }
  (<0>,5344) tree.c:3189: return 1;
  (<0>,5346) tree.c:3191: }
  (<0>,5353) fake_sched.h:43: return __running_cpu;
  (<0>,5357) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
  (<0>,5361) tree.c:2437: if (user)
  (<0>,5369) fake_sched.h:43: return __running_cpu;
  (<0>,5373) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
  (<0>,5375) fake_sched.h:43: return __running_cpu;
  (<0>,5379) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5385) fake_sched.h:43: return __running_cpu;
  (<0>,5389) fake_sched.h:182: if (need_softirq[get_cpu()]) {
  (<0>,5399) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,5402) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5403) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5404) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5408) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5409) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5410) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5412) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5416) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,5425) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5427) fake_sched.h:43: return __running_cpu;
  (<0>,5430) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5432) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5434) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5435) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5437) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5444) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5445) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5447) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5453) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5454) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5455) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5456) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5457) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5461)
  (<0>,5462)
  (<0>,5463) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5464) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5471)
  (<0>,5472)
  (<0>,5473) tree.c:1584: local_irq_save(flags);
  (<0>,5476) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5478) fake_sched.h:43: return __running_cpu;
  (<0>,5482) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5484) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5488) fake_sched.h:43: return __running_cpu;
  (<0>,5492) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5497) tree.c:1585: rnp = rdp->mynode;
  (<0>,5499) tree.c:1585: rnp = rdp->mynode;
  (<0>,5500) tree.c:1585: rnp = rdp->mynode;
  (<0>,5501) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5503) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5504) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5506) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5509) tree.c:1588: !raw_spin_trylock(&rnp->lock)) { /* irqs already off, so later. */
  (<0>,5514) fake_sync.h:122: preempt_disable();
  (<0>,5516) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
    (<0.0>,3067) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3069) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3070) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3072) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3077) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3078) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3080) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3081) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3083) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3087) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3088) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3089) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3090) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3092) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3093) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3095) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3096) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3097) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3099) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3102) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3103) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3104) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3110)
    (<0.0>,3111)
    (<0.0>,3112)
    (<0.0>,3113) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3115) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3116) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3118) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3121) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3122) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3123) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3132)
    (<0.0>,3133)
    (<0.0>,3134)
    (<0.0>,3135) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3138) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3141) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3144) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3145) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3148) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3149) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3154)
    (<0.0>,3155)
    (<0.0>,3156) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3159)
    (<0.0>,3160) tree.c:453: return &rsp->node[0];
    (<0.0>,3164) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3167) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3169) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3170) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3172) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3175) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3177) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3179) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3181) tree.c:1261: }
    (<0.0>,3183) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3184) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3186) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3189) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3191) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3194) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3195) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3198) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3201) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3205) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3207) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3209) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3212) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3214) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3217) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3218) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3221) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3224) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3227) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3229) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3232) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3233) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3238) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3240) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3244) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3247) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3250) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3251) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3253) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3256) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3257) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3258) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3260) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3263) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3265) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3267) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3269) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3272) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3275) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3276) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3278) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3281) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3282) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3283) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3285) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3288) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3290) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3292) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3294) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3297) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3298) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3307)
    (<0.0>,3308)
    (<0.0>,3309)
    (<0.0>,3310) tree.c:1289: bool ret = false;
    (<0.0>,3311) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3313) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3316)
    (<0.0>,3317) tree.c:453: return &rsp->node[0];
    (<0.0>,3321) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3322) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3324) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3325) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3330)
    (<0.0>,3331)
    (<0.0>,3332) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3335)
    (<0.0>,3336) tree.c:453: return &rsp->node[0];
    (<0.0>,3340) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3343) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3345) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3346) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3348) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3351) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3353) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3355) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3357) tree.c:1261: }
    (<0.0>,3359) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3360) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3361) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3362) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3368)
    (<0.0>,3369)
    (<0.0>,3370)
    (<0.0>,3371) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,3375) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3377) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3380) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3383) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3385) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3386) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3388) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3391) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3393) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3396) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3398) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3399) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3400) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3401) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3407)
    (<0.0>,3408)
    (<0.0>,3409)
    (<0.0>,3410) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,3415) tree.c:1372: if (c_out != NULL)
    (<0.0>,3418) tree.c:1374: return ret;
    (<0.0>,3422) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3423) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3426) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3427) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3433) tree.c:1482: return ret;
    (<0.0>,3435) tree.c:1482: return ret;
    (<0.0>,3437) tree.c:1483: }
    (<0.0>,3440) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3442) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3444) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3445) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3447) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3450) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3452) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3453) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3455) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3458) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3460) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3461) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3463) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3464) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3466) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3471) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3474) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3475) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,3478) tree_plugin.h:1950: }
    (<0.0>,3481) tree.c:1575: return ret;
    (<0.0>,3485) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,3488) tree_plugin.h:1487: }
    (<0.0>,3492) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,3496)
    (<0.0>,3497) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3498) fake_sync.h:100: if (pthread_mutex_unlock(l))
  (<0>,5517) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
  (<0>,5520) fake_sync.h:127: return 1;
  (<0>,5522) fake_sync.h:128: }
  (<0>,5528) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5529) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5530) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5536)
  (<0>,5537)
  (<0>,5538)
  (<0>,5539) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5541) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5542) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5544) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5547) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5548) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5549) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5558)
  (<0>,5559)
  (<0>,5560)
  (<0>,5561) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5564) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5567) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5570) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5571) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5574) tree.c:1434: return false;
  (<0>,5576) tree.c:1483: }
  (<0>,5579) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5581) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5583) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5584) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5586) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5589) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5591) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5592) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5594) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5597) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,5599) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,5600) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5602) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5603) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5605) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5610) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5613) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5614) tree.c:1573: zero_cpu_stall_ticks(rdp);
  (<0>,5617) tree_plugin.h:1950: }
  (<0>,5620) tree.c:1575: return ret;
  (<0>,5624) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5625) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,5627) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,5631)
  (<0>,5632)
  (<0>,5633) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,5634) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,5637) fake_sync.h:86: local_irq_restore(flags);
  (<0>,5640) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5642) fake_sched.h:43: return __running_cpu;
  (<0>,5646) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5648) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5652) fake_sched.h:43: return __running_cpu;
  (<0>,5656) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5663) tree.c:1595: if (needwake)
  (<0>,5667) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5669) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5672) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,5674) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,5679) tree.c:2558: local_irq_save(flags);
  (<0>,5682) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5684) fake_sched.h:43: return __running_cpu;
  (<0>,5688) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5690) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5694) fake_sched.h:43: return __running_cpu;
  (<0>,5698) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5703) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5704) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5710)
  (<0>,5711)
  (<0>,5712) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,5715)
  (<0>,5716) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5718) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5719) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5721) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5727) tree.c:481: return 0;  /* No, a grace period is already in progress. */
  (<0>,5729) tree.c:494: }
  (<0>,5733) tree.c:2566: local_irq_restore(flags);
  (<0>,5736) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5738) fake_sched.h:43: return __running_cpu;
  (<0>,5742) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5744) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5748) fake_sched.h:43: return __running_cpu;
  (<0>,5752) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5758) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,5761)
  (<0>,5762) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5764) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5767) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5774) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,5777) tree_plugin.h:2720: }
  (<0>,5781) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5784) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5785) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5786) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5790) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5791) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5792) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5794) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5798) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,5807) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5809) fake_sched.h:43: return __running_cpu;
  (<0>,5812) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5814) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5816) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5817) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5819) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5826) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5827) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5829) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5835) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5836) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5837) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5838) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5839) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5843)
  (<0>,5844)
  (<0>,5845) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5846) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5853)
  (<0>,5854)
  (<0>,5855) tree.c:1584: local_irq_save(flags);
  (<0>,5858) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5860) fake_sched.h:43: return __running_cpu;
  (<0>,5864) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5866) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5870) fake_sched.h:43: return __running_cpu;
  (<0>,5874) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5879) tree.c:1585: rnp = rdp->mynode;
  (<0>,5881) tree.c:1585: rnp = rdp->mynode;
  (<0>,5882) tree.c:1585: rnp = rdp->mynode;
  (<0>,5883) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5885) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5886) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5888) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5891) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5893) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5894) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5896) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5899) tree.c:1589: local_irq_restore(flags);
  (<0>,5902) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5904) fake_sched.h:43: return __running_cpu;
  (<0>,5908) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5910) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5914) fake_sched.h:43: return __running_cpu;
  (<0>,5918) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5925) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5927) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5932) tree.c:2558: local_irq_save(flags);
  (<0>,5935) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5937) fake_sched.h:43: return __running_cpu;
  (<0>,5941) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5943) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5947) fake_sched.h:43: return __running_cpu;
  (<0>,5951) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5956) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5957) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5963)
  (<0>,5964)
  (<0>,5965) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,5968)
  (<0>,5969) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5971) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5972) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5974) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5980) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5986)
  (<0>,5987) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5990)
  (<0>,5991) tree.c:453: return &rsp->node[0];
  (<0>,5995) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5996) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5998) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6002) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6003) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6005) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6008) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6009) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6010) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6014) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6017) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6020) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6023) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6024) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6027) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6029) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6032) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6035) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6038) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6039) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6041) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6044) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6048) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6050) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6052) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6055) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6058) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6061) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6062) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6064) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6067) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6071) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6073) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6075) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6078) tree.c:493: return 0; /* No grace period needed. */
  (<0>,6080) tree.c:494: }
  (<0>,6084) tree.c:2566: local_irq_restore(flags);
  (<0>,6087) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6089) fake_sched.h:43: return __running_cpu;
  (<0>,6093) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6095) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6099) fake_sched.h:43: return __running_cpu;
  (<0>,6103) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6109) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,6112)
  (<0>,6113) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6115) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6118) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6125) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,6128) tree_plugin.h:2720: }
  (<0>,6132) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6135) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6136) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6137) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6141) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6142) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6143) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6145) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6153) fake_sched.h:43: return __running_cpu;
  (<0>,6157) fake_sched.h:184: need_softirq[get_cpu()] = 0;
  (<0>,6166) tree.c:624: local_irq_save(flags);
  (<0>,6169) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6171) fake_sched.h:43: return __running_cpu;
  (<0>,6175) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6177) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6181) fake_sched.h:43: return __running_cpu;
  (<0>,6185) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6191) fake_sched.h:43: return __running_cpu;
  (<0>,6195) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6196) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6198) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6199) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6200) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6202) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6204) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6205) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6207) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6212) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6213) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6215) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6219) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6220) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6221) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6222) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,6224) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,6232) tree_plugin.h:3141: }
  (<0>,6234) tree.c:634: local_irq_restore(flags);
  (<0>,6237) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6239) fake_sched.h:43: return __running_cpu;
  (<0>,6243) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6245) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6249) fake_sched.h:43: return __running_cpu;
  (<0>,6253) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6267) fake_sched.h:43: return __running_cpu;
  (<0>,6272) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
  (<0>,6278) fake_sched.h:43: return __running_cpu;
  (<0>,6283) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
  (<0>,6289) fake_sched.h:43: return __running_cpu;
  (<0>,6293) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
  (<0>,6305) fake_sched.h:43: return __running_cpu;
  (<0>,6309) fake_sched.h:96: rcu_idle_enter();
  (<0>,6312) tree.c:580: local_irq_save(flags);
  (<0>,6315) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6317) fake_sched.h:43: return __running_cpu;
  (<0>,6321) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6323) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6327) fake_sched.h:43: return __running_cpu;
  (<0>,6331) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6344) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6346) fake_sched.h:43: return __running_cpu;
  (<0>,6350) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6351) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6353) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6354) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6355) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6361) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6362) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6367) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6368) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6369) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6370) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,6374) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,6376) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,6377) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,6378) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,6397)
  (<0>,6399) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6401) fake_sched.h:43: return __running_cpu;
  (<0>,6405) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6408) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,6412) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6413) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6414) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6418) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6419) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6420) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6422) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6427) fake_sched.h:43: return __running_cpu;
  (<0>,6430) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6432) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6434) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6435) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,6438) tree_plugin.h:2720: }
  (<0>,6441) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6444) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6445) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6446) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6450) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6451) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6452) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6454) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6459) fake_sched.h:43: return __running_cpu;
  (<0>,6462) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6464) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6466) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6467) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,6470) tree_plugin.h:2720: }
  (<0>,6473) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6476) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6477) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6478) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6482) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6483) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6484) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6486) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6493) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6496) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6497) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6498) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6500) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6501) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6503) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6506) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6512) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6513) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6516) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6521) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6522) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6523) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6537) tree_plugin.h:3141: }
  (<0>,6539) tree.c:583: local_irq_restore(flags);
  (<0>,6542) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6544) fake_sched.h:43: return __running_cpu;
  (<0>,6548) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6550) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6554) fake_sched.h:43: return __running_cpu;
  (<0>,6558) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6564) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6567) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6572) fake_sched.h:43: return __running_cpu;
  (<0>,6576)
  (<0>,6577) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,6580) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,6585) tree.c:704: local_irq_save(flags);
  (<0>,6588) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6590) fake_sched.h:43: return __running_cpu;
  (<0>,6594) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6596) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6600) fake_sched.h:43: return __running_cpu;
  (<0>,6604) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6617) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6619) fake_sched.h:43: return __running_cpu;
  (<0>,6623) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6624) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6626) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6627) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6628) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6633) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6634) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6638) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6639) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6640) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6641) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,6645) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6647) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6648) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,6649) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,6663)
  (<0>,6664) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6666) fake_sched.h:43: return __running_cpu;
  (<0>,6670) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6674) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6677) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6678) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6679) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6681) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6682) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6684) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6687) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6694) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6695) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6698) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6703) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6704) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6705) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6710) tree.c:656: if (!user && !is_idle_task(current)) {
  (<0>,6719) tree_plugin.h:3145: }
  (<0>,6721) tree.c:707: local_irq_restore(flags);
  (<0>,6724) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6726) fake_sched.h:43: return __running_cpu;
  (<0>,6730) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6732) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6736) fake_sched.h:43: return __running_cpu;
  (<0>,6740) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6751) fake_sched.h:43: return __running_cpu;
  (<0>,6755) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
  (<0>,6759) fake_sched.h:43: return __running_cpu;
  (<0>,6763) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6768) fake_sched.h:43: return __running_cpu;
  (<0>,6772) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
  (<0>,6782) tree.c:749: local_irq_save(flags);
  (<0>,6785) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6787) fake_sched.h:43: return __running_cpu;
  (<0>,6791) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6793) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6798) fake_sched.h:43: return __running_cpu;
  (<0>,6802) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6803) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6805) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6806) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6807) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6809) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6811) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6812) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6814) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6819) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6820) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6822) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6826) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6827) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6828) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6829) tree.c:754: if (oldval)
  (<0>,6837) tree_plugin.h:3145: }
  (<0>,6839) tree.c:759: local_irq_restore(flags);
  (<0>,6842) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6844) fake_sched.h:43: return __running_cpu;
  (<0>,6848) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6850) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6858) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,6863) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,6868) fake_sched.h:43: return __running_cpu;
  (<0>,6873) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,6881) fake_sched.h:43: return __running_cpu;
  (<0>,6886) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
  (<0>,6900) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6901) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6902) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6906) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6907) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6908) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6910) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6914) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6916) fake_sched.h:43: return __running_cpu;
  (<0>,6919) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6921) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6928)
  (<0>,6929)
  (<0>,6930) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6932) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6933) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6934) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,6936) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,6938) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,6939) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,6940) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,6950)
  (<0>,6951)
  (<0>,6952) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
  (<0>,6957) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
  (<0>,6960) tree_plugin.h:3185: return 0;
  (<0>,6963) tree.c:3135: if (rcu_scheduler_fully_active &&
  (<0>,6966) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,6968) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,6971) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,6973) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,6976) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,6978) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,6981) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,6983) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,6986) tree.c:3139: rdp->n_rp_report_qs++;
  (<0>,6988) tree.c:3139: rdp->n_rp_report_qs++;
  (<0>,6990) tree.c:3139: rdp->n_rp_report_qs++;
  (<0>,6991) tree.c:3140: return 1;
  (<0>,6993) tree.c:3176: }
  (<0>,6997) tree.c:3189: return 1;
  (<0>,6999) tree.c:3191: }
  (<0>,7006) fake_sched.h:43: return __running_cpu;
  (<0>,7010) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
  (<0>,7014) tree.c:2437: if (user)
  (<0>,7022) fake_sched.h:43: return __running_cpu;
  (<0>,7026) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
  (<0>,7028) fake_sched.h:43: return __running_cpu;
  (<0>,7032) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7038) fake_sched.h:43: return __running_cpu;
  (<0>,7042) fake_sched.h:182: if (need_softirq[get_cpu()]) {
  (<0>,7052) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,7055) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7056) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7057) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7061) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7062) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7063) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7065) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7069) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,7078) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7080) fake_sched.h:43: return __running_cpu;
  (<0>,7083) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7085) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7087) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7088) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7090) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7097) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7098) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7100) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7106) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7107) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7108) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7109) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7110) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7114)
  (<0>,7115)
  (<0>,7116) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7117) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7124)
  (<0>,7125)
  (<0>,7126) tree.c:1584: local_irq_save(flags);
  (<0>,7129) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7131) fake_sched.h:43: return __running_cpu;
  (<0>,7135) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7137) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7141) fake_sched.h:43: return __running_cpu;
  (<0>,7145) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7150) tree.c:1585: rnp = rdp->mynode;
  (<0>,7152) tree.c:1585: rnp = rdp->mynode;
  (<0>,7153) tree.c:1585: rnp = rdp->mynode;
  (<0>,7154) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7156) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7157) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7159) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7162) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7164) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7165) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7167) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7170) tree.c:1589: local_irq_restore(flags);
  (<0>,7173) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7175) fake_sched.h:43: return __running_cpu;
  (<0>,7179) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7181) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7185) fake_sched.h:43: return __running_cpu;
  (<0>,7189) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7196) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7198) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7201) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,7203) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,7206) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,7208) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,7209) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,7210) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,7219)
  (<0>,7220)
  (<0>,7221)
  (<0>,7222) tree.c:2034: rnp = rdp->mynode;
  (<0>,7224) tree.c:2034: rnp = rdp->mynode;
  (<0>,7225) tree.c:2034: rnp = rdp->mynode;
  (<0>,7226) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,7228) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,7232)
  (<0>,7233)
  (<0>,7234) fake_sync.h:76: local_irq_save(flags);
  (<0>,7237) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7239) fake_sched.h:43: return __running_cpu;
  (<0>,7243) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7245) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7249) fake_sched.h:43: return __running_cpu;
  (<0>,7253) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7259) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,7260) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,7266) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7268) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7273) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7275) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7276) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7278) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
  (<0>,7281) tree.c:2038: rnp->completed == rnp->gpnum) {
  (<0>,7283) tree.c:2038: rnp->completed == rnp->gpnum) {
  (<0>,7284) tree.c:2038: rnp->completed == rnp->gpnum) {
  (<0>,7286) tree.c:2038: rnp->completed == rnp->gpnum) {
  (<0>,7289) tree.c:2050: mask = rdp->grpmask;
  (<0>,7291) tree.c:2050: mask = rdp->grpmask;
  (<0>,7292) tree.c:2050: mask = rdp->grpmask;
  (<0>,7293) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
  (<0>,7295) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
  (<0>,7296) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
  (<0>,7300) tree.c:2054: rdp->qs_pending = 0;
  (<0>,7302) tree.c:2054: rdp->qs_pending = 0;
  (<0>,7303) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7304) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7305) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7314)
  (<0>,7315)
  (<0>,7316)
  (<0>,7317) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7320) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7323) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7326) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7327) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7330) tree.c:1434: return false;
  (<0>,7332) tree.c:1483: }
  (<0>,7335) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7336) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,7337) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,7338) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,7339) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
  (<0>,7349)
  (<0>,7350)
  (<0>,7351)
  (<0>,7352) tree.c:1974: for (;;) {
  (<0>,7354) tree.c:1975: if (!(rnp->qsmask & mask)) {
  (<0>,7356) tree.c:1975: if (!(rnp->qsmask & mask)) {
  (<0>,7357) tree.c:1975: if (!(rnp->qsmask & mask)) {
  (<0>,7361) tree.c:1981: rnp->qsmask &= ~mask;
  (<0>,7363) tree.c:1981: rnp->qsmask &= ~mask;
  (<0>,7365) tree.c:1981: rnp->qsmask &= ~mask;
  (<0>,7367) tree.c:1981: rnp->qsmask &= ~mask;
  (<0>,7370) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,7372) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,7375) tree.c:1990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,7377) tree.c:1990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,7381)
  (<0>,7382)
  (<0>,7383) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,7384) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,7387) fake_sync.h:86: local_irq_restore(flags);
  (<0>,7390) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7392) fake_sched.h:43: return __running_cpu;
  (<0>,7396) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7398) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7402) fake_sched.h:43: return __running_cpu;
  (<0>,7406) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7415) tree.c:2063: if (needwake)
  (<0>,7422) tree.c:2558: local_irq_save(flags);
  (<0>,7425) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7427) fake_sched.h:43: return __running_cpu;
  (<0>,7431) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7433) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7437) fake_sched.h:43: return __running_cpu;
  (<0>,7441) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7446) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7447) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7453)
  (<0>,7454)
  (<0>,7455) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,7458)
  (<0>,7459) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7461) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7462) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7464) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7470) tree.c:481: return 0;  /* No, a grace period is already in progress. */
  (<0>,7472) tree.c:494: }
  (<0>,7476) tree.c:2566: local_irq_restore(flags);
  (<0>,7479) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7481) fake_sched.h:43: return __running_cpu;
  (<0>,7485) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7487) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7491) fake_sched.h:43: return __running_cpu;
  (<0>,7495) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7501) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7504)
  (<0>,7505) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7507) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7510) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7517) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,7520) tree_plugin.h:2720: }
  (<0>,7524) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7527) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7528) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7529) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7533) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7534) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7535) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7537) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7541) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,7550) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7552) fake_sched.h:43: return __running_cpu;
  (<0>,7555) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7557) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7559) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7560) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7562) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7569) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7570) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7572) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7578) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7579) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7580) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7581) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7582) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7586)
  (<0>,7587)
  (<0>,7588) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7589) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7596)
  (<0>,7597)
  (<0>,7598) tree.c:1584: local_irq_save(flags);
  (<0>,7601) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7603) fake_sched.h:43: return __running_cpu;
  (<0>,7607) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7609) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7613) fake_sched.h:43: return __running_cpu;
  (<0>,7617) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7622) tree.c:1585: rnp = rdp->mynode;
  (<0>,7624) tree.c:1585: rnp = rdp->mynode;
  (<0>,7625) tree.c:1585: rnp = rdp->mynode;
  (<0>,7626) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7628) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7629) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7631) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7634) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7636) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7637) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7639) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7642) tree.c:1589: local_irq_restore(flags);
  (<0>,7645) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7647) fake_sched.h:43: return __running_cpu;
  (<0>,7651) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7653) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7657) fake_sched.h:43: return __running_cpu;
  (<0>,7661) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7668) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7670) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7675) tree.c:2558: local_irq_save(flags);
  (<0>,7678) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7680) fake_sched.h:43: return __running_cpu;
  (<0>,7684) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7686) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7690) fake_sched.h:43: return __running_cpu;
  (<0>,7694) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7699) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7700) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7706)
  (<0>,7707)
  (<0>,7708) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,7711)
  (<0>,7712) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7714) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7715) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7717) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7723) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,7729)
  (<0>,7730) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7733)
  (<0>,7734) tree.c:453: return &rsp->node[0];
  (<0>,7738) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7739) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7741) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7745) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7746) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7748) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7751) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7752) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7753) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7757) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7760) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7763) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7766) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7767) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7770) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7772) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7775) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7778) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7781) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7782) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7784) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7787) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7791) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7793) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7795) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7798) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7801) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7804) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7805) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7807) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7810) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7814) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7816) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7818) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7821) tree.c:493: return 0; /* No grace period needed. */
  (<0>,7823) tree.c:494: }
  (<0>,7827) tree.c:2566: local_irq_restore(flags);
  (<0>,7830) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7832) fake_sched.h:43: return __running_cpu;
  (<0>,7836) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7838) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7842) fake_sched.h:43: return __running_cpu;
  (<0>,7846) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7852) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7855)
  (<0>,7856) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7858) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7861) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7868) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,7871) tree_plugin.h:2720: }
  (<0>,7875) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7878) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7879) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7880) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7884) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7885) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7886) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7888) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7896) fake_sched.h:43: return __running_cpu;
  (<0>,7900) fake_sched.h:184: need_softirq[get_cpu()] = 0;
  (<0>,7909) tree.c:624: local_irq_save(flags);
  (<0>,7912) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7914) fake_sched.h:43: return __running_cpu;
  (<0>,7918) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7920) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7924) fake_sched.h:43: return __running_cpu;
  (<0>,7928) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7934) fake_sched.h:43: return __running_cpu;
  (<0>,7938) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,7939) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,7941) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,7942) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,7943) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,7945) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,7947) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,7948) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7950) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7955) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7956) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7958) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7962) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7963) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7964) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,7965) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,7967) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,7975) tree_plugin.h:3141: }
  (<0>,7977) tree.c:634: local_irq_restore(flags);
  (<0>,7980) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7982) fake_sched.h:43: return __running_cpu;
  (<0>,7986) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7988) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7992) fake_sched.h:43: return __running_cpu;
  (<0>,7996) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,719) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3499) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3503) fake_sched.h:43: return __running_cpu;
    (<0.0>,3507) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,3509) fake_sched.h:43: return __running_cpu;
    (<0.0>,3513) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3526) fake_sched.h:43: return __running_cpu;
    (<0.0>,3531) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,3537) fake_sched.h:43: return __running_cpu;
    (<0.0>,3542) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,3548) fake_sched.h:43: return __running_cpu;
    (<0.0>,3552) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,3564) fake_sched.h:43: return __running_cpu;
    (<0.0>,3568)
    (<0.0>,3571) tree.c:580: local_irq_save(flags);
    (<0.0>,3574)
    (<0.0>,3576) fake_sched.h:43: return __running_cpu;
    (<0.0>,3580) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3582) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3586) fake_sched.h:43: return __running_cpu;
    (<0.0>,3590) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3603)
    (<0.0>,3605) fake_sched.h:43: return __running_cpu;
    (<0.0>,3609) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3610) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3612) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3613) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3614) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3620) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3621) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3626) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3627) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3628) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3629) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3633) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3635) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3636) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3637) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3656)
    (<0.0>,3658)
    (<0.0>,3660) fake_sched.h:43: return __running_cpu;
    (<0.0>,3664) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3667) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,3671) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3672) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3673) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3677) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3678) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3679) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3681) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3686) fake_sched.h:43: return __running_cpu;
    (<0.0>,3689) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3691) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3693) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3694) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3697)
    (<0.0>,3700) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3703) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3704) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3705) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3709) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3710) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3711) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3713) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3718) fake_sched.h:43: return __running_cpu;
    (<0.0>,3721) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3723) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3725) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3726) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3729)
    (<0.0>,3732) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3735) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3736) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3737) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3741) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3742) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3743) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3745) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3752) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3755) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3756) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3757) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3759) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3760) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3762) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3765) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3771) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3772) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3775) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3780) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3781) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3782) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3796)
    (<0.0>,3798) tree.c:583: local_irq_restore(flags);
    (<0.0>,3801)
    (<0.0>,3803) fake_sched.h:43: return __running_cpu;
    (<0.0>,3807) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3809) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3813) fake_sched.h:43: return __running_cpu;
    (<0.0>,3817) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3823) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3826) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3831) fake_sched.h:43: return __running_cpu;
    (<0.0>,3835)
    (<0.0>,3836) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3839) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3844) tree.c:704: local_irq_save(flags);
    (<0.0>,3847)
    (<0.0>,3849) fake_sched.h:43: return __running_cpu;
    (<0.0>,3853) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3855) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3859) fake_sched.h:43: return __running_cpu;
    (<0.0>,3863) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3876)
    (<0.0>,3878) fake_sched.h:43: return __running_cpu;
    (<0.0>,3882) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3883) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3885) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3886) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3887) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3892) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3893) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3897) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3898) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3899) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3900) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3904) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3906) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3907) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3908) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3922)
    (<0.0>,3923)
    (<0.0>,3925) fake_sched.h:43: return __running_cpu;
    (<0.0>,3929) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3933) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3936) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3937) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3938) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3940) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3941) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3943) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3946) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3953) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3954) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3957) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3962) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3963) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3964) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3969) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,3978)
    (<0.0>,3980) tree.c:707: local_irq_restore(flags);
    (<0.0>,3983)
    (<0.0>,3985) fake_sched.h:43: return __running_cpu;
    (<0.0>,3989) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3991) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3995) fake_sched.h:43: return __running_cpu;
    (<0.0>,3999) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4014) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4016) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4018) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4019) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4021) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4026) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,4030)
    (<0.0>,4031) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4033) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4037) tree.c:1674: return 1;
    (<0.0>,4039) tree.c:1675: }
    (<0.0>,4044) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,4045) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4046) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4047) tree.c:1819: if (j > HZ) {
    (<0.0>,4050) tree.c:1823: ret = 0;
    (<0.0>,4052) tree.c:1825: if (!ret)
    (<0.0>,4055) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4056) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4058) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4060) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4064) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4066) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4070) fake_sched.h:43: return __running_cpu;
    (<0.0>,4074) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4078) fake_sched.h:43: return __running_cpu;
    (<0.0>,4082) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4087) fake_sched.h:43: return __running_cpu;
    (<0.0>,4091) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4101) tree.c:749: local_irq_save(flags);
    (<0.0>,4104)
    (<0.0>,4106) fake_sched.h:43: return __running_cpu;
    (<0.0>,4110) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4112) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4117) fake_sched.h:43: return __running_cpu;
    (<0.0>,4121) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4122) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4124) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4125) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4126) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4128) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4130) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4131) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4133) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4138) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4139) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4141) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4145) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4146) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4147) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4148) tree.c:754: if (oldval)
    (<0.0>,4156)
    (<0.0>,4158) tree.c:759: local_irq_restore(flags);
    (<0.0>,4161)
    (<0.0>,4163) fake_sched.h:43: return __running_cpu;
    (<0.0>,4167) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4169) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4177)
    (<0.0>,4182) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,4187) fake_sched.h:43: return __running_cpu;
    (<0.0>,4192) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,4200) fake_sched.h:43: return __running_cpu;
    (<0.0>,4205) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,4219) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4220) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4221) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4225) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4226) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4227) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4229) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4233) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4235) fake_sched.h:43: return __running_cpu;
    (<0.0>,4238) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4240) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4247)
    (<0.0>,4248)
    (<0.0>,4249) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4251) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4252) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4253) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4255) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4257) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4258) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4259) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4269)
    (<0.0>,4270)
    (<0.0>,4271) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,4276) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,4279)
    (<0.0>,4282) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,4285) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4287) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4290) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4292) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4295) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4297) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4300) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4302) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4305) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4307) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4309) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4310) tree.c:3140: return 1;
    (<0.0>,4312) tree.c:3176: }
    (<0.0>,4316) tree.c:3189: return 1;
    (<0.0>,4318) tree.c:3191: }
    (<0.0>,4325) fake_sched.h:43: return __running_cpu;
    (<0.0>,4329) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,4333) tree.c:2437: if (user)
    (<0.0>,4341) fake_sched.h:43: return __running_cpu;
    (<0.0>,4345) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4347) fake_sched.h:43: return __running_cpu;
    (<0.0>,4351) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4357) fake_sched.h:43: return __running_cpu;
    (<0.0>,4361) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,4371)
    (<0.0>,4374) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4375) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4376) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4380) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4381) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4382) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4384) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4388) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,4397)
    (<0.0>,4399) fake_sched.h:43: return __running_cpu;
    (<0.0>,4402) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4404) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4406) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4407) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4409) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4416) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4417) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4419) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4425) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4426) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4427) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4428) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4429) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4433)
    (<0.0>,4434)
    (<0.0>,4435) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4436) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4443)
    (<0.0>,4444)
    (<0.0>,4445) tree.c:1584: local_irq_save(flags);
    (<0.0>,4448)
    (<0.0>,4450) fake_sched.h:43: return __running_cpu;
    (<0.0>,4454) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4456) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4460) fake_sched.h:43: return __running_cpu;
    (<0.0>,4464) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4469) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4471) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4472) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4473) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4475) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4476) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4478) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4481) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4483) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4484) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4486) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4489) tree.c:1589: local_irq_restore(flags);
    (<0.0>,4492)
    (<0.0>,4494) fake_sched.h:43: return __running_cpu;
    (<0.0>,4498) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4500) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4504) fake_sched.h:43: return __running_cpu;
    (<0.0>,4508) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4515) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4517) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4520) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,4522) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,4525) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4527) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4528) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4529) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4538)
    (<0.0>,4539)
    (<0.0>,4540)
    (<0.0>,4541) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4543) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4544) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4545) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,4547) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,4551)
    (<0.0>,4552)
    (<0.0>,4553) fake_sync.h:76: local_irq_save(flags);
    (<0.0>,4556)
    (<0.0>,4558) fake_sched.h:43: return __running_cpu;
    (<0.0>,4562) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4564) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4568) fake_sched.h:43: return __running_cpu;
    (<0.0>,4572) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4578) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,4579) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,4585) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4587) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4592) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4594) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4595) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4597) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4600) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4602) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4603) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4605) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4608) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4610) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4611) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4612) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4614) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4615) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4619) tree.c:2054: rdp->qs_pending = 0;
    (<0.0>,4621) tree.c:2054: rdp->qs_pending = 0;
    (<0.0>,4622) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4623) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4624) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4633)
    (<0.0>,4634)
    (<0.0>,4635)
    (<0.0>,4636) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4639) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4642) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4645) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4646) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4649) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4650) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4655)
    (<0.0>,4656)
    (<0.0>,4657) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4660)
    (<0.0>,4661) tree.c:453: return &rsp->node[0];
    (<0.0>,4665) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4668) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4670) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4671) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4673) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4676) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4678) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4680) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4682) tree.c:1261: }
    (<0.0>,4684) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4685) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4687) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4690) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4692) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4695) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4696) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4699) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4702) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4706) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4708) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4710) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4713) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4715) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4718) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4719) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4722) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4725) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4728) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4730) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4733) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4734) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4739) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4741) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4745) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4748) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4751) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4752) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4754) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4757) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4758) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4759) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4761) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4764) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4766) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4768) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4770) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4773) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4776) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4777) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4779) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4782) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4783) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4784) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4786) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4789) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,4791) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4793) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4795) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4798) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4799) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4808)
    (<0.0>,4809)
    (<0.0>,4810)
    (<0.0>,4811) tree.c:1289: bool ret = false;
    (<0.0>,4812) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4814) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4817)
    (<0.0>,4818) tree.c:453: return &rsp->node[0];
    (<0.0>,4822) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4823) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4825) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4826) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4831)
    (<0.0>,4832)
    (<0.0>,4833) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4836)
    (<0.0>,4837) tree.c:453: return &rsp->node[0];
    (<0.0>,4841) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4844) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4846) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4847) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4849) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4852) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4854) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4856) tree.c:1260: return rnp->completed + 2;
    (<0.0>,4858) tree.c:1261: }
    (<0.0>,4860) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4861) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4862) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4863) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4869)
    (<0.0>,4870)
    (<0.0>,4871)
    (<0.0>,4872)
    (<0.0>,4876) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4878) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4881) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4884) tree.c:1304: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,4885) tree.c:1304: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,4886) tree.c:1304: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,4892)
    (<0.0>,4893)
    (<0.0>,4894)
    (<0.0>,4895)
    (<0.0>,4900) tree.c:1372: if (c_out != NULL)
    (<0.0>,4903) tree.c:1374: return ret;
    (<0.0>,4907) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4908) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4911) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4912) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4918) tree.c:1482: return ret;
    (<0.0>,4920) tree.c:1482: return ret;
    (<0.0>,4922) tree.c:1483: }
    (<0.0>,4925) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4926) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,4927) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,4928) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,4929) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,4939)
    (<0.0>,4940)
    (<0.0>,4941)
    (<0.0>,4942)
    (<0.0>,4944) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,4946) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,4947) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,4951) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,4953) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,4955) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,4957) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,4960) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,4962) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,4965) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,4968)
    (<0.0>,4972) tree.c:1994: mask = rnp->grpmask;
    (<0.0>,4974) tree.c:1994: mask = rnp->grpmask;
    (<0.0>,4975) tree.c:1994: mask = rnp->grpmask;
    (<0.0>,4976) tree.c:1995: if (rnp->parent == NULL) {
    (<0.0>,4978) tree.c:1995: if (rnp->parent == NULL) {
    (<0.0>,4982) tree.c:2014: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
    (<0.0>,4983) tree.c:2014: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
    (<0.0>,4990)
    (<0.0>,4991)
    (<0.0>,4992) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,4995)
    (<0.0>,4996) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4998) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4999) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5001) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5010) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,5011) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,5014)
    (<0.0>,5015) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5017) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5018) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5020) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5027) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,5028) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,5029) tree.c:1950: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,5030) tree.c:1951: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,5033)
    (<0.0>,5034) tree.c:453: return &rsp->node[0];
    (<0.0>,5039) tree.c:1951: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,5043)
    (<0.0>,5044)
    (<0.0>,5045) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,5046) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,5049) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,5052)
    (<0.0>,5054) fake_sched.h:43: return __running_cpu;
    (<0.0>,5058) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5060) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5064) fake_sched.h:43: return __running_cpu;
    (<0.0>,5068) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5075) tree.c:1952: rcu_gp_kthread_wake(rsp);
    (<0.0>,5078)
    (<0.0>,5079) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,5080) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,5082) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,5090) tree.c:2063: if (needwake)
    (<0.0>,5097) tree.c:2558: local_irq_save(flags);
    (<0.0>,5100)
    (<0.0>,5102) fake_sched.h:43: return __running_cpu;
    (<0.0>,5106) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5108) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5112) fake_sched.h:43: return __running_cpu;
    (<0.0>,5116) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5121) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,5122) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,5128)
    (<0.0>,5129)
    (<0.0>,5130) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,5133)
    (<0.0>,5134) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5136) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5137) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5139) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5145) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,5147) tree.c:494: }
    (<0.0>,5151) tree.c:2566: local_irq_restore(flags);
    (<0.0>,5154)
    (<0.0>,5156) fake_sched.h:43: return __running_cpu;
    (<0.0>,5160) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5162) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5166) fake_sched.h:43: return __running_cpu;
    (<0.0>,5170) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5176) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,5179)
    (<0.0>,5180) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5182) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5185) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5192) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5195)
    (<0.0>,5199) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5202) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5203) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5204) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5208) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5209) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5210) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5212) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5216) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,5225)
    (<0.0>,5227) fake_sched.h:43: return __running_cpu;
    (<0.0>,5230) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5232) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5234) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5235) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5237) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5244) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5245) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5247) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5253) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5254) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5255) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,5256) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,5257) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,5261)
    (<0.0>,5262)
    (<0.0>,5263) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,5264) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,5271)
    (<0.0>,5272)
    (<0.0>,5273) tree.c:1584: local_irq_save(flags);
    (<0.0>,5276)
    (<0.0>,5278) fake_sched.h:43: return __running_cpu;
    (<0.0>,5282) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5284) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5288) fake_sched.h:43: return __running_cpu;
    (<0.0>,5292) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5297) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,5299) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,5300) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,5301) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,5303) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,5304) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,5306) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,5309) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,5311) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,5312) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,5314) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,5317) tree.c:1589: local_irq_restore(flags);
    (<0.0>,5320)
    (<0.0>,5322) fake_sched.h:43: return __running_cpu;
    (<0.0>,5326) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5328) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5332) fake_sched.h:43: return __running_cpu;
    (<0.0>,5336) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5343) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,5345) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,5350) tree.c:2558: local_irq_save(flags);
    (<0.0>,5353)
    (<0.0>,5355) fake_sched.h:43: return __running_cpu;
    (<0.0>,5359) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5361) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5365) fake_sched.h:43: return __running_cpu;
    (<0.0>,5369) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5374) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,5375) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,5381)
    (<0.0>,5382)
    (<0.0>,5383) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,5386)
    (<0.0>,5387) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5389) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5390) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5392) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,5398) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,5404)
    (<0.0>,5405) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5408)
    (<0.0>,5409) tree.c:453: return &rsp->node[0];
    (<0.0>,5413) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5414) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,5416) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,5420) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,5421) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,5423) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,5426) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,5427) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,5428) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,5432) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5435) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5438) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5441) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5442) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5445) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5447) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5450) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5453) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5456) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5457) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5459) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5462) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5466) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5468) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5470) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5473) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5476) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5479) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5480) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5482) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5485) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5489) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5491) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5493) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5496) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,5498) tree.c:494: }
    (<0.0>,5502) tree.c:2566: local_irq_restore(flags);
    (<0.0>,5505)
    (<0.0>,5507) fake_sched.h:43: return __running_cpu;
    (<0.0>,5511) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5513) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5517) fake_sched.h:43: return __running_cpu;
    (<0.0>,5521) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5527) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,5530)
    (<0.0>,5531) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5533) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5536) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5543) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5546)
    (<0.0>,5550) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5553) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5554) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5555) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5559) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5560) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5561) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5563) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5571) fake_sched.h:43: return __running_cpu;
    (<0.0>,5575) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,5584) tree.c:624: local_irq_save(flags);
    (<0.0>,5587)
    (<0.0>,5589) fake_sched.h:43: return __running_cpu;
    (<0.0>,5593) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5595) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5599) fake_sched.h:43: return __running_cpu;
    (<0.0>,5603) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5609) fake_sched.h:43: return __running_cpu;
    (<0.0>,5613) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5614) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5616) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5617) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5618) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5620) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5622) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5623) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5625) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5630) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5631) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5633) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5637) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5638) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5639) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5640) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,5642) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,5650)
    (<0.0>,5652) tree.c:634: local_irq_restore(flags);
    (<0.0>,5655)
    (<0.0>,5657) fake_sched.h:43: return __running_cpu;
    (<0.0>,5661) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5663) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5667) fake_sched.h:43: return __running_cpu;
    (<0.0>,5671) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5680) fake_sched.h:43: return __running_cpu;
    (<0.0>,5684)
    (<0.0>,5687) tree.c:580: local_irq_save(flags);
    (<0.0>,5690)
    (<0.0>,5692) fake_sched.h:43: return __running_cpu;
    (<0.0>,5696) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5698) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5702) fake_sched.h:43: return __running_cpu;
    (<0.0>,5706) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5719)
    (<0.0>,5721) fake_sched.h:43: return __running_cpu;
    (<0.0>,5725) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5726) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5728) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5729) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5730) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5736) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5737) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5742) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5743) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5744) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5745) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5749) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5751) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5752) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5753) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5772)
    (<0.0>,5774)
    (<0.0>,5776) fake_sched.h:43: return __running_cpu;
    (<0.0>,5780) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5783) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,5787) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5788) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5789) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5793) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5794) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5795) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5797) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5802) fake_sched.h:43: return __running_cpu;
    (<0.0>,5805) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5807) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5809) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5810) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5813)
    (<0.0>,5816) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5819) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5820) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5821) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5825) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5826) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5827) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5829) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5834) fake_sched.h:43: return __running_cpu;
    (<0.0>,5837) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5839) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5841) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5842) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5845)
    (<0.0>,5848) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5851) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5852) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5853) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5857) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5858) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5859) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5861) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5868) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5871) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5872) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5873) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5875) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5876) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5878) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5881) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5887) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5888) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5891) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5896) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5897) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5898) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5912)
    (<0.0>,5914) tree.c:583: local_irq_restore(flags);
    (<0.0>,5917)
    (<0.0>,5919) fake_sched.h:43: return __running_cpu;
    (<0.0>,5923) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5925) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5929) fake_sched.h:43: return __running_cpu;
    (<0.0>,5933) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5939) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5942) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5947) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5949) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5951) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5955) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5957) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5960) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5963)
    (<0.0>,5972) fake_sched.h:43: return __running_cpu;
    (<0.0>,5976)
    (<0.0>,5977) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5980) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5985) tree.c:704: local_irq_save(flags);
    (<0.0>,5988)
    (<0.0>,5990) fake_sched.h:43: return __running_cpu;
    (<0.0>,5994) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5996) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6000) fake_sched.h:43: return __running_cpu;
    (<0.0>,6004) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6017)
    (<0.0>,6019) fake_sched.h:43: return __running_cpu;
    (<0.0>,6023) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6024) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6026) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6027) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6028) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6033) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6034) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6038) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6039) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6040) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6041) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,6045) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6047) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6048) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6049) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6063)
    (<0.0>,6064)
    (<0.0>,6066) fake_sched.h:43: return __running_cpu;
    (<0.0>,6070) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6074) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6077) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6078) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6079) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6081) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6082) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6084) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6087) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6094) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6095) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6098) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6103) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6104) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6105) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6110) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,6119)
    (<0.0>,6121) tree.c:707: local_irq_restore(flags);
    (<0.0>,6124)
    (<0.0>,6126) fake_sched.h:43: return __running_cpu;
    (<0.0>,6130) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6132) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6136) fake_sched.h:43: return __running_cpu;
    (<0.0>,6140) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6147) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,6148) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,6149) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,6150) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,6152) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,6155) tree.c:1840: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,6158)
    (<0.0>,6163) tree.c:1872: rcu_gp_cleanup(rsp);
    (<0.0>,6171)
    (<0.0>,6172) tree.c:1720: bool needgp = false;
    (<0.0>,6173) tree.c:1721: int nocb = 0;
    (<0.0>,6174) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6177)
    (<0.0>,6178) tree.c:453: return &rsp->node[0];
    (<0.0>,6182) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6183) tree.c:1725: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,6187)
    (<0.0>,6190) fake_sched.h:43: return __running_cpu;
    (<0.0>,6194) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6198) fake_sched.h:43: return __running_cpu;
    (<0.0>,6202) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6207) fake_sched.h:43: return __running_cpu;
    (<0.0>,6211) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,6214) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6215) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6221) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,6222) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,6224) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,6226) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,6227) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,6228) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,6230) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,6233) tree.c:1739: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,6237)
    (<0.0>,6238) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6239) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6244) fake_sched.h:43: return __running_cpu;
    (<0.0>,6248) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6250) fake_sched.h:43: return __running_cpu;
    (<0.0>,6254) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6260) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6263) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6265) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6266) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6268) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6273) tree.c:1751: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,6277)
    (<0.0>,6280) fake_sched.h:43: return __running_cpu;
    (<0.0>,6284) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6288) fake_sched.h:43: return __running_cpu;
    (<0.0>,6292) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6297) fake_sched.h:43: return __running_cpu;
    (<0.0>,6301) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,6304) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6305) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6311) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,6313) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,6314) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,6316) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,6318) fake_sched.h:43: return __running_cpu;
    (<0.0>,6321) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6323) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6325) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6326) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,6327) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,6329) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,6332) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6333) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6334) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6340)
    (<0.0>,6341)
    (<0.0>,6342)
    (<0.0>,6343) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,6345) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,6346) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,6348) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,6351) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,6352) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,6353) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,6361)
    (<0.0>,6362)
    (<0.0>,6363)
    (<0.0>,6364) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6367) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6370) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6373) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6374) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6377) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6379) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6382) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6384) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6385) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6387) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6390) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6394) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6396) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6399) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6400) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6403) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,6405) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6407) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6409) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,6412) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6414) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6415) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6417) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6420) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,6425) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6427) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6428) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6431) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6434) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6435) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6437) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6440) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6442) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6444) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6446) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6447) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6450) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,6452) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,6455) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6457) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6460) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6461) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6464) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6468) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6469) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6470) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6479)
    (<0.0>,6480)
    (<0.0>,6481)
    (<0.0>,6482) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6485) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6488) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6491) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6492) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6495) tree.c:1434: return false;
    (<0.0>,6497) tree.c:1483: }
    (<0.0>,6499) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6501) tree.c:1527: }
    (<0.0>,6504) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,6505) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6507) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6508) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6510) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6514) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6516) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6517) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6519) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6522) tree.c:1575: return ret;
    (<0.0>,6526) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6530) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6532) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6533) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6540)
    (<0.0>,6541)
    (<0.0>,6542) tree.c:1385: int c = rnp->completed;
    (<0.0>,6544) tree.c:1385: int c = rnp->completed;
    (<0.0>,6546) tree.c:1385: int c = rnp->completed;
    (<0.0>,6548) fake_sched.h:43: return __running_cpu;
    (<0.0>,6551) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6553) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6555) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6556) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,6557) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,6561)
    (<0.0>,6562)
    (<0.0>,6564) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6567) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6570) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6571) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6575) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6578) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6579) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6580) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6581) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6582) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6584) tree.c:1393: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,6592)
    (<0.0>,6593)
    (<0.0>,6594)
    (<0.0>,6595)
    (<0.0>,6599) tree.c:1394: return needmore;
    (<0.0>,6601) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6603) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6604) tree.c:1759: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,6608)
    (<0.0>,6609) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6610) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6615) fake_sched.h:43: return __running_cpu;
    (<0.0>,6619) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6621) fake_sched.h:43: return __running_cpu;
    (<0.0>,6625) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6638) fake_sched.h:43: return __running_cpu;
    (<0.0>,6643) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,6650) fake_sched.h:43: return __running_cpu;
    (<0.0>,6654) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,6666) fake_sched.h:43: return __running_cpu;
    (<0.0>,6670)
    (<0.0>,6673) tree.c:580: local_irq_save(flags);
    (<0.0>,6676)
    (<0.0>,6678) fake_sched.h:43: return __running_cpu;
    (<0.0>,6682) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6684) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6688) fake_sched.h:43: return __running_cpu;
    (<0.0>,6692) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6705)
    (<0.0>,6707) fake_sched.h:43: return __running_cpu;
    (<0.0>,6711) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6712) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6714) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6715) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6716) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6722) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6723) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6728) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6729) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6730) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6731) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,6735) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,6737) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,6738) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6739) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6758)
    (<0.0>,6760)
    (<0.0>,6762) fake_sched.h:43: return __running_cpu;
    (<0.0>,6766) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6769) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,6773) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6774) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6775) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6779) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6780) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6781) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6783) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6788) fake_sched.h:43: return __running_cpu;
    (<0.0>,6791) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6793) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6795) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6796) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6799)
    (<0.0>,6802) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6805) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6806) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6807) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6811) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6812) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6813) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6815) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6820) fake_sched.h:43: return __running_cpu;
    (<0.0>,6823) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6825) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6827) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6828) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6831)
    (<0.0>,6834) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6837) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6838) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6839) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6843) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6844) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6845) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6847) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6854) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6857) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6858) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6859) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6861) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6862) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6864) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6867) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6873) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6874) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6877) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6882) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6883) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6884) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6898)
    (<0.0>,6900) tree.c:583: local_irq_restore(flags);
    (<0.0>,6903)
    (<0.0>,6905) fake_sched.h:43: return __running_cpu;
    (<0.0>,6909) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6911) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6915) fake_sched.h:43: return __running_cpu;
    (<0.0>,6919) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6925) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6928) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6933) fake_sched.h:43: return __running_cpu;
    (<0.0>,6937)
    (<0.0>,6938) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6941) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6946) tree.c:704: local_irq_save(flags);
    (<0.0>,6949)
    (<0.0>,6951) fake_sched.h:43: return __running_cpu;
    (<0.0>,6955) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6957) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6961) fake_sched.h:43: return __running_cpu;
    (<0.0>,6965) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6978)
    (<0.0>,6980) fake_sched.h:43: return __running_cpu;
    (<0.0>,6984) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6985) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6987) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6988) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6989) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6994) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6995) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6999) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,7000) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,7001) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,7002) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,7006) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,7008) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,7009) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,7010) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,7024)
    (<0.0>,7025)
    (<0.0>,7027) fake_sched.h:43: return __running_cpu;
    (<0.0>,7031) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,7035) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7038) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7039) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7040) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7042) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7043) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,7045) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7048) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7055) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7056) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7059) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7064) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7065) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7066) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,7071) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,7080)
    (<0.0>,7082) tree.c:707: local_irq_restore(flags);
    (<0.0>,7085)
    (<0.0>,7087) fake_sched.h:43: return __running_cpu;
    (<0.0>,7091) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7093) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7097) fake_sched.h:43: return __running_cpu;
    (<0.0>,7101) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7116) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7118) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7120) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7121) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7123) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7128) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,7131)
    (<0.0>,7132) tree.c:453: return &rsp->node[0];
    (<0.0>,7136) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,7137) tree.c:1763: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,7141)
    (<0.0>,7144) fake_sched.h:43: return __running_cpu;
    (<0.0>,7148) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,7152) fake_sched.h:43: return __running_cpu;
    (<0.0>,7156) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7161) fake_sched.h:43: return __running_cpu;
    (<0.0>,7165) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,7168) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,7169) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,7175) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,7176) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,7180)
    (<0.0>,7181)
    (<0.0>,7183) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,7185) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,7186) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,7188) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,7191) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,7193) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,7195) fake_sched.h:43: return __running_cpu;
    (<0.0>,7198) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,7200) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,7202) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,7203) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,7204) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,7205) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,7213)
    (<0.0>,7214)
    (<0.0>,7215)
    (<0.0>,7216) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7219) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7222) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7225) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7226) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7229) tree.c:1502: return false;
    (<0.0>,7231) tree.c:1527: }
    (<0.0>,7234) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,7238) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,7239) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7242) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7243) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7249)
    (<0.0>,7250)
    (<0.0>,7251) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7254)
    (<0.0>,7255) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7257) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7258) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7260) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7266) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7272)
    (<0.0>,7273) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7276)
    (<0.0>,7277) tree.c:453: return &rsp->node[0];
    (<0.0>,7281) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7282) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7284) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7288) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7289) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7291) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7294) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7295) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7296) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7300) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7302) tree.c:494: }
    (<0.0>,7306) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7308) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7312) tree.c:1780: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,7316)
    (<0.0>,7317) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,7318) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,7323) fake_sched.h:43: return __running_cpu;
    (<0.0>,7327) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,7329) fake_sched.h:43: return __running_cpu;
    (<0.0>,7333) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7344) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,7346) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,7350) fake_sched.h:43: return __running_cpu;
    (<0.0>,7354) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,7358) fake_sched.h:43: return __running_cpu;
    (<0.0>,7362) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7367) fake_sched.h:43: return __running_cpu;
    (<0.0>,7371) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,7381) tree.c:749: local_irq_save(flags);
    (<0.0>,7384)
    (<0.0>,7386) fake_sched.h:43: return __running_cpu;
    (<0.0>,7390) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7392) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7397) fake_sched.h:43: return __running_cpu;
    (<0.0>,7401) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,7402) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,7404) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,7405) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,7406) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,7408) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,7410) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,7411) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7413) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7418) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7419) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7421) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7425) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7426) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7427) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7428) tree.c:754: if (oldval)
    (<0.0>,7436)
    (<0.0>,7438) tree.c:759: local_irq_restore(flags);
    (<0.0>,7441)
    (<0.0>,7443) fake_sched.h:43: return __running_cpu;
    (<0.0>,7447) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7449) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7457)
    (<0.0>,7462) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,7467) fake_sched.h:43: return __running_cpu;
    (<0.0>,7472) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,7480) fake_sched.h:43: return __running_cpu;
    (<0.0>,7485) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,7499) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7500) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7501) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7505) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7506) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7507) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7509) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7513) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7515) fake_sched.h:43: return __running_cpu;
    (<0.0>,7518) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7520) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7527)
    (<0.0>,7528)
    (<0.0>,7529) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7531) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7532) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7533) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7535) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7537) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7538) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,7539) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,7549)
    (<0.0>,7550)
    (<0.0>,7551) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,7556) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,7559)
    (<0.0>,7562) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,7565) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7567) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7570) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7572) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7576) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,7579)
    (<0.0>,7580) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7582) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7585) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7588) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7591) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7598) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,7600) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,7602) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,7603) tree.c:3146: return 1;
    (<0.0>,7605) tree.c:3176: }
    (<0.0>,7609) tree.c:3189: return 1;
    (<0.0>,7611) tree.c:3191: }
    (<0.0>,7618) fake_sched.h:43: return __running_cpu;
    (<0.0>,7622) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,7626) tree.c:2437: if (user)
    (<0.0>,7634) fake_sched.h:43: return __running_cpu;
    (<0.0>,7638) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,7640) fake_sched.h:43: return __running_cpu;
    (<0.0>,7644) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7650) fake_sched.h:43: return __running_cpu;
    (<0.0>,7654) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,7664)
    (<0.0>,7667) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7668) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7669) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7673) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7674) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7675) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7677) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7681) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,7690)
    (<0.0>,7692) fake_sched.h:43: return __running_cpu;
    (<0.0>,7695) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7697) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7699) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7700) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7702) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7709) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7710) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7712) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7718) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7719) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7720) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7721) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7722) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7726)
    (<0.0>,7727)
    (<0.0>,7728) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7729) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7736)
    (<0.0>,7737)
    (<0.0>,7738) tree.c:1584: local_irq_save(flags);
    (<0.0>,7741)
    (<0.0>,7743) fake_sched.h:43: return __running_cpu;
    (<0.0>,7747) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7749) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7753) fake_sched.h:43: return __running_cpu;
    (<0.0>,7757) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7762) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7764) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7765) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7766) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7768) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7769) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7771) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7774) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7776) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7777) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7779) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7782) tree.c:1589: local_irq_restore(flags);
    (<0.0>,7785)
    (<0.0>,7787) fake_sched.h:43: return __running_cpu;
    (<0.0>,7791) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7793) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7797) fake_sched.h:43: return __running_cpu;
    (<0.0>,7801) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7808) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7810) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7815) tree.c:2558: local_irq_save(flags);
    (<0.0>,7818)
    (<0.0>,7820) fake_sched.h:43: return __running_cpu;
    (<0.0>,7824) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7826) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7830) fake_sched.h:43: return __running_cpu;
    (<0.0>,7834) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7839) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7840) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7846)
    (<0.0>,7847)
    (<0.0>,7848) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7851)
    (<0.0>,7852) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7854) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7855) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7857) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7863) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7869)
    (<0.0>,7870) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7873)
    (<0.0>,7874) tree.c:453: return &rsp->node[0];
    (<0.0>,7878) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7879) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7881) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7885) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7886) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7888) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7891) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7892) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7893) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7897) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7899) tree.c:494: }
    (<0.0>,7903) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,7906)
    (<0.0>,7907) tree.c:453: return &rsp->node[0];
    (<0.0>,7914)
    (<0.0>,7916) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7917) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7921) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,7927)
    (<0.0>,7929) fake_sched.h:43: return __running_cpu;
    (<0.0>,7932) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7934) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7936) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7937) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7940)
    (<0.0>,7941) tree.c:453: return &rsp->node[0];
    (<0.0>,7945) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7946) tree.c:1925: bool ret = false;
    (<0.0>,7947) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7948) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7949) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7957)
    (<0.0>,7958)
    (<0.0>,7959)
    (<0.0>,7960) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7963) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7966) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7969) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7970) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7973) tree.c:1502: return false;
    (<0.0>,7975) tree.c:1527: }
    (<0.0>,7978) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7982) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7983) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7984) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7985) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7991)
    (<0.0>,7992)
    (<0.0>,7993)
    (<0.0>,7994) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7996) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7999) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8000) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8006)
    (<0.0>,8007)
    (<0.0>,8008) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,8011)
    (<0.0>,8012) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8014) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8015) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8017) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8023) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,8029)
    (<0.0>,8030) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8033)
    (<0.0>,8034) tree.c:453: return &rsp->node[0];
    (<0.0>,8038) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8039) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8041) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8045) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8046) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8048) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8051) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8052) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8053) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8057) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,8059) tree.c:494: }
    (<0.0>,8063) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,8065) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,8068) tree.c:1909: return true;
    (<0.0>,8070) tree.c:1910: }
    (<0.0>,8074) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,8075) tree.c:1937: return ret;
    (<0.0>,8079) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,8080) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,8083)
    (<0.0>,8084) tree.c:453: return &rsp->node[0];
    (<0.0>,8089) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,8093)
    (<0.0>,8094)
    (<0.0>,8095) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,8096) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,8099) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,8102)
    (<0.0>,8104) fake_sched.h:43: return __running_cpu;
    (<0.0>,8108) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8110) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8114) fake_sched.h:43: return __running_cpu;
    (<0.0>,8118) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8125) tree.c:2563: if (needwake)
    (<0.0>,8128) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,8131)
    (<0.0>,8132) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,8133) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,8135) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,8142) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,8145)
    (<0.0>,8146) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8148) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8151) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8154) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,8157) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,8164) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,8165) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,8169)
    (<0.0>,8170)
    (<0.0>,8171) tree.c:2601: if (unlikely(!ACCESS_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,8180) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,8182) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,8191) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,8192) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,8209)
    (<0.0>,8210)
    (<0.0>,8211) tree.c:2313: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,8214)
    (<0.0>,8215) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8217) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8220) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8223) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,8226) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,8233) tree.c:2325: local_irq_save(flags);
    (<0.0>,8236)
    (<0.0>,8238) fake_sched.h:43: return __running_cpu;
    (<0.0>,8242) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8244) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8248) fake_sched.h:43: return __running_cpu;
    (<0.0>,8252) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8257) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8258) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8259) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8260) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,8261) tree.c:2327: bl = rdp->blimit;
    (<0.0>,8263) tree.c:2327: bl = rdp->blimit;
    (<0.0>,8264) tree.c:2327: bl = rdp->blimit;
    (<0.0>,8267) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,8269) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,8270) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,8271) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8274) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8275) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8276) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8278) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8279) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,8282) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,8283) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,8284) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8287) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8288) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8289) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8291) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8294) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8296) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8299) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8300) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8303) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8306) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8308) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8310) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8313) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8316) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8318) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8320) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8323) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8325) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8328) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8329) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8332) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8335) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8337) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8339) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8342) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8345) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8347) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8349) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8352) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8354) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8357) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8358) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8361) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8364) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8366) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8368) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8371) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8374) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8376) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8378) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8381) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8383) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8386) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8387) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8390) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8393) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8395) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8397) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8400) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8403) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8405) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8407) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8410) tree.c:2336: local_irq_restore(flags);
    (<0.0>,8413)
    (<0.0>,8415) fake_sched.h:43: return __running_cpu;
    (<0.0>,8419) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8421) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8425) fake_sched.h:43: return __running_cpu;
    (<0.0>,8429) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8434) tree.c:2339: count = count_lazy = 0;
    (<0.0>,8435) tree.c:2339: count = count_lazy = 0;
    (<0.0>,8437) tree.c:2340: while (list) {
    (<0.0>,8440) tree.c:2341: next = list->next;
    (<0.0>,8442) tree.c:2341: next = list->next;
    (<0.0>,8443) tree.c:2341: next = list->next;
    (<0.0>,8446) tree.c:2343: debug_rcu_head_unqueue(list);
    (<0.0>,8449)
    (<0.0>,8451) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8453) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8454) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8460)
    (<0.0>,8461)
    (<0.0>,8462) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8464) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8466) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8469) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,8472) rcu.h:118: head->func(head);
    (<0.0>,8474) rcu.h:118: head->func(head);
    (<0.0>,8475) rcu.h:118: head->func(head);
    (<0.0>,8481)
    (<0.0>,8482) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8483) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8484) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8488) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8489) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8490) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8491) update.c:216: complete(&rcu->completion);
    (<0.0>,8495)
    (<0.0>,8496) fake_sync.h:248: x->done++;
    (<0.0>,8498) fake_sync.h:248: x->done++;
    (<0.0>,8500) fake_sync.h:248: x->done++;
    (<0.0>,8505) rcu.h:120: return false;
    (<0.0>,8507) rcu.h:122: }
    (<0.0>,8510) tree.c:2346: list = next;
    (<0.0>,8511) tree.c:2346: list = next;
    (<0.0>,8512) tree.c:2348: if (++count >= bl &&
    (<0.0>,8514) tree.c:2348: if (++count >= bl &&
    (<0.0>,8515) tree.c:2348: if (++count >= bl &&
    (<0.0>,8519) tree.c:2340: while (list) {
    (<0.0>,8522) tree.c:2354: local_irq_save(flags);
    (<0.0>,8525)
    (<0.0>,8527) fake_sched.h:43: return __running_cpu;
    (<0.0>,8531) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8533) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8537) fake_sched.h:43: return __running_cpu;
    (<0.0>,8541) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8548) tree.c:2360: if (list != NULL) {
    (<0.0>,8552) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8553) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8555) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8557) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8558) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8560) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8561) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8563) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8565) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8566) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8567) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8569) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8571) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8572) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,8574) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,8577) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8579) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8582) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8584) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8587) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8589) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8590) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8592) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8593) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8598) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8600) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8603) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8605) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8612) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8613) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8615) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8618) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8620) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8626) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8627) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8628) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8629) tree.c:2386: local_irq_restore(flags);
    (<0.0>,8632)
    (<0.0>,8634) fake_sched.h:43: return __running_cpu;
    (<0.0>,8638) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8640) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8644) fake_sched.h:43: return __running_cpu;
    (<0.0>,8648) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8653) tree.c:2389: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,8656)
    (<0.0>,8657) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8659) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8662) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8673) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8676)
    (<0.0>,8680) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8683) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8684) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8685) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8689) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8690) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8691) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8693) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8697) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,8706)
    (<0.0>,8708) fake_sched.h:43: return __running_cpu;
    (<0.0>,8711) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8713) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8715) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8716) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8718) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8725) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8726) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8728) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8734) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8735) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8736) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8737) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,8738) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,8742)
    (<0.0>,8743)
    (<0.0>,8744) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,8745) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,8752)
    (<0.0>,8753)
    (<0.0>,8754) tree.c:1584: local_irq_save(flags);
    (<0.0>,8757)
    (<0.0>,8759) fake_sched.h:43: return __running_cpu;
    (<0.0>,8763) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8765) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8769) fake_sched.h:43: return __running_cpu;
    (<0.0>,8773) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8778) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8780) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8781) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8782) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8784) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8785) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8787) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8790) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8792) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8793) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8795) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8798) tree.c:1589: local_irq_restore(flags);
    (<0.0>,8801)
    (<0.0>,8803) fake_sched.h:43: return __running_cpu;
    (<0.0>,8807) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8809) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8813) fake_sched.h:43: return __running_cpu;
    (<0.0>,8817) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8824) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,8826) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,8831) tree.c:2558: local_irq_save(flags);
    (<0.0>,8834)
    (<0.0>,8836) fake_sched.h:43: return __running_cpu;
    (<0.0>,8840) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8842) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8846) fake_sched.h:43: return __running_cpu;
    (<0.0>,8850) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8855) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8856) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8862)
    (<0.0>,8863)
    (<0.0>,8864) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,8867)
    (<0.0>,8868) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8870) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8871) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8873) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8879) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,8885)
    (<0.0>,8886) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8889)
    (<0.0>,8890) tree.c:453: return &rsp->node[0];
    (<0.0>,8894) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8895) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8897) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8901) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8902) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8904) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8907) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8908) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8909) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8913) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8916) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8919) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8922) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8923) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8926) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8928) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8931) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8934) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8937) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8938) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8940) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8943) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8947) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8949) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8951) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8954) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8957) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8960) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8961) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8963) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8966) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8970) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8972) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8974) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8977) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,8979) tree.c:494: }
    (<0.0>,8983) tree.c:2566: local_irq_restore(flags);
    (<0.0>,8986)
    (<0.0>,8988) fake_sched.h:43: return __running_cpu;
    (<0.0>,8992) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8994) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8998) fake_sched.h:43: return __running_cpu;
    (<0.0>,9002) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9008) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,9011)
    (<0.0>,9012) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9014) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9017) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9024) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9027)
    (<0.0>,9031) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9034) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9035) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9036) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9040) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9041) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9042) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9044) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,9052) fake_sched.h:43: return __running_cpu;
    (<0.0>,9056) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,9065) tree.c:624: local_irq_save(flags);
    (<0.0>,9068)
    (<0.0>,9070) fake_sched.h:43: return __running_cpu;
    (<0.0>,9074) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9076) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9080) fake_sched.h:43: return __running_cpu;
    (<0.0>,9084) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9090) fake_sched.h:43: return __running_cpu;
    (<0.0>,9094) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9095) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,9097) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,9098) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,9099) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,9101) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,9103) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,9104) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9106) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9111) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9112) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9114) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9118) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9119) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9120) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,9121) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,9123) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,9131)
    (<0.0>,9133) tree.c:634: local_irq_restore(flags);
    (<0.0>,9136)
    (<0.0>,9138) fake_sched.h:43: return __running_cpu;
    (<0.0>,9142) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9144) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9148) fake_sched.h:43: return __running_cpu;
    (<0.0>,9152) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9161) fake_sched.h:43: return __running_cpu;
    (<0.0>,9165)
    (<0.0>,9168) tree.c:580: local_irq_save(flags);
    (<0.0>,9171)
    (<0.0>,9173) fake_sched.h:43: return __running_cpu;
    (<0.0>,9177) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9179) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9183) fake_sched.h:43: return __running_cpu;
    (<0.0>,9187) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9200)
    (<0.0>,9202) fake_sched.h:43: return __running_cpu;
    (<0.0>,9206) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9207) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9209) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9210) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9211) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9217) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9218) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9223) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9224) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9225) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9226) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,9230) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9232) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9233) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9234) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9253)
    (<0.0>,9255)
    (<0.0>,9257) fake_sched.h:43: return __running_cpu;
    (<0.0>,9261) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9264) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,9268) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9269) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9270) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9274) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9275) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9276) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9278) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9283) fake_sched.h:43: return __running_cpu;
    (<0.0>,9286) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9288) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9290) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9291) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9294)
    (<0.0>,9297) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9300) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9301) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9302) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9306) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9307) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9308) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9310) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9315) fake_sched.h:43: return __running_cpu;
    (<0.0>,9318) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9320) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9322) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9323) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9326)
    (<0.0>,9329) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9332) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9333) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9334) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9338) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9339) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9340) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9342) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9349) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9352) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9353) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9354) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9356) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9357) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9359) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9362) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9368) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9369) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9372) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9377) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9378) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9379) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9393)
    (<0.0>,9395) tree.c:583: local_irq_restore(flags);
    (<0.0>,9398)
    (<0.0>,9400) fake_sched.h:43: return __running_cpu;
    (<0.0>,9404) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9406) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9410) fake_sched.h:43: return __running_cpu;
    (<0.0>,9414) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9420) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9423) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9428) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,9430) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,9439) fake_sched.h:43: return __running_cpu;
    (<0.0>,9443)
    (<0.0>,9444) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9447) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9452) tree.c:704: local_irq_save(flags);
    (<0.0>,9455)
    (<0.0>,9457) fake_sched.h:43: return __running_cpu;
    (<0.0>,9461) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9463) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9467) fake_sched.h:43: return __running_cpu;
    (<0.0>,9471) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9484)
    (<0.0>,9486) fake_sched.h:43: return __running_cpu;
    (<0.0>,9490) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9491) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9493) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9494) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9495) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9500) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9501) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9505) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9506) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9507) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9508) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,9512) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9514) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9515) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9516) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9530)
    (<0.0>,9531)
    (<0.0>,9533) fake_sched.h:43: return __running_cpu;
    (<0.0>,9537) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9541) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9544) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9545) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9546) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9548) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9549) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9551) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9554) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9561) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9562) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9565) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9570) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9571) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9572) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9577) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,9586)
    (<0.0>,9588) tree.c:707: local_irq_restore(flags);
    (<0.0>,9591)
    (<0.0>,9593) fake_sched.h:43: return __running_cpu;
    (<0.0>,9597) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9599) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9603) fake_sched.h:43: return __running_cpu;
    (<0.0>,9607) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9614) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,9626)
    (<0.0>,9627) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9630)
    (<0.0>,9631) tree.c:453: return &rsp->node[0];
    (<0.0>,9635) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9639) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,9643)
    (<0.0>,9646) fake_sched.h:43: return __running_cpu;
    (<0.0>,9650) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9654) fake_sched.h:43: return __running_cpu;
    (<0.0>,9658) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9663) fake_sched.h:43: return __running_cpu;
    (<0.0>,9667) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9670) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9671) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9677) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,9679) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,9682) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,9684) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,9685) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9688)
    (<0.0>,9689) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9691) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9692) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9694) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9702) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9703) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9706)
    (<0.0>,9707) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9709) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9710) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9712) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9719) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9720) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9721) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9724) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,9729)
    (<0.0>,9730) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,9731) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,9732) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9733) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9735) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9739) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,9740) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,9741) update.c:344: if (till_stall_check < 3) {
    (<0.0>,9744) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,9748) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,9753) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,9754) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9755) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9757) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9759) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9760) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9761) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9764) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9766) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9770) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9772) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9774) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9776) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9780) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,9784)
    (<0.0>,9785) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9786) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9791) fake_sched.h:43: return __running_cpu;
    (<0.0>,9795) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9797) fake_sched.h:43: return __running_cpu;
    (<0.0>,9801) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9807) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,9811)
    (<0.0>,9812) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,9814) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,9820) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9823) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9825) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9826) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9828) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9833) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,9837)
    (<0.0>,9840) fake_sched.h:43: return __running_cpu;
    (<0.0>,9844) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9848) fake_sched.h:43: return __running_cpu;
    (<0.0>,9852) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9857) fake_sched.h:43: return __running_cpu;
    (<0.0>,9861) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9864) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9865) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9872) fake_sched.h:43: return __running_cpu;
    (<0.0>,9875) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9877) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9879) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9880) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,9886)
    (<0.0>,9887) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9889) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9894) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9895) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9897) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9901) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9902) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9903) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9905) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9907) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9908) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9910) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9911) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9913) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9914) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9916) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9917) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9919) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9920) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9922) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9927) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9928) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9930) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9931) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9933) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9937) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9938) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9939) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9940) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9942) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9943) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9945) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9946) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9947) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9949) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9952) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9953) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9954) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9960)
    (<0.0>,9961)
    (<0.0>,9962)
    (<0.0>,9963) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9965) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9966) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9968) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9971) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9972) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9973) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9982)
    (<0.0>,9983)
    (<0.0>,9984)
    (<0.0>,9985) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9988) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9991) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9994) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9995) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9998) tree.c:1434: return false;
    (<0.0>,10000) tree.c:1483: }
    (<0.0>,10003) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,10005) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,10007) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,10008) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,10010) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,10013) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,10015) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,10016) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,10018) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,10021) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,10023) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,10024) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10026) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10027) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10029) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10034) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10037) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,10038) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,10041)
    (<0.0>,10044) tree.c:1575: return ret;
    (<0.0>,10048) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,10051)
    (<0.0>,10055) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,10059)
    (<0.0>,10060) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,10061) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,10066) fake_sched.h:43: return __running_cpu;
    (<0.0>,10070) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,10072) fake_sched.h:43: return __running_cpu;
    (<0.0>,10076) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10089) fake_sched.h:43: return __running_cpu;
    (<0.0>,10094) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,10100) fake_sched.h:43: return __running_cpu;
    (<0.0>,10105) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,10111) fake_sched.h:43: return __running_cpu;
    (<0.0>,10115) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,10127) fake_sched.h:43: return __running_cpu;
    (<0.0>,10131)
    (<0.0>,10134) tree.c:580: local_irq_save(flags);
    (<0.0>,10137)
    (<0.0>,10139) fake_sched.h:43: return __running_cpu;
    (<0.0>,10143) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10145) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10149) fake_sched.h:43: return __running_cpu;
    (<0.0>,10153) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10166)
    (<0.0>,10168) fake_sched.h:43: return __running_cpu;
    (<0.0>,10172) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10173) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10175) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10176) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10177) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10183) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10184) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10189) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10190) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10191) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10192) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,10196) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,10198) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,10199) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,10200) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,10219)
    (<0.0>,10221)
    (<0.0>,10223) fake_sched.h:43: return __running_cpu;
    (<0.0>,10227) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10230) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,10234) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10235) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10236) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10240) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10241) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10242) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10244) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10249) fake_sched.h:43: return __running_cpu;
    (<0.0>,10252) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10254) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10256) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10257) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10260)
    (<0.0>,10263) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10266) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10267) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10268) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10272) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10273) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10274) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10276) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10281) fake_sched.h:43: return __running_cpu;
    (<0.0>,10284) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10286) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10288) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10289) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10292)
    (<0.0>,10295) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10298) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10299) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10300) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10304) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10305) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10306) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10308) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10315) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10318) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10319) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10320) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10322) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10323) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10325) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10328) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10334) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10335) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10338) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10343) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10344) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10345) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10359)
    (<0.0>,10361) tree.c:583: local_irq_restore(flags);
    (<0.0>,10364)
    (<0.0>,10366) fake_sched.h:43: return __running_cpu;
    (<0.0>,10370) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10372) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10376) fake_sched.h:43: return __running_cpu;
    (<0.0>,10380) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10386) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10389) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10394) fake_sched.h:43: return __running_cpu;
    (<0.0>,10398)
    (<0.0>,10399) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10402) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10407) tree.c:704: local_irq_save(flags);
    (<0.0>,10410)
    (<0.0>,10412) fake_sched.h:43: return __running_cpu;
    (<0.0>,10416) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10418) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10422) fake_sched.h:43: return __running_cpu;
    (<0.0>,10426) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10439)
    (<0.0>,10441) fake_sched.h:43: return __running_cpu;
    (<0.0>,10445) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10446) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10448) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10449) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10450) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10455) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10456) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10460) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10461) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10462) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10463) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,10467) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10469) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10470) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10471) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10485)
    (<0.0>,10486)
    (<0.0>,10488) fake_sched.h:43: return __running_cpu;
    (<0.0>,10492) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10496) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10499) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10500) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10501) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10503) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10504) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10506) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10509) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10516) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10517) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10520) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10525) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10526) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10527) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10532) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,10541)
    (<0.0>,10543) tree.c:707: local_irq_restore(flags);
    (<0.0>,10546)
    (<0.0>,10548) fake_sched.h:43: return __running_cpu;
    (<0.0>,10552) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10554) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10558) fake_sched.h:43: return __running_cpu;
    (<0.0>,10562) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10577) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10579) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10581) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10582) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10584) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10589) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,10593)
    (<0.0>,10594) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,10596) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,10600) tree.c:1674: return 1;
    (<0.0>,10602) tree.c:1675: }
    (<0.0>,10607) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,10608) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,10609) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,10610) tree.c:1819: if (j > HZ) {
    (<0.0>,10613) tree.c:1823: ret = 0;
    (<0.0>,10615) tree.c:1825: if (!ret)
    (<0.0>,10618) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10619) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10621) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10623) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10627) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,10629) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,10633) fake_sched.h:43: return __running_cpu;
    (<0.0>,10637) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,10641) fake_sched.h:43: return __running_cpu;
    (<0.0>,10645) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10650) fake_sched.h:43: return __running_cpu;
    (<0.0>,10654) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,10664) tree.c:749: local_irq_save(flags);
    (<0.0>,10667)
    (<0.0>,10669) fake_sched.h:43: return __running_cpu;
    (<0.0>,10673) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10675) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10680) fake_sched.h:43: return __running_cpu;
    (<0.0>,10684) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10685) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10687) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10688) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10689) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10691) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10693) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10694) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10696) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10701) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10702) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10704) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10708) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10709) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10710) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10711) tree.c:754: if (oldval)
    (<0.0>,10719)
    (<0.0>,10721) tree.c:759: local_irq_restore(flags);
    (<0.0>,10724)
    (<0.0>,10726) fake_sched.h:43: return __running_cpu;
    (<0.0>,10730) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10732) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10740)
    (<0.0>,10745) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,10750) fake_sched.h:43: return __running_cpu;
    (<0.0>,10755) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,10763) fake_sched.h:43: return __running_cpu;
    (<0.0>,10768) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,10782) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10783) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10784) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10788) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10789) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10790) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10792) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10796) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10798) fake_sched.h:43: return __running_cpu;
    (<0.0>,10801) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10803) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10810)
    (<0.0>,10811)
    (<0.0>,10812) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10814) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10815) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10816) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10818) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10820) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10821) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10822) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10832)
    (<0.0>,10833)
    (<0.0>,10834) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,10839) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,10842)
    (<0.0>,10845) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,10848) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10850) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10853) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10855) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10858) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10860) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10863) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10865) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10868) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10870) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10872) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10873) tree.c:3140: return 1;
    (<0.0>,10875) tree.c:3176: }
    (<0.0>,10879) tree.c:3189: return 1;
    (<0.0>,10881) tree.c:3191: }
    (<0.0>,10888) fake_sched.h:43: return __running_cpu;
    (<0.0>,10892) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,10896) tree.c:2437: if (user)
    (<0.0>,10904) fake_sched.h:43: return __running_cpu;
    (<0.0>,10908) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,10910) fake_sched.h:43: return __running_cpu;
    (<0.0>,10914) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10920) fake_sched.h:43: return __running_cpu;
    (<0.0>,10924) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,10934)
    (<0.0>,10937) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10938) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10939) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10943) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10944) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10945) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10947) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10951) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,10960)
    (<0.0>,10962) fake_sched.h:43: return __running_cpu;
    (<0.0>,10965) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10967) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10969) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10970) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10972) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10979) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10980) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10982) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10988) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10989) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10990) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10991) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10992) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10996)
    (<0.0>,10997)
    (<0.0>,10998) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,10999) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,11006)
    (<0.0>,11007)
    (<0.0>,11008) tree.c:1584: local_irq_save(flags);
    (<0.0>,11011)
    (<0.0>,11013) fake_sched.h:43: return __running_cpu;
    (<0.0>,11017) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11019) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11023) fake_sched.h:43: return __running_cpu;
    (<0.0>,11027) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11032) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11034) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11035) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11036) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11038) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11039) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11041) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11044) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11046) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11047) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11049) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11052) tree.c:1589: local_irq_restore(flags);
    (<0.0>,11055)
    (<0.0>,11057) fake_sched.h:43: return __running_cpu;
    (<0.0>,11061) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11063) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11067) fake_sched.h:43: return __running_cpu;
    (<0.0>,11071) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11078) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11080) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11083) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,11085) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,11088) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,11090) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,11091) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,11092) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,11101)
    (<0.0>,11102)
    (<0.0>,11103)
    (<0.0>,11104) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,11106) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,11107) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,11108) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,11110) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,11114)
    (<0.0>,11115)
    (<0.0>,11116) fake_sync.h:76: local_irq_save(flags);
    (<0.0>,11119)
    (<0.0>,11121) fake_sched.h:43: return __running_cpu;
    (<0.0>,11125) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11127) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11131) fake_sched.h:43: return __running_cpu;
    (<0.0>,11135) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11141) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,11142) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,11148) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11150) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11155) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11157) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11158) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11160) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,11163) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,11165) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,11166) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,11168) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,11171) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,11173) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,11174) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,11175) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,11177) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,11178) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,11182) tree.c:2054: rdp->qs_pending = 0;
    (<0.0>,11184) tree.c:2054: rdp->qs_pending = 0;
    (<0.0>,11185) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,11186) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,11187) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,11196)
    (<0.0>,11197)
    (<0.0>,11198)
    (<0.0>,11199) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11202) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11205) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11208) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11209) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11212) tree.c:1434: return false;
    (<0.0>,11214) tree.c:1483: }
    (<0.0>,11217) tree.c:2060: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,11218) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,11219) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,11220) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,11221) tree.c:2062: rcu_report_qs_rnp(mask, rsp, rnp, flags); /* rlses rnp->lock */
    (<0.0>,11231)
    (<0.0>,11232)
    (<0.0>,11233)
    (<0.0>,11234)
    (<0.0>,11236) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,11238) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,11239) tree.c:1975: if (!(rnp->qsmask & mask)) {
    (<0.0>,11243) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,11245) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,11247) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,11249) tree.c:1981: rnp->qsmask &= ~mask;
    (<0.0>,11252) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,11254) tree.c:1987: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,11257) tree.c:1990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,11259) tree.c:1990: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,11263)
    (<0.0>,11264)
    (<0.0>,11265) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,11266) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,11269) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,11272)
    (<0.0>,11274) fake_sched.h:43: return __running_cpu;
    (<0.0>,11278) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11280) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11284) fake_sched.h:43: return __running_cpu;
    (<0.0>,11288) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11297) tree.c:2063: if (needwake)
    (<0.0>,11304) tree.c:2558: local_irq_save(flags);
    (<0.0>,11307)
    (<0.0>,11309) fake_sched.h:43: return __running_cpu;
    (<0.0>,11313) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11315) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11319) fake_sched.h:43: return __running_cpu;
    (<0.0>,11323) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11328) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11329) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11335)
    (<0.0>,11336)
    (<0.0>,11337) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,11340)
    (<0.0>,11341) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11343) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11344) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11346) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11352) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,11354) tree.c:494: }
    (<0.0>,11358) tree.c:2566: local_irq_restore(flags);
    (<0.0>,11361)
    (<0.0>,11363) fake_sched.h:43: return __running_cpu;
    (<0.0>,11367) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11369) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11373) fake_sched.h:43: return __running_cpu;
    (<0.0>,11377) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11383) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11386)
    (<0.0>,11387) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11389) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11392) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11399) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11402)
    (<0.0>,11406) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11409) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11410) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11411) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11415) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11416) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11417) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11419) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11423) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,11432)
    (<0.0>,11434) fake_sched.h:43: return __running_cpu;
    (<0.0>,11437) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11439) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11441) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11442) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11444) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11451) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11452) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11454) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11460) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11461) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11462) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11463) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11464) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11468)
    (<0.0>,11469)
    (<0.0>,11470) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,11471) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,11478)
    (<0.0>,11479)
    (<0.0>,11480) tree.c:1584: local_irq_save(flags);
    (<0.0>,11483)
    (<0.0>,11485) fake_sched.h:43: return __running_cpu;
    (<0.0>,11489) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11491) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11495) fake_sched.h:43: return __running_cpu;
    (<0.0>,11499) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11504) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11506) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11507) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11508) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11510) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11511) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11513) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11516) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11518) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11519) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11521) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11524) tree.c:1589: local_irq_restore(flags);
    (<0.0>,11527)
    (<0.0>,11529) fake_sched.h:43: return __running_cpu;
    (<0.0>,11533) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11535) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11539) fake_sched.h:43: return __running_cpu;
    (<0.0>,11543) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11550) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11552) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11557) tree.c:2558: local_irq_save(flags);
    (<0.0>,11560)
    (<0.0>,11562) fake_sched.h:43: return __running_cpu;
    (<0.0>,11566) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11568) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11572) fake_sched.h:43: return __running_cpu;
    (<0.0>,11576) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11581) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11582) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11588)
    (<0.0>,11589)
    (<0.0>,11590) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,11593)
    (<0.0>,11594) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11596) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11597) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11599) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11605) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,11611)
    (<0.0>,11612) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11615)
    (<0.0>,11616) tree.c:453: return &rsp->node[0];
    (<0.0>,11620) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11621) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11623) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11627) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11628) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11630) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11633) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11634) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11635) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11639) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11642) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11645) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11648) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11649) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11652) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11654) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11657) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11660) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11663) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11664) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11666) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11669) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11673) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11675) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11677) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11680) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11683) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11686) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11687) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11689) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11692) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11696) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11698) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11700) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11703) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,11705) tree.c:494: }
    (<0.0>,11709) tree.c:2566: local_irq_restore(flags);
    (<0.0>,11712)
    (<0.0>,11714) fake_sched.h:43: return __running_cpu;
    (<0.0>,11718) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11720) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11724) fake_sched.h:43: return __running_cpu;
    (<0.0>,11728) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11734) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11737)
    (<0.0>,11738) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11740) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11743) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11750) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11753)
    (<0.0>,11757) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11760) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11761) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11762) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11766) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11767) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11768) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11770) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11778) fake_sched.h:43: return __running_cpu;
    (<0.0>,11782) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,11791) tree.c:624: local_irq_save(flags);
    (<0.0>,11794)
    (<0.0>,11796) fake_sched.h:43: return __running_cpu;
    (<0.0>,11800) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11802) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11806) fake_sched.h:43: return __running_cpu;
    (<0.0>,11810) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11816) fake_sched.h:43: return __running_cpu;
    (<0.0>,11820) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11821) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11823) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11824) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11825) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11827) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11829) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11830) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11832) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11837) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11838) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11840) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11844) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11845) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11846) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11847) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,11849) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,11857)
    (<0.0>,11859) tree.c:634: local_irq_restore(flags);
    (<0.0>,11862)
    (<0.0>,11864) fake_sched.h:43: return __running_cpu;
    (<0.0>,11868) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11870) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11874) fake_sched.h:43: return __running_cpu;
    (<0.0>,11878) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11887) fake_sched.h:43: return __running_cpu;
    (<0.0>,11891)
    (<0.0>,11894) tree.c:580: local_irq_save(flags);
    (<0.0>,11897)
    (<0.0>,11899) fake_sched.h:43: return __running_cpu;
    (<0.0>,11903) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11905) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11909) fake_sched.h:43: return __running_cpu;
    (<0.0>,11913) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11926)
    (<0.0>,11928) fake_sched.h:43: return __running_cpu;
    (<0.0>,11932) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11933) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11935) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11936) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11937) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11943) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11944) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11949) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11950) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11951) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11952) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,11956) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11958) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11959) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11960) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11979)
    (<0.0>,11981)
    (<0.0>,11983) fake_sched.h:43: return __running_cpu;
    (<0.0>,11987) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11990) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,11994) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11995) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11996) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12000) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12001) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12002) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12004) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12009) fake_sched.h:43: return __running_cpu;
    (<0.0>,12012) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,12014) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,12016) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,12017) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12020)
    (<0.0>,12023) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12026) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12027) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12028) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12032) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12033) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12034) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12036) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12041) fake_sched.h:43: return __running_cpu;
    (<0.0>,12044) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,12046) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,12048) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,12049) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12052)
    (<0.0>,12055) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12058) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12059) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12060) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12064) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12065) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12066) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12068) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12075) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12078) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12079) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12080) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12082) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12083) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,12085) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12088) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12094) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12095) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12098) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12103) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12104) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12105) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,12119)
    (<0.0>,12121) tree.c:583: local_irq_restore(flags);
    (<0.0>,12124)
    (<0.0>,12126) fake_sched.h:43: return __running_cpu;
    (<0.0>,12130) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12132) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12136) fake_sched.h:43: return __running_cpu;
    (<0.0>,12140) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12146) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12149) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12154) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12156) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12158) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12162) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12164) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12171) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12173) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12175) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12179) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12181) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12188) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12190) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12192) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12196) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12198) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12205) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12207) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12209) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12213) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12215) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12222) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12224) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12226) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12230) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,12232) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
      (<0.1>,720) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,723) fake_sync.h:241: while (!x->done)
      (<0.1>,725) fake_sync.h:241: while (!x->done)
      (<0.1>,732) fake_sched.h:43: return __running_cpu;
      (<0.1>,736)
      (<0.1>,737) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,740) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,745) tree.c:704: local_irq_save(flags);
      (<0.1>,748)
      (<0.1>,750) fake_sched.h:43: return __running_cpu;
      (<0.1>,754) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,756) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,760) fake_sched.h:43: return __running_cpu;
      (<0.1>,764) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,777)
      (<0.1>,779) fake_sched.h:43: return __running_cpu;
      (<0.1>,783) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,784) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,786) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,787) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,788) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,793) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,794) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,798) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,799) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,800) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,801) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,805) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,807) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,808) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,809) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,823)
      (<0.1>,824)
      (<0.1>,826) fake_sched.h:43: return __running_cpu;
      (<0.1>,830) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,834) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,837) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,838) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,839) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,841) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,842) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,844) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,847) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,854) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,855) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,858) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,864) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,865) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,870) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,879)
      (<0.1>,881) tree.c:707: local_irq_restore(flags);
      (<0.1>,884)
      (<0.1>,886) fake_sched.h:43: return __running_cpu;
      (<0.1>,890) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,892) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,896) fake_sched.h:43: return __running_cpu;
      (<0.1>,900) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,911)
      (<0.1>,917) litmus.c:91: y = 1;
  (<0>,8004) litmus.c:69: r_y = y;
  (<0>,8005) litmus.c:69: r_y = y;
  (<0>,8016) fake_sched.h:43: return __running_cpu;
  (<0>,8020)
  (<0>,8023) tree.c:580: local_irq_save(flags);
  (<0>,8026)
  (<0>,8028) fake_sched.h:43: return __running_cpu;
  (<0>,8032) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8034) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8038) fake_sched.h:43: return __running_cpu;
  (<0>,8042) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8055)
  (<0>,8057) fake_sched.h:43: return __running_cpu;
  (<0>,8061) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,8062) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,8064) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,8065) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,8066) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8072) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8073) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8078) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8079) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8080) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8081) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,8085) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,8087) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,8088) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,8089) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,8108)
  (<0>,8110)
  (<0>,8112) fake_sched.h:43: return __running_cpu;
  (<0>,8116) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,8119) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,8123) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8124) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8125) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8129) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8130) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8131) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8133) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8138) fake_sched.h:43: return __running_cpu;
  (<0>,8141) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8143) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8145) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8146) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,8149)
  (<0>,8152) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8155) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8156) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8157) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8161) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8162) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8163) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8165) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8170) fake_sched.h:43: return __running_cpu;
  (<0>,8173) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8175) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8177) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8178) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,8181)
  (<0>,8184) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8187) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8188) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8189) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8193) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8194) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8195) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8197) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8204) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8207) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8208) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8209) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8211) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8212) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8214) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8217) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8223) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8224) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8227) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8232) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8233) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8234) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8248)
  (<0>,8250) tree.c:583: local_irq_restore(flags);
  (<0>,8253)
  (<0>,8255) fake_sched.h:43: return __running_cpu;
  (<0>,8259) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8261) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8265) fake_sched.h:43: return __running_cpu;
  (<0>,8269) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8275) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8278) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8283) litmus.c:138: if (pthread_join(tu, NULL))
      (<0.1>,918) litmus.c:93: fake_release_cpu(get_cpu());
      (<0.1>,919) fake_sched.h:43: return __running_cpu;
      (<0.1>,923)
      (<0.1>,926) tree.c:580: local_irq_save(flags);
      (<0.1>,929)
      (<0.1>,931) fake_sched.h:43: return __running_cpu;
      (<0.1>,935) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,937) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,941) fake_sched.h:43: return __running_cpu;
      (<0.1>,945) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,958)
      (<0.1>,960) fake_sched.h:43: return __running_cpu;
      (<0.1>,964) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,965) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,967) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,968) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,969) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,975) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,976) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,981) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,982) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,983) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,984) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,988) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,990) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,991) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,992) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1011)
      (<0.1>,1013)
      (<0.1>,1015) fake_sched.h:43: return __running_cpu;
      (<0.1>,1019) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1022) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,1026) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1027) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1028) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1033) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1034) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1041) fake_sched.h:43: return __running_cpu;
      (<0.1>,1044) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1046) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1048) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1049) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1052)
      (<0.1>,1055) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1058) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1059) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1060) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1065) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1066) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1073) fake_sched.h:43: return __running_cpu;
      (<0.1>,1076) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1078) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1080) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1081) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1084)
      (<0.1>,1087) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1090) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1091) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1092) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1097) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1098) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1107) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1110) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1111) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1112) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1117) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1120) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1126) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1127) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1130) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1135) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1136) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1137) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1151)
      (<0.1>,1153) tree.c:583: local_irq_restore(flags);
      (<0.1>,1156)
      (<0.1>,1158) fake_sched.h:43: return __running_cpu;
      (<0.1>,1162) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1164) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1168) fake_sched.h:43: return __running_cpu;
      (<0.1>,1172) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1178) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1181) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8284) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,8287) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8290) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8294) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8295) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8296) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
             Error: Assertion violation at (<0>,8299): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpusehb7q3/tmph1bgpo2c.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DFORCE_FAILURE_1 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpusehb7q3/tmpppijug27.ll /tmp/tmpusehb7q3/tmph1bgpo2c.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=-1 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpusehb7q3/tmpppijug27.ll
Total wall-clock time: 1.60 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 13256 (also 8 sleepset blocked)

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,73) tree_plugin.h:931: pr_info("Hierarchical RCU implementation.\n");
  (<0>,77) tree_plugin.h:91: if (rcu_fanout_leaf != CONFIG_RCU_FANOUT_LEAF)
  (<0>,89) tree.c:3718: ulong d;
  (<0>,90) tree.c:3722: int rcu_capacity[MAX_RCU_LVLS + 1];
  (<0>,91) tree.c:3732: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,94) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,95) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,97) tree.c:3734: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,100) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,101) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,103) tree.c:3738: if (rcu_fanout_leaf == CONFIG_RCU_FANOUT_LEAF &&
  (<0>,115)
  (<0>,116) tree.c:3628: static void __init rcu_init_one(struct rcu_state *rsp,
  (<0>,117) tree.c:3629: struct rcu_data __percpu *rda)
  (<0>,118) tree.c:3643: int i;
  (<0>,121) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,123) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,124) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,127) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,130) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,133) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,136) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,140) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,142) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,143) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,146) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,149) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,152) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,158)
  (<0>,159) tree.c:3610: static void __init rcu_init_levelspread(struct rcu_state *rsp)
  (<0>,160) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,162) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,164) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,169) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,172) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,173) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,174) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,175) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,183) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,186) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,187) tree.c:3620: cprv = ccur;
  (<0>,188) tree.c:3620: cprv = ccur;
  (<0>,190) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,192) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,194) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,198) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,199) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,201) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,202) tree.c:3661: fl_mask <<= 1;
  (<0>,206) tree.c:3661: fl_mask <<= 1;
  (<0>,207) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,209) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,211) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,219) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,221) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,223) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,224) tree.c:3667: rnp = rsp->level[i];
  (<0>,226) tree.c:3667: rnp = rsp->level[i];
  (<0>,229) tree.c:3667: rnp = rsp->level[i];
  (<0>,230) tree.c:3667: rnp = rsp->level[i];
  (<0>,231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,233) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,236) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,242) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,246)
  (<0>,247) fake_sync.h:68: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,248) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,254) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,258)
  (<0>,259) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,260) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,268) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,269) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,271) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,274) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,275) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,277) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,278) tree.c:3677: rnp->qsmask = 0;
  (<0>,280) tree.c:3677: rnp->qsmask = 0;
  (<0>,281) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,283) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,285) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,287) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,295) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,297) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,298) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,300) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,303) tree.c:3683: if (i == 0) {
  (<0>,306) tree.c:3684: rnp->grpnum = 0;
  (<0>,308) tree.c:3684: rnp->grpnum = 0;
  (<0>,309) tree.c:3685: rnp->grpmask = 0;
  (<0>,311) tree.c:3685: rnp->grpmask = 0;
  (<0>,312) tree.c:3686: rnp->parent = NULL;
  (<0>,314) tree.c:3686: rnp->parent = NULL;
  (<0>,316) tree.c:3693: rnp->level = i;
  (<0>,318) tree.c:3693: rnp->level = i;
  (<0>,320) tree.c:3693: rnp->level = i;
  (<0>,321) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,325)
  (<0>,326) fake_defs.h:273: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,327) fake_defs.h:275: list->next = list;
  (<0>,329) fake_defs.h:275: list->next = list;
  (<0>,330) fake_defs.h:276: list->prev = list;
  (<0>,331) fake_defs.h:276: list->prev = list;
  (<0>,333) fake_defs.h:276: list->prev = list;
  (<0>,335) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,338) tree_plugin.h:2694: }
  (<0>,341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,348) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,351) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,354) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,358) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,360) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,362) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,365) tree.c:3699: rsp->rda = rda;
  (<0>,366) tree.c:3699: rsp->rda = rda;
  (<0>,368) tree.c:3699: rsp->rda = rda;
  (<0>,371) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,374) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,377) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,378) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,379) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,381) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,382) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,389)
  (<0>,390) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,391) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,393) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,394) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,397) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,399) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,400) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,403) fake_defs.h:536: if (offset & mask)
  (<0>,404) fake_defs.h:536: if (offset & mask)
  (<0>,408) fake_defs.h:537: return cpu;
  (<0>,409) fake_defs.h:537: return cpu;
  (<0>,411) fake_defs.h:540: }
  (<0>,413) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,414) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,418) tree.c:3703: while (i > rnp->grphi)
  (<0>,419) tree.c:3703: while (i > rnp->grphi)
  (<0>,421) tree.c:3703: while (i > rnp->grphi)
  (<0>,424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,425) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,429) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,432) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,433) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,434) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,447)
  (<0>,448) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,449) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,451) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,453) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,455) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,456) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,459)
  (<0>,460) tree.c:451: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,464) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,465) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,467) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,471)
  (<0>,472) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,473) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,476) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,478) fake_sched.h:43: return __running_cpu;
  (<0>,482) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,484) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,488) fake_sched.h:43: return __running_cpu;
  (<0>,492) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,498) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,499) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,504) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,506) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,508) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,512) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,514) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,515) tree.c:3412: init_callback_list(rdp);
  (<0>,519)
  (<0>,520) tree.c:1223: static void init_callback_list(struct rcu_data *rdp)
  (<0>,523) tree_plugin.h:2732: return false;
  (<0>,526) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,528) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,529) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,531) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,534) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,538) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,541) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,543) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,545) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,547) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,550) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,554) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,557) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,559) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,561) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,563) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,566) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,570) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,573) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,575) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,577) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,579) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,582) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,586) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,589) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,591) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,593) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,595) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,599) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,601) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,602) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,604) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,608) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,610) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,611) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,613) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,621) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,623) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,629) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,630) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,631) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,634) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,642) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,645) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,652) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,653) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,654) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,655) tree.c:3418: rdp->cpu = cpu;
  (<0>,656) tree.c:3418: rdp->cpu = cpu;
  (<0>,658) tree.c:3418: rdp->cpu = cpu;
  (<0>,659) tree.c:3419: rdp->rsp = rsp;
  (<0>,660) tree.c:3419: rdp->rsp = rsp;
  (<0>,662) tree.c:3419: rdp->rsp = rsp;
  (<0>,663) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,666) tree_plugin.h:2711: }
  (<0>,668) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,670) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,674)
  (<0>,675) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,676) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,677) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,680) fake_sync.h:86: local_irq_restore(flags);
  (<0>,683) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,685) fake_sched.h:43: return __running_cpu;
  (<0>,689) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,691) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,695) fake_sched.h:43: return __running_cpu;
  (<0>,699) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,708) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,709) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,716)
  (<0>,717)
  (<0>,718) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,720) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,721) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,724) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,726) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,727) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,730) fake_defs.h:536: if (offset & mask)
  (<0>,731) fake_defs.h:536: if (offset & mask)
  (<0>,735) fake_defs.h:537: return cpu;
  (<0>,736) fake_defs.h:537: return cpu;
  (<0>,738) fake_defs.h:540: }
  (<0>,740) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,741) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,745) tree.c:3703: while (i > rnp->grphi)
  (<0>,746) tree.c:3703: while (i > rnp->grphi)
  (<0>,748) tree.c:3703: while (i > rnp->grphi)
  (<0>,751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,752) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,756) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,759) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,760) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,761) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,774)
  (<0>,775)
  (<0>,776) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,778) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,780) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,782) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,783) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,786)
  (<0>,787) tree.c:453: return &rsp->node[0];
  (<0>,791) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,792) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,794) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,798)
  (<0>,799)
  (<0>,800) fake_sync.h:76: local_irq_save(flags);
  (<0>,803) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,805) fake_sched.h:43: return __running_cpu;
  (<0>,809) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,811) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,815) fake_sched.h:43: return __running_cpu;
  (<0>,819) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,825) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,826) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,831) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,833) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,835) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,839) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,841) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,842) tree.c:3412: init_callback_list(rdp);
  (<0>,846)
  (<0>,847) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,850) tree_plugin.h:2732: return false;
  (<0>,853) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,855) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,856) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,858) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,861) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,865) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,868) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,870) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,872) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,874) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,877) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,881) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,884) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,886) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,888) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,890) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,893) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,897) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,900) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,902) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,904) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,906) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,909) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,913) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,916) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,918) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,920) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,922) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,926) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,928) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,929) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,931) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,935) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,937) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,938) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,940) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,948) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,950) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,956) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,957) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,958) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,961) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,969) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,972) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,979) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,980) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,981) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,982) tree.c:3418: rdp->cpu = cpu;
  (<0>,983) tree.c:3418: rdp->cpu = cpu;
  (<0>,985) tree.c:3418: rdp->cpu = cpu;
  (<0>,986) tree.c:3419: rdp->rsp = rsp;
  (<0>,987) tree.c:3419: rdp->rsp = rsp;
  (<0>,989) tree.c:3419: rdp->rsp = rsp;
  (<0>,990) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,993) tree_plugin.h:2711: }
  (<0>,995) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,997) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1001)
  (<0>,1002)
  (<0>,1003) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1004) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1007) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1010) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1012) fake_sched.h:43: return __running_cpu;
  (<0>,1016) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1018) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1022) fake_sched.h:43: return __running_cpu;
  (<0>,1026) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1035) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1036) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1043)
  (<0>,1044)
  (<0>,1045) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1047) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1048) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1051) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1053) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1054) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1057) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,1059) fake_defs.h:540: }
  (<0>,1061) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1062) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1065) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1070)
  (<0>,1071) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1072) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1073) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1074) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1076) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1081)
  (<0>,1082) fake_defs.h:279: static inline void __list_add(struct list_head *new,
  (<0>,1083) fake_defs.h:280: struct list_head *prev,
  (<0>,1084) fake_defs.h:281: struct list_head *next)
  (<0>,1085) fake_defs.h:283: next->prev = new;
  (<0>,1087) fake_defs.h:283: next->prev = new;
  (<0>,1088) fake_defs.h:284: new->next = next;
  (<0>,1089) fake_defs.h:284: new->next = next;
  (<0>,1091) fake_defs.h:284: new->next = next;
  (<0>,1092) fake_defs.h:285: new->prev = prev;
  (<0>,1093) fake_defs.h:285: new->prev = prev;
  (<0>,1095) fake_defs.h:285: new->prev = prev;
  (<0>,1096) fake_defs.h:286: prev->next = new;
  (<0>,1097) fake_defs.h:286: prev->next = new;
  (<0>,1099) fake_defs.h:286: prev->next = new;
  (<0>,1110)
  (<0>,1111)
  (<0>,1112) tree.c:3642: int cpustride = 1;
  (<0>,1113) tree.c:3650: if (rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1116) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1118) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1119) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1122) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1125) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1126) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1128) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1133) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1135) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1137) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1141) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1143) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1144) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1147) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,1153)
  (<0>,1154) tree.c:3616: cprv = nr_cpu_ids;
  (<0>,1155) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1157) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1159) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1162) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1164) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1168) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1169) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1170) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1173) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1176) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1182) tree.c:3620: cprv = ccur;
  (<0>,1183) tree.c:3620: cprv = ccur;
  (<0>,1185) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1187) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1189) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1193) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1194) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1196) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1197) tree.c:3661: fl_mask <<= 1;
  (<0>,1201) tree.c:3661: fl_mask <<= 1;
  (<0>,1202) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1204) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1206) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1209) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1211) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1218) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1219) tree.c:3667: rnp = rsp->level[i];
  (<0>,1221) tree.c:3667: rnp = rsp->level[i];
  (<0>,1224) tree.c:3667: rnp = rsp->level[i];
  (<0>,1225) tree.c:3667: rnp = rsp->level[i];
  (<0>,1226) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1228) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1229) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1237) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,1241)
  (<0>,1242) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1243) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1249) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1253)
  (<0>,1254) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1255) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1261) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1263) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1264) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1267) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1269) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1270) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1273) tree.c:3677: rnp->qsmask = 0;
  (<0>,1275) tree.c:3677: rnp->qsmask = 0;
  (<0>,1276) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1278) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1279) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1280) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1282) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1285) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1287) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1293) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1295) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1298) tree.c:3683: if (i == 0) {
  (<0>,1301) tree.c:3684: rnp->grpnum = 0;
  (<0>,1303) tree.c:3684: rnp->grpnum = 0;
  (<0>,1304) tree.c:3685: rnp->grpmask = 0;
  (<0>,1306) tree.c:3685: rnp->grpmask = 0;
  (<0>,1307) tree.c:3686: rnp->parent = NULL;
  (<0>,1309) tree.c:3686: rnp->parent = NULL;
  (<0>,1311) tree.c:3693: rnp->level = i;
  (<0>,1313) tree.c:3693: rnp->level = i;
  (<0>,1315) tree.c:3693: rnp->level = i;
  (<0>,1316) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1320)
  (<0>,1321) fake_defs.h:275: list->next = list;
  (<0>,1322) fake_defs.h:275: list->next = list;
  (<0>,1324) fake_defs.h:275: list->next = list;
  (<0>,1325) fake_defs.h:276: list->prev = list;
  (<0>,1326) fake_defs.h:276: list->prev = list;
  (<0>,1328) fake_defs.h:276: list->prev = list;
  (<0>,1330) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,1333) tree_plugin.h:2694: }
  (<0>,1336) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1338) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1339) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1353) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1355) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1357) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1360) tree.c:3699: rsp->rda = rda;
  (<0>,1361) tree.c:3699: rsp->rda = rda;
  (<0>,1363) tree.c:3699: rsp->rda = rda;
  (<0>,1366) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1369) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1372) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1373) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1374) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1376) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1377) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1384)
  (<0>,1385)
  (<0>,1386) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1388) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1389) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1392) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1394) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1395) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1398) fake_defs.h:536: if (offset & mask)
  (<0>,1399) fake_defs.h:536: if (offset & mask)
  (<0>,1403) fake_defs.h:537: return cpu;
  (<0>,1404) fake_defs.h:537: return cpu;
  (<0>,1406) fake_defs.h:540: }
  (<0>,1408) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1409) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1413) tree.c:3703: while (i > rnp->grphi)
  (<0>,1414) tree.c:3703: while (i > rnp->grphi)
  (<0>,1416) tree.c:3703: while (i > rnp->grphi)
  (<0>,1419) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1420) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1422) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1428) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1429) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1442)
  (<0>,1443)
  (<0>,1444) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1446) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1448) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1450) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1451) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1454)
  (<0>,1455) tree.c:453: return &rsp->node[0];
  (<0>,1459) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1460) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1462) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1466)
  (<0>,1467)
  (<0>,1468) fake_sync.h:76: local_irq_save(flags);
  (<0>,1471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1473) fake_sched.h:43: return __running_cpu;
  (<0>,1477) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1479) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1483) fake_sched.h:43: return __running_cpu;
  (<0>,1487) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1493) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1494) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1498) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1499) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1501) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1507) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1509) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1510) tree.c:3412: init_callback_list(rdp);
  (<0>,1514)
  (<0>,1515) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1518) tree_plugin.h:2732: return false;
  (<0>,1521) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1523) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1529) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1531) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1533) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1538) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1540) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1542) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1545) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1547) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1549) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1554) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1556) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1558) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1561) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1563) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1565) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1570) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1572) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1574) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1577) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1579) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1581) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1586) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1588) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1590) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1594) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1596) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1597) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1599) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1600) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1603) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1606) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1608) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1610) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1616) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1618) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1624) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1626) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1627) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1629) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1638) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1640) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1647) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1649) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1650) tree.c:3418: rdp->cpu = cpu;
  (<0>,1651) tree.c:3418: rdp->cpu = cpu;
  (<0>,1653) tree.c:3418: rdp->cpu = cpu;
  (<0>,1654) tree.c:3419: rdp->rsp = rsp;
  (<0>,1655) tree.c:3419: rdp->rsp = rsp;
  (<0>,1657) tree.c:3419: rdp->rsp = rsp;
  (<0>,1658) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1661) tree_plugin.h:2711: }
  (<0>,1663) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1665) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1669)
  (<0>,1670)
  (<0>,1671) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1672) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1675) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1678) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1680) fake_sched.h:43: return __running_cpu;
  (<0>,1684) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1686) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1690) fake_sched.h:43: return __running_cpu;
  (<0>,1694) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1703) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1704) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1711)
  (<0>,1712)
  (<0>,1713) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1715) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1716) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1719) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1721) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1722) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1725) fake_defs.h:536: if (offset & mask)
  (<0>,1726) fake_defs.h:536: if (offset & mask)
  (<0>,1730) fake_defs.h:537: return cpu;
  (<0>,1731) fake_defs.h:537: return cpu;
  (<0>,1733) fake_defs.h:540: }
  (<0>,1735) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1736) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1740) tree.c:3703: while (i > rnp->grphi)
  (<0>,1741) tree.c:3703: while (i > rnp->grphi)
  (<0>,1743) tree.c:3703: while (i > rnp->grphi)
  (<0>,1746) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1747) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1749) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1755) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1756) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1769)
  (<0>,1770)
  (<0>,1771) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1773) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1775) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1777) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1778) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1781)
  (<0>,1782) tree.c:453: return &rsp->node[0];
  (<0>,1786) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1787) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1789) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1793)
  (<0>,1794)
  (<0>,1795) fake_sync.h:76: local_irq_save(flags);
  (<0>,1798) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1800) fake_sched.h:43: return __running_cpu;
  (<0>,1804) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1806) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1810) fake_sched.h:43: return __running_cpu;
  (<0>,1814) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1820) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1821) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1825) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1826) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1828) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1834) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1836) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1837) tree.c:3412: init_callback_list(rdp);
  (<0>,1841)
  (<0>,1842) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1845) tree_plugin.h:2732: return false;
  (<0>,1848) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1850) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1851) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1853) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1856) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1858) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1860) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1865) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1867) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1869) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1872) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1874) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1876) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1881) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1883) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1885) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1888) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1890) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1892) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1897) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1899) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1901) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1904) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1906) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1908) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1913) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1915) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1917) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1921) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1923) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1924) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1926) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1927) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1930) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1933) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1935) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1937) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1943) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1945) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1951) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1953) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1954) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1956) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1965) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1967) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1974) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1976) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1977) tree.c:3418: rdp->cpu = cpu;
  (<0>,1978) tree.c:3418: rdp->cpu = cpu;
  (<0>,1980) tree.c:3418: rdp->cpu = cpu;
  (<0>,1981) tree.c:3419: rdp->rsp = rsp;
  (<0>,1982) tree.c:3419: rdp->rsp = rsp;
  (<0>,1984) tree.c:3419: rdp->rsp = rsp;
  (<0>,1985) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1988) tree_plugin.h:2711: }
  (<0>,1990) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1992) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1996)
  (<0>,1997)
  (<0>,1998) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1999) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,2002) fake_sync.h:86: local_irq_restore(flags);
  (<0>,2005) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2007) fake_sched.h:43: return __running_cpu;
  (<0>,2011) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2013) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2017) fake_sched.h:43: return __running_cpu;
  (<0>,2021) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2030) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2031) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2038)
  (<0>,2039)
  (<0>,2040) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2042) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2043) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2046) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2048) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2049) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2052) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,2054) fake_defs.h:540: }
  (<0>,2056) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2057) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2060) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,2065)
  (<0>,2066)
  (<0>,2067) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2068) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2069) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2071) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2076)
  (<0>,2077)
  (<0>,2078)
  (<0>,2079) fake_defs.h:283: next->prev = new;
  (<0>,2080) fake_defs.h:283: next->prev = new;
  (<0>,2082) fake_defs.h:283: next->prev = new;
  (<0>,2083) fake_defs.h:284: new->next = next;
  (<0>,2084) fake_defs.h:284: new->next = next;
  (<0>,2086) fake_defs.h:284: new->next = next;
  (<0>,2087) fake_defs.h:285: new->prev = prev;
  (<0>,2088) fake_defs.h:285: new->prev = prev;
  (<0>,2090) fake_defs.h:285: new->prev = prev;
  (<0>,2091) fake_defs.h:286: prev->next = new;
  (<0>,2092) fake_defs.h:286: prev->next = new;
  (<0>,2094) fake_defs.h:286: prev->next = new;
  (<0>,2106) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2108) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2109) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2116)
  (<0>,2117)
  (<0>,2118) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2120) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2121) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2124) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2126) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2127) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2130) fake_defs.h:536: if (offset & mask)
  (<0>,2131) fake_defs.h:536: if (offset & mask)
  (<0>,2135) fake_defs.h:537: return cpu;
  (<0>,2136) fake_defs.h:537: return cpu;
  (<0>,2138) fake_defs.h:540: }
  (<0>,2140) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2141) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2144) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2163)
  (<0>,2164) tree.c:3493: static int rcu_cpu_notify(struct notifier_block *self,
  (<0>,2165) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2166) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2168) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2169) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2170) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2172) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2174) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2175) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2177) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2178) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2181) tree.c:3502: switch (action) {
  (<0>,2183) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2192)
  (<0>,2193) tree.c:3482: static void rcu_prepare_cpu(int cpu)
  (<0>,2194) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2195) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2199) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2200) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2201) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2203) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2207) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2208) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2216)
  (<0>,2217) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2218) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2220) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2222) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2224) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2225) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2228)
  (<0>,2229) tree.c:453: return &rsp->node[0];
  (<0>,2233) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2234) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2238)
  (<0>,2239) fake_sync.h:172: void mutex_lock(struct mutex *l)
  (<0>,2241) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2245) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2247) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2251)
  (<0>,2252)
  (<0>,2253) fake_sync.h:76: local_irq_save(flags);
  (<0>,2256) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2258) fake_sched.h:43: return __running_cpu;
  (<0>,2262) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2264) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2268) fake_sched.h:43: return __running_cpu;
  (<0>,2272) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2278) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2279) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2283) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2285) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2286) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2288) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2289) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2291) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2292) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2294) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2295) tree.c:3446: rdp->blimit = blimit;
  (<0>,2296) tree.c:3446: rdp->blimit = blimit;
  (<0>,2298) tree.c:3446: rdp->blimit = blimit;
  (<0>,2299) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2303)
  (<0>,2304) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2307) tree_plugin.h:2732: return false;
  (<0>,2310) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2312) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2313) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2315) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2318) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2320) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2322) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2325) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2327) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2329) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2331) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2334) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2336) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2338) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2341) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2343) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2345) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2347) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2350) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2352) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2354) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2357) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2359) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2361) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2363) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2366) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2368) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2370) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2373) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2375) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2377) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2379) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2383) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2385) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2387) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2388) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2390) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2393) tree_plugin.h:3164: }
  (<0>,2395) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2397) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2400) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2403) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2405) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2408) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2409) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2413)
  (<0>,2414) fake_sync.h:113: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2415) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2420) tree.c:3455: rnp = rdp->mynode;
  (<0>,2422) tree.c:3455: rnp = rdp->mynode;
  (<0>,2423) tree.c:3455: rnp = rdp->mynode;
  (<0>,2424) tree.c:3456: mask = rdp->grpmask;
  (<0>,2426) tree.c:3456: mask = rdp->grpmask;
  (<0>,2427) tree.c:3456: mask = rdp->grpmask;
  (<0>,2429) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2433) fake_sync.h:108: preempt_disable();
  (<0>,2435) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2436) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2440) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2441) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2443) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2445) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2446) tree.c:3461: mask = rnp->grpmask;
  (<0>,2448) tree.c:3461: mask = rnp->grpmask;
  (<0>,2449) tree.c:3461: mask = rnp->grpmask;
  (<0>,2450) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2451) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2453) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2456) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2458) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2459) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2461) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2462) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2464) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2465) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2467) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2468) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2470) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2471) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2473) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2477) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2481)
  (<0>,2482) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2483) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2488) tree.c:3475: rnp = rnp->parent;
  (<0>,2490) tree.c:3475: rnp = rnp->parent;
  (<0>,2491) tree.c:3475: rnp = rnp->parent;
  (<0>,2493) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2497) tree.c:3477: local_irq_restore(flags);
  (<0>,2500) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2502) fake_sched.h:43: return __running_cpu;
  (<0>,2506) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2508) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2512) fake_sched.h:43: return __running_cpu;
  (<0>,2516) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2521) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2525)
  (<0>,2526) fake_sync.h:178: void mutex_unlock(struct mutex *l)
  (<0>,2528) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2534) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2537) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2538) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2539) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2543) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2544) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2545) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2547) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2551) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2552) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2560)
  (<0>,2561)
  (<0>,2562) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2564) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2566) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2568) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2569) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2572)
  (<0>,2573) tree.c:453: return &rsp->node[0];
  (<0>,2577) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2578) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2582)
  (<0>,2583) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2585) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2589) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2591) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2595)
  (<0>,2596)
  (<0>,2597) fake_sync.h:76: local_irq_save(flags);
  (<0>,2600) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2602) fake_sched.h:43: return __running_cpu;
  (<0>,2606) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2608) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2612) fake_sched.h:43: return __running_cpu;
  (<0>,2616) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2622) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2623) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2627) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2629) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2630) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2632) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2633) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2635) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2636) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2638) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2639) tree.c:3446: rdp->blimit = blimit;
  (<0>,2640) tree.c:3446: rdp->blimit = blimit;
  (<0>,2642) tree.c:3446: rdp->blimit = blimit;
  (<0>,2643) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2647)
  (<0>,2648) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2651) tree_plugin.h:2732: return false;
  (<0>,2654) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2656) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2657) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2659) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2662) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2664) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2666) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2669) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2671) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2673) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2675) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2678) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2680) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2682) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2685) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2687) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2689) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2691) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2694) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2696) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2698) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2701) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2703) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2705) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2707) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2710) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2712) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2714) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2717) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2719) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2721) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2723) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2727) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2729) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2731) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2732) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2734) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2737) tree_plugin.h:3164: }
  (<0>,2739) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2741) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2744) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2747) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2749) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2752) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2753) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2757)
  (<0>,2758) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2759) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2764) tree.c:3455: rnp = rdp->mynode;
  (<0>,2766) tree.c:3455: rnp = rdp->mynode;
  (<0>,2767) tree.c:3455: rnp = rdp->mynode;
  (<0>,2768) tree.c:3456: mask = rdp->grpmask;
  (<0>,2770) tree.c:3456: mask = rdp->grpmask;
  (<0>,2771) tree.c:3456: mask = rdp->grpmask;
  (<0>,2773) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2777) fake_sync.h:108: preempt_disable();
  (<0>,2779) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2780) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2784) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2785) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2787) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2789) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2790) tree.c:3461: mask = rnp->grpmask;
  (<0>,2792) tree.c:3461: mask = rnp->grpmask;
  (<0>,2793) tree.c:3461: mask = rnp->grpmask;
  (<0>,2794) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2795) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2797) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2800) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2802) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2803) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2805) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2806) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2808) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2809) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2811) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2812) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2814) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2815) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2817) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2821) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2825)
  (<0>,2826) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2827) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2832) tree.c:3475: rnp = rnp->parent;
  (<0>,2834) tree.c:3475: rnp = rnp->parent;
  (<0>,2835) tree.c:3475: rnp = rnp->parent;
  (<0>,2837) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2841) tree.c:3477: local_irq_restore(flags);
  (<0>,2844) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2846) fake_sched.h:43: return __running_cpu;
  (<0>,2850) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2852) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2856) fake_sched.h:43: return __running_cpu;
  (<0>,2860) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2865) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2869)
  (<0>,2870) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2872) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2878) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2881) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2882) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2883) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2887) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2888) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2889) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2891) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2896) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,2900) tree_plugin.h:1499: }
  (<0>,2902) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2906) tree_plugin.h:2724: }
  (<0>,2913) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2914) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2921)
  (<0>,2922)
  (<0>,2923) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2925) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2926) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2929) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2931) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2932) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2935) fake_defs.h:536: if (offset & mask)
  (<0>,2936) fake_defs.h:536: if (offset & mask)
  (<0>,2940) fake_defs.h:537: return cpu;
  (<0>,2941) fake_defs.h:537: return cpu;
  (<0>,2943) fake_defs.h:540: }
  (<0>,2945) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2946) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2949) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2968)
  (<0>,2969)
  (<0>,2970)
  (<0>,2971) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2973) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2974) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2975) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2977) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2979) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2980) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2982) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2983) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2986) tree.c:3502: switch (action) {
  (<0>,2988) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2997)
  (<0>,2998) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2999) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3000) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3004) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3005) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3006) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3008) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3012) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3013) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3021)
  (<0>,3022)
  (<0>,3023) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3025) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3027) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3029) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3030) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3033)
  (<0>,3034) tree.c:453: return &rsp->node[0];
  (<0>,3038) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3039) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3043)
  (<0>,3044) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3046) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3050) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3052) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3056)
  (<0>,3057)
  (<0>,3058) fake_sync.h:76: local_irq_save(flags);
  (<0>,3061) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3063) fake_sched.h:43: return __running_cpu;
  (<0>,3067) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3069) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3073) fake_sched.h:43: return __running_cpu;
  (<0>,3077) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3083) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3084) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3088) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3090) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3091) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3093) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3094) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3096) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3097) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3099) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3100) tree.c:3446: rdp->blimit = blimit;
  (<0>,3101) tree.c:3446: rdp->blimit = blimit;
  (<0>,3103) tree.c:3446: rdp->blimit = blimit;
  (<0>,3104) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3108)
  (<0>,3109) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3112) tree_plugin.h:2732: return false;
  (<0>,3115) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3117) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3118) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3120) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3123) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3125) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3127) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3130) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3132) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3134) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3136) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3139) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3141) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3143) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3146) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3148) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3150) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3152) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3155) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3157) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3159) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3162) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3164) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3166) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3168) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3171) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3173) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3175) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3178) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3180) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3182) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3184) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3188) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3190) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3192) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3193) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3195) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3198) tree_plugin.h:3164: }
  (<0>,3200) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3202) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3205) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3208) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3210) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3213) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3214) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3218)
  (<0>,3219) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3220) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3225) tree.c:3455: rnp = rdp->mynode;
  (<0>,3227) tree.c:3455: rnp = rdp->mynode;
  (<0>,3228) tree.c:3455: rnp = rdp->mynode;
  (<0>,3229) tree.c:3456: mask = rdp->grpmask;
  (<0>,3231) tree.c:3456: mask = rdp->grpmask;
  (<0>,3232) tree.c:3456: mask = rdp->grpmask;
  (<0>,3234) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3238) fake_sync.h:108: preempt_disable();
  (<0>,3240) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3241) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3245) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3246) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3248) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3250) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3251) tree.c:3461: mask = rnp->grpmask;
  (<0>,3253) tree.c:3461: mask = rnp->grpmask;
  (<0>,3254) tree.c:3461: mask = rnp->grpmask;
  (<0>,3255) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3256) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3258) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3261) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3263) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3264) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3266) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3267) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3269) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3270) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3272) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3273) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3275) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3276) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3278) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3282) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3286)
  (<0>,3287) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3288) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3293) tree.c:3475: rnp = rnp->parent;
  (<0>,3295) tree.c:3475: rnp = rnp->parent;
  (<0>,3296) tree.c:3475: rnp = rnp->parent;
  (<0>,3298) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3302) tree.c:3477: local_irq_restore(flags);
  (<0>,3305) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3307) fake_sched.h:43: return __running_cpu;
  (<0>,3311) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3313) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3317) fake_sched.h:43: return __running_cpu;
  (<0>,3321) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3326) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3330)
  (<0>,3331) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3333) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3339) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3342) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3343) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3344) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3348) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3349) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3350) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3352) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3356) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3357) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3365)
  (<0>,3366)
  (<0>,3367) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3369) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3371) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3373) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3374) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3377)
  (<0>,3378) tree.c:453: return &rsp->node[0];
  (<0>,3382) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3383) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3387)
  (<0>,3388) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3390) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3394) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3396) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3400)
  (<0>,3401)
  (<0>,3402) fake_sync.h:76: local_irq_save(flags);
  (<0>,3405) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3407) fake_sched.h:43: return __running_cpu;
  (<0>,3411) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3413) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3417) fake_sched.h:43: return __running_cpu;
  (<0>,3421) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3427) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3428) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3432) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3434) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3435) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3437) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3438) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3440) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3441) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3443) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3444) tree.c:3446: rdp->blimit = blimit;
  (<0>,3445) tree.c:3446: rdp->blimit = blimit;
  (<0>,3447) tree.c:3446: rdp->blimit = blimit;
  (<0>,3448) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3452)
  (<0>,3453) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3456) tree_plugin.h:2732: return false;
  (<0>,3459) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3461) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3462) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3464) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3467) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3469) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3471) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3474) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3476) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3478) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3480) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3483) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3485) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3487) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3490) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3492) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3494) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3496) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3499) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3501) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3503) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3506) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3508) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3510) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3512) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3515) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3517) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3519) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3522) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3528) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3532) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3534) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3536) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3537) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3539) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3542) tree_plugin.h:3164: }
  (<0>,3544) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3546) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3549) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3552) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3554) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3557) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3558) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3562)
  (<0>,3563) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3564) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3569) tree.c:3455: rnp = rdp->mynode;
  (<0>,3571) tree.c:3455: rnp = rdp->mynode;
  (<0>,3572) tree.c:3455: rnp = rdp->mynode;
  (<0>,3573) tree.c:3456: mask = rdp->grpmask;
  (<0>,3575) tree.c:3456: mask = rdp->grpmask;
  (<0>,3576) tree.c:3456: mask = rdp->grpmask;
  (<0>,3578) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3582) fake_sync.h:108: preempt_disable();
  (<0>,3584) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3585) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3589) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3590) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3592) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3594) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3595) tree.c:3461: mask = rnp->grpmask;
  (<0>,3597) tree.c:3461: mask = rnp->grpmask;
  (<0>,3598) tree.c:3461: mask = rnp->grpmask;
  (<0>,3599) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3600) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3602) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3605) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3607) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3608) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3610) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3611) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3613) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3614) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3616) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3617) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3619) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3620) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3622) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3626) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3630)
  (<0>,3631) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3632) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3637) tree.c:3475: rnp = rnp->parent;
  (<0>,3639) tree.c:3475: rnp = rnp->parent;
  (<0>,3640) tree.c:3475: rnp = rnp->parent;
  (<0>,3642) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3646) tree.c:3477: local_irq_restore(flags);
  (<0>,3649) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3651) fake_sched.h:43: return __running_cpu;
  (<0>,3655) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3657) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3661) fake_sched.h:43: return __running_cpu;
  (<0>,3665) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3670) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3674)
  (<0>,3675) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3677) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3683) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3686) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3687) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3688) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3692) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3693) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3694) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3696) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3701) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,3705) tree_plugin.h:1499: }
  (<0>,3707) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,3711) tree_plugin.h:2724: }
  (<0>,3718) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3719) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3726)
  (<0>,3727)
  (<0>,3728) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3730) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3731) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3734) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3736) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3737) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3740) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,3742) fake_defs.h:540: }
  (<0>,3744) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3745) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3751) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3753) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3756) litmus.c:123: set_cpu(i);
  (<0>,3759)
  (<0>,3760) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,3761) fake_sched.h:56: __running_cpu = cpu;
  (<0>,3765) tree.c:578: unsigned long flags;
  (<0>,3768) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3770) fake_sched.h:43: return __running_cpu;
  (<0>,3774) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3776) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3780) fake_sched.h:43: return __running_cpu;
  (<0>,3784) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3797) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3799) fake_sched.h:43: return __running_cpu;
  (<0>,3803) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3804) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3806) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3807) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3808) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3814) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3815) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3820) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3821) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3822) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3823) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,3827) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3829) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3830) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3831) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3850) tree.c:503: static void rcu_eqs_enter_common(long long oldval, bool user)
  (<0>,3852) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3854) fake_sched.h:43: return __running_cpu;
  (<0>,3858) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3861) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,3865) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3866) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3867) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3871) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3872) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3873) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3875) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3880) fake_sched.h:43: return __running_cpu;
  (<0>,3883) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3885) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3887) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3888) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3891) tree_plugin.h:2720: }
  (<0>,3894) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3897) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3898) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3899) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3903) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3904) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3905) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3907) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3912) fake_sched.h:43: return __running_cpu;
  (<0>,3915) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3917) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3919) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3920) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3923) tree_plugin.h:2720: }
  (<0>,3926) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3929) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3930) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3931) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3935) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3936) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3937) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3939) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3946) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3949) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3950) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3951) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3953) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3954) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3956) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3959) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3965) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3966) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3969) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3974) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3975) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3976) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3990) tree_plugin.h:3141: }
  (<0>,3992) tree.c:583: local_irq_restore(flags);
  (<0>,3995) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3997) fake_sched.h:43: return __running_cpu;
  (<0>,4001) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4003) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4007) fake_sched.h:43: return __running_cpu;
  (<0>,4011) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4018) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4020) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4022) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4025) litmus.c:123: set_cpu(i);
  (<0>,4028)
  (<0>,4029) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4030) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4034) tree.c:580: local_irq_save(flags);
  (<0>,4037) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4039) fake_sched.h:43: return __running_cpu;
  (<0>,4043) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4045) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4049) fake_sched.h:43: return __running_cpu;
  (<0>,4053) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4066) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4068) fake_sched.h:43: return __running_cpu;
  (<0>,4072) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4073) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4075) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4076) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4077) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4083) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4084) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4089) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4090) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4091) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4092) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4096) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4098) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4099) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4100) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4119)
  (<0>,4121) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4123) fake_sched.h:43: return __running_cpu;
  (<0>,4127) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4130) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,4134) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4135) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4136) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4140) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4141) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4142) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4144) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4149) fake_sched.h:43: return __running_cpu;
  (<0>,4152) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4154) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4156) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4157) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4160) tree_plugin.h:2720: }
  (<0>,4163) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4166) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4167) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4168) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4172) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4173) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4181) fake_sched.h:43: return __running_cpu;
  (<0>,4184) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4186) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4188) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4189) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4192) tree_plugin.h:2720: }
  (<0>,4195) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4198) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4199) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4200) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4204) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4205) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4206) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4208) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4215) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4218) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4219) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4220) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4222) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4223) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4225) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4228) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4234) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4235) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4238) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4243) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4244) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4245) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4259) tree_plugin.h:3141: }
  (<0>,4261) tree.c:583: local_irq_restore(flags);
  (<0>,4264) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4266) fake_sched.h:43: return __running_cpu;
  (<0>,4270) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4272) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4276) fake_sched.h:43: return __running_cpu;
  (<0>,4280) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4287) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4289) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4291) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4303) tree.c:3561: unsigned long flags;
  (<0>,4304) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4305) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4306) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4310) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4314) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4320) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4322) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4331)
  (<0>,4332) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4333) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4334) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4335) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4336) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4339)
  (<0>,4340)
  (<0>,4347)
  (<0>,4348)
  (<0>,4355)
  (<0>,4356)
  (<0>,4363)
  (<0>,4364)
  (<0>,4371)
  (<0>,4372)
  (<0>,4379)
  (<0>,4380)
  (<0>,4387)
  (<0>,4388)
  (<0>,4395)
  (<0>,4396)
  (<0>,4403)
  (<0>,4404)
  (<0>,4411)
  (<0>,4412) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4421) fake_defs.h:730: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,4427) fake_defs.h:732: task = malloc(sizeof(*task));
  (<0>,4428) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4430) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4432) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4433) fake_defs.h:734: task->tid = t;
  (<0>,4434) fake_defs.h:734: task->tid = t;
  (<0>,4436) fake_defs.h:734: task->tid = t;
  (<0>,4437) fake_defs.h:735: return task;
  (<0>,4438) fake_defs.h:735: return task;
  (<0>,4440) fake_defs.h:738: }
  (<0>,4442) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4443) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4446)
  (<0>,4447) tree.c:453: return &rsp->node[0];
  (<0>,4451) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4452) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4454) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4458)
  (<0>,4459)
  (<0>,4460) fake_sync.h:76: local_irq_save(flags);
  (<0>,4463) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4465) fake_sched.h:43: return __running_cpu;
  (<0>,4469) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4475) fake_sched.h:43: return __running_cpu;
  (<0>,4479) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4485) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4486) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4490) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4491) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4493) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4494) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4496) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4500)
  (<0>,4501)
  (<0>,4502) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4503) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4506) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4509) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4511) fake_sched.h:43: return __running_cpu;
  (<0>,4515) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4517) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4521) fake_sched.h:43: return __running_cpu;
  (<0>,4525) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4533) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4537) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4538) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4542) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4543) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4544) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4546) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4550) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4552) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4554) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4563)
  (<0>,4564)
  (<0>,4565)
  (<0>,4566)
  (<0>,4567) fake_defs.h:727: struct task_struct *task = NULL;
  (<0>,4568) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4571)
  (<0>,4572)
  (<0>,4579)
  (<0>,4580)
  (<0>,4587)
  (<0>,4588)
  (<0>,4595)
  (<0>,4596)
  (<0>,4603)
  (<0>,4604) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4617) fake_defs.h:737: return NULL;
  (<0>,4619) fake_defs.h:738: }
  (<0>,4621) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4622) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4625)
  (<0>,4626) tree.c:453: return &rsp->node[0];
  (<0>,4630) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4631) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4633) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4637)
  (<0>,4638)
  (<0>,4639) fake_sync.h:76: local_irq_save(flags);
  (<0>,4642) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4644) fake_sched.h:43: return __running_cpu;
  (<0>,4648) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4650) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4654) fake_sched.h:43: return __running_cpu;
  (<0>,4658) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4664) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4665) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4669) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4670) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4672) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4673) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4675) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4679)
  (<0>,4680)
  (<0>,4681) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4682) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4685) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4688) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4690) fake_sched.h:43: return __running_cpu;
  (<0>,4694) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4696) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4700) fake_sched.h:43: return __running_cpu;
  (<0>,4704) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4712) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4715) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4716) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4717) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4721) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4722) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4723) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4725) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4739)
  (<0>,4740) litmus.c:51: void *thread_reader(void *arg)
  (<0>,4743)
  (<0>,4744) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4745) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4748) fake_sched.h:43: return __running_cpu;
  (<0>,4752)
  (<0>,4753) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,4756) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,4761) tree.c:702: unsigned long flags;
  (<0>,4764) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4766) fake_sched.h:43: return __running_cpu;
  (<0>,4770) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4772) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4780) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4793) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4795) fake_sched.h:43: return __running_cpu;
  (<0>,4799) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4800) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4802) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4803) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4804) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4809) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4810) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4814) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4815) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4816) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4817) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,4821) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4823) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4824) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4825) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4839)
  (<0>,4840) tree.c:644: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,4842) fake_sched.h:43: return __running_cpu;
  (<0>,4846) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4850) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4853) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4854) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4855) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4857) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4858) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4860) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4870) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4871) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4874) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4879) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4880) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4881) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4886) tree.c:656: if (!user && !is_idle_task(current)) {
  (<0>,4895) tree_plugin.h:3145: }
  (<0>,4897) tree.c:707: local_irq_restore(flags);
  (<0>,4900) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4902) fake_sched.h:43: return __running_cpu;
  (<0>,4906) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4908) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4912) fake_sched.h:43: return __running_cpu;
  (<0>,4916) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4932) litmus.c:57: r_x = x;
  (<0>,4933) litmus.c:57: r_x = x;
  (<0>,4937) fake_sched.h:43: return __running_cpu;
  (<0>,4941) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4949) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4954) fake_sched.h:43: return __running_cpu;
  (<0>,4958) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
  (<0>,4968) tree.c:745: unsigned long flags;
  (<0>,4971) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4973) fake_sched.h:43: return __running_cpu;
  (<0>,4977) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4984) fake_sched.h:43: return __running_cpu;
  (<0>,4988) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4989) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4991) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4992) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4993) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4995) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4997) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4998) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5000) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5005) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5006) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5008) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5012) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5013) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5014) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5015) tree.c:754: if (oldval)
  (<0>,5023) tree_plugin.h:3145: }
  (<0>,5025) tree.c:759: local_irq_restore(flags);
  (<0>,5028) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5030) fake_sched.h:43: return __running_cpu;
  (<0>,5034) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5036) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5044) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,5049) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,5054) fake_sched.h:43: return __running_cpu;
  (<0>,5059) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,5067) fake_sched.h:43: return __running_cpu;
  (<0>,5072) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
  (<0>,5078) fake_sched.h:43: return __running_cpu;
  (<0>,5083) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
  (<0>,5096) tree.c:3185: struct rcu_state *rsp;
  (<0>,5097) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5098) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5102) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5103) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5104) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5106) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5110) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5112) fake_sched.h:43: return __running_cpu;
  (<0>,5115) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5117) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5124)
  (<0>,5125) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5126) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5128) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5129) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5130) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5132) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5134) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5135) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5136) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5146)
  (<0>,5147) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5148) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5153) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
  (<0>,5156) tree_plugin.h:3185: return 0;
  (<0>,5159) tree.c:3135: if (rcu_scheduler_fully_active &&
  (<0>,5162) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5164) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5167) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5169) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5173) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,5176)
  (<0>,5177) tree.c:442: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,5179) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5182) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5189) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5190) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5196)
  (<0>,5197) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5198) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5201)
  (<0>,5202) tree.c:176: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,5204) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5205) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5207) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5213) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5219)
  (<0>,5220) tree.c:461: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,5223)
  (<0>,5224) tree.c:453: return &rsp->node[0];
  (<0>,5228) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5229) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5231) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5235) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5236) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5238) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5241) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5242) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,1)
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:704: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,56) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,58) fake_sched.h:43: return __running_cpu;
      (<0.1>,62) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,63) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,67) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,72) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,73) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,77) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,78) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,79) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,80) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,84) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,86) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,87) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,88) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,102)
      (<0.1>,103) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,105) fake_sched.h:43: return __running_cpu;
      (<0.1>,109) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,113) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,116) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,117) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,118) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,120) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,123) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,126) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,133) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,137) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,143) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,144) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,149) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,158) tree_plugin.h:3145: }
      (<0.1>,160) tree.c:707: local_irq_restore(flags);
      (<0.1>,163) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,165) fake_sched.h:43: return __running_cpu;
      (<0.1>,169) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,171) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,175) fake_sched.h:43: return __running_cpu;
      (<0.1>,179) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,186) litmus.c:87: x = 1;
      (<0.1>,196) tree.c:2787: ret = num_online_cpus() <= 1;
      (<0.1>,198) tree.c:2789: return ret;
      (<0.1>,202) tree.c:2841: if (rcu_expedited)
      (<0.1>,208) update.c:223: init_rcu_head_on_stack(&rcu.head);
      (<0.1>,212) rcupdate.h:401: }
      (<0.1>,217)
      (<0.1>,218) fake_sync.h:232: x->done = 0;
      (<0.1>,220) fake_sync.h:232: x->done = 0;
      (<0.1>,224) update.c:226: crf(&rcu.head, wakeme_after_rcu);
      (<0.1>,229)
      (<0.1>,230) tree.c:2743: void call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
      (<0.1>,231) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,232) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,249)
      (<0.1>,250) tree.c:2683: __call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu),
      (<0.1>,251) tree.c:2683: __call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu),
      (<0.1>,252) tree.c:2684: struct rcu_state *rsp, int cpu, bool lazy)
      (<0.1>,254)
      (<0.1>,255) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,262) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,263) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,269) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,270) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,271) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,272) tree.c:2690: if (debug_rcu_head_queue(head)) {
      (<0.1>,275) rcu.h:92: return 0;
      (<0.1>,279) tree.c:2696: head->func = func;
      (<0.1>,280) tree.c:2696: head->func = func;
      (<0.1>,282) tree.c:2696: head->func = func;
      (<0.1>,283) tree.c:2697: head->next = NULL;
      (<0.1>,285) tree.c:2697: head->next = NULL;
      (<0.1>,286) tree.c:2705: local_irq_save(flags);
      (<0.1>,289) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,291) fake_sched.h:43: return __running_cpu;
      (<0.1>,295) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,297) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,301) fake_sched.h:43: return __running_cpu;
      (<0.1>,305) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,311) fake_sched.h:43: return __running_cpu;
      (<0.1>,314) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,316) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,318) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,319) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,322) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,330) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,333) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,335) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,337) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,339) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,340) tree.c:2721: if (lazy)
      (<0.1>,347) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,348) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,351) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,352) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,353) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,355) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,358) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,359) tree.c:2729: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,366) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,367) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,368) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,369) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,377)
      (<0.1>,378) tree.c:2619: static void __call_rcu_core(struct rcu_state *rsp, struct rcu_data *rdp,
      (<0.1>,379) tree.c:2619: static void __call_rcu_core(struct rcu_state *rsp, struct rcu_data *rdp,
      (<0.1>,380) tree.c:2628: if (!rcu_is_watching() && cpu_online(smp_processor_id()))
      (<0.1>,386) fake_sched.h:43: return __running_cpu;
      (<0.1>,392) tree.c:815: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,397) tree.c:829: ret = __rcu_is_watching();
      (<0.1>,399) tree.c:831: return ret;
      (<0.1>,403) tree.c:2632: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,406) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,408) fake_sched.h:43: return __running_cpu;
      (<0.1>,412) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,422) tree.c:2737: local_irq_restore(flags);
      (<0.1>,425) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,427) fake_sched.h:43: return __running_cpu;
      (<0.1>,431) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,433) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,437) fake_sched.h:43: return __running_cpu;
      (<0.1>,441) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,452) fake_sync.h:238: might_sleep();
      (<0.1>,456) fake_sched.h:43: return __running_cpu;
      (<0.1>,460) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,463) tree.c:580: local_irq_save(flags);
      (<0.1>,466) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,468) fake_sched.h:43: return __running_cpu;
      (<0.1>,472) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,474) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,478) fake_sched.h:43: return __running_cpu;
      (<0.1>,482) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,495) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,497) fake_sched.h:43: return __running_cpu;
      (<0.1>,501) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,502) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,504) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,505) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,506) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,512) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,513) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,518) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,519) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,520) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,521) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,525) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,527) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,528) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,529) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,548)
      (<0.1>,550) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,552) fake_sched.h:43: return __running_cpu;
      (<0.1>,556) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,559) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,563) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,564) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,565) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,569) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,570) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,571) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,573) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,578) fake_sched.h:43: return __running_cpu;
      (<0.1>,581) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,583) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,585) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,586) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,589) tree_plugin.h:2720: }
      (<0.1>,592) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,595) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,596) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,597) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,601) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,602) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,603) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,605) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,610) fake_sched.h:43: return __running_cpu;
      (<0.1>,613) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,615) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,617) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,618) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,621) tree_plugin.h:2720: }
      (<0.1>,624) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,627) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,628) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,629) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,633) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,635) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,637) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,644) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,647) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,648) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,649) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,651) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,652) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,654) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,657) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,663) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,664) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,667) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,672) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,673) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,674) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,688) tree_plugin.h:3141: }
      (<0.1>,690) tree.c:583: local_irq_restore(flags);
      (<0.1>,693) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,695) fake_sched.h:43: return __running_cpu;
      (<0.1>,699) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,701) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,705) fake_sched.h:43: return __running_cpu;
      (<0.1>,709) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,715) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,718) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:704: local_irq_save(flags);
    (<0.0>,35) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,64) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,66) fake_sched.h:43: return __running_cpu;
    (<0.0>,70) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,71) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,75) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,80) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,81) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,85) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,86) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,87) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,88) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,92) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,94) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,95) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,96) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,110)
    (<0.0>,111) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,113) fake_sched.h:43: return __running_cpu;
    (<0.0>,117) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,124) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,125) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,126) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,128) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,129) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,131) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,141) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,145) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,150) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,151) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,152) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,157) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,166) tree_plugin.h:3145: }
    (<0.0>,168) tree.c:707: local_irq_restore(flags);
    (<0.0>,171) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,173) fake_sched.h:43: return __running_cpu;
    (<0.0>,177) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,179) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,183) fake_sched.h:43: return __running_cpu;
    (<0.0>,187) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,194) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,206)
    (<0.0>,207) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,209) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,213)
    (<0.0>,214) tree.c:453: return &rsp->node[0];
    (<0.0>,218) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,223) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,225) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) fake_sched.h:43: return __running_cpu;
    (<0.0>,233) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,237) fake_sched.h:43: return __running_cpu;
    (<0.0>,241) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,246) fake_sched.h:43: return __running_cpu;
    (<0.0>,250) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,260) tree.c:749: local_irq_save(flags);
    (<0.0>,263) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,271) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,276) fake_sched.h:43: return __running_cpu;
    (<0.0>,280) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,281) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,283) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,284) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,285) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,287) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,289) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,290) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,292) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,297) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,298) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,300) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,304) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,305) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,306) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,307) tree.c:754: if (oldval)
    (<0.0>,315) tree_plugin.h:3145: }
    (<0.0>,317) tree.c:759: local_irq_restore(flags);
    (<0.0>,320) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,322) fake_sched.h:43: return __running_cpu;
    (<0.0>,326) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,328) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,336) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,341) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,346) fake_sched.h:43: return __running_cpu;
    (<0.0>,351) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,359) fake_sched.h:43: return __running_cpu;
    (<0.0>,364) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,370) fake_sched.h:43: return __running_cpu;
    (<0.0>,375) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,388) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,389) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,390) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,394) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,395) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,396) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,398) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,402) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,404) fake_sched.h:43: return __running_cpu;
    (<0.0>,407) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,409) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,416)
    (<0.0>,417)
    (<0.0>,418) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,420) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,421) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,422) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,424) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,426) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,427) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,428) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,438)
    (<0.0>,439)
    (<0.0>,440) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,445) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,448) tree_plugin.h:3185: return 0;
    (<0.0>,451) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,454) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,456) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,459) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,461) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,465) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,468)
    (<0.0>,469) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,471) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,474) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,481) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,482) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,488)
    (<0.0>,489)
    (<0.0>,490) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,493)
    (<0.0>,494) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,496) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,497) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,499) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,505) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,511)
    (<0.0>,512) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,515)
    (<0.0>,516) tree.c:453: return &rsp->node[0];
    (<0.0>,520) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,521) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,523) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,527) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,528) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,530) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,533) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,534) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,535) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,539) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,542) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,545) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,548) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,549) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,552) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,554) tree.c:494: }
    (<0.0>,558) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,560) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,562) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,563) tree.c:3152: return 1;
    (<0.0>,565) tree.c:3176: }
    (<0.0>,569) tree.c:3189: return 1;
    (<0.0>,571) tree.c:3191: }
    (<0.0>,578) fake_sched.h:43: return __running_cpu;
    (<0.0>,582) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,586) tree.c:2437: if (user)
    (<0.0>,594) fake_sched.h:43: return __running_cpu;
    (<0.0>,598) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,600) fake_sched.h:43: return __running_cpu;
    (<0.0>,604) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,610) fake_sched.h:43: return __running_cpu;
    (<0.0>,614) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,624) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,627) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,628) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,629) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,633) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,634) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,635) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,637) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,641) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,650) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,652) fake_sched.h:43: return __running_cpu;
    (<0.0>,655) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,657) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,659) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,660) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,662) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,669) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,670) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,672) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,678) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,679) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,680) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,681) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,682) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,686)
    (<0.0>,687) tree.c:2075: rcu_check_quiescent_state(struct rcu_state *rsp, struct rcu_data *rdp)
    (<0.0>,688) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,689) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,696)
    (<0.0>,697) tree.c:1578: static void note_gp_changes(struct rcu_state *rsp, struct rcu_data *rdp)
    (<0.0>,698) tree.c:1584: local_irq_save(flags);
    (<0.0>,701) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,703) fake_sched.h:43: return __running_cpu;
    (<0.0>,707) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,709) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,713) fake_sched.h:43: return __running_cpu;
    (<0.0>,717) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,722) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,724) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,725) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,726) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,728) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,729) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,731) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,734) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,736) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,737) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,739) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,742) tree.c:1589: local_irq_restore(flags);
    (<0.0>,745) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,747) fake_sched.h:43: return __running_cpu;
    (<0.0>,751) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,753) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,757) fake_sched.h:43: return __running_cpu;
    (<0.0>,761) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,768) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,770) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,775) tree.c:2558: local_irq_save(flags);
    (<0.0>,778) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,780) fake_sched.h:43: return __running_cpu;
    (<0.0>,784) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,786) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,790) fake_sched.h:43: return __running_cpu;
    (<0.0>,794) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,799) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,800) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,806)
    (<0.0>,807)
    (<0.0>,808) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,811)
    (<0.0>,812) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,814) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,815) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,817) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,823) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,829)
    (<0.0>,830) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,833)
    (<0.0>,834) tree.c:453: return &rsp->node[0];
    (<0.0>,838) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,839) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,841) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,845) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,846) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,848) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,851) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,852) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,853) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,857) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,860) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,863) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,866) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,867) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,870) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,872) tree.c:494: }
    (<0.0>,876) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,879)
    (<0.0>,880) tree.c:453: return &rsp->node[0];
    (<0.0>,887) fake_sync.h:108: preempt_disable();
    (<0.0>,889) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,890) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,894) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,900) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,902) fake_sched.h:43: return __running_cpu;
    (<0.0>,905) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,907) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,909) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,910) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,913)
    (<0.0>,914) tree.c:453: return &rsp->node[0];
    (<0.0>,918) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,919) tree.c:1925: bool ret = false;
    (<0.0>,920) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,921) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,922) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,930)
    (<0.0>,931) tree.c:1495: static bool rcu_advance_cbs(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,932) tree.c:1495: static bool rcu_advance_cbs(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,933) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,936) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,939) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,942) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,943) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,946) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,948) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,951) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,953) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,954) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,956) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,959) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,964) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,966) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,967) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,970) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,972) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,975) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,977) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,980) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,981) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,984) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,987) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,989) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,992) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,993) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,995) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,998) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,999) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1001) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1004) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1005) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1007) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1010) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1012) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1014) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1015) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1017) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1019) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1022) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1024) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1027) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1028) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1031) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1034) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1036) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1039) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1040) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1042) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1045) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1046) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1048) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1051) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1052) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1054) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1057) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1059) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1061) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1062) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1064) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1066) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1069) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1070) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1071) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1080)
    (<0.0>,1081) tree.c:1425: static bool rcu_accelerate_cbs(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,1082) tree.c:1425: static bool rcu_accelerate_cbs(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,1083) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1086) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1089) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1092) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1093) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1096) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1097) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1102)
    (<0.0>,1103) tree.c:1243: static unsigned long rcu_cbs_completed(struct rcu_state *rsp,
    (<0.0>,1104) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1107)
    (<0.0>,1108) tree.c:453: return &rsp->node[0];
    (<0.0>,1112) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1115) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1117) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1118) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1120) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1123) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1125) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1127) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1129) tree.c:1261: }
    (<0.0>,1131) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1132) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1134) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1137) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1139) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1142) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1143) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1146) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1149) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1153) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1155) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1157) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1160) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1162) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1165) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1166) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1169) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1172) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1176) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1178) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1180) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1183) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1185) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1189) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1192) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1195) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1196) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1198) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1201) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1202) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1203) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1205) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1208) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1210) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1212) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1214) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1217) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1220) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1221) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1223) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1226) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1227) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1228) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1230) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1233) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1235) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1237) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1239) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1242) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1245) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1246) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1248) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1251) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1252) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1253) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1255) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1258) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1260) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1262) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1264) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1267) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1268) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1277)
    (<0.0>,1278) tree.c:1284: rcu_start_future_gp(struct rcu_node *rnp, struct rcu_data *rdp,
    (<0.0>,1279) tree.c:1284: rcu_start_future_gp(struct rcu_node *rnp, struct rcu_data *rdp,
    (<0.0>,1280) tree.c:1289: bool ret = false;
    (<0.0>,1281) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1283) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1286)
    (<0.0>,1287) tree.c:453: return &rsp->node[0];
    (<0.0>,1291) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1292) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1294) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1295) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1300)
    (<0.0>,1301)
    (<0.0>,1302) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1305)
    (<0.0>,1306) tree.c:453: return &rsp->node[0];
    (<0.0>,1310) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1313) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1315) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1316) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1318) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1321) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1323) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1325) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1327) tree.c:1261: }
    (<0.0>,1329) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1330) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1331) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1332) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1338)
    (<0.0>,1339) tree.c:1267: static void trace_rcu_future_gp(struct rcu_node *rnp, struct rcu_data *rdp,
    (<0.0>,1340) tree.c:1267: static void trace_rcu_future_gp(struct rcu_node *rnp, struct rcu_data *rdp,
    (<0.0>,1341) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1345) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1347) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1350) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1353) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1355) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1356) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1358) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1361) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1363) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1364) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1366) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1369) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1370) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1373) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1375) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1376) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1381)
    (<0.0>,1382)
    (<0.0>,1383) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1386)
    (<0.0>,1387) tree.c:453: return &rsp->node[0];
    (<0.0>,1391) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1394) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1396) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1397) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1399) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1402) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1404) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1406) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1408) tree.c:1261: }
    (<0.0>,1410) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1411) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1413) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1416) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1417) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1419) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1422) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1426) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1427) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1429) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1432) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1435) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1437) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1439) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1442) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1443) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1445) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1448) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1453) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1455) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1457) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1460) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1461) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1463) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1466) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1471) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1473) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1475) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1478) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1480) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1483) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1486) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1488) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1491) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1493) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
  (<0>,5243) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5247) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
  (<0>,5249) tree.c:494: }
  (<0>,5253) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
  (<0>,5255) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
  (<0>,5257) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
  (<0>,5258) tree.c:3152: return 1;
  (<0>,5260) tree.c:3176: }
  (<0>,5264) tree.c:3189: return 1;
  (<0>,5266) tree.c:3191: }
  (<0>,5273) fake_sched.h:43: return __running_cpu;
  (<0>,5277) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
  (<0>,5281) tree.c:2437: if (user)
  (<0>,5289) fake_sched.h:43: return __running_cpu;
  (<0>,5293) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
  (<0>,5295) fake_sched.h:43: return __running_cpu;
  (<0>,5299) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5305) fake_sched.h:43: return __running_cpu;
  (<0>,5309) fake_sched.h:182: if (need_softirq[get_cpu()]) {
  (<0>,5319) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,5322) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5323) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5324) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5328) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5329) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5330) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5332) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5336) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,5345) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5347) fake_sched.h:43: return __running_cpu;
  (<0>,5350) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5352) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5354) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5355) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5357) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5364) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5365) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5367) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5373) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5374) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5375) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5376) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5377) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5381)
  (<0>,5382)
  (<0>,5383) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5384) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5391)
  (<0>,5392)
  (<0>,5393) tree.c:1584: local_irq_save(flags);
  (<0>,5396) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5398) fake_sched.h:43: return __running_cpu;
  (<0>,5402) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5404) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5408) fake_sched.h:43: return __running_cpu;
  (<0>,5412) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5417) tree.c:1585: rnp = rdp->mynode;
  (<0>,5419) tree.c:1585: rnp = rdp->mynode;
  (<0>,5420) tree.c:1585: rnp = rdp->mynode;
  (<0>,5421) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5423) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5424) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5426) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5429) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5431) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5432) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5434) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5437) tree.c:1589: local_irq_restore(flags);
  (<0>,5440) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5442) fake_sched.h:43: return __running_cpu;
  (<0>,5446) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5448) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5452) fake_sched.h:43: return __running_cpu;
  (<0>,5456) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5463) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5465) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5470) tree.c:2558: local_irq_save(flags);
  (<0>,5473) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5475) fake_sched.h:43: return __running_cpu;
  (<0>,5479) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5481) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5485) fake_sched.h:43: return __running_cpu;
  (<0>,5489) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5494) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5495) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5501)
  (<0>,5502)
  (<0>,5503) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,5506)
  (<0>,5507) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5509) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5510) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5512) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5518) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5524)
  (<0>,5525) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5528)
  (<0>,5529) tree.c:453: return &rsp->node[0];
  (<0>,5533) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5534) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5536) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5540) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5541) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5543) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5546) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5547) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5548) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5552) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
  (<0>,5554) tree.c:494: }
  (<0>,5558) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
  (<0>,5561)
  (<0>,5562) tree.c:453: return &rsp->node[0];
  (<0>,5569) fake_sync.h:108: preempt_disable();
  (<0>,5571) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,1494) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1496) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1497) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1499) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1502) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1503) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1504) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1510)
    (<0.0>,1511)
    (<0.0>,1512)
    (<0.0>,1513) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1517) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1519) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1520) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1521) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1527)
    (<0.0>,1528) tree.c:1888: rcu_start_gp_advanced(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,1529) tree.c:1888: rcu_start_gp_advanced(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,1530) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1532) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1535) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1536) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1542)
    (<0.0>,1543)
    (<0.0>,1544) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1547)
    (<0.0>,1548) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1550) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1551) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1553) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1559) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1565)
    (<0.0>,1566) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1569)
    (<0.0>,1570) tree.c:453: return &rsp->node[0];
    (<0.0>,1574) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1575) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1577) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1581) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1582) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1584) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1587) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1588) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1589) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1593) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1595) tree.c:494: }
    (<0.0>,1599) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1601) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1604) tree.c:1909: return true;
    (<0.0>,1606) tree.c:1910: }
    (<0.0>,1609) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1612) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1613) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1617) tree.c:1372: if (c_out != NULL)
    (<0.0>,1620) tree.c:1374: return ret;
    (<0.0>,1624) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1625) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1628) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1629) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1635) tree.c:1482: return ret;
    (<0.0>,1637) tree.c:1482: return ret;
    (<0.0>,1639) tree.c:1483: }
    (<0.0>,1641) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1643) tree.c:1527: }
    (<0.0>,1647) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1648) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1649) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1650) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1656)
    (<0.0>,1657)
    (<0.0>,1658)
    (<0.0>,1659) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1661) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1664) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1665) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1671)
    (<0.0>,1672)
    (<0.0>,1673) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1676)
    (<0.0>,1677) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1679) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1680) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1682) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1688) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1694)
    (<0.0>,1695) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1698)
    (<0.0>,1699) tree.c:453: return &rsp->node[0];
    (<0.0>,1703) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1704) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1706) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1710) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1711) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1713) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1716) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1717) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1718) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1722) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1724) tree.c:494: }
    (<0.0>,1728) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1730) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1733) tree.c:1909: return true;
    (<0.0>,1735) tree.c:1910: }
    (<0.0>,1739) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1740) tree.c:1937: return ret;
    (<0.0>,1744) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,1745) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1748)
    (<0.0>,1749) tree.c:453: return &rsp->node[0];
    (<0.0>,1754) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1758)
    (<0.0>,1759)
    (<0.0>,1760) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1761) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1762) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1764) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,1767) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1769) fake_sched.h:43: return __running_cpu;
    (<0.0>,1773) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1775) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1779) fake_sched.h:43: return __running_cpu;
    (<0.0>,1783) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1790) tree.c:2563: if (needwake)
    (<0.0>,1793) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,1796)
    (<0.0>,1797) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1798) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1800) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1807) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,1810)
    (<0.0>,1811) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1813) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1816) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1823) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,1826) tree_plugin.h:2720: }
    (<0.0>,1830) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1833) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1834) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1835) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1839) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1840) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1841) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1843) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1847) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,1856) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1858) fake_sched.h:43: return __running_cpu;
    (<0.0>,1861) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1863) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1865) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1866) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1868) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1875) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1876) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1878) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1884) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1885) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1886) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1887) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1888) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1892)
    (<0.0>,1893)
    (<0.0>,1894) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1895) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1902)
    (<0.0>,1903)
    (<0.0>,1904) tree.c:1584: local_irq_save(flags);
    (<0.0>,1907) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1909) fake_sched.h:43: return __running_cpu;
    (<0.0>,1913) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1915) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1919) fake_sched.h:43: return __running_cpu;
    (<0.0>,1923) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1928) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1930) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1931) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1932) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1934) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1935) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1937) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1940) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1942) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1943) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1945) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1948) tree.c:1589: local_irq_restore(flags);
    (<0.0>,1951) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1953) fake_sched.h:43: return __running_cpu;
    (<0.0>,1957) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1959) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1963) fake_sched.h:43: return __running_cpu;
    (<0.0>,1967) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1974) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1976) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1981) tree.c:2558: local_irq_save(flags);
    (<0.0>,1984) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1986) fake_sched.h:43: return __running_cpu;
    (<0.0>,1990) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1992) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1996) fake_sched.h:43: return __running_cpu;
    (<0.0>,2000) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2005) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2006) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2012)
    (<0.0>,2013)
    (<0.0>,2014) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,2017)
    (<0.0>,2018) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2020) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2021) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2023) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2029) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,2035)
    (<0.0>,2036) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2039)
    (<0.0>,2040) tree.c:453: return &rsp->node[0];
    (<0.0>,2044) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2045) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2047) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2051) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2052) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2054) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2057) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2058) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2059) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2063) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2066) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2069) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2072) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2073) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2076) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2078) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2081) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2084) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2087) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2088) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2090) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2093) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2097) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2099) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2101) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2104) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2107) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2110) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2111) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2113) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2116) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2120) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2122) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2124) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2127) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,2129) tree.c:494: }
    (<0.0>,2133) tree.c:2566: local_irq_restore(flags);
    (<0.0>,2136) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2138) fake_sched.h:43: return __running_cpu;
    (<0.0>,2142) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2144) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2148) fake_sched.h:43: return __running_cpu;
    (<0.0>,2152) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2158) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2161)
    (<0.0>,2162) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2164) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2167) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2174) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2177) tree_plugin.h:2720: }
    (<0.0>,2181) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2184) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2185) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2186) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2190) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2191) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2192) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2194) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2202) fake_sched.h:43: return __running_cpu;
    (<0.0>,2206) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,2215) tree.c:624: local_irq_save(flags);
    (<0.0>,2218) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2220) fake_sched.h:43: return __running_cpu;
    (<0.0>,2224) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2226) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2230) fake_sched.h:43: return __running_cpu;
    (<0.0>,2234) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2240) fake_sched.h:43: return __running_cpu;
    (<0.0>,2244) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2245) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2247) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2248) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2249) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2251) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2253) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2254) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2256) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2261) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2262) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2264) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2268) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2269) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2270) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2271) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2273) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2281) tree_plugin.h:3141: }
    (<0.0>,2283) tree.c:634: local_irq_restore(flags);
    (<0.0>,2286) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2288) fake_sched.h:43: return __running_cpu;
    (<0.0>,2292) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2294) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2298) fake_sched.h:43: return __running_cpu;
    (<0.0>,2302) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2311) fake_sched.h:43: return __running_cpu;
    (<0.0>,2315) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,2318) tree.c:580: local_irq_save(flags);
    (<0.0>,2321) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2323) fake_sched.h:43: return __running_cpu;
    (<0.0>,2327) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2329) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2333) fake_sched.h:43: return __running_cpu;
    (<0.0>,2337) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2350) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2352) fake_sched.h:43: return __running_cpu;
    (<0.0>,2356) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2357) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2359) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2360) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2361) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2367) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2368) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2373) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2374) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2375) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2376) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,2380) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2382) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2383) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2384) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2403)
    (<0.0>,2405) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2407) fake_sched.h:43: return __running_cpu;
    (<0.0>,2411) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2414) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,2418) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2419) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2420) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2424) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2425) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2426) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2428) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2433) fake_sched.h:43: return __running_cpu;
    (<0.0>,2436) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2438) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2440) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2441) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2444) tree_plugin.h:2720: }
    (<0.0>,2447) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2450) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2451) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2452) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2456) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2457) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2458) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2460) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2465) fake_sched.h:43: return __running_cpu;
    (<0.0>,2468) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2470) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2472) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2473) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2476) tree_plugin.h:2720: }
    (<0.0>,2479) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2482) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2483) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2484) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2488) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2489) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2490) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2492) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2499) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2502) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2503) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2504) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2506) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2507) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2509) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2512) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2518) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2519) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2522) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2527) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2528) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2529) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2543) tree_plugin.h:3141: }
    (<0.0>,2545) tree.c:583: local_irq_restore(flags);
    (<0.0>,2548) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2550) fake_sched.h:43: return __running_cpu;
    (<0.0>,2554) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2556) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2560) fake_sched.h:43: return __running_cpu;
    (<0.0>,2564) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2570) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2573) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2578) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2580) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2589) fake_sched.h:43: return __running_cpu;
    (<0.0>,2593)
    (<0.0>,2594) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2597) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2602) tree.c:704: local_irq_save(flags);
    (<0.0>,2605) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2607) fake_sched.h:43: return __running_cpu;
    (<0.0>,2611) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2613) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2617) fake_sched.h:43: return __running_cpu;
    (<0.0>,2621) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2634) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2636) fake_sched.h:43: return __running_cpu;
    (<0.0>,2640) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2641) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2643) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2644) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2645) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2650) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2651) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2655) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2656) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2657) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2658) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,2662) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2664) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2665) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2666) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2680)
    (<0.0>,2681) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2683) fake_sched.h:43: return __running_cpu;
    (<0.0>,2687) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2691) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2694) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2695) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2696) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2698) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2699) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2701) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2704) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2711) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2712) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2715) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2720) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2721) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2722) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2727) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,2736) tree_plugin.h:3145: }
    (<0.0>,2738) tree.c:707: local_irq_restore(flags);
    (<0.0>,2741) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2743) fake_sched.h:43: return __running_cpu;
    (<0.0>,2747) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2749) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2753) fake_sched.h:43: return __running_cpu;
    (<0.0>,2757) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2764) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,2776)
    (<0.0>,2777) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2780)
    (<0.0>,2781) tree.c:453: return &rsp->node[0];
    (<0.0>,2785) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2789) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2793) fake_sync.h:92: local_irq_disable();
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2804) fake_sched.h:43: return __running_cpu;
    (<0.0>,2808) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2813) fake_sched.h:43: return __running_cpu;
    (<0.0>,2817) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,2820) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2821) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2827) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2829) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2832) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2834) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2835) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2838)
    (<0.0>,2839) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2841) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2842) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2844) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2852) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2853) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2856)
    (<0.0>,2857) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2859) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2860) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2870) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2871) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2874) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,2879)
    (<0.0>,2880) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2881) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2882) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2883) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2885) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2889) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2890) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2891) update.c:344: if (till_stall_check < 3) {
    (<0.0>,2894) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,2898) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,2903) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,2904) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2905) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2907) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2909) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2910) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2911) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2914) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2916) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2920) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2922) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2924) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2926) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2930) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,2934)
    (<0.0>,2935) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2936) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2937) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2941) fake_sched.h:43: return __running_cpu;
    (<0.0>,2945) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,2947) fake_sched.h:43: return __running_cpu;
    (<0.0>,2951) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2957) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,2961)
    (<0.0>,2962) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2964) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2970) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2973) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2975) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2976) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2978) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2983) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2987) fake_sync.h:92: local_irq_disable();
    (<0.0>,2990) fake_sched.h:43: return __running_cpu;
    (<0.0>,2994) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2998) fake_sched.h:43: return __running_cpu;
    (<0.0>,3002) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3007) fake_sched.h:43: return __running_cpu;
    (<0.0>,3011) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3014) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3015) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3022) fake_sched.h:43: return __running_cpu;
    (<0.0>,3025) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3027) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3029) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3030) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,3036)
    (<0.0>,3037) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3039) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3044) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3045) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3047) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3051) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3052) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3053) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3055) tree.c:1656: rnp->qsmask = 0;
    (<0.0>,3057) tree.c:1656: rnp->qsmask = 0;
    (<0.0>,3058) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3060) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3061) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3063) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3064) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3066) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3067) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3069) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3074) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3075) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3077) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3078) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3080) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3084) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3085) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3086) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3087) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3089) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3090) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3092) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3093) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3094) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3096) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3099) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3100) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3101) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3107)
    (<0.0>,3108) tree.c:1535: static bool __note_gp_changes(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,3109) tree.c:1535: static bool __note_gp_changes(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,3110) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3112) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3113) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3115) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3118) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3119) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3120) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3129)
    (<0.0>,3130)
    (<0.0>,3131)
    (<0.0>,3132) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3135) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3138) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3141) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3142) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3145) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3146) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3151)
    (<0.0>,3152)
    (<0.0>,3153) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3156)
    (<0.0>,3157) tree.c:453: return &rsp->node[0];
    (<0.0>,3161) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3164) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3166) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3167) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3169) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3172) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3174) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3176) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3178) tree.c:1261: }
    (<0.0>,3180) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3181) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3183) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3186) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3188) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3191) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3192) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3195) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3198) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3202) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3204) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3206) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3209) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3211) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3214) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3215) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3218) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3221) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3224) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3226) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3229) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3230) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3235) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3237) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3241) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3244) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3247) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3248) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3250) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3253) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3254) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3255) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3257) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3260) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3262) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3264) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3266) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3269) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3272) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3273) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3275) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3278) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3279) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3280) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3282) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3285) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3287) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3289) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3291) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3294) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3295) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3304)
    (<0.0>,3305)
    (<0.0>,3306)
    (<0.0>,3307) tree.c:1289: bool ret = false;
    (<0.0>,3308) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3310) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3313)
    (<0.0>,3314) tree.c:453: return &rsp->node[0];
    (<0.0>,3318) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3319) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3321) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3322) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3327)
    (<0.0>,3328)
    (<0.0>,3329) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3332)
    (<0.0>,3333) tree.c:453: return &rsp->node[0];
    (<0.0>,3337) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3340) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3342) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3343) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3345) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3348) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3350) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3352) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3354) tree.c:1261: }
    (<0.0>,3356) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3357) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3358) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3359) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3365)
    (<0.0>,3366)
    (<0.0>,3367)
    (<0.0>,3368) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,3372) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3374) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3377) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3380) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3382) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3383) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3385) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3388) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3390) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3393) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3395) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3396) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3397) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3398) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3404)
    (<0.0>,3405)
    (<0.0>,3406)
    (<0.0>,3407) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,3412) tree.c:1372: if (c_out != NULL)
    (<0.0>,3415) tree.c:1374: return ret;
    (<0.0>,3419) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3420) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3423) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3424) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3430) tree.c:1482: return ret;
    (<0.0>,3432) tree.c:1482: return ret;
    (<0.0>,3434) tree.c:1483: }
    (<0.0>,3437) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3439) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3441) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3442) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3444) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3447) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3449) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3450) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3452) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3455) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3457) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3458) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3460) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3461) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3463) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3468) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3471) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3472) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,3475) tree_plugin.h:1950: }
    (<0.0>,3478) tree.c:1575: return ret;
    (<0.0>,3482) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,3485) tree_plugin.h:1487: }
    (<0.0>,3489) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,3493)
    (<0.0>,3494) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3495) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3496) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3500) fake_sched.h:43: return __running_cpu;
    (<0.0>,3504) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,3506) fake_sched.h:43: return __running_cpu;
    (<0.0>,3510) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3523) fake_sched.h:43: return __running_cpu;
    (<0.0>,3528) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,3534) fake_sched.h:43: return __running_cpu;
    (<0.0>,3539) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,3545) fake_sched.h:43: return __running_cpu;
    (<0.0>,3549) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,3561) fake_sched.h:43: return __running_cpu;
    (<0.0>,3565) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,3568) tree.c:580: local_irq_save(flags);
    (<0.0>,3571) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3573) fake_sched.h:43: return __running_cpu;
    (<0.0>,3577) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3579) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3583) fake_sched.h:43: return __running_cpu;
    (<0.0>,3587) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3600) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3602) fake_sched.h:43: return __running_cpu;
    (<0.0>,3606) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3607) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3609) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3610) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3611) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3617) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3618) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3623) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3624) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3625) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3626) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3630) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3632) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3633) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3634) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3653)
    (<0.0>,3655) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3657) fake_sched.h:43: return __running_cpu;
    (<0.0>,3661) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3664) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,3668) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3669) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3670) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3674) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3675) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3676) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3678) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3683) fake_sched.h:43: return __running_cpu;
    (<0.0>,3686) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3688) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3690) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3691) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3694) tree_plugin.h:2720: }
    (<0.0>,3697) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3700) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3701) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3702) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3706) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3707) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3708) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3710) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3715) fake_sched.h:43: return __running_cpu;
    (<0.0>,3718) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3720) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3722) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3723) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3726) tree_plugin.h:2720: }
    (<0.0>,3729) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3732) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3733) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3734) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3738) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3739) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3740) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3742) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3749) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3752) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3753) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3754) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3756) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3757) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3759) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3762) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3768) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3769) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3772) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3777) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3778) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3779) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3793) tree_plugin.h:3141: }
    (<0.0>,3795) tree.c:583: local_irq_restore(flags);
    (<0.0>,3798) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3800) fake_sched.h:43: return __running_cpu;
    (<0.0>,3804) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3806) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3810) fake_sched.h:43: return __running_cpu;
    (<0.0>,3814) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3820) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3823) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3828) fake_sched.h:43: return __running_cpu;
    (<0.0>,3832)
    (<0.0>,3833) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3836) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3841) tree.c:704: local_irq_save(flags);
    (<0.0>,3844) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3846) fake_sched.h:43: return __running_cpu;
    (<0.0>,3850) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3852) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3856) fake_sched.h:43: return __running_cpu;
    (<0.0>,3860) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3873) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3875) fake_sched.h:43: return __running_cpu;
    (<0.0>,3879) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3880) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3882) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3883) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3884) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3889) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3890) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3894) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3895) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3896) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3897) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3901) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3903) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3904) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3905) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3919)
    (<0.0>,3920) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3922) fake_sched.h:43: return __running_cpu;
    (<0.0>,3926) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3930) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3933) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3934) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3935) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3937) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3938) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3940) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3943) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3950) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3951) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3954) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3959) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3960) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3961) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3966) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,3975) tree_plugin.h:3145: }
    (<0.0>,3977) tree.c:707: local_irq_restore(flags);
    (<0.0>,3980) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3982) fake_sched.h:43: return __running_cpu;
    (<0.0>,3986) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3988) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3992) fake_sched.h:43: return __running_cpu;
    (<0.0>,3996) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4011) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4013) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4015) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4016) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4018) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4023) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,4027)
    (<0.0>,4028) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4030) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4034) tree.c:1674: return 1;
    (<0.0>,4036) tree.c:1675: }
    (<0.0>,4041) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,4042) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4043) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4044) tree.c:1819: if (j > HZ) {
    (<0.0>,4047) tree.c:1823: ret = 0;
    (<0.0>,4049) tree.c:1825: if (!ret)
    (<0.0>,4052) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4053) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4055) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4057) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4061) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4063) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4067) fake_sched.h:43: return __running_cpu;
    (<0.0>,4071) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4075) fake_sched.h:43: return __running_cpu;
    (<0.0>,4079) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4084) fake_sched.h:43: return __running_cpu;
    (<0.0>,4088) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4098) tree.c:749: local_irq_save(flags);
    (<0.0>,4101) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4103) fake_sched.h:43: return __running_cpu;
    (<0.0>,4107) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4109) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4114) fake_sched.h:43: return __running_cpu;
    (<0.0>,4118) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4119) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4121) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4122) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4123) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4125) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4127) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4128) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4130) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4135) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4136) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4138) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4142) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4143) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4144) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4145) tree.c:754: if (oldval)
    (<0.0>,4153) tree_plugin.h:3145: }
    (<0.0>,4155) tree.c:759: local_irq_restore(flags);
    (<0.0>,4158) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4160) fake_sched.h:43: return __running_cpu;
    (<0.0>,4164) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4166) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4174) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,4179) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,4184) fake_sched.h:43: return __running_cpu;
    (<0.0>,4189) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,4197) fake_sched.h:43: return __running_cpu;
    (<0.0>,4202) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,4216) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4217) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4218) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4222) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4223) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4224) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4226) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4230) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4232) fake_sched.h:43: return __running_cpu;
    (<0.0>,4235) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4237) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4244)
    (<0.0>,4245)
    (<0.0>,4246) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4248) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4249) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4250) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4252) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4254) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4255) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4256) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4266)
    (<0.0>,4267)
    (<0.0>,4268) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,4273) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,4276) tree_plugin.h:3185: return 0;
    (<0.0>,4279) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,4282) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4284) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4287) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4289) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4293) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,4296)
    (<0.0>,4297) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4299) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4302) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4309) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4310) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4316)
    (<0.0>,4317)
    (<0.0>,4318) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,4321)
    (<0.0>,4322) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4324) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4325) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4327) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4333) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,4335) tree.c:494: }
    (<0.0>,4339) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4341) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4342) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4344) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4347) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4349) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4350) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4352) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4355) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,4358) tree_plugin.h:2715: return false;
    (<0.0>,4362) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4364) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4366) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4367) tree.c:3175: return 0;
    (<0.0>,4369) tree.c:3176: }
    (<0.0>,4374) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4377) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4378) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4379) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4383) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4384) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4385) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4387) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4391) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4393) fake_sched.h:43: return __running_cpu;
    (<0.0>,4396) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4398) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4405)
    (<0.0>,4406)
    (<0.0>,4407) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4409) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4410) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4411) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4413) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4415) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4416) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4417) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4427)
    (<0.0>,4428)
    (<0.0>,4429) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,4434) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,4437) tree_plugin.h:3185: return 0;
    (<0.0>,4440) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,4443) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4445) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4448) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4450) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4454) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,4457)
    (<0.0>,4458) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4460) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4463) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4470) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4471) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4477)
    (<0.0>,4478)
    (<0.0>,4479) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,4482)
    (<0.0>,4483) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4485) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4486) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4488) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4494) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,4500)
    (<0.0>,4501) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,4504)
    (<0.0>,4505) tree.c:453: return &rsp->node[0];
    (<0.0>,4509) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,4510) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4512) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4516) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4517) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,4519) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,4522) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,4523) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,4524) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,4528) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,4531) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,4534) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,4537) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,4538) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,4541) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4543) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4546) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4549) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4552) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4553) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4555) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4558) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4562) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4564) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4566) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4569) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4572) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4575) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4576) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4578) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4581) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,4585) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4587) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4589) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,4592) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,4594) tree.c:494: }
    (<0.0>,4598) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4600) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4601) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4603) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,4606) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4608) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4609) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4611) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,4614) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,4617) tree_plugin.h:2715: return false;
    (<0.0>,4621) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4623) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4625) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,4626) tree.c:3175: return 0;
    (<0.0>,4628) tree.c:3176: }
    (<0.0>,4633) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4636) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4637) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4638) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4642) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4643) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4644) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4646) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4650) tree.c:3190: return 0;
    (<0.0>,4652) tree.c:3191: }
    (<0.0>,4656) tree.c:2437: if (user)
    (<0.0>,4664) fake_sched.h:43: return __running_cpu;
    (<0.0>,4668) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4670) fake_sched.h:43: return __running_cpu;
    (<0.0>,4674) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4680) fake_sched.h:43: return __running_cpu;
    (<0.0>,4684) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,4694) tree.c:624: local_irq_save(flags);
    (<0.0>,4697) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4699) fake_sched.h:43: return __running_cpu;
    (<0.0>,4703) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4705) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4709) fake_sched.h:43: return __running_cpu;
    (<0.0>,4713) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4719) fake_sched.h:43: return __running_cpu;
    (<0.0>,4723) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4724) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,4726) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,4727) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,4728) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,4730) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,4732) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,4733) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4735) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4740) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4741) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4743) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4747) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4748) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4749) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,4750) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,4752) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,4760) tree_plugin.h:3141: }
    (<0.0>,4762) tree.c:634: local_irq_restore(flags);
    (<0.0>,4765) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4767) fake_sched.h:43: return __running_cpu;
    (<0.0>,4771) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4773) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4777) fake_sched.h:43: return __running_cpu;
    (<0.0>,4781) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4790) fake_sched.h:43: return __running_cpu;
    (<0.0>,4794) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,4797) tree.c:580: local_irq_save(flags);
    (<0.0>,4800) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4802) fake_sched.h:43: return __running_cpu;
    (<0.0>,4806) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4808) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4812) fake_sched.h:43: return __running_cpu;
    (<0.0>,4816) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4829) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4831) fake_sched.h:43: return __running_cpu;
    (<0.0>,4835) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4836) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,4838) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,4839) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,4840) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4846) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4847) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4852) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4853) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4854) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,4855) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,4859) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,4861) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,4862) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,4863) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,4882)
    (<0.0>,4884) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4886) fake_sched.h:43: return __running_cpu;
    (<0.0>,4890) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4893) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,4897) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4898) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4899) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4903) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4904) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4905) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4907) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4912) fake_sched.h:43: return __running_cpu;
    (<0.0>,4915) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4917) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4919) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4920) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,4923) tree_plugin.h:2720: }
    (<0.0>,4926) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4929) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4930) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4931) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4935) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4936) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4937) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4939) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4944) fake_sched.h:43: return __running_cpu;
    (<0.0>,4947) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4949) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4951) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,4952) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,4955) tree_plugin.h:2720: }
    (<0.0>,4958) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4961) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4962) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4963) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4967) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4968) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4969) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4971) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,4978) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4981) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4982) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4983) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4985) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4986) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,4988) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,4991) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,4997) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,4998) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5001) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5006) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5007) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5008) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5022) tree_plugin.h:3141: }
    (<0.0>,5024) tree.c:583: local_irq_restore(flags);
    (<0.0>,5027) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5029) fake_sched.h:43: return __running_cpu;
    (<0.0>,5033) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5035) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5039) fake_sched.h:43: return __running_cpu;
    (<0.0>,5043) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5049) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5052) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5057) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5059) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5061) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5065) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5067) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5070) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5073) tree_plugin.h:958: return 0;
    (<0.0>,5082) fake_sched.h:43: return __running_cpu;
    (<0.0>,5086)
    (<0.0>,5087) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5090) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5095) tree.c:704: local_irq_save(flags);
    (<0.0>,5098) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5100) fake_sched.h:43: return __running_cpu;
    (<0.0>,5104) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5106) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5110) fake_sched.h:43: return __running_cpu;
    (<0.0>,5114) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5127) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5129) fake_sched.h:43: return __running_cpu;
    (<0.0>,5133) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5134) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5136) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5137) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5138) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5143) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5144) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5148) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5149) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5150) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5151) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,5155) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5157) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5158) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5159) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5173)
    (<0.0>,5174) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5176) fake_sched.h:43: return __running_cpu;
    (<0.0>,5180) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5184) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5187) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5188) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5189) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5191) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5192) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5194) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5197) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5204) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5205) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5208) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5213) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5214) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5215) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5220) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,5229) tree_plugin.h:3145: }
    (<0.0>,5231) tree.c:707: local_irq_restore(flags);
    (<0.0>,5234) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5236) fake_sched.h:43: return __running_cpu;
    (<0.0>,5240) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5242) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5246) fake_sched.h:43: return __running_cpu;
    (<0.0>,5250) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5257) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5258) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5259) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5260) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,5262) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,5265) tree.c:1840: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,5268) tree_plugin.h:958: return 0;
    (<0.0>,5273) tree.c:1872: rcu_gp_cleanup(rsp);
    (<0.0>,5281)
    (<0.0>,5282) tree.c:1720: bool needgp = false;
    (<0.0>,5283) tree.c:1721: int nocb = 0;
    (<0.0>,5284) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5287)
    (<0.0>,5288) tree.c:453: return &rsp->node[0];
    (<0.0>,5292) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5293) tree.c:1725: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,5297) fake_sync.h:92: local_irq_disable();
    (<0.0>,5300) fake_sched.h:43: return __running_cpu;
    (<0.0>,5304) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5308) fake_sched.h:43: return __running_cpu;
    (<0.0>,5312) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5317) fake_sched.h:43: return __running_cpu;
    (<0.0>,5321) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5324) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5325) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5331) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5332) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5334) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5336) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5337) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5338) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5340) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5343) tree.c:1739: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,5347)
    (<0.0>,5348) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5349) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5350) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5354) fake_sched.h:43: return __running_cpu;
    (<0.0>,5358) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,5360) fake_sched.h:43: return __running_cpu;
    (<0.0>,5364) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5370) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5373) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5375) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5376) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5378) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5383) tree.c:1751: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,5387) fake_sync.h:92: local_irq_disable();
    (<0.0>,5390) fake_sched.h:43: return __running_cpu;
    (<0.0>,5394) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5398) fake_sched.h:43: return __running_cpu;
    (<0.0>,5402) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5407) fake_sched.h:43: return __running_cpu;
    (<0.0>,5411) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5414) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5415) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5421) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5423) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5424) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5426) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5428) fake_sched.h:43: return __running_cpu;
    (<0.0>,5431) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5433) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5435) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5436) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5437) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5439) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5442) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5443) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5444) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5450)
    (<0.0>,5451)
    (<0.0>,5452)
    (<0.0>,5453) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5455) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5456) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5458) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5461) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5462) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5463) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5471)
    (<0.0>,5472)
    (<0.0>,5473)
    (<0.0>,5474) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5477) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5480) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5483) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5484) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5487) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5489) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5492) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5494) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5495) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5497) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5500) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5504) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5506) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5509) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5510) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5513) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5515) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5517) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5519) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5522) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5524) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5525) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5527) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5530) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5535) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5537) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5538) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5541) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,5544) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,5545) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,5547) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,5550) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,5552) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5554) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5556) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5557) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,5560) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,5562) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,5565) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5567) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5570) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5571) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5574) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5578) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,5579) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,5580) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,5589)
    (<0.0>,5590)
    (<0.0>,5591)
    (<0.0>,5592) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5595) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5598) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5601) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5602) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5605) tree.c:1434: return false;
    (<0.0>,5607) tree.c:1483: }
    (<0.0>,5609) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,5611) tree.c:1527: }
    (<0.0>,5614) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5615) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,5617) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,5618) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,5620) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,5624) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,5626) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,5627) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,5629) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,5632) tree.c:1575: return ret;
    (<0.0>,5636) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5640) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5642) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,5643) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,5650)
    (<0.0>,5651) tree.c:1383: static int rcu_future_gp_cleanup(struct rcu_state *rsp, struct rcu_node *rnp)
    (<0.0>,5652) tree.c:1385: int c = rnp->completed;
    (<0.0>,5654) tree.c:1385: int c = rnp->completed;
    (<0.0>,5656) tree.c:1385: int c = rnp->completed;
    (<0.0>,5658) fake_sched.h:43: return __running_cpu;
    (<0.0>,5661) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5663) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5665) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,5666) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,5667) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,5671)
    (<0.0>,5672) tree_plugin.h:2686: }
    (<0.0>,5674) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,5677) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,5680) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,5681) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,5685) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,5688) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,5689) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,5690) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,5691) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,5692) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,5694) tree.c:1393: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,5702)
    (<0.0>,5703)
    (<0.0>,5704)
    (<0.0>,5705) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,5709) tree.c:1394: return needmore;
    (<0.0>,5711) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,5713) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,5714) tree.c:1759: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,5718)
    (<0.0>,5719) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5720) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5721) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5725) fake_sched.h:43: return __running_cpu;
    (<0.0>,5729) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,5731) fake_sched.h:43: return __running_cpu;
    (<0.0>,5735) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5748) fake_sched.h:43: return __running_cpu;
    (<0.0>,5753) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,5760) fake_sched.h:43: return __running_cpu;
    (<0.0>,5764) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,5776) fake_sched.h:43: return __running_cpu;
    (<0.0>,5780) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,5783) tree.c:580: local_irq_save(flags);
    (<0.0>,5786) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5788) fake_sched.h:43: return __running_cpu;
    (<0.0>,5792) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5794) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5798) fake_sched.h:43: return __running_cpu;
    (<0.0>,5802) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5815) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5817) fake_sched.h:43: return __running_cpu;
    (<0.0>,5821) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5822) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5824) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5825) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5826) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5832) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5833) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5838) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5839) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5840) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5841) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5845) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5847) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5848) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5849) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5868)
    (<0.0>,5870) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5872) fake_sched.h:43: return __running_cpu;
    (<0.0>,5876) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5879) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,5883) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5884) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5885) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5889) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5890) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5891) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5893) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5898) fake_sched.h:43: return __running_cpu;
    (<0.0>,5901) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5903) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5905) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5906) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5909) tree_plugin.h:2720: }
    (<0.0>,5912) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5915) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5916) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5917) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5921) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5922) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5923) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5925) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5930) fake_sched.h:43: return __running_cpu;
    (<0.0>,5933) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5935) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5937) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5938) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5941) tree_plugin.h:2720: }
    (<0.0>,5944) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5947) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5948) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5949) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5953) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5954) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5955) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5957) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5964) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5967) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5968) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5969) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5971) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5972) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5974) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5977) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5983) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5984) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5987) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5992) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5993) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5994) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6008) tree_plugin.h:3141: }
    (<0.0>,6010) tree.c:583: local_irq_restore(flags);
    (<0.0>,6013) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6015) fake_sched.h:43: return __running_cpu;
    (<0.0>,6019) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6021) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6025) fake_sched.h:43: return __running_cpu;
    (<0.0>,6029) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6035) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6038) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6043) fake_sched.h:43: return __running_cpu;
    (<0.0>,6047)
    (<0.0>,6048) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6051) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6056) tree.c:704: local_irq_save(flags);
    (<0.0>,6059) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6061) fake_sched.h:43: return __running_cpu;
    (<0.0>,6065) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6067) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6071) fake_sched.h:43: return __running_cpu;
    (<0.0>,6075) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6088) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6090) fake_sched.h:43: return __running_cpu;
    (<0.0>,6094) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6095) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6097) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6098) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6099) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6104) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6105) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6109) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6110) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6111) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6112) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,6116) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6118) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6119) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6120) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6134)
    (<0.0>,6135) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6137) fake_sched.h:43: return __running_cpu;
    (<0.0>,6141) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6145) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6148) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6149) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6150) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6152) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6153) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6155) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6158) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6165) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6166) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6169) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6174) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6175) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6176) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6181) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,6190) tree_plugin.h:3145: }
    (<0.0>,6192) tree.c:707: local_irq_restore(flags);
    (<0.0>,6195) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6197) fake_sched.h:43: return __running_cpu;
    (<0.0>,6201) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6203) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6207) fake_sched.h:43: return __running_cpu;
    (<0.0>,6211) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6226) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6228) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6230) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6231) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6233) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6238) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,6241)
    (<0.0>,6242) tree.c:453: return &rsp->node[0];
    (<0.0>,6246) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,6247) tree.c:1763: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,6251) fake_sync.h:92: local_irq_disable();
    (<0.0>,6254) fake_sched.h:43: return __running_cpu;
    (<0.0>,6258) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6262) fake_sched.h:43: return __running_cpu;
    (<0.0>,6266) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6271) fake_sched.h:43: return __running_cpu;
    (<0.0>,6275) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,6278) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6279) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6285) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,6286) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,6290)
    (<0.0>,6291) tree_plugin.h:2690: }
    (<0.0>,6293) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6295) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6296) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6298) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6301) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,6303) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,6305) fake_sched.h:43: return __running_cpu;
    (<0.0>,6308) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6310) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6312) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6313) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6314) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6315) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6323)
    (<0.0>,6324)
    (<0.0>,6325)
    (<0.0>,6326) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6329) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6332) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6335) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6336) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6339) tree.c:1502: return false;
    (<0.0>,6341) tree.c:1527: }
    (<0.0>,6344) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6348) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6349) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6352) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6353) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6359)
    (<0.0>,6360)
    (<0.0>,6361) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,6364)
    (<0.0>,6365) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6367) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6368) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6370) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6376) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,6382)
    (<0.0>,6383) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6386)
    (<0.0>,6387) tree.c:453: return &rsp->node[0];
    (<0.0>,6391) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6392) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6394) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6398) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6399) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6401) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6404) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6405) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,6406) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,6410) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,6412) tree.c:494: }
    (<0.0>,6416) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,6418) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,6422) tree.c:1780: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,6426)
    (<0.0>,6427) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6428) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6429) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6433) fake_sched.h:43: return __running_cpu;
    (<0.0>,6437) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6439) fake_sched.h:43: return __running_cpu;
    (<0.0>,6443) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6454) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,6456) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,6460) fake_sched.h:43: return __running_cpu;
    (<0.0>,6464) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6468) fake_sched.h:43: return __running_cpu;
    (<0.0>,6472) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6477) fake_sched.h:43: return __running_cpu;
    (<0.0>,6481) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,6491) tree.c:749: local_irq_save(flags);
    (<0.0>,6494) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6496) fake_sched.h:43: return __running_cpu;
    (<0.0>,6500) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6502) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6507) fake_sched.h:43: return __running_cpu;
    (<0.0>,6511) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6512) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,6514) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,6515) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,6516) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,6518) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,6520) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,6521) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6523) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6528) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6529) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6531) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6535) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6536) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6537) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6538) tree.c:754: if (oldval)
    (<0.0>,6546) tree_plugin.h:3145: }
    (<0.0>,6548) tree.c:759: local_irq_restore(flags);
    (<0.0>,6551) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6553) fake_sched.h:43: return __running_cpu;
    (<0.0>,6557) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6559) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6567) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,6572) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,6577) fake_sched.h:43: return __running_cpu;
    (<0.0>,6582) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,6590) fake_sched.h:43: return __running_cpu;
    (<0.0>,6595) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,6609) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6610) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6611) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6615) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6616) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6617) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6619) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,6623) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,6625) fake_sched.h:43: return __running_cpu;
    (<0.0>,6628) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,6630) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,6637)
    (<0.0>,6638)
    (<0.0>,6639) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6641) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6642) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6643) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,6645) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,6647) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,6648) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,6649) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,6659)
    (<0.0>,6660)
    (<0.0>,6661) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,6666) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,6669) tree_plugin.h:3185: return 0;
    (<0.0>,6672) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,6675) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,6677) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,6680) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,6682) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,6686) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,6689)
    (<0.0>,6690) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6692) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6695) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6698) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,6701) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,6708) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,6710) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,6712) tree.c:3145: rdp->n_rp_cb_ready++;
    (<0.0>,6713) tree.c:3146: return 1;
    (<0.0>,6715) tree.c:3176: }
    (<0.0>,6719) tree.c:3189: return 1;
    (<0.0>,6721) tree.c:3191: }
    (<0.0>,6728) fake_sched.h:43: return __running_cpu;
    (<0.0>,6732) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,6736) tree.c:2437: if (user)
    (<0.0>,6744) fake_sched.h:43: return __running_cpu;
    (<0.0>,6748) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6750) fake_sched.h:43: return __running_cpu;
    (<0.0>,6754) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6760) fake_sched.h:43: return __running_cpu;
    (<0.0>,6764) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,6774) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,6777) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6778) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6779) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6783) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6784) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6785) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6787) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,6791) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,6800) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6802) fake_sched.h:43: return __running_cpu;
    (<0.0>,6805) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6807) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6809) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6810) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6812) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6819) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6820) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6822) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6828) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6829) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6830) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6831) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6832) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6836)
    (<0.0>,6837)
    (<0.0>,6838) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,6839) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,6846)
    (<0.0>,6847)
    (<0.0>,6848) tree.c:1584: local_irq_save(flags);
    (<0.0>,6851) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6853) fake_sched.h:43: return __running_cpu;
    (<0.0>,6857) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6859) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6863) fake_sched.h:43: return __running_cpu;
    (<0.0>,6867) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6872) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,6874) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,6875) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,6876) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,6878) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,6879) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,6881) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,6884) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,6886) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,6887) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,6889) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,6892) tree.c:1589: local_irq_restore(flags);
    (<0.0>,6895) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6897) fake_sched.h:43: return __running_cpu;
    (<0.0>,6901) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6903) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6907) fake_sched.h:43: return __running_cpu;
    (<0.0>,6911) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6918) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,6920) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,6925) tree.c:2558: local_irq_save(flags);
    (<0.0>,6928) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6930) fake_sched.h:43: return __running_cpu;
    (<0.0>,6934) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6936) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6940) fake_sched.h:43: return __running_cpu;
    (<0.0>,6944) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6949) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6950) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6956)
    (<0.0>,6957)
    (<0.0>,6958) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,6961)
    (<0.0>,6962) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6964) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6965) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6967) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6973) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,6979)
    (<0.0>,6980) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6983)
    (<0.0>,6984) tree.c:453: return &rsp->node[0];
    (<0.0>,6988) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6989) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6991) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6995) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6996) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6998) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7001) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7002) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7003) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7007) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7009) tree.c:494: }
    (<0.0>,7013) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,7016)
    (<0.0>,7017) tree.c:453: return &rsp->node[0];
    (<0.0>,7024) fake_sync.h:108: preempt_disable();
    (<0.0>,7026) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7027) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7031) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,7037) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7039) fake_sched.h:43: return __running_cpu;
    (<0.0>,7042) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7044) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7046) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7047) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7050)
    (<0.0>,7051) tree.c:453: return &rsp->node[0];
    (<0.0>,7055) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7056) tree.c:1925: bool ret = false;
    (<0.0>,7057) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7058) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7059) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7067)
    (<0.0>,7068)
    (<0.0>,7069)
    (<0.0>,7070) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7073) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7076) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7079) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7080) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7083) tree.c:1502: return false;
    (<0.0>,7085) tree.c:1527: }
    (<0.0>,7088) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7092) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7093) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7094) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7095) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7101)
    (<0.0>,7102)
    (<0.0>,7103)
    (<0.0>,7104) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7106) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7109) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7110) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7116)
    (<0.0>,7117)
    (<0.0>,7118) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7121)
    (<0.0>,7122) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7124) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7125) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7127) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7133) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7139)
    (<0.0>,7140) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7143)
    (<0.0>,7144) tree.c:453: return &rsp->node[0];
    (<0.0>,7148) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7149) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7151) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7155) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7156) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7158) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7161) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7162) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7163) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7167) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7169) tree.c:494: }
    (<0.0>,7173) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7175) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7178) tree.c:1909: return true;
    (<0.0>,7180) tree.c:1910: }
    (<0.0>,7184) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7185) tree.c:1937: return ret;
    (<0.0>,7189) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,7190) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,7193)
    (<0.0>,7194) tree.c:453: return &rsp->node[0];
    (<0.0>,7199) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,7203)
    (<0.0>,7204)
    (<0.0>,7205) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7206) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,5572) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,5576) tree.c:2561: needwake = rcu_start_gp(rsp);
  (<0>,5582) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5584) fake_sched.h:43: return __running_cpu;
  (<0>,5587) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5589) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5591) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5592) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5595)
  (<0>,5596) tree.c:453: return &rsp->node[0];
  (<0>,5600) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5601) tree.c:1925: bool ret = false;
  (<0>,5602) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
  (<0>,5603) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
  (<0>,5604) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
  (<0>,5612)
  (<0>,5613)
  (<0>,5614)
  (<0>,5615) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5618) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5621) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5624) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5625) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5628) tree.c:1502: return false;
  (<0>,5630) tree.c:1527: }
  (<0>,5633) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
  (<0>,5637) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
  (<0>,5638) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
  (<0>,5639) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
  (<0>,5640) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
  (<0>,5646)
  (<0>,5647)
  (<0>,5648)
  (<0>,5649) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5651) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5654) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5655) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5661)
  (<0>,5662)
  (<0>,5663) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,5666)
  (<0>,5667) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5669) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5670) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5672) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5678) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5684)
  (<0>,5685) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5688)
  (<0>,5689) tree.c:453: return &rsp->node[0];
  (<0>,5693) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5694) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5696) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5700) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5701) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5703) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5706) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5707) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5708) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5712) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
  (<0>,5714) tree.c:494: }
  (<0>,5718) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
  (<0>,5720) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
  (<0>,5723) tree.c:1909: return true;
  (<0>,5725) tree.c:1910: }
  (<0>,5729) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
  (<0>,5730) tree.c:1937: return ret;
  (<0>,5734) tree.c:2561: needwake = rcu_start_gp(rsp);
  (<0>,5735) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
  (<0>,5738)
  (<0>,5739) tree.c:453: return &rsp->node[0];
  (<0>,5744) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
  (<0>,5748)
  (<0>,5749)
  (<0>,5750) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,5751) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,5754) fake_sync.h:86: local_irq_restore(flags);
  (<0>,5757) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5759) fake_sched.h:43: return __running_cpu;
  (<0>,5763) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5765) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5769) fake_sched.h:43: return __running_cpu;
  (<0>,5773) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5780) tree.c:2563: if (needwake)
  (<0>,5783) tree.c:2564: rcu_gp_kthread_wake(rsp);
  (<0>,5786)
  (<0>,5787) tree.c:1406: if (current == rsp->gp_kthread ||
  (<0>,5788) tree.c:1406: if (current == rsp->gp_kthread ||
  (<0>,5790) tree.c:1406: if (current == rsp->gp_kthread ||
  (<0>,5793) tree.c:1407: !ACCESS_ONCE(rsp->gp_flags) ||
  (<0>,5795) tree.c:1407: !ACCESS_ONCE(rsp->gp_flags) ||
  (<0>,5798) tree.c:1408: !rsp->gp_kthread)
  (<0>,5800) tree.c:1408: !rsp->gp_kthread)
  (<0>,5808) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,5811)
  (<0>,5812) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5814) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5817) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5824) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,5827) tree_plugin.h:2720: }
  (<0>,5831) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5834) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5835) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5836) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5840) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5841) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5842) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5844) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5848) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,5857) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5859) fake_sched.h:43: return __running_cpu;
  (<0>,5862) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5864) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5866) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5867) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5869) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5876) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5877) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5879) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5885) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5886) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5887) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5888) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5889) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5893)
  (<0>,5894)
  (<0>,5895) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5896) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5903)
  (<0>,5904)
  (<0>,5905) tree.c:1584: local_irq_save(flags);
  (<0>,5908) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5910) fake_sched.h:43: return __running_cpu;
  (<0>,5914) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5916) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5920) fake_sched.h:43: return __running_cpu;
  (<0>,5924) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5929) tree.c:1585: rnp = rdp->mynode;
  (<0>,5931) tree.c:1585: rnp = rdp->mynode;
  (<0>,5932) tree.c:1585: rnp = rdp->mynode;
  (<0>,5933) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5935) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5936) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5938) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5941) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5943) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5944) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5946) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5949) tree.c:1589: local_irq_restore(flags);
  (<0>,5952) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5954) fake_sched.h:43: return __running_cpu;
  (<0>,5958) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5960) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5964) fake_sched.h:43: return __running_cpu;
  (<0>,5968) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5975) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5977) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5982) tree.c:2558: local_irq_save(flags);
  (<0>,5985) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5987) fake_sched.h:43: return __running_cpu;
  (<0>,5991) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5993) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5997) fake_sched.h:43: return __running_cpu;
  (<0>,6001) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6006) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6007) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6013)
  (<0>,6014)
  (<0>,6015) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,6018)
  (<0>,6019) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,6021) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,6022) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,6024) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,6030) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,6036)
  (<0>,6037) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6040)
  (<0>,6041) tree.c:453: return &rsp->node[0];
  (<0>,6045) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6046) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6048) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6052) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6053) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6055) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6058) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6059) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6060) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6064) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6067) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6070) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6073) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6074) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6077) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6079) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6082) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6085) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6088) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6089) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6091) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6094) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6098) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6100) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6102) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6105) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6108) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6111) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6112) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6114) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6117) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6121) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6123) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6125) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6128) tree.c:493: return 0; /* No grace period needed. */
  (<0>,6130) tree.c:494: }
  (<0>,6134) tree.c:2566: local_irq_restore(flags);
  (<0>,6137) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6139) fake_sched.h:43: return __running_cpu;
  (<0>,6143) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6145) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6149) fake_sched.h:43: return __running_cpu;
  (<0>,6153) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6159) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,6162)
  (<0>,6163) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6165) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6168) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6175) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,6178) tree_plugin.h:2720: }
  (<0>,6182) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6185) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6186) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6187) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6191) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6192) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6193) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6195) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6203) fake_sched.h:43: return __running_cpu;
  (<0>,6207) fake_sched.h:184: need_softirq[get_cpu()] = 0;
  (<0>,6216) tree.c:624: local_irq_save(flags);
  (<0>,6219) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6221) fake_sched.h:43: return __running_cpu;
  (<0>,6225) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6227) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6231) fake_sched.h:43: return __running_cpu;
  (<0>,6235) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6241) fake_sched.h:43: return __running_cpu;
  (<0>,6245) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6246) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6248) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6249) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6250) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6252) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6254) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6255) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6257) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6262) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6263) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6265) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6269) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6270) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6271) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6272) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,6274) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,6282) tree_plugin.h:3141: }
  (<0>,6284) tree.c:634: local_irq_restore(flags);
  (<0>,6287) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6289) fake_sched.h:43: return __running_cpu;
  (<0>,6293) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6295) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6299) fake_sched.h:43: return __running_cpu;
  (<0>,6303) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,719) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,7207) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7209) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,7212)
    (<0.0>,7214) fake_sched.h:43: return __running_cpu;
    (<0.0>,7218) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7220) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7224) fake_sched.h:43: return __running_cpu;
    (<0.0>,7228) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7235) tree.c:2563: if (needwake)
    (<0.0>,7238) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,7241)
    (<0.0>,7242) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7243) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7245) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7252) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7255)
    (<0.0>,7256) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7258) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7261) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7264) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7267) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7274) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,7275) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,7279)
    (<0.0>,7280)
    (<0.0>,7281) tree.c:2601: if (unlikely(!ACCESS_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,7290) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,7292) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,7301) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,7302) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,7319)
    (<0.0>,7320)
    (<0.0>,7321) tree.c:2313: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,7324)
    (<0.0>,7325) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7327) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7330) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7333) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7336) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7343) tree.c:2325: local_irq_save(flags);
    (<0.0>,7346)
    (<0.0>,7348) fake_sched.h:43: return __running_cpu;
    (<0.0>,7352) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7354) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7358) fake_sched.h:43: return __running_cpu;
    (<0.0>,7362) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7367) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7368) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7369) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7370) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7371) tree.c:2327: bl = rdp->blimit;
    (<0.0>,7373) tree.c:2327: bl = rdp->blimit;
    (<0.0>,7374) tree.c:2327: bl = rdp->blimit;
    (<0.0>,7377) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,7379) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,7380) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,7381) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7384) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7385) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7386) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7388) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7389) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,7392) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,7393) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,7394) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7397) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7398) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7399) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7401) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7404) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7406) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7409) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7410) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7413) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7416) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7418) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7420) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7423) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7426) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7428) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7430) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7433) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7435) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7438) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7439) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7442) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7445) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7447) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7449) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7452) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7455) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7457) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7459) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7462) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7464) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7467) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7468) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7471) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7474) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7476) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7478) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7481) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7484) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7486) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7488) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7491) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7493) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7496) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7497) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7500) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7503) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7505) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7507) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7510) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7513) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7515) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7517) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7520) tree.c:2336: local_irq_restore(flags);
    (<0.0>,7523)
    (<0.0>,7525) fake_sched.h:43: return __running_cpu;
    (<0.0>,7529) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7531) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7535) fake_sched.h:43: return __running_cpu;
    (<0.0>,7539) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7544) tree.c:2339: count = count_lazy = 0;
    (<0.0>,7545) tree.c:2339: count = count_lazy = 0;
    (<0.0>,7547) tree.c:2340: while (list) {
    (<0.0>,7550) tree.c:2341: next = list->next;
    (<0.0>,7552) tree.c:2341: next = list->next;
    (<0.0>,7553) tree.c:2341: next = list->next;
    (<0.0>,7556) tree.c:2343: debug_rcu_head_unqueue(list);
    (<0.0>,7559)
    (<0.0>,7561) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,7563) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,7564) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,7570)
    (<0.0>,7571)
    (<0.0>,7572) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,7574) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,7576) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,7579) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,7582) rcu.h:118: head->func(head);
    (<0.0>,7584) rcu.h:118: head->func(head);
    (<0.0>,7585) rcu.h:118: head->func(head);
    (<0.0>,7591)
    (<0.0>,7592) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7593) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7594) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7598) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7599) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7600) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,7601) update.c:216: complete(&rcu->completion);
    (<0.0>,7605)
    (<0.0>,7606) fake_sync.h:248: x->done++;
    (<0.0>,7608) fake_sync.h:248: x->done++;
    (<0.0>,7610) fake_sync.h:248: x->done++;
    (<0.0>,7615) rcu.h:120: return false;
    (<0.0>,7617) rcu.h:122: }
    (<0.0>,7620) tree.c:2346: list = next;
    (<0.0>,7621) tree.c:2346: list = next;
    (<0.0>,7622) tree.c:2348: if (++count >= bl &&
    (<0.0>,7624) tree.c:2348: if (++count >= bl &&
    (<0.0>,7625) tree.c:2348: if (++count >= bl &&
    (<0.0>,7629) tree.c:2340: while (list) {
    (<0.0>,7632) tree.c:2354: local_irq_save(flags);
    (<0.0>,7635)
    (<0.0>,7637) fake_sched.h:43: return __running_cpu;
    (<0.0>,7641) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7643) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7647) fake_sched.h:43: return __running_cpu;
    (<0.0>,7651) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7658) tree.c:2360: if (list != NULL) {
    (<0.0>,7662) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,7663) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,7665) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,7667) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,7668) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,7670) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,7671) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,7673) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,7675) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,7676) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,7677) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,7679) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,7681) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,7682) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,7684) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,7687) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,7689) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,7692) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,7694) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,7697) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,7699) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,7700) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,7702) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,7703) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,7708) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7710) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7713) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7715) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7722) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7723) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7725) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7728) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7730) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7736) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7737) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7738) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,7739) tree.c:2386: local_irq_restore(flags);
    (<0.0>,7742)
    (<0.0>,7744) fake_sched.h:43: return __running_cpu;
    (<0.0>,7748) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7750) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7754) fake_sched.h:43: return __running_cpu;
    (<0.0>,7758) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7763) tree.c:2389: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7766)
    (<0.0>,7767) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7769) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7772) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7783) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7786)
    (<0.0>,7790) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7793) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7794) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7795) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7799) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7800) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7801) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7803) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7807) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,7816)
    (<0.0>,7818) fake_sched.h:43: return __running_cpu;
    (<0.0>,7821) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7823) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7825) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7826) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7828) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7835) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7836) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7838) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7844) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7845) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7846) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7847) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7848) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7852)
    (<0.0>,7853)
    (<0.0>,7854) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7855) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7862)
    (<0.0>,7863)
    (<0.0>,7864) tree.c:1584: local_irq_save(flags);
    (<0.0>,7867)
    (<0.0>,7869) fake_sched.h:43: return __running_cpu;
    (<0.0>,7873) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7875) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7879) fake_sched.h:43: return __running_cpu;
    (<0.0>,7883) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7888) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7890) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7891) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7892) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7894) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7895) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7897) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7900) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7902) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7903) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7905) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7908) tree.c:1589: local_irq_restore(flags);
    (<0.0>,7911)
    (<0.0>,7913) fake_sched.h:43: return __running_cpu;
    (<0.0>,7917) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7919) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7923) fake_sched.h:43: return __running_cpu;
    (<0.0>,7927) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7934) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7936) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7941) tree.c:2558: local_irq_save(flags);
    (<0.0>,7944)
    (<0.0>,7946) fake_sched.h:43: return __running_cpu;
    (<0.0>,7950) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7952) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7956) fake_sched.h:43: return __running_cpu;
    (<0.0>,7960) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7965) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7966) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7972)
    (<0.0>,7973)
    (<0.0>,7974) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7977)
    (<0.0>,7978) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7980) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7981) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7983) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7989) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7995)
    (<0.0>,7996) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7999)
    (<0.0>,8000) tree.c:453: return &rsp->node[0];
    (<0.0>,8004) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8005) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8007) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8011) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8012) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8014) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8017) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8018) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8019) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8023) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8026) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8029) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8032) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8033) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8036) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8038) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8041) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8044) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8047) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8048) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8050) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8053) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8057) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8059) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8061) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8064) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8067) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8070) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8071) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8073) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8076) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8080) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8082) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8084) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8087) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,8089) tree.c:494: }
    (<0.0>,8093) tree.c:2566: local_irq_restore(flags);
    (<0.0>,8096)
    (<0.0>,8098) fake_sched.h:43: return __running_cpu;
    (<0.0>,8102) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8104) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8108) fake_sched.h:43: return __running_cpu;
    (<0.0>,8112) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8118) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,8121)
    (<0.0>,8122) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8124) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8127) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8134) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8137)
    (<0.0>,8141) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8144) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8145) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8146) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8150) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8151) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8152) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8154) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8162) fake_sched.h:43: return __running_cpu;
    (<0.0>,8166) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,8175) tree.c:624: local_irq_save(flags);
    (<0.0>,8178)
    (<0.0>,8180) fake_sched.h:43: return __running_cpu;
    (<0.0>,8184) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8186) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8190) fake_sched.h:43: return __running_cpu;
    (<0.0>,8194) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8200) fake_sched.h:43: return __running_cpu;
    (<0.0>,8204) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8205) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8207) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8208) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8209) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8211) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8213) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8214) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8216) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8221) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8222) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8224) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8228) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8229) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8230) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8231) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,8233) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,8241)
    (<0.0>,8243) tree.c:634: local_irq_restore(flags);
    (<0.0>,8246)
    (<0.0>,8248) fake_sched.h:43: return __running_cpu;
    (<0.0>,8252) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8254) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8258) fake_sched.h:43: return __running_cpu;
    (<0.0>,8262) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8271) fake_sched.h:43: return __running_cpu;
    (<0.0>,8275)
    (<0.0>,8278) tree.c:580: local_irq_save(flags);
    (<0.0>,8281)
    (<0.0>,8283) fake_sched.h:43: return __running_cpu;
    (<0.0>,8287) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8289) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8293) fake_sched.h:43: return __running_cpu;
    (<0.0>,8297) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8310)
    (<0.0>,8312) fake_sched.h:43: return __running_cpu;
    (<0.0>,8316) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8317) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8319) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8320) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8321) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8327) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8328) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8333) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8334) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8335) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8336) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,8340) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,8342) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,8343) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8344) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8363)
    (<0.0>,8365)
    (<0.0>,8367) fake_sched.h:43: return __running_cpu;
    (<0.0>,8371) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8374) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,8378) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8379) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8380) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8384) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8385) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8386) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8388) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8393) fake_sched.h:43: return __running_cpu;
    (<0.0>,8396) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8398) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8400) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8401) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8404)
    (<0.0>,8407) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8410) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8411) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8412) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8416) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8417) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8418) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8420) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8425) fake_sched.h:43: return __running_cpu;
    (<0.0>,8428) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8430) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8432) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8433) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8436)
    (<0.0>,8439) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8442) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8443) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8444) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8448) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8449) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8450) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8452) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8459) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8462) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8463) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8464) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8466) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8467) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,8469) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8472) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8478) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8479) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8482) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8487) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8488) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8489) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,8503)
    (<0.0>,8505) tree.c:583: local_irq_restore(flags);
    (<0.0>,8508)
    (<0.0>,8510) fake_sched.h:43: return __running_cpu;
    (<0.0>,8514) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8516) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8520) fake_sched.h:43: return __running_cpu;
    (<0.0>,8524) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8530) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8533) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8538) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,8540) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,8549) fake_sched.h:43: return __running_cpu;
    (<0.0>,8553)
    (<0.0>,8554) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8557) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8562) tree.c:704: local_irq_save(flags);
    (<0.0>,8565)
    (<0.0>,8567) fake_sched.h:43: return __running_cpu;
    (<0.0>,8571) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8573) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8577) fake_sched.h:43: return __running_cpu;
    (<0.0>,8581) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8594)
    (<0.0>,8596) fake_sched.h:43: return __running_cpu;
    (<0.0>,8600) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8601) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,8603) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,8604) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,8605) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8610) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8611) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8615) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8616) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8617) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,8618) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,8622) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8624) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8625) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8626) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8640)
    (<0.0>,8641)
    (<0.0>,8643) fake_sched.h:43: return __running_cpu;
    (<0.0>,8647) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8651) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8654) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8655) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8656) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8658) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8659) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,8661) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8664) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8671) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8672) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8675) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8680) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8681) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8682) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,8687) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,8696)
    (<0.0>,8698) tree.c:707: local_irq_restore(flags);
    (<0.0>,8701)
    (<0.0>,8703) fake_sched.h:43: return __running_cpu;
    (<0.0>,8707) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8709) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8713) fake_sched.h:43: return __running_cpu;
    (<0.0>,8717) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8724) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,8736)
    (<0.0>,8737) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8740)
    (<0.0>,8741) tree.c:453: return &rsp->node[0];
    (<0.0>,8745) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8749) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,8753)
    (<0.0>,8756) fake_sched.h:43: return __running_cpu;
    (<0.0>,8760) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8764) fake_sched.h:43: return __running_cpu;
    (<0.0>,8768) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8773) fake_sched.h:43: return __running_cpu;
    (<0.0>,8777) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8780) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,8781) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,8787) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,8789) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,8792) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,8794) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,8795) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8798)
    (<0.0>,8799) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8801) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8802) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8804) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8812) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8813) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8816)
    (<0.0>,8817) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8819) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8820) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8822) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8829) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8830) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8831) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,8834) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,8839)
    (<0.0>,8840) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,8841) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,8842) tree.c:1012: rsp->gp_start = j;
    (<0.0>,8843) tree.c:1012: rsp->gp_start = j;
    (<0.0>,8845) tree.c:1012: rsp->gp_start = j;
    (<0.0>,8849) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,8850) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,8851) update.c:344: if (till_stall_check < 3) {
    (<0.0>,8854) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,8858) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,8863) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,8864) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,8865) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,8867) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,8869) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,8870) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,8871) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,8874) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,8876) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,8880) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,8882) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,8884) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,8886) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,8890) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,8894)
    (<0.0>,8895) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,8896) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,8901) fake_sched.h:43: return __running_cpu;
    (<0.0>,8905) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,8907) fake_sched.h:43: return __running_cpu;
    (<0.0>,8911) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8917) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,8921)
    (<0.0>,8922) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,8924) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,8930) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8933) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8935) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8936) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8938) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8943) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,8947)
    (<0.0>,8950) fake_sched.h:43: return __running_cpu;
    (<0.0>,8954) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8958) fake_sched.h:43: return __running_cpu;
    (<0.0>,8962) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8967) fake_sched.h:43: return __running_cpu;
    (<0.0>,8971) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8974) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,8975) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,8982) fake_sched.h:43: return __running_cpu;
    (<0.0>,8985) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8987) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8989) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8990) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,8996)
    (<0.0>,8997) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8999) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9004) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9005) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9007) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9011) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9012) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9013) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9015) tree.c:1656: rnp->qsmask = 0;
    (<0.0>,9017) tree.c:1656: rnp->qsmask = 0;
    (<0.0>,9018) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9020) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9021) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9023) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9024) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9026) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9027) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9029) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9034) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9035) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9037) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9038) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9040) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9044) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9045) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9046) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9047) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9049) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9050) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9052) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9053) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9054) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9056) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9059) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9060) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9061) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9067)
    (<0.0>,9068)
    (<0.0>,9069)
    (<0.0>,9070) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9072) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9073) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9075) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9078) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9079) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9080) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9089)
    (<0.0>,9090)
    (<0.0>,9091)
    (<0.0>,9092) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9095) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9098) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9101) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9102) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9105) tree.c:1434: return false;
    (<0.0>,9107) tree.c:1483: }
    (<0.0>,9110) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9112) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9114) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9115) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9117) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9120) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9122) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9123) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9125) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9128) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,9130) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,9131) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9133) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9134) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9136) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9141) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9144) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9145) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,9148)
    (<0.0>,9151) tree.c:1575: return ret;
    (<0.0>,9155) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,9158)
    (<0.0>,9162) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,9166)
    (<0.0>,9167) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9168) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9173) fake_sched.h:43: return __running_cpu;
    (<0.0>,9177) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9179) fake_sched.h:43: return __running_cpu;
    (<0.0>,9183) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9196) fake_sched.h:43: return __running_cpu;
    (<0.0>,9201) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,9207) fake_sched.h:43: return __running_cpu;
    (<0.0>,9212) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,9218) fake_sched.h:43: return __running_cpu;
    (<0.0>,9222) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,9234) fake_sched.h:43: return __running_cpu;
    (<0.0>,9238)
    (<0.0>,9241) tree.c:580: local_irq_save(flags);
    (<0.0>,9244)
    (<0.0>,9246) fake_sched.h:43: return __running_cpu;
    (<0.0>,9250) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9252) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9256) fake_sched.h:43: return __running_cpu;
    (<0.0>,9260) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9273)
    (<0.0>,9275) fake_sched.h:43: return __running_cpu;
    (<0.0>,9279) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9280) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9282) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9283) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9284) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9290) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9291) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9296) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9297) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9298) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9299) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,9303) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9305) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9306) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9307) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9326)
    (<0.0>,9328)
    (<0.0>,9330) fake_sched.h:43: return __running_cpu;
    (<0.0>,9334) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9337) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,9341) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9342) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9343) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9347) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9348) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9349) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9351) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9356) fake_sched.h:43: return __running_cpu;
    (<0.0>,9359) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9361) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9363) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9364) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9367)
    (<0.0>,9370) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9373) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9374) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9375) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9379) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9380) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9381) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9383) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9388) fake_sched.h:43: return __running_cpu;
    (<0.0>,9391) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9393) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9395) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9396) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9399)
    (<0.0>,9402) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9405) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9406) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9407) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9411) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9412) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9413) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9415) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9422) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9425) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9426) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9427) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9429) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9430) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9432) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9435) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9441) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9442) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9445) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9450) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9451) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9452) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9466)
    (<0.0>,9468) tree.c:583: local_irq_restore(flags);
    (<0.0>,9471)
    (<0.0>,9473) fake_sched.h:43: return __running_cpu;
    (<0.0>,9477) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9479) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9483) fake_sched.h:43: return __running_cpu;
    (<0.0>,9487) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9493) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9496) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9501) fake_sched.h:43: return __running_cpu;
    (<0.0>,9505)
    (<0.0>,9506) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9509) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9514) tree.c:704: local_irq_save(flags);
    (<0.0>,9517)
    (<0.0>,9519) fake_sched.h:43: return __running_cpu;
    (<0.0>,9523) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9525) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9529) fake_sched.h:43: return __running_cpu;
    (<0.0>,9533) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9546)
    (<0.0>,9548) fake_sched.h:43: return __running_cpu;
    (<0.0>,9552) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9553) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9555) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9556) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9557) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9562) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9563) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9567) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9568) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9569) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9570) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,9574) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9576) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9577) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9578) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9592)
    (<0.0>,9593)
    (<0.0>,9595) fake_sched.h:43: return __running_cpu;
    (<0.0>,9599) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9603) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9606) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9607) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9608) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9610) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9611) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9613) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9616) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9623) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9624) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9627) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9632) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9633) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9634) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9639) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,9648)
    (<0.0>,9650) tree.c:707: local_irq_restore(flags);
    (<0.0>,9653)
    (<0.0>,9655) fake_sched.h:43: return __running_cpu;
    (<0.0>,9659) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9661) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9665) fake_sched.h:43: return __running_cpu;
    (<0.0>,9669) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9684) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9686) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9688) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9689) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9691) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9696) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,9700)
    (<0.0>,9701) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,9703) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,9707) tree.c:1674: return 1;
    (<0.0>,9709) tree.c:1675: }
    (<0.0>,9714) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,9715) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,9716) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,9717) tree.c:1819: if (j > HZ) {
    (<0.0>,9720) tree.c:1823: ret = 0;
    (<0.0>,9722) tree.c:1825: if (!ret)
    (<0.0>,9725) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,9726) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,9728) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,9730) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,9734) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,9736) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,9740) fake_sched.h:43: return __running_cpu;
    (<0.0>,9744) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9748) fake_sched.h:43: return __running_cpu;
    (<0.0>,9752) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9757) fake_sched.h:43: return __running_cpu;
    (<0.0>,9761) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9771) tree.c:749: local_irq_save(flags);
    (<0.0>,9774)
    (<0.0>,9776) fake_sched.h:43: return __running_cpu;
    (<0.0>,9780) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9782) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9787) fake_sched.h:43: return __running_cpu;
    (<0.0>,9791) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9792) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,9794) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,9795) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,9796) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,9798) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,9800) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,9801) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9803) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9808) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9809) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9811) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9815) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9816) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9817) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,9818) tree.c:754: if (oldval)
    (<0.0>,9826)
    (<0.0>,9828) tree.c:759: local_irq_restore(flags);
    (<0.0>,9831)
    (<0.0>,9833) fake_sched.h:43: return __running_cpu;
    (<0.0>,9837) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9839) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9847)
    (<0.0>,9852) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,9857) fake_sched.h:43: return __running_cpu;
    (<0.0>,9862) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,9870) fake_sched.h:43: return __running_cpu;
    (<0.0>,9875) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,9889) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9890) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9891) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9895) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9896) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9897) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9899) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,9903) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,9905) fake_sched.h:43: return __running_cpu;
    (<0.0>,9908) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,9910) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,9917)
    (<0.0>,9918)
    (<0.0>,9919) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9921) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9922) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9923) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,9925) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,9927) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,9928) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,9929) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,9939)
    (<0.0>,9940)
    (<0.0>,9941) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,9946) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,9949)
    (<0.0>,9952) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,9955) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,9957) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,9960) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,9962) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,9966) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,9969)
    (<0.0>,9970) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9972) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9975) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9982) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9983) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9989)
    (<0.0>,9990)
    (<0.0>,9991) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,9994)
    (<0.0>,9995) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9997) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9998) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10000) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10006) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,10008) tree.c:494: }
    (<0.0>,10012) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10014) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10015) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10017) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10020) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10022) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10023) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10025) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10028) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,10031)
    (<0.0>,10035) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10037) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10039) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10040) tree.c:3175: return 0;
    (<0.0>,10042) tree.c:3176: }
    (<0.0>,10047) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10050) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10051) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10052) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10056) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10057) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10058) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10060) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10064) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10066) fake_sched.h:43: return __running_cpu;
    (<0.0>,10069) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10071) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10078)
    (<0.0>,10079)
    (<0.0>,10080) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10082) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10083) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10084) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10086) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10088) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10089) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10090) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10100)
    (<0.0>,10101)
    (<0.0>,10102) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,10107) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,10110)
    (<0.0>,10113) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,10116) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10118) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10121) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10123) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10127) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,10130)
    (<0.0>,10131) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10133) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10136) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10143) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10144) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10150)
    (<0.0>,10151)
    (<0.0>,10152) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,10155)
    (<0.0>,10156) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10158) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10159) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10161) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10167) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,10173)
    (<0.0>,10174) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10177)
    (<0.0>,10178) tree.c:453: return &rsp->node[0];
    (<0.0>,10182) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10183) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10185) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10189) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10190) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10192) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10195) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10196) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,10197) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,10201) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,10204) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,10207) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,10210) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,10211) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,10214) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10216) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10219) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10222) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10225) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10226) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10228) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10231) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10235) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10237) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10239) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10242) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10245) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10248) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10249) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10251) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10254) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,10258) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10260) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10262) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,10265) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,10267) tree.c:494: }
    (<0.0>,10271) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10273) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10274) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10276) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,10279) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10281) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10282) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10284) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,10287) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,10290)
    (<0.0>,10294) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10296) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10298) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,10299) tree.c:3175: return 0;
    (<0.0>,10301) tree.c:3176: }
    (<0.0>,10306) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10309) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10310) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10311) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10315) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10316) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10317) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10319) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10323) tree.c:3190: return 0;
    (<0.0>,10325) tree.c:3191: }
    (<0.0>,10329) tree.c:2437: if (user)
    (<0.0>,10337) fake_sched.h:43: return __running_cpu;
    (<0.0>,10341) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,10343) fake_sched.h:43: return __running_cpu;
    (<0.0>,10347) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10353) fake_sched.h:43: return __running_cpu;
    (<0.0>,10357) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,10367) tree.c:624: local_irq_save(flags);
    (<0.0>,10370)
    (<0.0>,10372) fake_sched.h:43: return __running_cpu;
    (<0.0>,10376) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10378) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10382) fake_sched.h:43: return __running_cpu;
    (<0.0>,10386) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10392) fake_sched.h:43: return __running_cpu;
    (<0.0>,10396) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10397) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,10399) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,10400) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,10401) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,10403) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,10405) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,10406) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10408) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10413) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10414) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10416) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10420) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10421) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10422) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,10423) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,10425) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,10433)
    (<0.0>,10435) tree.c:634: local_irq_restore(flags);
    (<0.0>,10438)
    (<0.0>,10440) fake_sched.h:43: return __running_cpu;
    (<0.0>,10444) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10446) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10450) fake_sched.h:43: return __running_cpu;
    (<0.0>,10454) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10463) fake_sched.h:43: return __running_cpu;
    (<0.0>,10467)
    (<0.0>,10470) tree.c:580: local_irq_save(flags);
    (<0.0>,10473)
    (<0.0>,10475) fake_sched.h:43: return __running_cpu;
    (<0.0>,10479) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10481) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10485) fake_sched.h:43: return __running_cpu;
    (<0.0>,10489) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10502)
    (<0.0>,10504) fake_sched.h:43: return __running_cpu;
    (<0.0>,10508) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10509) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10511) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10512) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,10513) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10519) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10520) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10525) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10526) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10527) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,10528) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,10532) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,10534) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,10535) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,10536) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,10555)
    (<0.0>,10557)
    (<0.0>,10559) fake_sched.h:43: return __running_cpu;
    (<0.0>,10563) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10566) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,10570) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10571) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10572) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10576) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10577) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10578) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10580) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10585) fake_sched.h:43: return __running_cpu;
    (<0.0>,10588) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10590) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10592) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10593) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10596)
    (<0.0>,10599) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10602) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10603) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10604) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10608) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10609) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10610) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10612) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10617) fake_sched.h:43: return __running_cpu;
    (<0.0>,10620) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10622) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10624) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,10625) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10628)
    (<0.0>,10631) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10634) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10635) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10636) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10640) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10641) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10642) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10644) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10651) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10654) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10655) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10656) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10658) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10659) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10661) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10664) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10670) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10671) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10674) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10679) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10680) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10681) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10695)
    (<0.0>,10697) tree.c:583: local_irq_restore(flags);
    (<0.0>,10700)
    (<0.0>,10702) fake_sched.h:43: return __running_cpu;
    (<0.0>,10706) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10708) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10712) fake_sched.h:43: return __running_cpu;
    (<0.0>,10716) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10722) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10725) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10730) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10732) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10734) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10738) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10740) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10743) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10746)
    (<0.0>,10755) fake_sched.h:43: return __running_cpu;
    (<0.0>,10759)
    (<0.0>,10760) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10763) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10768) tree.c:704: local_irq_save(flags);
    (<0.0>,10771)
    (<0.0>,10773) fake_sched.h:43: return __running_cpu;
    (<0.0>,10777) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10779) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10783) fake_sched.h:43: return __running_cpu;
    (<0.0>,10787) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10800)
    (<0.0>,10802) fake_sched.h:43: return __running_cpu;
    (<0.0>,10806) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10807) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10809) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10810) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10811) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10816) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10817) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10821) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10822) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10823) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10824) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,10828) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10830) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10831) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10832) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10846)
    (<0.0>,10847)
    (<0.0>,10849) fake_sched.h:43: return __running_cpu;
    (<0.0>,10853) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10857) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10860) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10861) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10862) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10864) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10865) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10867) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10870) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10877) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10878) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10881) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10886) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10887) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10888) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10893) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,10902)
    (<0.0>,10904) tree.c:707: local_irq_restore(flags);
    (<0.0>,10907)
    (<0.0>,10909) fake_sched.h:43: return __running_cpu;
    (<0.0>,10913) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10915) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10919) fake_sched.h:43: return __running_cpu;
    (<0.0>,10923) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10930) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10931) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10932) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,10933) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,10935) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,10938) tree.c:1840: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,10941)
    (<0.0>,10946) tree.c:1872: rcu_gp_cleanup(rsp);
    (<0.0>,10954)
    (<0.0>,10955) tree.c:1720: bool needgp = false;
    (<0.0>,10956) tree.c:1721: int nocb = 0;
    (<0.0>,10957) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10960)
    (<0.0>,10961) tree.c:453: return &rsp->node[0];
    (<0.0>,10965) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10966) tree.c:1725: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,10970)
    (<0.0>,10973) fake_sched.h:43: return __running_cpu;
    (<0.0>,10977) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,10981) fake_sched.h:43: return __running_cpu;
    (<0.0>,10985) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10990) fake_sched.h:43: return __running_cpu;
    (<0.0>,10994) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,10997) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,10998) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,11004) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,11005) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,11007) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,11009) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,11010) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,11011) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,11013) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,11016) tree.c:1739: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,11020)
    (<0.0>,11021) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,11022) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,11027) fake_sched.h:43: return __running_cpu;
    (<0.0>,11031) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,11033) fake_sched.h:43: return __running_cpu;
    (<0.0>,11037) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11043) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11046) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11048) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11049) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11051) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11056) tree.c:1751: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,11060)
    (<0.0>,11063) fake_sched.h:43: return __running_cpu;
    (<0.0>,11067) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,11071) fake_sched.h:43: return __running_cpu;
    (<0.0>,11075) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11080) fake_sched.h:43: return __running_cpu;
    (<0.0>,11084) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,11087) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,11088) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,11094) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,11096) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,11097) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,11099) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,11101) fake_sched.h:43: return __running_cpu;
    (<0.0>,11104) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11106) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11108) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11109) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,11110) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,11112) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,11115) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,11116) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,11117) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,11123)
    (<0.0>,11124)
    (<0.0>,11125)
    (<0.0>,11126) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,11128) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,11129) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,11131) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,11134) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,11135) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,11136) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,11144)
    (<0.0>,11145)
    (<0.0>,11146)
    (<0.0>,11147) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11150) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11153) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11156) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11157) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11160) tree.c:1502: return false;
    (<0.0>,11162) tree.c:1527: }
    (<0.0>,11165) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,11166) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,11168) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,11169) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,11171) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,11175) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,11177) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,11178) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,11180) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,11183) tree.c:1575: return ret;
    (<0.0>,11187) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,11191) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,11193) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,11194) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,11201)
    (<0.0>,11202)
    (<0.0>,11203) tree.c:1385: int c = rnp->completed;
    (<0.0>,11205) tree.c:1385: int c = rnp->completed;
    (<0.0>,11207) tree.c:1385: int c = rnp->completed;
    (<0.0>,11209) fake_sched.h:43: return __running_cpu;
    (<0.0>,11212) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11214) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11216) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11217) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,11218) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,11222)
    (<0.0>,11223)
    (<0.0>,11225) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,11228) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,11231) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,11232) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,11236) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,11239) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,11240) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,11241) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,11242) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,11243) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,11245) tree.c:1393: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,11253)
    (<0.0>,11254)
    (<0.0>,11255)
    (<0.0>,11256)
    (<0.0>,11260) tree.c:1394: return needmore;
    (<0.0>,11262) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,11264) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,11265) tree.c:1759: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,11269)
    (<0.0>,11270) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,11271) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,11276) fake_sched.h:43: return __running_cpu;
    (<0.0>,11280) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,11282) fake_sched.h:43: return __running_cpu;
    (<0.0>,11286) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11299) fake_sched.h:43: return __running_cpu;
    (<0.0>,11304) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,11311) fake_sched.h:43: return __running_cpu;
    (<0.0>,11315) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,11327) fake_sched.h:43: return __running_cpu;
    (<0.0>,11331)
    (<0.0>,11334) tree.c:580: local_irq_save(flags);
    (<0.0>,11337)
    (<0.0>,11339) fake_sched.h:43: return __running_cpu;
    (<0.0>,11343) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11345) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11349) fake_sched.h:43: return __running_cpu;
    (<0.0>,11353) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11366)
    (<0.0>,11368) fake_sched.h:43: return __running_cpu;
    (<0.0>,11372) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11373) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11375) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11376) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11377) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11383) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11384) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11389) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11390) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11391) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11392) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,11396) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11398) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11399) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11400) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11419)
    (<0.0>,11421)
    (<0.0>,11423) fake_sched.h:43: return __running_cpu;
    (<0.0>,11427) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11430) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,11434) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11435) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11436) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11440) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11441) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11442) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11444) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11449) fake_sched.h:43: return __running_cpu;
    (<0.0>,11452) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11454) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11456) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11457) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11460)
    (<0.0>,11463) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11466) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11467) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11468) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11472) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11473) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11474) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11476) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11481) fake_sched.h:43: return __running_cpu;
    (<0.0>,11484) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11486) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11488) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11489) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11492)
    (<0.0>,11495) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11498) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11499) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11500) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11504) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11505) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11506) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11508) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11515) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11518) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11519) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11520) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11522) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11523) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11525) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11528) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11534) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11535) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11538) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11543) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11544) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11545) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11559)
    (<0.0>,11561) tree.c:583: local_irq_restore(flags);
    (<0.0>,11564)
    (<0.0>,11566) fake_sched.h:43: return __running_cpu;
    (<0.0>,11570) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11572) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11576) fake_sched.h:43: return __running_cpu;
    (<0.0>,11580) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11586) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11589) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11594) fake_sched.h:43: return __running_cpu;
    (<0.0>,11598)
    (<0.0>,11599) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,11602) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,11607) tree.c:704: local_irq_save(flags);
    (<0.0>,11610)
    (<0.0>,11612) fake_sched.h:43: return __running_cpu;
    (<0.0>,11616) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11618) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11622) fake_sched.h:43: return __running_cpu;
    (<0.0>,11626) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11639)
    (<0.0>,11641) fake_sched.h:43: return __running_cpu;
    (<0.0>,11645) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11646) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,11648) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,11649) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,11650) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11655) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11656) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11660) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11661) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11662) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,11663) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,11667) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,11669) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,11670) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,11671) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,11685)
    (<0.0>,11686)
    (<0.0>,11688) fake_sched.h:43: return __running_cpu;
    (<0.0>,11692) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11696) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11699) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11700) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11701) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11703) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11704) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,11706) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11709) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11716) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11717) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11720) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11725) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11726) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11727) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,11732) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,11741)
    (<0.0>,11743) tree.c:707: local_irq_restore(flags);
    (<0.0>,11746)
    (<0.0>,11748) fake_sched.h:43: return __running_cpu;
    (<0.0>,11752) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11754) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11758) fake_sched.h:43: return __running_cpu;
    (<0.0>,11762) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11777) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11779) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11781) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11782) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11784) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,11789) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,11792)
    (<0.0>,11793) tree.c:453: return &rsp->node[0];
    (<0.0>,11797) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,11798) tree.c:1763: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,11802)
    (<0.0>,11805) fake_sched.h:43: return __running_cpu;
    (<0.0>,11809) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,11813) fake_sched.h:43: return __running_cpu;
    (<0.0>,11817) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11822) fake_sched.h:43: return __running_cpu;
    (<0.0>,11826) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,11829) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,11830) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,11836) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,11837) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,11841)
    (<0.0>,11842)
    (<0.0>,11844) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,11846) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,11847) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,11849) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,11852) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,11854) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,11856) fake_sched.h:43: return __running_cpu;
    (<0.0>,11859) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11861) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11863) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11864) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,11865) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,11866) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,11874)
    (<0.0>,11875)
    (<0.0>,11876)
    (<0.0>,11877) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11880) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11883) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11886) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11887) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11890) tree.c:1502: return false;
    (<0.0>,11892) tree.c:1527: }
    (<0.0>,11895) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,11899) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,11900) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11903) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11904) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11910)
    (<0.0>,11911)
    (<0.0>,11912) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,11915)
    (<0.0>,11916) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11918) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11919) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11921) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11927) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,11933)
    (<0.0>,11934) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11937)
    (<0.0>,11938) tree.c:453: return &rsp->node[0];
    (<0.0>,11942) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11943) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11945) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11949) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11950) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11952) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11955) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11956) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11957) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11961) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11964) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11967) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11970) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11971) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11974) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11976) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11979) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11982) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11985) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11986) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11988) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11991) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11995) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11997) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11999) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12002) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12005) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12008) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12009) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12011) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12014) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12018) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12020) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12022) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12025) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,12027) tree.c:494: }
    (<0.0>,12031) tree.c:1780: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,12035)
    (<0.0>,12036) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,12037) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,12042) fake_sched.h:43: return __running_cpu;
    (<0.0>,12046) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12048) fake_sched.h:43: return __running_cpu;
    (<0.0>,12052) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12063) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,12065) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,12069) fake_sched.h:43: return __running_cpu;
    (<0.0>,12073) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12077) fake_sched.h:43: return __running_cpu;
    (<0.0>,12081) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12086) fake_sched.h:43: return __running_cpu;
    (<0.0>,12090) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12100) tree.c:749: local_irq_save(flags);
    (<0.0>,12103)
    (<0.0>,12105) fake_sched.h:43: return __running_cpu;
    (<0.0>,12109) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12111) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12116) fake_sched.h:43: return __running_cpu;
    (<0.0>,12120) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,12121) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,12123) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,12124) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,12125) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,12127) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,12129) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,12130) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12132) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12137) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12138) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12140) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12144) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12145) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12146) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,12147) tree.c:754: if (oldval)
    (<0.0>,12155)
    (<0.0>,12157) tree.c:759: local_irq_restore(flags);
    (<0.0>,12160)
    (<0.0>,12162) fake_sched.h:43: return __running_cpu;
    (<0.0>,12166) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12168) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12176)
    (<0.0>,12181) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,12186) fake_sched.h:43: return __running_cpu;
    (<0.0>,12191) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,12199) fake_sched.h:43: return __running_cpu;
    (<0.0>,12204) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,12218) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12219) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12220) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12224) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12225) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12226) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12228) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12232) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12234) fake_sched.h:43: return __running_cpu;
    (<0.0>,12237) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12239) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12246)
    (<0.0>,12247)
    (<0.0>,12248) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12250) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12251) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12252) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12254) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12256) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12257) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,12258) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,12268)
    (<0.0>,12269)
    (<0.0>,12270) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,12275) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,12278)
    (<0.0>,12281) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,12284) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,12286) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,12289) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,12291) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,12295) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,12298)
    (<0.0>,12299) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12301) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12304) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12311) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,12312) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,12318)
    (<0.0>,12319)
    (<0.0>,12320) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,12323)
    (<0.0>,12324) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12326) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12327) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12329) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12335) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,12341)
    (<0.0>,12342) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12345)
    (<0.0>,12346) tree.c:453: return &rsp->node[0];
    (<0.0>,12350) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12351) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12353) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12357) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12358) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12360) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12363) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12364) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,12365) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,12369) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12372) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12375) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12378) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12379) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12382) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12384) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12387) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12390) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12393) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12394) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12396) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12399) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12403) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12405) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12407) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12410) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12413) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12416) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12417) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12419) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12422) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12426) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12428) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12430) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12433) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,12435) tree.c:494: }
    (<0.0>,12439) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12441) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12442) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12444) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12447) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12449) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12450) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12452) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12455) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,12458)
    (<0.0>,12462) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12464) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12466) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12467) tree.c:3175: return 0;
    (<0.0>,12469) tree.c:3176: }
    (<0.0>,12474) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12477) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12478) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12479) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12483) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12484) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12485) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12487) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12491) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12493) fake_sched.h:43: return __running_cpu;
    (<0.0>,12496) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12498) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,12505)
    (<0.0>,12506)
    (<0.0>,12507) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12509) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12510) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,12511) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12513) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12515) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,12516) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,12517) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,12527)
    (<0.0>,12528)
    (<0.0>,12529) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,12534) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,12537)
    (<0.0>,12540) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,12543) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,12545) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,12548) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,12550) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,12554) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,12557)
    (<0.0>,12558) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12560) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12563) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12570) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,12571) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,12577)
    (<0.0>,12578)
    (<0.0>,12579) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,12582)
    (<0.0>,12583) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12585) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12586) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12588) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,12594) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,12600)
    (<0.0>,12601) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12604)
    (<0.0>,12605) tree.c:453: return &rsp->node[0];
    (<0.0>,12609) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12610) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12612) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12616) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12617) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12619) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12622) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12623) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,12624) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,12628) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12631) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12634) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12637) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12638) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12641) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12643) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12646) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12649) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12652) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12653) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12655) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12658) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12662) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12664) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12666) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12669) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12672) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12675) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12676) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12678) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12681) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12685) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12687) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12689) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12692) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,12694) tree.c:494: }
    (<0.0>,12698) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12700) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12701) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12703) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,12706) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12708) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12709) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12711) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
    (<0.0>,12714) tree.c:3168: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,12717)
    (<0.0>,12721) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12723) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12725) tree.c:3174: rdp->n_rp_need_nothing++;
    (<0.0>,12726) tree.c:3175: return 0;
    (<0.0>,12728) tree.c:3176: }
    (<0.0>,12733) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12736) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12737) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12738) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12742) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12743) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12744) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12746) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,12750) tree.c:3190: return 0;
    (<0.0>,12752) tree.c:3191: }
    (<0.0>,12756) tree.c:2437: if (user)
    (<0.0>,12764) fake_sched.h:43: return __running_cpu;
    (<0.0>,12768) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12770) fake_sched.h:43: return __running_cpu;
    (<0.0>,12774) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12780) fake_sched.h:43: return __running_cpu;
    (<0.0>,12784) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,12794) tree.c:624: local_irq_save(flags);
    (<0.0>,12797)
    (<0.0>,12799) fake_sched.h:43: return __running_cpu;
    (<0.0>,12803) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12805) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12809) fake_sched.h:43: return __running_cpu;
    (<0.0>,12813) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12819) fake_sched.h:43: return __running_cpu;
    (<0.0>,12823) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,12824) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,12826) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,12827) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,12828) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,12830) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,12832) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,12833) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12835) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12840) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12841) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12843) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12847) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12848) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12849) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,12850) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,12852) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,12860)
    (<0.0>,12862) tree.c:634: local_irq_restore(flags);
    (<0.0>,12865)
    (<0.0>,12867) fake_sched.h:43: return __running_cpu;
    (<0.0>,12871) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12873) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12877) fake_sched.h:43: return __running_cpu;
    (<0.0>,12881) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12890) fake_sched.h:43: return __running_cpu;
    (<0.0>,12894)
    (<0.0>,12897) tree.c:580: local_irq_save(flags);
    (<0.0>,12900)
    (<0.0>,12902) fake_sched.h:43: return __running_cpu;
    (<0.0>,12906) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12908) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12912) fake_sched.h:43: return __running_cpu;
    (<0.0>,12916) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12929)
    (<0.0>,12931) fake_sched.h:43: return __running_cpu;
    (<0.0>,12935) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,12936) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,12938) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,12939) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,12940) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12946) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12947) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12952) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12953) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12954) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,12955) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,12959) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,12961) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,12962) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12963) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12982)
    (<0.0>,12984)
    (<0.0>,12986) fake_sched.h:43: return __running_cpu;
    (<0.0>,12990) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,12993) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,12997) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12998) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,12999) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13003) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13004) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13005) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13007) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13012) fake_sched.h:43: return __running_cpu;
    (<0.0>,13015) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13017) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13019) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13020) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,13023)
    (<0.0>,13026) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13029) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13030) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13031) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13035) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13036) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13037) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13039) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13044) fake_sched.h:43: return __running_cpu;
    (<0.0>,13047) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13049) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13051) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,13052) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,13055)
    (<0.0>,13058) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13061) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13062) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13063) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13067) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13068) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13069) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13071) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,13078) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13081) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13082) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13083) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13085) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13086) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,13088) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13091) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13097) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13098) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13101) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13106) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13107) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13108) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,13122)
    (<0.0>,13124) tree.c:583: local_irq_restore(flags);
    (<0.0>,13127)
    (<0.0>,13129) fake_sched.h:43: return __running_cpu;
    (<0.0>,13133) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13135) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13139) fake_sched.h:43: return __running_cpu;
    (<0.0>,13143) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13149) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,13152) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,13157) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,13159) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
      (<0.1>,720) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,723) fake_sync.h:241: while (!x->done)
      (<0.1>,725) fake_sync.h:241: while (!x->done)
      (<0.1>,732) fake_sched.h:43: return __running_cpu;
      (<0.1>,736)
      (<0.1>,737) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,740) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,745) tree.c:704: local_irq_save(flags);
      (<0.1>,748)
      (<0.1>,750) fake_sched.h:43: return __running_cpu;
      (<0.1>,754) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,756) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,760) fake_sched.h:43: return __running_cpu;
      (<0.1>,764) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,777)
      (<0.1>,779) fake_sched.h:43: return __running_cpu;
      (<0.1>,783) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,784) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,786) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,787) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,788) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,793) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,794) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,798) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,799) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,800) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,801) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,805) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,807) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,808) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,809) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,823)
      (<0.1>,824)
      (<0.1>,826) fake_sched.h:43: return __running_cpu;
      (<0.1>,830) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,834) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,837) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,838) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,839) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,841) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,842) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,844) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,847) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,854) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,855) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,858) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,864) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,865) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,870) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,879)
      (<0.1>,881) tree.c:707: local_irq_restore(flags);
      (<0.1>,884)
      (<0.1>,886) fake_sched.h:43: return __running_cpu;
      (<0.1>,890) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,892) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,896) fake_sched.h:43: return __running_cpu;
      (<0.1>,900) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,911)
      (<0.1>,917) litmus.c:91: y = 1;
  (<0>,6311) litmus.c:69: r_y = y;
  (<0>,6312) litmus.c:69: r_y = y;
  (<0>,6328) fake_sched.h:43: return __running_cpu;
  (<0>,6333) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
  (<0>,6339) fake_sched.h:43: return __running_cpu;
  (<0>,6344) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
  (<0>,6350) fake_sched.h:43: return __running_cpu;
  (<0>,6354) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
  (<0>,6366) fake_sched.h:43: return __running_cpu;
  (<0>,6370)
  (<0>,6373) tree.c:580: local_irq_save(flags);
  (<0>,6376)
  (<0>,6378) fake_sched.h:43: return __running_cpu;
  (<0>,6382) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6384) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6388) fake_sched.h:43: return __running_cpu;
  (<0>,6392) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6405)
  (<0>,6407) fake_sched.h:43: return __running_cpu;
  (<0>,6411) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6412) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6414) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6415) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6416) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6422) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6423) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6428) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6429) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6430) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6431) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,6435) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,6437) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,6438) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,6439) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,6458)
  (<0>,6460)
  (<0>,6462) fake_sched.h:43: return __running_cpu;
  (<0>,6466) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6469) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,6473) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6474) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6475) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6479) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6480) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6481) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6483) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6488) fake_sched.h:43: return __running_cpu;
  (<0>,6491) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6493) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6495) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6496) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,6499)
  (<0>,6502) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6505) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6506) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6507) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6511) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6512) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6513) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6515) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6520) fake_sched.h:43: return __running_cpu;
  (<0>,6523) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6525) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6527) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6528) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,6531)
  (<0>,6534) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6537) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6538) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6539) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6543) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6544) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6545) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6547) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6554) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6557) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6558) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6559) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6561) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6562) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6564) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6567) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6573) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6574) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6577) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6582) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6583) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6584) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6598)
  (<0>,6600) tree.c:583: local_irq_restore(flags);
  (<0>,6603)
  (<0>,6605) fake_sched.h:43: return __running_cpu;
  (<0>,6609) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6611) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6615) fake_sched.h:43: return __running_cpu;
  (<0>,6619) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6625) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6628) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6633) fake_sched.h:43: return __running_cpu;
  (<0>,6637)
  (<0>,6638) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,6641) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,6646) tree.c:704: local_irq_save(flags);
  (<0>,6649)
  (<0>,6651) fake_sched.h:43: return __running_cpu;
  (<0>,6655) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6657) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6661) fake_sched.h:43: return __running_cpu;
  (<0>,6665) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6678)
  (<0>,6680) fake_sched.h:43: return __running_cpu;
  (<0>,6684) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6685) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6687) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6688) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,6689) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6694) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6695) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6699) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6700) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6701) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,6702) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,6706) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6708) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6709) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,6710) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,6724)
  (<0>,6725)
  (<0>,6727) fake_sched.h:43: return __running_cpu;
  (<0>,6731) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6735) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6738) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6739) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6740) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6742) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6743) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,6745) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6748) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6755) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6756) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6759) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6764) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6765) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6766) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,6771) tree.c:656: if (!user && !is_idle_task(current)) {
  (<0>,6780)
  (<0>,6782) tree.c:707: local_irq_restore(flags);
  (<0>,6785)
  (<0>,6787) fake_sched.h:43: return __running_cpu;
  (<0>,6791) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6793) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6797) fake_sched.h:43: return __running_cpu;
  (<0>,6801) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6812) fake_sched.h:43: return __running_cpu;
  (<0>,6816) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
  (<0>,6820) fake_sched.h:43: return __running_cpu;
  (<0>,6824) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6829) fake_sched.h:43: return __running_cpu;
  (<0>,6833) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
  (<0>,6843) tree.c:749: local_irq_save(flags);
  (<0>,6846)
  (<0>,6848) fake_sched.h:43: return __running_cpu;
  (<0>,6852) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6854) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6859) fake_sched.h:43: return __running_cpu;
  (<0>,6863) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6864) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6866) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6867) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,6868) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6870) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6872) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,6873) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6875) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6880) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6881) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6883) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6887) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6888) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6889) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,6890) tree.c:754: if (oldval)
  (<0>,6898)
  (<0>,6900) tree.c:759: local_irq_restore(flags);
  (<0>,6903)
  (<0>,6905) fake_sched.h:43: return __running_cpu;
  (<0>,6909) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6911) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6919)
  (<0>,6924) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,6929) fake_sched.h:43: return __running_cpu;
  (<0>,6934) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,6942) fake_sched.h:43: return __running_cpu;
  (<0>,6947) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
  (<0>,6961) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6962) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6963) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6967) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6968) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6969) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6971) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,6975) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6977) fake_sched.h:43: return __running_cpu;
  (<0>,6980) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6982) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,6989)
  (<0>,6990)
  (<0>,6991) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6993) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6994) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,6995) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,6997) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,6999) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,7000) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,7001) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,7011)
  (<0>,7012)
  (<0>,7013) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
  (<0>,7018) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
  (<0>,7021)
  (<0>,7024) tree.c:3135: if (rcu_scheduler_fully_active &&
  (<0>,7027) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,7029) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,7032) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,7034) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,7038) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,7041)
  (<0>,7042) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7044) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7047) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7054) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7055) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7061)
  (<0>,7062)
  (<0>,7063) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,7066)
  (<0>,7067) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7069) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7070) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7072) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7078) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,7084)
  (<0>,7085) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7088)
  (<0>,7089) tree.c:453: return &rsp->node[0];
  (<0>,7093) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7094) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7096) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7100) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7101) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7103) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7106) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7107) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7108) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7112) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7115) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7118) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7121) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7122) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7125) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7127) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7130) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7133) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7136) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7137) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7139) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7142) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7146) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7148) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7150) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7153) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7156) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7159) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7160) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7162) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7165) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7169) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7171) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7173) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7176) tree.c:493: return 0; /* No grace period needed. */
  (<0>,7178) tree.c:494: }
  (<0>,7182) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7184) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7185) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7187) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,7190) tree.c:3157: rdp->n_rp_gp_completed++;
  (<0>,7192) tree.c:3157: rdp->n_rp_gp_completed++;
  (<0>,7194) tree.c:3157: rdp->n_rp_gp_completed++;
  (<0>,7195) tree.c:3158: return 1;
  (<0>,7197) tree.c:3176: }
  (<0>,7201) tree.c:3189: return 1;
  (<0>,7203) tree.c:3191: }
  (<0>,7210) fake_sched.h:43: return __running_cpu;
  (<0>,7214) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
  (<0>,7218) tree.c:2437: if (user)
  (<0>,7226) fake_sched.h:43: return __running_cpu;
  (<0>,7230) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
  (<0>,7232) fake_sched.h:43: return __running_cpu;
  (<0>,7236) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7242) fake_sched.h:43: return __running_cpu;
  (<0>,7246) fake_sched.h:182: if (need_softirq[get_cpu()]) {
  (<0>,7256)
  (<0>,7259) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7260) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7261) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7265) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7266) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7267) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7269) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7273) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,7282)
  (<0>,7284) fake_sched.h:43: return __running_cpu;
  (<0>,7287) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7289) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7291) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7292) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7294) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7301) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7302) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7304) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7310) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7311) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7312) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7313) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7314) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7318)
  (<0>,7319)
  (<0>,7320) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7321) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7328)
  (<0>,7329)
  (<0>,7330) tree.c:1584: local_irq_save(flags);
  (<0>,7333)
  (<0>,7335) fake_sched.h:43: return __running_cpu;
  (<0>,7339) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7341) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7345) fake_sched.h:43: return __running_cpu;
  (<0>,7349) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7354) tree.c:1585: rnp = rdp->mynode;
  (<0>,7356) tree.c:1585: rnp = rdp->mynode;
  (<0>,7357) tree.c:1585: rnp = rdp->mynode;
  (<0>,7358) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7360) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7361) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7363) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7366) tree.c:1588: !raw_spin_trylock(&rnp->lock)) { /* irqs already off, so later. */
  (<0>,7371)
  (<0>,7373) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
  (<0>,7374) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
  (<0>,7377) fake_sync.h:127: return 1;
  (<0>,7379) fake_sync.h:128: }
  (<0>,7385) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7386) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7387) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7393)
  (<0>,7394)
  (<0>,7395)
  (<0>,7396) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,7398) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,7399) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,7401) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,7404) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,7405) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,7406) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,7414)
  (<0>,7415)
  (<0>,7416)
  (<0>,7417) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7420) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7423) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7426) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7427) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7430) tree.c:1502: return false;
  (<0>,7432) tree.c:1527: }
  (<0>,7435) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,7436) tree.c:1552: rdp->completed = rnp->completed;
  (<0>,7438) tree.c:1552: rdp->completed = rnp->completed;
  (<0>,7439) tree.c:1552: rdp->completed = rnp->completed;
  (<0>,7441) tree.c:1552: rdp->completed = rnp->completed;
  (<0>,7445) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,7447) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,7448) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,7450) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,7453) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,7455) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,7456) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,7458) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,7461) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,7463) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,7464) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7466) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7467) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7469) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7474) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7477) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7478) tree.c:1573: zero_cpu_stall_ticks(rdp);
  (<0>,7481)
  (<0>,7484) tree.c:1575: return ret;
  (<0>,7488) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7489) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,7491) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,7495)
  (<0>,7496)
  (<0>,7497) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,7498) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,7501) fake_sync.h:86: local_irq_restore(flags);
  (<0>,7504)
  (<0>,7506) fake_sched.h:43: return __running_cpu;
  (<0>,7510) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7512) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7516) fake_sched.h:43: return __running_cpu;
  (<0>,7520) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7527) tree.c:1595: if (needwake)
  (<0>,7531) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7533) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7538) tree.c:2558: local_irq_save(flags);
  (<0>,7541)
  (<0>,7543) fake_sched.h:43: return __running_cpu;
  (<0>,7547) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7549) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7553) fake_sched.h:43: return __running_cpu;
  (<0>,7557) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7562) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7563) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7569)
  (<0>,7570)
  (<0>,7571) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,7574)
  (<0>,7575) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7577) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7578) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7580) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7586) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,7592)
  (<0>,7593) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7596)
  (<0>,7597) tree.c:453: return &rsp->node[0];
  (<0>,7601) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7602) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7604) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7608) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7609) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7611) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7614) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7615) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7616) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7620) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7623) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7626) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7629) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7630) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7633) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7635) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7638) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7641) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7644) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7645) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7647) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7650) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7654) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7656) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7658) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7661) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7664) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7667) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7668) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7670) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7673) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7677) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7679) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7681) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7684) tree.c:493: return 0; /* No grace period needed. */
  (<0>,7686) tree.c:494: }
  (<0>,7690) tree.c:2566: local_irq_restore(flags);
  (<0>,7693)
  (<0>,7695) fake_sched.h:43: return __running_cpu;
  (<0>,7699) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7701) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7705) fake_sched.h:43: return __running_cpu;
  (<0>,7709) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7715) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7718)
  (<0>,7719) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7721) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7724) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7731) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,7734)
  (<0>,7738) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7741) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7742) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7743) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7747) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7748) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7749) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7751) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,7755) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,7764)
  (<0>,7766) fake_sched.h:43: return __running_cpu;
  (<0>,7769) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7771) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7773) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,7774) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7776) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7783) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7784) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7786) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7792) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7793) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7794) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7795) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7796) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7800)
  (<0>,7801)
  (<0>,7802) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7803) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,7810)
  (<0>,7811)
  (<0>,7812) tree.c:1584: local_irq_save(flags);
  (<0>,7815)
  (<0>,7817) fake_sched.h:43: return __running_cpu;
  (<0>,7821) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7823) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7827) fake_sched.h:43: return __running_cpu;
  (<0>,7831) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7836) tree.c:1585: rnp = rdp->mynode;
  (<0>,7838) tree.c:1585: rnp = rdp->mynode;
  (<0>,7839) tree.c:1585: rnp = rdp->mynode;
  (<0>,7840) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7842) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7843) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7845) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,7848) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7850) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7851) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7853) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,7856) tree.c:1589: local_irq_restore(flags);
  (<0>,7859)
  (<0>,7861) fake_sched.h:43: return __running_cpu;
  (<0>,7865) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7867) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7871) fake_sched.h:43: return __running_cpu;
  (<0>,7875) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7882) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7884) tree.c:2084: if (!rdp->qs_pending)
  (<0>,7889) tree.c:2558: local_irq_save(flags);
  (<0>,7892)
  (<0>,7894) fake_sched.h:43: return __running_cpu;
  (<0>,7898) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7900) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7904) fake_sched.h:43: return __running_cpu;
  (<0>,7908) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7913) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7914) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7920)
  (<0>,7921)
  (<0>,7922) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,7925)
  (<0>,7926) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7928) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7929) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7931) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,7937) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,7943)
  (<0>,7944) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7947)
  (<0>,7948) tree.c:453: return &rsp->node[0];
  (<0>,7952) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7953) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7955) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7959) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7960) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7962) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7965) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,7966) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7967) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,7971) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7974) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7977) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7980) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7981) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7984) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7986) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7989) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7992) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7995) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7996) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7998) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8001) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8005) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8007) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8009) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8012) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8015) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8018) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8019) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8021) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8024) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8028) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8030) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8032) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8035) tree.c:493: return 0; /* No grace period needed. */
  (<0>,8037) tree.c:494: }
  (<0>,8041) tree.c:2566: local_irq_restore(flags);
  (<0>,8044)
  (<0>,8046) fake_sched.h:43: return __running_cpu;
  (<0>,8050) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8052) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8056) fake_sched.h:43: return __running_cpu;
  (<0>,8060) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8066) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,8069)
  (<0>,8070) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,8072) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,8075) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,8082) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,8085)
  (<0>,8089) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,8092) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,8093) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,8094) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,8098) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,8099) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,8100) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,8102) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,8110) fake_sched.h:43: return __running_cpu;
  (<0>,8114) fake_sched.h:184: need_softirq[get_cpu()] = 0;
  (<0>,8123) tree.c:624: local_irq_save(flags);
  (<0>,8126)
  (<0>,8128) fake_sched.h:43: return __running_cpu;
  (<0>,8132) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8134) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8138) fake_sched.h:43: return __running_cpu;
  (<0>,8142) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8148) fake_sched.h:43: return __running_cpu;
  (<0>,8152) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,8153) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,8155) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,8156) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,8157) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,8159) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,8161) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,8162) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8164) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8169) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8170) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8172) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8176) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8177) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8178) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,8179) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,8181) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,8189)
  (<0>,8191) tree.c:634: local_irq_restore(flags);
  (<0>,8194)
  (<0>,8196) fake_sched.h:43: return __running_cpu;
  (<0>,8200) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8202) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8206) fake_sched.h:43: return __running_cpu;
  (<0>,8210) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8219) fake_sched.h:43: return __running_cpu;
  (<0>,8223)
  (<0>,8226) tree.c:580: local_irq_save(flags);
  (<0>,8229)
  (<0>,8231) fake_sched.h:43: return __running_cpu;
  (<0>,8235) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8237) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8241) fake_sched.h:43: return __running_cpu;
  (<0>,8245) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8258)
  (<0>,8260) fake_sched.h:43: return __running_cpu;
  (<0>,8264) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,8265) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,8267) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,8268) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,8269) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8275) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8276) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8281) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8282) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8283) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,8284) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,8288) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,8290) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,8291) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,8292) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,8311)
  (<0>,8313)
  (<0>,8315) fake_sched.h:43: return __running_cpu;
  (<0>,8319) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,8322) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,8326) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8327) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8328) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8332) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8333) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8334) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8336) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8341) fake_sched.h:43: return __running_cpu;
  (<0>,8344) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8346) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8348) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8349) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,8352)
  (<0>,8355) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8358) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8359) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8360) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8364) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8365) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8366) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8368) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8373) fake_sched.h:43: return __running_cpu;
  (<0>,8376) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8378) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8380) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,8381) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,8384)
  (<0>,8387) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8390) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8391) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8392) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8396) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8397) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8398) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8400) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,8407) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8410) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8411) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8412) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8414) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8415) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,8417) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8420) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8426) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8427) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8430) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8435) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8436) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8437) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,8451)
  (<0>,8453) tree.c:583: local_irq_restore(flags);
  (<0>,8456)
  (<0>,8458) fake_sched.h:43: return __running_cpu;
  (<0>,8462) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8464) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8468) fake_sched.h:43: return __running_cpu;
  (<0>,8472) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8478) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8481) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8486) litmus.c:138: if (pthread_join(tu, NULL))
      (<0.1>,918) litmus.c:93: fake_release_cpu(get_cpu());
      (<0.1>,919) fake_sched.h:43: return __running_cpu;
      (<0.1>,923)
      (<0.1>,926) tree.c:580: local_irq_save(flags);
      (<0.1>,929)
      (<0.1>,931) fake_sched.h:43: return __running_cpu;
      (<0.1>,935) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,937) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,941) fake_sched.h:43: return __running_cpu;
      (<0.1>,945) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,958)
      (<0.1>,960) fake_sched.h:43: return __running_cpu;
      (<0.1>,964) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,965) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,967) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,968) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,969) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,975) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,976) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,981) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,982) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,983) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,984) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,988) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,990) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,991) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,992) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1011)
      (<0.1>,1013)
      (<0.1>,1015) fake_sched.h:43: return __running_cpu;
      (<0.1>,1019) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1022) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,1026) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1027) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1028) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1033) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1034) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1041) fake_sched.h:43: return __running_cpu;
      (<0.1>,1044) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1046) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1048) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1049) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1052)
      (<0.1>,1055) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1058) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1059) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1060) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1065) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1066) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1073) fake_sched.h:43: return __running_cpu;
      (<0.1>,1076) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1078) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1080) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1081) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1084)
      (<0.1>,1087) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1090) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1091) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1092) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1097) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1098) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1107) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1110) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1111) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1112) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1117) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1120) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1126) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1127) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1130) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1135) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1136) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1137) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1151)
      (<0.1>,1153) tree.c:583: local_irq_restore(flags);
      (<0.1>,1156)
      (<0.1>,1158) fake_sched.h:43: return __running_cpu;
      (<0.1>,1162) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1164) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1168) fake_sched.h:43: return __running_cpu;
      (<0.1>,1172) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1178) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1181) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8487) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,8490) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8493) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8497) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8498) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8499) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
             Error: Assertion violation at (<0>,8502): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpvzhoas5a/tmp38_mq2bp.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DFORCE_FAILURE_3 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpvzhoas5a/tmp8pj536ln.ll /tmp/tmpvzhoas5a/tmp38_mq2bp.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=-1 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpvzhoas5a/tmp8pj536ln.ll
Total wall-clock time: 539.05 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 8 (also 1 sleepset blocked)

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,73) tree_plugin.h:931: pr_info("Hierarchical RCU implementation.\n");
  (<0>,77) tree_plugin.h:91: if (rcu_fanout_leaf != CONFIG_RCU_FANOUT_LEAF)
  (<0>,89) tree.c:3718: ulong d;
  (<0>,90) tree.c:3722: int rcu_capacity[MAX_RCU_LVLS + 1];
  (<0>,91) tree.c:3732: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,94) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,95) tree.c:3733: jiffies_till_first_fqs = d;
  (<0>,97) tree.c:3734: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,100) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,101) tree.c:3735: jiffies_till_next_fqs = d;
  (<0>,103) tree.c:3738: if (rcu_fanout_leaf == CONFIG_RCU_FANOUT_LEAF &&
  (<0>,115)
  (<0>,116) tree.c:3628: static void __init rcu_init_one(struct rcu_state *rsp,
  (<0>,117) tree.c:3629: struct rcu_data __percpu *rda)
  (<0>,118) tree.c:3643: int i;
  (<0>,121) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,123) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,124) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,127) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,130) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,133) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,136) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,140) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,142) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,143) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,146) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,149) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,152) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,158)
  (<0>,159) tree.c:3610: static void __init rcu_init_levelspread(struct rcu_state *rsp)
  (<0>,160) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,162) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,164) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,169) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,172) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,173) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,174) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,175) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,183) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,186) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,187) tree.c:3620: cprv = ccur;
  (<0>,188) tree.c:3620: cprv = ccur;
  (<0>,190) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,192) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,194) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,198) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,199) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,201) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,202) tree.c:3661: fl_mask <<= 1;
  (<0>,206) tree.c:3661: fl_mask <<= 1;
  (<0>,207) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,209) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,211) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,219) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,221) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,223) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,224) tree.c:3667: rnp = rsp->level[i];
  (<0>,226) tree.c:3667: rnp = rsp->level[i];
  (<0>,229) tree.c:3667: rnp = rsp->level[i];
  (<0>,230) tree.c:3667: rnp = rsp->level[i];
  (<0>,231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,233) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,236) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,242) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,246)
  (<0>,247) fake_sync.h:68: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,248) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,254) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,258)
  (<0>,259) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,260) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,268) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,269) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,271) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,274) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,275) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,277) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,278) tree.c:3677: rnp->qsmask = 0;
  (<0>,280) tree.c:3677: rnp->qsmask = 0;
  (<0>,281) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,283) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,285) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,287) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,295) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,297) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,298) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,300) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,303) tree.c:3683: if (i == 0) {
  (<0>,306) tree.c:3684: rnp->grpnum = 0;
  (<0>,308) tree.c:3684: rnp->grpnum = 0;
  (<0>,309) tree.c:3685: rnp->grpmask = 0;
  (<0>,311) tree.c:3685: rnp->grpmask = 0;
  (<0>,312) tree.c:3686: rnp->parent = NULL;
  (<0>,314) tree.c:3686: rnp->parent = NULL;
  (<0>,316) tree.c:3693: rnp->level = i;
  (<0>,318) tree.c:3693: rnp->level = i;
  (<0>,320) tree.c:3693: rnp->level = i;
  (<0>,321) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,325)
  (<0>,326) fake_defs.h:273: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,327) fake_defs.h:275: list->next = list;
  (<0>,329) fake_defs.h:275: list->next = list;
  (<0>,330) fake_defs.h:276: list->prev = list;
  (<0>,331) fake_defs.h:276: list->prev = list;
  (<0>,333) fake_defs.h:276: list->prev = list;
  (<0>,335) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,338) tree_plugin.h:2694: }
  (<0>,341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,348) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,351) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,354) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,358) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,360) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,362) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,365) tree.c:3699: rsp->rda = rda;
  (<0>,366) tree.c:3699: rsp->rda = rda;
  (<0>,368) tree.c:3699: rsp->rda = rda;
  (<0>,371) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,374) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,377) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,378) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,379) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,381) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,382) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,389)
  (<0>,390) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,391) fake_defs.h:530: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,393) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,394) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,397) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,399) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,400) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,403) fake_defs.h:536: if (offset & mask)
  (<0>,404) fake_defs.h:536: if (offset & mask)
  (<0>,408) fake_defs.h:537: return cpu;
  (<0>,409) fake_defs.h:537: return cpu;
  (<0>,411) fake_defs.h:540: }
  (<0>,413) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,414) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,418) tree.c:3703: while (i > rnp->grphi)
  (<0>,419) tree.c:3703: while (i > rnp->grphi)
  (<0>,421) tree.c:3703: while (i > rnp->grphi)
  (<0>,424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,425) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,429) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,432) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,433) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,434) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,447)
  (<0>,448) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,449) tree.c:3403: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,451) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,453) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,455) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,456) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,459)
  (<0>,460) tree.c:451: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,464) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,465) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,467) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,471)
  (<0>,472) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,473) fake_sync.h:74: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,476) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,478) fake_sched.h:43: return __running_cpu;
  (<0>,482) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,484) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,488) fake_sched.h:43: return __running_cpu;
  (<0>,492) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,498) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,499) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,504) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,506) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,508) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,512) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,514) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,515) tree.c:3412: init_callback_list(rdp);
  (<0>,519)
  (<0>,520) tree.c:1223: static void init_callback_list(struct rcu_data *rdp)
  (<0>,523) tree_plugin.h:2732: return false;
  (<0>,526) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,528) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,529) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,531) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,534) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,538) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,541) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,543) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,545) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,547) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,550) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,554) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,557) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,559) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,561) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,563) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,566) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,570) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,573) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,575) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,577) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,579) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,582) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,586) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,589) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,591) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,593) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,595) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,599) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,601) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,602) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,604) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,608) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,610) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,611) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,613) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,621) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,623) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,629) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,630) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,631) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,634) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,642) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,645) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,652) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,653) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,654) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,655) tree.c:3418: rdp->cpu = cpu;
  (<0>,656) tree.c:3418: rdp->cpu = cpu;
  (<0>,658) tree.c:3418: rdp->cpu = cpu;
  (<0>,659) tree.c:3419: rdp->rsp = rsp;
  (<0>,660) tree.c:3419: rdp->rsp = rsp;
  (<0>,662) tree.c:3419: rdp->rsp = rsp;
  (<0>,663) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,666) tree_plugin.h:2711: }
  (<0>,668) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,670) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,674)
  (<0>,675) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,676) fake_sync.h:82: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,677) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,680) fake_sync.h:86: local_irq_restore(flags);
  (<0>,683) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,685) fake_sched.h:43: return __running_cpu;
  (<0>,689) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,691) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,695) fake_sched.h:43: return __running_cpu;
  (<0>,699) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,708) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,709) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,716)
  (<0>,717)
  (<0>,718) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,720) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,721) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,724) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,726) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,727) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,730) fake_defs.h:536: if (offset & mask)
  (<0>,731) fake_defs.h:536: if (offset & mask)
  (<0>,735) fake_defs.h:537: return cpu;
  (<0>,736) fake_defs.h:537: return cpu;
  (<0>,738) fake_defs.h:540: }
  (<0>,740) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,741) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,745) tree.c:3703: while (i > rnp->grphi)
  (<0>,746) tree.c:3703: while (i > rnp->grphi)
  (<0>,748) tree.c:3703: while (i > rnp->grphi)
  (<0>,751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,752) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,756) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,759) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,760) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,761) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,774)
  (<0>,775)
  (<0>,776) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,778) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,780) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,782) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,783) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,786)
  (<0>,787) tree.c:453: return &rsp->node[0];
  (<0>,791) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,792) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,794) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,798)
  (<0>,799)
  (<0>,800) fake_sync.h:76: local_irq_save(flags);
  (<0>,803) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,805) fake_sched.h:43: return __running_cpu;
  (<0>,809) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,811) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,815) fake_sched.h:43: return __running_cpu;
  (<0>,819) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,825) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,826) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,831) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,833) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,835) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,839) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,841) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,842) tree.c:3412: init_callback_list(rdp);
  (<0>,846)
  (<0>,847) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,850) tree_plugin.h:2732: return false;
  (<0>,853) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,855) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,856) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,858) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,861) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,865) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,868) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,870) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,872) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,874) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,877) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,881) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,884) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,886) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,888) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,890) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,893) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,897) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,900) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,902) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,904) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,906) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,909) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,913) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,916) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,918) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,920) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,922) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,926) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,928) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,929) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,931) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,935) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,937) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,938) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,940) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,948) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,950) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,956) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,957) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,958) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,961) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,969) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,972) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,979) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,980) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,981) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,982) tree.c:3418: rdp->cpu = cpu;
  (<0>,983) tree.c:3418: rdp->cpu = cpu;
  (<0>,985) tree.c:3418: rdp->cpu = cpu;
  (<0>,986) tree.c:3419: rdp->rsp = rsp;
  (<0>,987) tree.c:3419: rdp->rsp = rsp;
  (<0>,989) tree.c:3419: rdp->rsp = rsp;
  (<0>,990) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,993) tree_plugin.h:2711: }
  (<0>,995) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,997) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1001)
  (<0>,1002)
  (<0>,1003) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1004) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1007) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1010) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1012) fake_sched.h:43: return __running_cpu;
  (<0>,1016) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1018) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1022) fake_sched.h:43: return __running_cpu;
  (<0>,1026) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1035) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1036) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1043)
  (<0>,1044)
  (<0>,1045) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1047) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1048) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1051) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1053) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1054) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1057) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,1059) fake_defs.h:540: }
  (<0>,1061) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1062) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1065) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1070)
  (<0>,1071) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1072) fake_defs.h:289: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,1073) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1074) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1076) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,1081)
  (<0>,1082) fake_defs.h:279: static inline void __list_add(struct list_head *new,
  (<0>,1083) fake_defs.h:280: struct list_head *prev,
  (<0>,1084) fake_defs.h:281: struct list_head *next)
  (<0>,1085) fake_defs.h:283: next->prev = new;
  (<0>,1087) fake_defs.h:283: next->prev = new;
  (<0>,1088) fake_defs.h:284: new->next = next;
  (<0>,1089) fake_defs.h:284: new->next = next;
  (<0>,1091) fake_defs.h:284: new->next = next;
  (<0>,1092) fake_defs.h:285: new->prev = prev;
  (<0>,1093) fake_defs.h:285: new->prev = prev;
  (<0>,1095) fake_defs.h:285: new->prev = prev;
  (<0>,1096) fake_defs.h:286: prev->next = new;
  (<0>,1097) fake_defs.h:286: prev->next = new;
  (<0>,1099) fake_defs.h:286: prev->next = new;
  (<0>,1110)
  (<0>,1111)
  (<0>,1112) tree.c:3642: int cpustride = 1;
  (<0>,1113) tree.c:3650: if (rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1116) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1118) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1119) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1122) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1125) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1126) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1128) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1131) tree.c:3656: rsp->levelcnt[i] = num_rcu_lvl[i];
  (<0>,1133) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1135) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1137) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1138) tree.c:3655: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1141) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1143) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1144) tree.c:3657: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1147) tree.c:3659: rcu_init_levelspread(rsp);
  (<0>,1153)
  (<0>,1154) tree.c:3616: cprv = nr_cpu_ids;
  (<0>,1155) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1157) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1159) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1162) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1164) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1167) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1168) tree.c:3618: ccur = rsp->levelcnt[i];
  (<0>,1169) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1170) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1173) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1176) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1178) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1181) tree.c:3619: rsp->levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1182) tree.c:3620: cprv = ccur;
  (<0>,1183) tree.c:3620: cprv = ccur;
  (<0>,1185) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1187) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1189) tree.c:3617: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1193) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1194) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1196) tree.c:3660: rsp->flavor_mask = fl_mask;
  (<0>,1197) tree.c:3661: fl_mask <<= 1;
  (<0>,1201) tree.c:3661: fl_mask <<= 1;
  (<0>,1202) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1204) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1206) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1209) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1211) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1214) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1216) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1218) tree.c:3666: cpustride *= rsp->levelspread[i];
  (<0>,1219) tree.c:3667: rnp = rsp->level[i];
  (<0>,1221) tree.c:3667: rnp = rsp->level[i];
  (<0>,1224) tree.c:3667: rnp = rsp->level[i];
  (<0>,1225) tree.c:3667: rnp = rsp->level[i];
  (<0>,1226) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1228) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1229) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1231) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1234) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1237) tree.c:3669: raw_spin_lock_init(&rnp->lock);
  (<0>,1241)
  (<0>,1242) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1243) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1249) tree.c:3672: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1253)
  (<0>,1254) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1255) fake_sync.h:70: if (pthread_mutex_init(l, NULL))
  (<0>,1261) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1263) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1264) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1266) tree.c:3675: rnp->gpnum = rsp->gpnum;
  (<0>,1267) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1269) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1270) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1272) tree.c:3676: rnp->completed = rsp->completed;
  (<0>,1273) tree.c:3677: rnp->qsmask = 0;
  (<0>,1275) tree.c:3677: rnp->qsmask = 0;
  (<0>,1276) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1278) tree.c:3678: rnp->qsmaskinit = 0;
  (<0>,1279) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1280) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1282) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1284) tree.c:3679: rnp->grplo = j * cpustride;
  (<0>,1285) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1287) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1290) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1292) tree.c:3680: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1293) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1295) tree.c:3681: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1298) tree.c:3683: if (i == 0) {
  (<0>,1301) tree.c:3684: rnp->grpnum = 0;
  (<0>,1303) tree.c:3684: rnp->grpnum = 0;
  (<0>,1304) tree.c:3685: rnp->grpmask = 0;
  (<0>,1306) tree.c:3685: rnp->grpmask = 0;
  (<0>,1307) tree.c:3686: rnp->parent = NULL;
  (<0>,1309) tree.c:3686: rnp->parent = NULL;
  (<0>,1311) tree.c:3693: rnp->level = i;
  (<0>,1313) tree.c:3693: rnp->level = i;
  (<0>,1315) tree.c:3693: rnp->level = i;
  (<0>,1316) tree.c:3694: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1320)
  (<0>,1321) fake_defs.h:275: list->next = list;
  (<0>,1322) fake_defs.h:275: list->next = list;
  (<0>,1324) fake_defs.h:275: list->next = list;
  (<0>,1325) fake_defs.h:276: list->prev = list;
  (<0>,1326) fake_defs.h:276: list->prev = list;
  (<0>,1328) fake_defs.h:276: list->prev = list;
  (<0>,1330) tree.c:3695: rcu_init_one_nocb(rnp);
  (<0>,1333) tree_plugin.h:2694: }
  (<0>,1336) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1338) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1339) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1341) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1343) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1344) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1346) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1349) tree.c:3668: for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
  (<0>,1353) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1355) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1357) tree.c:3665: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1360) tree.c:3699: rsp->rda = rda;
  (<0>,1361) tree.c:3699: rsp->rda = rda;
  (<0>,1363) tree.c:3699: rsp->rda = rda;
  (<0>,1366) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1369) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1372) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1373) tree.c:3701: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1374) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1376) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1377) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1384)
  (<0>,1385)
  (<0>,1386) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1388) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1389) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1392) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1394) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1395) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1398) fake_defs.h:536: if (offset & mask)
  (<0>,1399) fake_defs.h:536: if (offset & mask)
  (<0>,1403) fake_defs.h:537: return cpu;
  (<0>,1404) fake_defs.h:537: return cpu;
  (<0>,1406) fake_defs.h:540: }
  (<0>,1408) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1409) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1413) tree.c:3703: while (i > rnp->grphi)
  (<0>,1414) tree.c:3703: while (i > rnp->grphi)
  (<0>,1416) tree.c:3703: while (i > rnp->grphi)
  (<0>,1419) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1420) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1422) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1424) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1427) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1428) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1429) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1442)
  (<0>,1443)
  (<0>,1444) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1446) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1448) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1450) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1451) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1454)
  (<0>,1455) tree.c:453: return &rsp->node[0];
  (<0>,1459) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1460) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1462) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1466)
  (<0>,1467)
  (<0>,1468) fake_sync.h:76: local_irq_save(flags);
  (<0>,1471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1473) fake_sched.h:43: return __running_cpu;
  (<0>,1477) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1479) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1483) fake_sched.h:43: return __running_cpu;
  (<0>,1487) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1493) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1494) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1498) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1499) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1501) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1503) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1507) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1509) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1510) tree.c:3412: init_callback_list(rdp);
  (<0>,1514)
  (<0>,1515) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1518) tree_plugin.h:2732: return false;
  (<0>,1521) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1523) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1529) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1531) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1533) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1536) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1538) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1540) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1542) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1545) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1547) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1549) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1552) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1554) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1556) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1558) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1561) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1563) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1565) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1568) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1570) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1572) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1574) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1577) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1579) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1581) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1584) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1586) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1588) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1590) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1594) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1596) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1597) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1599) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1600) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1603) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1605) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1606) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1608) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1610) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1615) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1616) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1618) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1620) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1624) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1625) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1626) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1627) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1629) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1632) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1637) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1638) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1640) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1643) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1647) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1648) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1649) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1650) tree.c:3418: rdp->cpu = cpu;
  (<0>,1651) tree.c:3418: rdp->cpu = cpu;
  (<0>,1653) tree.c:3418: rdp->cpu = cpu;
  (<0>,1654) tree.c:3419: rdp->rsp = rsp;
  (<0>,1655) tree.c:3419: rdp->rsp = rsp;
  (<0>,1657) tree.c:3419: rdp->rsp = rsp;
  (<0>,1658) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1661) tree_plugin.h:2711: }
  (<0>,1663) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1665) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1669)
  (<0>,1670)
  (<0>,1671) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1672) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1675) fake_sync.h:86: local_irq_restore(flags);
  (<0>,1678) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1680) fake_sched.h:43: return __running_cpu;
  (<0>,1684) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1686) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1690) fake_sched.h:43: return __running_cpu;
  (<0>,1694) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1703) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1704) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1711)
  (<0>,1712)
  (<0>,1713) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1715) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1716) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1719) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1721) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1722) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1725) fake_defs.h:536: if (offset & mask)
  (<0>,1726) fake_defs.h:536: if (offset & mask)
  (<0>,1730) fake_defs.h:537: return cpu;
  (<0>,1731) fake_defs.h:537: return cpu;
  (<0>,1733) fake_defs.h:540: }
  (<0>,1735) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1736) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,1740) tree.c:3703: while (i > rnp->grphi)
  (<0>,1741) tree.c:3703: while (i > rnp->grphi)
  (<0>,1743) tree.c:3703: while (i > rnp->grphi)
  (<0>,1746) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1747) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1749) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1751) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1754) tree.c:3705: rsp->rda[i].mynode = rnp;
  (<0>,1755) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1756) tree.c:3706: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1769)
  (<0>,1770)
  (<0>,1771) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1773) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1775) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1777) tree.c:3406: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,1778) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1781)
  (<0>,1782) tree.c:453: return &rsp->node[0];
  (<0>,1786) tree.c:3407: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1787) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1789) tree.c:3410: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,1793)
  (<0>,1794)
  (<0>,1795) fake_sync.h:76: local_irq_save(flags);
  (<0>,1798) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1800) fake_sched.h:43: return __running_cpu;
  (<0>,1804) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1806) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1810) fake_sched.h:43: return __running_cpu;
  (<0>,1814) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1820) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1821) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,1825) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1826) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1828) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1830) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1834) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1836) tree.c:3411: rdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);
  (<0>,1837) tree.c:3412: init_callback_list(rdp);
  (<0>,1841)
  (<0>,1842) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,1845) tree_plugin.h:2732: return false;
  (<0>,1848) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1850) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,1851) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1853) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1856) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1858) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1860) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1863) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1865) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1867) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1869) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1872) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1874) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1876) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1879) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1881) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1883) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1885) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1888) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1890) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1892) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1895) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1897) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1899) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1901) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1904) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1906) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1908) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1911) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,1913) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1915) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1917) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,1921) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1923) tree.c:3413: rdp->qlen_lazy = 0;
  (<0>,1924) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1926) tree.c:3414: ACCESS_ONCE(rdp->qlen) = 0;
  (<0>,1927) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1930) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1932) tree.c:3415: rdp->dynticks = &rcu_dynticks[cpu];
  (<0>,1933) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1935) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1937) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1942) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1943) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1945) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1947) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1951) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1952) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1953) tree.c:3416: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1954) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1956) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1959) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1964) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1965) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1967) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1970) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1974) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1975) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1976) tree.c:3417: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1977) tree.c:3418: rdp->cpu = cpu;
  (<0>,1978) tree.c:3418: rdp->cpu = cpu;
  (<0>,1980) tree.c:3418: rdp->cpu = cpu;
  (<0>,1981) tree.c:3419: rdp->rsp = rsp;
  (<0>,1982) tree.c:3419: rdp->rsp = rsp;
  (<0>,1984) tree.c:3419: rdp->rsp = rsp;
  (<0>,1985) tree.c:3420: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1988) tree_plugin.h:2711: }
  (<0>,1990) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1992) tree.c:3421: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,1996)
  (<0>,1997)
  (<0>,1998) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,1999) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,2002) fake_sync.h:86: local_irq_restore(flags);
  (<0>,2005) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2007) fake_sched.h:43: return __running_cpu;
  (<0>,2011) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2013) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2017) fake_sched.h:43: return __running_cpu;
  (<0>,2021) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2030) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2031) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2038)
  (<0>,2039)
  (<0>,2040) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2042) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2043) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2046) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2048) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2049) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2052) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,2054) fake_defs.h:540: }
  (<0>,2056) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2057) tree.c:3702: for_each_possible_cpu(i) {
  (<0>,2060) tree.c:3708: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,2065)
  (<0>,2066)
  (<0>,2067) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2068) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2069) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2071) fake_defs.h:291: __list_add(new, head, head->next);
  (<0>,2076)
  (<0>,2077)
  (<0>,2078)
  (<0>,2079) fake_defs.h:283: next->prev = new;
  (<0>,2080) fake_defs.h:283: next->prev = new;
  (<0>,2082) fake_defs.h:283: next->prev = new;
  (<0>,2083) fake_defs.h:284: new->next = next;
  (<0>,2084) fake_defs.h:284: new->next = next;
  (<0>,2086) fake_defs.h:284: new->next = next;
  (<0>,2087) fake_defs.h:285: new->prev = prev;
  (<0>,2088) fake_defs.h:285: new->prev = prev;
  (<0>,2090) fake_defs.h:285: new->prev = prev;
  (<0>,2091) fake_defs.h:286: prev->next = new;
  (<0>,2092) fake_defs.h:286: prev->next = new;
  (<0>,2094) fake_defs.h:286: prev->next = new;
  (<0>,2106) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2108) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2109) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2116)
  (<0>,2117)
  (<0>,2118) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2120) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2121) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2124) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2126) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2127) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2130) fake_defs.h:536: if (offset & mask)
  (<0>,2131) fake_defs.h:536: if (offset & mask)
  (<0>,2135) fake_defs.h:537: return cpu;
  (<0>,2136) fake_defs.h:537: return cpu;
  (<0>,2138) fake_defs.h:540: }
  (<0>,2140) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2141) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2144) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2163)
  (<0>,2164) tree.c:3493: static int rcu_cpu_notify(struct notifier_block *self,
  (<0>,2165) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2166) tree.c:3494: unsigned long action, void *hcpu)
  (<0>,2168) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2169) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2170) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2172) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2174) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2175) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2177) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2178) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2181) tree.c:3502: switch (action) {
  (<0>,2183) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2192)
  (<0>,2193) tree.c:3482: static void rcu_prepare_cpu(int cpu)
  (<0>,2194) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2195) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2199) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2200) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2201) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2203) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2207) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2208) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2216)
  (<0>,2217) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2218) tree.c:3431: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,2220) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2222) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2224) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2225) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2228)
  (<0>,2229) tree.c:453: return &rsp->node[0];
  (<0>,2233) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2234) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2238)
  (<0>,2239) fake_sync.h:172: void mutex_lock(struct mutex *l)
  (<0>,2241) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2245) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2247) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2251)
  (<0>,2252)
  (<0>,2253) fake_sync.h:76: local_irq_save(flags);
  (<0>,2256) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2258) fake_sched.h:43: return __running_cpu;
  (<0>,2262) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2264) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2268) fake_sched.h:43: return __running_cpu;
  (<0>,2272) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2278) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2279) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2283) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2285) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2286) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2288) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2289) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2291) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2292) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2294) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2295) tree.c:3446: rdp->blimit = blimit;
  (<0>,2296) tree.c:3446: rdp->blimit = blimit;
  (<0>,2298) tree.c:3446: rdp->blimit = blimit;
  (<0>,2299) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2303)
  (<0>,2304) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2307) tree_plugin.h:2732: return false;
  (<0>,2310) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2312) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2313) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2315) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2318) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2320) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2322) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2325) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2327) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2329) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2331) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2334) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2336) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2338) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2341) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2343) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2345) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2347) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2350) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2352) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2354) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2357) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2359) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2361) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2363) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2366) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2368) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2370) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2373) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2375) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2377) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2379) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2383) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2385) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2387) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2388) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2390) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2393) tree_plugin.h:3164: }
  (<0>,2395) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2397) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2400) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2403) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2405) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2408) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2409) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2413)
  (<0>,2414) fake_sync.h:113: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2415) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2420) tree.c:3455: rnp = rdp->mynode;
  (<0>,2422) tree.c:3455: rnp = rdp->mynode;
  (<0>,2423) tree.c:3455: rnp = rdp->mynode;
  (<0>,2424) tree.c:3456: mask = rdp->grpmask;
  (<0>,2426) tree.c:3456: mask = rdp->grpmask;
  (<0>,2427) tree.c:3456: mask = rdp->grpmask;
  (<0>,2429) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2433) fake_sync.h:108: preempt_disable();
  (<0>,2435) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2436) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2440) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2441) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2443) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2445) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2446) tree.c:3461: mask = rnp->grpmask;
  (<0>,2448) tree.c:3461: mask = rnp->grpmask;
  (<0>,2449) tree.c:3461: mask = rnp->grpmask;
  (<0>,2450) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2451) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2453) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2456) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2458) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2459) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2461) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2462) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2464) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2465) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2467) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2468) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2470) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2471) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2473) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2477) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2481)
  (<0>,2482) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2483) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2488) tree.c:3475: rnp = rnp->parent;
  (<0>,2490) tree.c:3475: rnp = rnp->parent;
  (<0>,2491) tree.c:3475: rnp = rnp->parent;
  (<0>,2493) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2497) tree.c:3477: local_irq_restore(flags);
  (<0>,2500) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2502) fake_sched.h:43: return __running_cpu;
  (<0>,2506) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2508) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2512) fake_sched.h:43: return __running_cpu;
  (<0>,2516) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2521) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2525)
  (<0>,2526) fake_sync.h:178: void mutex_unlock(struct mutex *l)
  (<0>,2528) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2534) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2537) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2538) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2539) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2543) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2544) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2545) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2547) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2551) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2552) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,2560)
  (<0>,2561)
  (<0>,2562) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2564) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2566) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2568) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,2569) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2572)
  (<0>,2573) tree.c:453: return &rsp->node[0];
  (<0>,2577) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2578) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,2582)
  (<0>,2583) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2585) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,2589) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2591) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,2595)
  (<0>,2596)
  (<0>,2597) fake_sync.h:76: local_irq_save(flags);
  (<0>,2600) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2602) fake_sched.h:43: return __running_cpu;
  (<0>,2606) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2608) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2612) fake_sched.h:43: return __running_cpu;
  (<0>,2616) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2622) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2623) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,2627) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2629) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,2630) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2632) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,2633) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2635) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2636) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2638) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2639) tree.c:3446: rdp->blimit = blimit;
  (<0>,2640) tree.c:3446: rdp->blimit = blimit;
  (<0>,2642) tree.c:3446: rdp->blimit = blimit;
  (<0>,2643) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2647)
  (<0>,2648) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,2651) tree_plugin.h:2732: return false;
  (<0>,2654) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2656) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,2657) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2659) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2662) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2664) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2666) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2669) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2671) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2673) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2675) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2678) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2680) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2682) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2685) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2687) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2689) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2691) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2694) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2696) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2698) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2701) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2703) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2705) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2707) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2710) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2712) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2714) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2717) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2719) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2721) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2723) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2727) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2729) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2731) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2732) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2734) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2737) tree_plugin.h:3164: }
  (<0>,2739) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2741) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2744) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2747) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2749) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2752) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2753) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,2757)
  (<0>,2758) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2759) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2764) tree.c:3455: rnp = rdp->mynode;
  (<0>,2766) tree.c:3455: rnp = rdp->mynode;
  (<0>,2767) tree.c:3455: rnp = rdp->mynode;
  (<0>,2768) tree.c:3456: mask = rdp->grpmask;
  (<0>,2770) tree.c:3456: mask = rdp->grpmask;
  (<0>,2771) tree.c:3456: mask = rdp->grpmask;
  (<0>,2773) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,2777) fake_sync.h:108: preempt_disable();
  (<0>,2779) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2780) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,2784) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2785) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2787) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2789) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,2790) tree.c:3461: mask = rnp->grpmask;
  (<0>,2792) tree.c:3461: mask = rnp->grpmask;
  (<0>,2793) tree.c:3461: mask = rnp->grpmask;
  (<0>,2794) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2795) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2797) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,2800) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2802) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2803) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2805) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,2806) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2808) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2809) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2811) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,2812) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2814) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,2815) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2817) tree.c:3471: rdp->qs_pending = 0;
  (<0>,2821) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,2825)
  (<0>,2826) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2827) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,2832) tree.c:3475: rnp = rnp->parent;
  (<0>,2834) tree.c:3475: rnp = rnp->parent;
  (<0>,2835) tree.c:3475: rnp = rnp->parent;
  (<0>,2837) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,2841) tree.c:3477: local_irq_restore(flags);
  (<0>,2844) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2846) fake_sched.h:43: return __running_cpu;
  (<0>,2850) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2852) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2856) fake_sched.h:43: return __running_cpu;
  (<0>,2860) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2865) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,2869)
  (<0>,2870) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2872) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,2878) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2881) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2882) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2883) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2887) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2888) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2889) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2891) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2896) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,2900) tree_plugin.h:1499: }
  (<0>,2902) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2906) tree_plugin.h:2724: }
  (<0>,2913) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2914) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2921)
  (<0>,2922)
  (<0>,2923) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2925) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2926) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2929) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,2931) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2932) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,2935) fake_defs.h:536: if (offset & mask)
  (<0>,2936) fake_defs.h:536: if (offset & mask)
  (<0>,2940) fake_defs.h:537: return cpu;
  (<0>,2941) fake_defs.h:537: return cpu;
  (<0>,2943) fake_defs.h:540: }
  (<0>,2945) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2946) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,2949) tree.c:3807: rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
  (<0>,2968)
  (<0>,2969)
  (<0>,2970)
  (<0>,2971) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2973) tree.c:3496: long cpu = (long)hcpu;
  (<0>,2974) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2975) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2977) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2979) tree.c:3497: struct rcu_data *rdp = &rcu_state_p->rda[cpu];
  (<0>,2980) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2982) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2983) tree.c:3498: struct rcu_node *rnp = rdp->mynode;
  (<0>,2986) tree.c:3502: switch (action) {
  (<0>,2988) tree.c:3505: rcu_prepare_cpu(cpu);
  (<0>,2997)
  (<0>,2998) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,2999) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3000) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3004) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3005) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3006) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3008) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3012) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3013) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3021)
  (<0>,3022)
  (<0>,3023) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3025) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3027) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3029) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3030) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3033)
  (<0>,3034) tree.c:453: return &rsp->node[0];
  (<0>,3038) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3039) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3043)
  (<0>,3044) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3046) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3050) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3052) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3056)
  (<0>,3057)
  (<0>,3058) fake_sync.h:76: local_irq_save(flags);
  (<0>,3061) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3063) fake_sched.h:43: return __running_cpu;
  (<0>,3067) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3069) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3073) fake_sched.h:43: return __running_cpu;
  (<0>,3077) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3083) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3084) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3088) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3090) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3091) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3093) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3094) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3096) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3097) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3099) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3100) tree.c:3446: rdp->blimit = blimit;
  (<0>,3101) tree.c:3446: rdp->blimit = blimit;
  (<0>,3103) tree.c:3446: rdp->blimit = blimit;
  (<0>,3104) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3108)
  (<0>,3109) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3112) tree_plugin.h:2732: return false;
  (<0>,3115) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3117) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3118) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3120) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3123) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3125) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3127) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3130) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3132) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3134) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3136) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3139) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3141) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3143) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3146) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3148) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3150) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3152) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3155) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3157) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3159) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3162) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3164) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3166) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3168) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3171) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3173) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3175) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3178) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3180) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3182) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3184) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3188) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3190) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3192) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3193) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3195) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3198) tree_plugin.h:3164: }
  (<0>,3200) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3202) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3205) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3208) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3210) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3213) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3214) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3218)
  (<0>,3219) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3220) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3225) tree.c:3455: rnp = rdp->mynode;
  (<0>,3227) tree.c:3455: rnp = rdp->mynode;
  (<0>,3228) tree.c:3455: rnp = rdp->mynode;
  (<0>,3229) tree.c:3456: mask = rdp->grpmask;
  (<0>,3231) tree.c:3456: mask = rdp->grpmask;
  (<0>,3232) tree.c:3456: mask = rdp->grpmask;
  (<0>,3234) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3238) fake_sync.h:108: preempt_disable();
  (<0>,3240) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3241) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3245) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3246) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3248) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3250) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3251) tree.c:3461: mask = rnp->grpmask;
  (<0>,3253) tree.c:3461: mask = rnp->grpmask;
  (<0>,3254) tree.c:3461: mask = rnp->grpmask;
  (<0>,3255) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3256) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3258) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3261) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3263) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3264) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3266) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3267) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3269) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3270) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3272) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3273) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3275) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3276) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3278) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3282) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3286)
  (<0>,3287) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3288) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3293) tree.c:3475: rnp = rnp->parent;
  (<0>,3295) tree.c:3475: rnp = rnp->parent;
  (<0>,3296) tree.c:3475: rnp = rnp->parent;
  (<0>,3298) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3302) tree.c:3477: local_irq_restore(flags);
  (<0>,3305) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3307) fake_sched.h:43: return __running_cpu;
  (<0>,3311) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3313) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3317) fake_sched.h:43: return __running_cpu;
  (<0>,3321) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3326) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3330)
  (<0>,3331) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3333) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3339) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3342) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3343) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3344) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3348) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3349) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3350) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3352) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3356) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3357) tree.c:3487: rcu_init_percpu_data(cpu, rsp);
  (<0>,3365)
  (<0>,3366)
  (<0>,3367) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3369) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3371) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3373) tree.c:3435: struct rcu_data *rdp = &rsp->rda[cpu];
  (<0>,3374) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3377)
  (<0>,3378) tree.c:453: return &rsp->node[0];
  (<0>,3382) tree.c:3436: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3383) tree.c:3439: mutex_lock(&rsp->onoff_mutex);
  (<0>,3387)
  (<0>,3388) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3390) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
  (<0>,3394) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3396) tree.c:3442: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,3400)
  (<0>,3401)
  (<0>,3402) fake_sync.h:76: local_irq_save(flags);
  (<0>,3405) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3407) fake_sched.h:43: return __running_cpu;
  (<0>,3411) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3413) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3417) fake_sched.h:43: return __running_cpu;
  (<0>,3421) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3427) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3428) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,3432) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3434) tree.c:3443: rdp->beenonline = 1;	 /* We have now been online. */
  (<0>,3435) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3437) tree.c:3444: rdp->qlen_last_fqs_check = 0;
  (<0>,3438) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3440) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3441) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3443) tree.c:3445: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3444) tree.c:3446: rdp->blimit = blimit;
  (<0>,3445) tree.c:3446: rdp->blimit = blimit;
  (<0>,3447) tree.c:3446: rdp->blimit = blimit;
  (<0>,3448) tree.c:3447: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3452)
  (<0>,3453) tree.c:1227: if (init_nocb_callback_list(rdp))
  (<0>,3456) tree_plugin.h:2732: return false;
  (<0>,3459) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3461) tree.c:1229: rdp->nxtlist = NULL;
  (<0>,3462) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3464) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3467) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3469) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3471) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3474) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3476) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3478) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3480) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3483) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3485) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3487) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3490) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3492) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3494) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3496) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3499) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3501) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3503) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3506) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3508) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3510) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3512) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3515) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3517) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3519) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3522) tree.c:1231: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3524) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3526) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3528) tree.c:1230: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3532) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3534) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3536) tree.c:3448: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3537) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3539) tree.c:3449: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3542) tree_plugin.h:3164: }
  (<0>,3544) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3546) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3549) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3552) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3554) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3557) tree.c:3450: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3558) tree.c:3452: raw_spin_unlock(&rnp->lock);		/* irqs remain disabled. */
  (<0>,3562)
  (<0>,3563) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3564) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3569) tree.c:3455: rnp = rdp->mynode;
  (<0>,3571) tree.c:3455: rnp = rdp->mynode;
  (<0>,3572) tree.c:3455: rnp = rdp->mynode;
  (<0>,3573) tree.c:3456: mask = rdp->grpmask;
  (<0>,3575) tree.c:3456: mask = rdp->grpmask;
  (<0>,3576) tree.c:3456: mask = rdp->grpmask;
  (<0>,3578) tree.c:3459: raw_spin_lock(&rnp->lock);	/* irqs already disabled. */
  (<0>,3582) fake_sync.h:108: preempt_disable();
  (<0>,3584) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3585) fake_sync.h:109: if (pthread_mutex_lock(l))
  (<0>,3589) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3590) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3592) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3594) tree.c:3460: rnp->qsmaskinit |= mask;
  (<0>,3595) tree.c:3461: mask = rnp->grpmask;
  (<0>,3597) tree.c:3461: mask = rnp->grpmask;
  (<0>,3598) tree.c:3461: mask = rnp->grpmask;
  (<0>,3599) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3600) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3602) tree.c:3462: if (rnp == rdp->mynode) {
  (<0>,3605) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3607) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3608) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3610) tree.c:3468: rdp->gpnum = rnp->completed;
  (<0>,3611) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3613) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3614) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3616) tree.c:3469: rdp->completed = rnp->completed;
  (<0>,3617) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3619) tree.c:3470: rdp->passed_quiesce = 0;
  (<0>,3620) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3622) tree.c:3471: rdp->qs_pending = 0;
  (<0>,3626) tree.c:3474: raw_spin_unlock(&rnp->lock); /* irqs already disabled. */
  (<0>,3630)
  (<0>,3631) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3632) fake_sync.h:115: if (pthread_mutex_unlock(l))
  (<0>,3637) tree.c:3475: rnp = rnp->parent;
  (<0>,3639) tree.c:3475: rnp = rnp->parent;
  (<0>,3640) tree.c:3475: rnp = rnp->parent;
  (<0>,3642) tree.c:3476: } while (rnp != NULL && !(rnp->qsmaskinit & mask));
  (<0>,3646) tree.c:3477: local_irq_restore(flags);
  (<0>,3649) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3651) fake_sched.h:43: return __running_cpu;
  (<0>,3655) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3657) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3661) fake_sched.h:43: return __running_cpu;
  (<0>,3665) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3670) tree.c:3479: mutex_unlock(&rsp->onoff_mutex);
  (<0>,3674)
  (<0>,3675) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3677) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
  (<0>,3683) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3686) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3687) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3688) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3692) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3693) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3694) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3696) tree.c:3486: for_each_rcu_flavor(rsp)
  (<0>,3701) tree.c:3506: rcu_prepare_kthreads(cpu);
  (<0>,3705) tree_plugin.h:1499: }
  (<0>,3707) tree.c:3507: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,3711) tree_plugin.h:2724: }
  (<0>,3718) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3719) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3726)
  (<0>,3727)
  (<0>,3728) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3730) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3731) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3734) fake_defs.h:534: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3736) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3737) fake_defs.h:535: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3740) fake_defs.h:539: return nr_cpu_ids + 1;
  (<0>,3742) fake_defs.h:540: }
  (<0>,3744) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3745) tree.c:3806: for_each_online_cpu(cpu)
  (<0>,3751) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3753) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,3756) litmus.c:123: set_cpu(i);
  (<0>,3759)
  (<0>,3760) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,3761) fake_sched.h:56: __running_cpu = cpu;
  (<0>,3765) tree.c:578: unsigned long flags;
  (<0>,3768) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3770) fake_sched.h:43: return __running_cpu;
  (<0>,3774) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3776) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3780) fake_sched.h:43: return __running_cpu;
  (<0>,3784) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3797) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3799) fake_sched.h:43: return __running_cpu;
  (<0>,3803) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3804) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3806) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3807) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,3808) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3814) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3815) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3820) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3821) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3822) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,3823) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,3827) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3829) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,3830) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3831) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,3850)
  (<0>,3852) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3854) fake_sched.h:43: return __running_cpu;
  (<0>,3858) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,3861) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,3865) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3866) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3867) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3871) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3872) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3873) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3875) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3880) fake_sched.h:43: return __running_cpu;
  (<0>,3883) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3885) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3887) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3888) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3891) tree_plugin.h:2720: }
  (<0>,3894) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3897) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3898) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3899) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3903) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3904) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3905) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3907) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3912) fake_sched.h:43: return __running_cpu;
  (<0>,3915) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3917) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3919) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,3920) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,3923) tree_plugin.h:2720: }
  (<0>,3926) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3929) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3930) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3931) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3935) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3936) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3937) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3939) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,3946) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3949) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3950) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3951) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3953) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3954) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,3956) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3959) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3965) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3966) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3969) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3974) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3975) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3976) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,3990) tree_plugin.h:3141: }
  (<0>,3992) tree.c:583: local_irq_restore(flags);
  (<0>,3995) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3997) fake_sched.h:43: return __running_cpu;
  (<0>,4001) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4003) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4007) fake_sched.h:43: return __running_cpu;
  (<0>,4011) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4018) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4020) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4022) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4025) litmus.c:123: set_cpu(i);
  (<0>,4028)
  (<0>,4029) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4030) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4034) tree.c:580: local_irq_save(flags);
  (<0>,4037) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4039) fake_sched.h:43: return __running_cpu;
  (<0>,4043) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4045) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4049) fake_sched.h:43: return __running_cpu;
  (<0>,4053) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4066) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4068) fake_sched.h:43: return __running_cpu;
  (<0>,4072) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4073) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4075) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4076) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,4077) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4083) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4084) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4089) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4090) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4091) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,4092) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4096) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4098) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,4099) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4100) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,4119)
  (<0>,4121) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4123) fake_sched.h:43: return __running_cpu;
  (<0>,4127) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4130) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,4134) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4135) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4136) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4140) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4141) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4142) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4144) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4149) fake_sched.h:43: return __running_cpu;
  (<0>,4152) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4154) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4156) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4157) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4160) tree_plugin.h:2720: }
  (<0>,4163) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4166) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4167) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4168) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4172) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4173) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4181) fake_sched.h:43: return __running_cpu;
  (<0>,4184) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4186) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4188) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,4189) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,4192) tree_plugin.h:2720: }
  (<0>,4195) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4198) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4199) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4200) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4204) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4205) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4206) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4208) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,4215) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4218) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4219) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4220) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4222) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4223) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,4225) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4228) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4234) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4235) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4238) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4243) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4244) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4245) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,4259) tree_plugin.h:3141: }
  (<0>,4261) tree.c:583: local_irq_restore(flags);
  (<0>,4264) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4266) fake_sched.h:43: return __running_cpu;
  (<0>,4270) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4272) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4276) fake_sched.h:43: return __running_cpu;
  (<0>,4280) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4287) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4289) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4291) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4303) tree.c:3561: unsigned long flags;
  (<0>,4304) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4305) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4306) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4310) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4314) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4320) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4322) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4331)
  (<0>,4332) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4333) fake_defs.h:723: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,4334) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4335) fake_defs.h:724: const char namefmt[], const char name[])
  (<0>,4336) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4339)
  (<0>,4340)
  (<0>,4347)
  (<0>,4348)
  (<0>,4355)
  (<0>,4356)
  (<0>,4363)
  (<0>,4364)
  (<0>,4371)
  (<0>,4372)
  (<0>,4379)
  (<0>,4380)
  (<0>,4387)
  (<0>,4388)
  (<0>,4395)
  (<0>,4396)
  (<0>,4403)
  (<0>,4404)
  (<0>,4411)
  (<0>,4412) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4421) fake_defs.h:730: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,4427) fake_defs.h:732: task = malloc(sizeof(*task));
  (<0>,4428) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4430) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4432) fake_defs.h:733: task->pid = (unsigned long) t;
  (<0>,4433) fake_defs.h:734: task->tid = t;
  (<0>,4434) fake_defs.h:734: task->tid = t;
  (<0>,4436) fake_defs.h:734: task->tid = t;
  (<0>,4437) fake_defs.h:735: return task;
  (<0>,4438) fake_defs.h:735: return task;
  (<0>,4440) fake_defs.h:738: }
  (<0>,4442) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4443) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4446)
  (<0>,4447) tree.c:453: return &rsp->node[0];
  (<0>,4451) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4452) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4454) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4458)
  (<0>,4459)
  (<0>,4460) fake_sync.h:76: local_irq_save(flags);
  (<0>,4463) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4465) fake_sched.h:43: return __running_cpu;
  (<0>,4469) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4471) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4475) fake_sched.h:43: return __running_cpu;
  (<0>,4479) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4485) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4486) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4490) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4491) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4493) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4494) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4496) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4500)
  (<0>,4501)
  (<0>,4502) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4503) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4506) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4509) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4511) fake_sched.h:43: return __running_cpu;
  (<0>,4515) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4517) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4521) fake_sched.h:43: return __running_cpu;
  (<0>,4525) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4533) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4537) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4538) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4542) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4543) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4544) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4546) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4550) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4552) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4554) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4563)
  (<0>,4564)
  (<0>,4565)
  (<0>,4566)
  (<0>,4567) fake_defs.h:727: struct task_struct *task = NULL;
  (<0>,4568) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4571)
  (<0>,4572)
  (<0>,4579)
  (<0>,4580)
  (<0>,4587)
  (<0>,4588)
  (<0>,4595)
  (<0>,4596)
  (<0>,4603)
  (<0>,4604) fake_defs.h:729: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,4617) fake_defs.h:737: return NULL;
  (<0>,4619) fake_defs.h:738: }
  (<0>,4621) tree.c:3568: t = kthread_run(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,4622) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4625)
  (<0>,4626) tree.c:453: return &rsp->node[0];
  (<0>,4630) tree.c:3570: rnp = rcu_get_root(rsp);
  (<0>,4631) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4633) tree.c:3571: raw_spin_lock_irqsave(&rnp->lock, flags);
  (<0>,4637)
  (<0>,4638)
  (<0>,4639) fake_sync.h:76: local_irq_save(flags);
  (<0>,4642) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4644) fake_sched.h:43: return __running_cpu;
  (<0>,4648) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4650) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4654) fake_sched.h:43: return __running_cpu;
  (<0>,4658) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4664) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4665) fake_sync.h:78: if (pthread_mutex_lock(l))
  (<0>,4669) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4670) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4672) tree.c:3572: rsp->gp_kthread = t;
  (<0>,4673) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4675) tree.c:3573: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,4679)
  (<0>,4680)
  (<0>,4681) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4682) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,4685) fake_sync.h:86: local_irq_restore(flags);
  (<0>,4688) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4690) fake_sched.h:43: return __running_cpu;
  (<0>,4694) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4696) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4700) fake_sched.h:43: return __running_cpu;
  (<0>,4704) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4712) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4715) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4716) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4717) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4721) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4722) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4723) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4725) tree.c:3567: for_each_rcu_flavor(rsp) {
  (<0>,4739)
  (<0>,4740) litmus.c:51: void *thread_reader(void *arg)
  (<0>,4743)
  (<0>,4744) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4745) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4748) fake_sched.h:43: return __running_cpu;
  (<0>,4752)
  (<0>,4753) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,4756) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,4761) tree.c:702: unsigned long flags;
  (<0>,4764) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4766) fake_sched.h:43: return __running_cpu;
  (<0>,4770) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4772) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4780) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4793) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4795) fake_sched.h:43: return __running_cpu;
  (<0>,4799) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4800) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4802) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4803) tree.c:679: oldval = rdtp->dynticks_nesting;
  (<0>,4804) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4809) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4810) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4814) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4815) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4816) tree.c:680: WARN_ON_ONCE(oldval < 0);
  (<0>,4817) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,4821) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4823) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,4824) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4825) tree.c:685: rcu_eqs_exit_common(oldval, user);
  (<0>,4839)
  (<0>,4840) tree.c:644: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,4842) fake_sched.h:43: return __running_cpu;
  (<0>,4846) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4850) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4853) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4854) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4855) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4857) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4858) tree.c:650: atomic_inc(&rdtp->dynticks);
  (<0>,4860) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4870) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4871) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4874) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4879) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4880) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4881) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
  (<0>,4886) tree.c:656: if (!user && !is_idle_task(current)) {
  (<0>,4895) tree_plugin.h:3145: }
  (<0>,4897) tree.c:707: local_irq_restore(flags);
  (<0>,4900) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4902) fake_sched.h:43: return __running_cpu;
  (<0>,4906) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4908) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4912) fake_sched.h:43: return __running_cpu;
  (<0>,4916) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4932) litmus.c:57: r_x = x;
  (<0>,4933) litmus.c:57: r_x = x;
  (<0>,4937) fake_sched.h:43: return __running_cpu;
  (<0>,4941) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4949) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4954) fake_sched.h:43: return __running_cpu;
  (<0>,4958) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
  (<0>,4968) tree.c:745: unsigned long flags;
  (<0>,4971) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4973) fake_sched.h:43: return __running_cpu;
  (<0>,4977) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4984) fake_sched.h:43: return __running_cpu;
  (<0>,4988) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,4989) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4991) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4992) tree.c:751: oldval = rdtp->dynticks_nesting;
  (<0>,4993) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4995) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4997) tree.c:752: rdtp->dynticks_nesting++;
  (<0>,4998) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5000) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5005) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5006) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5008) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5012) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5013) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5014) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
  (<0>,5015) tree.c:754: if (oldval)
  (<0>,5023) tree_plugin.h:3145: }
  (<0>,5025) tree.c:759: local_irq_restore(flags);
  (<0>,5028) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5030) fake_sched.h:43: return __running_cpu;
  (<0>,5034) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5036) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5044) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,5049) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,5054) fake_sched.h:43: return __running_cpu;
  (<0>,5059) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,5067) fake_sched.h:43: return __running_cpu;
  (<0>,5072) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
  (<0>,5078) fake_sched.h:43: return __running_cpu;
  (<0>,5083) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
  (<0>,5096) tree.c:3185: struct rcu_state *rsp;
  (<0>,5097) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5098) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5102) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5103) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5104) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5106) tree.c:3187: for_each_rcu_flavor(rsp)
  (<0>,5110) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5112) fake_sched.h:43: return __running_cpu;
  (<0>,5115) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5117) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
  (<0>,5124)
  (<0>,5125) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5126) tree.c:3121: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5128) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5129) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
  (<0>,5130) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5132) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5134) tree.c:3125: rdp->n_rcu_pending++;
  (<0>,5135) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5136) tree.c:3128: check_cpu_stall(rsp, rdp);
  (<0>,5146)
  (<0>,5147) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5148) tree.c:1147: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5153) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
  (<0>,5156) tree_plugin.h:3185: return 0;
  (<0>,5159) tree.c:3135: if (rcu_scheduler_fully_active &&
  (<0>,5162) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5164) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
  (<0>,5167) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5169) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
  (<0>,5173) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,5176)
  (<0>,5177) tree.c:442: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,5179) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5182) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5189) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5190) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5196)
  (<0>,5197) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5198) tree.c:476: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,5201)
  (<0>,5202) tree.c:176: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,5204) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5205) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5207) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5213) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5219)
  (<0>,5220) tree.c:461: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,5223)
  (<0>,5224) tree.c:453: return &rsp->node[0];
  (<0>,5228) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5229) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5231) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5235) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,5236) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5238) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5241) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,5242) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5243) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,5247) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5250) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,5253) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5256) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5257) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,5260) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5262) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5265) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5268) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5271) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5272) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5274) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5277) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5281) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5283) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5285) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5288) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5291) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5294) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5295) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5297) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5300) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,5304) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5306) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5308) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,5311) tree.c:493: return 0; /* No grace period needed. */
  (<0>,5313) tree.c:494: }
  (<0>,5317) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5319) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5320) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5322) tree.c:3156: if (ACCESS_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,5325) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
      (<0.1>,1)
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:704: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,56) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,58) fake_sched.h:43: return __running_cpu;
      (<0.1>,62) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,63) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,67) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,72) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,73) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,77) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,78) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,79) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,80) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,84) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,86) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,87) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,88) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,102)
      (<0.1>,103) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,105) fake_sched.h:43: return __running_cpu;
      (<0.1>,109) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,113) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,116) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,117) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,118) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,120) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,123) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,126) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,133) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,137) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,143) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,144) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,149) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,158) tree_plugin.h:3145: }
      (<0.1>,160) tree.c:707: local_irq_restore(flags);
      (<0.1>,163) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,165) fake_sched.h:43: return __running_cpu;
      (<0.1>,169) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,171) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,175) fake_sched.h:43: return __running_cpu;
      (<0.1>,179) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,186) litmus.c:87: x = 1;
      (<0.1>,196) tree.c:2787: ret = num_online_cpus() <= 1;
      (<0.1>,198) tree.c:2789: return ret;
      (<0.1>,202) tree.c:2841: if (rcu_expedited)
      (<0.1>,208) update.c:223: init_rcu_head_on_stack(&rcu.head);
      (<0.1>,212) rcupdate.h:401: }
      (<0.1>,217)
      (<0.1>,218) fake_sync.h:232: x->done = 0;
      (<0.1>,220) fake_sync.h:232: x->done = 0;
      (<0.1>,224) update.c:226: crf(&rcu.head, wakeme_after_rcu);
      (<0.1>,229)
      (<0.1>,230)
      (<0.1>,231) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,232) tree.c:2745: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,249)
      (<0.1>,250)
      (<0.1>,251)
      (<0.1>,252)
      (<0.1>,254)
      (<0.1>,255) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,262) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,263) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,269) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,270) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,271) tree.c:2689: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,272) tree.c:2690: if (debug_rcu_head_queue(head)) {
      (<0.1>,275) rcu.h:92: return 0;
      (<0.1>,279) tree.c:2696: head->func = func;
      (<0.1>,280) tree.c:2696: head->func = func;
      (<0.1>,282) tree.c:2696: head->func = func;
      (<0.1>,283) tree.c:2697: head->next = NULL;
      (<0.1>,285) tree.c:2697: head->next = NULL;
      (<0.1>,286) tree.c:2705: local_irq_save(flags);
      (<0.1>,289) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,291) fake_sched.h:43: return __running_cpu;
      (<0.1>,295) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,297) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,301) fake_sched.h:43: return __running_cpu;
      (<0.1>,305) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,311) fake_sched.h:43: return __running_cpu;
      (<0.1>,314) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,316) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,318) tree.c:2706: rdp = &rsp->rda[get_cpu()];
      (<0.1>,319) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,322) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,330) tree.c:2709: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,333) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,335) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,337) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,339) tree.c:2720: ACCESS_ONCE(rdp->qlen) = rdp->qlen + 1;
      (<0.1>,340) tree.c:2721: if (lazy)
      (<0.1>,347) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,348) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,351) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,352) tree.c:2726: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,353) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,355) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,358) tree.c:2727: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,359) tree.c:2729: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,366) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,367) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,368) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,369) tree.c:2736: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,377)
      (<0.1>,378)
      (<0.1>,379)
      (<0.1>,380) tree.c:2628: if (!rcu_is_watching() && cpu_online(smp_processor_id()))
      (<0.1>,386) fake_sched.h:43: return __running_cpu;
      (<0.1>,392) tree.c:815: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,397) tree.c:829: ret = __rcu_is_watching();
      (<0.1>,399) tree.c:831: return ret;
      (<0.1>,403) tree.c:2632: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,406) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,408) fake_sched.h:43: return __running_cpu;
      (<0.1>,412) fake_sched.h:164: return !!local_irq_depth[get_cpu()];
      (<0.1>,422) tree.c:2737: local_irq_restore(flags);
      (<0.1>,425) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,427) fake_sched.h:43: return __running_cpu;
      (<0.1>,431) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,433) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,437) fake_sched.h:43: return __running_cpu;
      (<0.1>,441) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,452) fake_sync.h:238: might_sleep();
      (<0.1>,456) fake_sched.h:43: return __running_cpu;
      (<0.1>,460) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,463) tree.c:580: local_irq_save(flags);
      (<0.1>,466) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,468) fake_sched.h:43: return __running_cpu;
      (<0.1>,472) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,474) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,478) fake_sched.h:43: return __running_cpu;
      (<0.1>,482) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,495) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,497) fake_sched.h:43: return __running_cpu;
      (<0.1>,501) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,502) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,504) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,505) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,506) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,512) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,513) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,518) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,519) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,520) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,521) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,525) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,527) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,528) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,529) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,548)
      (<0.1>,550) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,552) fake_sched.h:43: return __running_cpu;
      (<0.1>,556) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,559) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,563) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,564) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,565) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,569) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,570) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,571) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,573) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,578) fake_sched.h:43: return __running_cpu;
      (<0.1>,581) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,583) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,585) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,586) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,589) tree_plugin.h:2720: }
      (<0.1>,592) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,595) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,596) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,597) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,601) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,602) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,603) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,605) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,610) fake_sched.h:43: return __running_cpu;
      (<0.1>,613) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,615) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,617) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,618) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,621) tree_plugin.h:2720: }
      (<0.1>,624) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,627) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,628) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,629) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,633) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,635) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,637) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,644) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,647) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,648) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,649) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,651) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,652) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,654) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,657) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,663) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,664) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,667) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,672) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,673) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,674) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,688) tree_plugin.h:3141: }
      (<0.1>,690) tree.c:583: local_irq_restore(flags);
      (<0.1>,693) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,695) fake_sched.h:43: return __running_cpu;
      (<0.1>,699) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,701) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,705) fake_sched.h:43: return __running_cpu;
      (<0.1>,709) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,715) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,718) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:704: local_irq_save(flags);
    (<0.0>,35) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,64) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,66) fake_sched.h:43: return __running_cpu;
    (<0.0>,70) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,71) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,75) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,80) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,81) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,85) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,86) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,87) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,88) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,92) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,94) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,95) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,96) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,110)
    (<0.0>,111) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,113) fake_sched.h:43: return __running_cpu;
    (<0.0>,117) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,121) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,124) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,125) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,126) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,128) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,129) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,131) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,134) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,141) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,142) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,145) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,150) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,151) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,152) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,157) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,166) tree_plugin.h:3145: }
    (<0.0>,168) tree.c:707: local_irq_restore(flags);
    (<0.0>,171) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,173) fake_sched.h:43: return __running_cpu;
    (<0.0>,177) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,179) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,183) fake_sched.h:43: return __running_cpu;
    (<0.0>,187) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,194) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,206)
    (<0.0>,207) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,209) tree.c:1792: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,213)
    (<0.0>,214) tree.c:453: return &rsp->node[0];
    (<0.0>,218) tree.c:1793: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,223) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,225) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) fake_sched.h:43: return __running_cpu;
    (<0.0>,233) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,237) fake_sched.h:43: return __running_cpu;
    (<0.0>,241) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,246) fake_sched.h:43: return __running_cpu;
    (<0.0>,250) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,260) tree.c:749: local_irq_save(flags);
    (<0.0>,263) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,271) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,276) fake_sched.h:43: return __running_cpu;
    (<0.0>,280) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,281) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,283) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,284) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,285) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,287) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,289) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,290) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,292) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,297) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,298) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,300) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,304) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,305) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,306) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,307) tree.c:754: if (oldval)
    (<0.0>,315) tree_plugin.h:3145: }
    (<0.0>,317) tree.c:759: local_irq_restore(flags);
    (<0.0>,320) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,322) fake_sched.h:43: return __running_cpu;
    (<0.0>,326) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,328) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,336) tree.c:2404: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,341) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,346) fake_sched.h:43: return __running_cpu;
    (<0.0>,351) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,359) fake_sched.h:43: return __running_cpu;
    (<0.0>,364) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,370) fake_sched.h:43: return __running_cpu;
    (<0.0>,375) tree.c:206: rcu_bh_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,388) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,389) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,390) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,394) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,395) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,396) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,398) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,402) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,404) fake_sched.h:43: return __running_cpu;
    (<0.0>,407) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,409) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,416)
    (<0.0>,417)
    (<0.0>,418) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,420) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,421) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,422) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,424) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,426) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,427) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,428) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,438)
    (<0.0>,439)
    (<0.0>,440) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,445) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,448) tree_plugin.h:3185: return 0;
    (<0.0>,451) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,454) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,456) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,459) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,461) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,465) tree.c:3144: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,468)
    (<0.0>,469) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,471) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,474) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,481) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,482) tree.c:3150: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,488)
    (<0.0>,489)
    (<0.0>,490) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,493)
    (<0.0>,494) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,496) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,497) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,499) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,505) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,511)
    (<0.0>,512) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,515)
    (<0.0>,516) tree.c:453: return &rsp->node[0];
    (<0.0>,520) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,521) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,523) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,527) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,528) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,530) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,533) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,534) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,535) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,539) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,542) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,545) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,548) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,549) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,552) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,554) tree.c:494: }
    (<0.0>,558) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,560) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,562) tree.c:3151: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,563) tree.c:3152: return 1;
    (<0.0>,565) tree.c:3176: }
    (<0.0>,569) tree.c:3189: return 1;
    (<0.0>,571) tree.c:3191: }
    (<0.0>,578) fake_sched.h:43: return __running_cpu;
    (<0.0>,582) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,586) tree.c:2437: if (user)
    (<0.0>,594) fake_sched.h:43: return __running_cpu;
    (<0.0>,598) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,600) fake_sched.h:43: return __running_cpu;
    (<0.0>,604) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,610) fake_sched.h:43: return __running_cpu;
    (<0.0>,614) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,624) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,627) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,628) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,629) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,633) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,634) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,635) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,637) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,641) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,650) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,652) fake_sched.h:43: return __running_cpu;
    (<0.0>,655) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,657) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,659) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,660) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,662) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,669) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,670) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,672) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,678) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,679) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,680) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,681) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,682) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,686)
    (<0.0>,687)
    (<0.0>,688) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,689) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,696)
    (<0.0>,697)
    (<0.0>,698) tree.c:1584: local_irq_save(flags);
    (<0.0>,701) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,703) fake_sched.h:43: return __running_cpu;
    (<0.0>,707) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,709) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,713) fake_sched.h:43: return __running_cpu;
    (<0.0>,717) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,722) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,724) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,725) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,726) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,728) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,729) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,731) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,734) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,736) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,737) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,739) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,742) tree.c:1589: local_irq_restore(flags);
    (<0.0>,745) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,747) fake_sched.h:43: return __running_cpu;
    (<0.0>,751) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,753) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,757) fake_sched.h:43: return __running_cpu;
    (<0.0>,761) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,768) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,770) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,775) tree.c:2558: local_irq_save(flags);
    (<0.0>,778) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,780) fake_sched.h:43: return __running_cpu;
    (<0.0>,784) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,786) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,790) fake_sched.h:43: return __running_cpu;
    (<0.0>,794) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,799) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,800) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,806)
    (<0.0>,807)
    (<0.0>,808) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,811)
    (<0.0>,812) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,814) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,815) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,817) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,823) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,829)
    (<0.0>,830) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,833)
    (<0.0>,834) tree.c:453: return &rsp->node[0];
    (<0.0>,838) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,839) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,841) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,845) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,846) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,848) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,851) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,852) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,853) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,857) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,860) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,863) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,866) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,867) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,870) tree.c:487: return 1;  /* Yes, this CPU has newly registered callbacks. */
    (<0.0>,872) tree.c:494: }
    (<0.0>,876) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,879)
    (<0.0>,880) tree.c:453: return &rsp->node[0];
    (<0.0>,887) fake_sync.h:108: preempt_disable();
    (<0.0>,889) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,890) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,894) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,900) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,902) fake_sched.h:43: return __running_cpu;
    (<0.0>,905) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,907) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,909) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,910) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,913)
    (<0.0>,914) tree.c:453: return &rsp->node[0];
    (<0.0>,918) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,919) tree.c:1925: bool ret = false;
    (<0.0>,920) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,921) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,922) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,930)
    (<0.0>,931)
    (<0.0>,932)
    (<0.0>,933) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,936) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,939) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,942) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,943) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,946) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,948) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,951) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,953) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,954) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,956) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,959) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,964) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,966) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,967) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,970) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,972) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,975) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,977) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,980) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,981) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,984) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,987) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,989) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,992) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,993) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,995) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,998) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,999) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1001) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1004) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1005) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1007) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1010) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1012) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1014) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1015) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1017) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1019) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1022) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1024) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1027) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1028) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1031) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1034) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1036) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1039) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1040) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1042) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1045) tree.c:1521: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1046) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1048) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1051) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1052) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1054) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1057) tree.c:1522: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1059) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1061) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1062) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1064) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1066) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1069) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1070) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1071) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1080)
    (<0.0>,1081)
    (<0.0>,1082)
    (<0.0>,1083) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1086) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1089) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1092) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1093) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1096) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1097) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1102)
    (<0.0>,1103)
    (<0.0>,1104) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1107)
    (<0.0>,1108) tree.c:453: return &rsp->node[0];
    (<0.0>,1112) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1115) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1117) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1118) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1120) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1123) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1125) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1127) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1129) tree.c:1261: }
    (<0.0>,1131) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1132) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1134) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1137) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1139) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1142) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1143) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1146) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1149) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1153) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1155) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1157) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1160) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1162) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1165) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1166) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1169) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1172) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1176) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1178) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1180) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1183) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1185) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1189) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1192) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1195) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1196) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1198) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1201) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1202) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1203) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1205) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1208) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1210) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1212) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1214) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1217) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1220) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1221) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1223) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1226) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1227) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1228) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1230) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1233) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1235) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1237) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1239) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1242) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1245) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1246) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1248) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1251) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1252) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1253) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1255) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1258) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,1260) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1262) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1264) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1267) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1268) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1277)
    (<0.0>,1278)
    (<0.0>,1279)
    (<0.0>,1280) tree.c:1289: bool ret = false;
    (<0.0>,1281) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1283) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1286)
    (<0.0>,1287) tree.c:453: return &rsp->node[0];
    (<0.0>,1291) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1292) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1294) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1295) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1300)
    (<0.0>,1301)
    (<0.0>,1302) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1305)
    (<0.0>,1306) tree.c:453: return &rsp->node[0];
    (<0.0>,1310) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1313) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1315) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1316) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1318) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1321) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1323) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1325) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1327) tree.c:1261: }
    (<0.0>,1329) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1330) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1331) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1332) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1338)
    (<0.0>,1339)
    (<0.0>,1340)
    (<0.0>,1341) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1345) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1347) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1350) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1353) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1355) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1356) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1358) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1361) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1363) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1364) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1366) tree.c:1322: ACCESS_ONCE(rnp_root->gpnum) != ACCESS_ONCE(rnp_root->completed)) {
    (<0.0>,1369) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1370) tree.c:1333: if (rnp != rnp_root) {
    (<0.0>,1373) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1375) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1376) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1381)
    (<0.0>,1382)
    (<0.0>,1383) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1386)
    (<0.0>,1387) tree.c:453: return &rsp->node[0];
    (<0.0>,1391) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1394) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1396) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1397) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1399) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1402) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1404) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1406) tree.c:1254: return rnp->completed + 1;
    (<0.0>,1408) tree.c:1261: }
    (<0.0>,1410) tree.c:1344: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1411) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1413) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1416) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1417) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1419) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1422) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1426) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1427) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1429) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1432) tree.c:1347: rdp->nxtcompleted[i] = c;
    (<0.0>,1435) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1437) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1439) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1442) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1443) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1445) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1448) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1453) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1455) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1457) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1460) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1461) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1463) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1466) tree.c:1346: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1471) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1473) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1475) tree.c:1345: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1478) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1480) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1483) tree.c:1353: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1486) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1488) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1491) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1493) tree.c:1359: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1494) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1496) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1497) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1499) tree.c:1362: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1502) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1503) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1504) tree.c:1365: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1510)
    (<0.0>,1511)
    (<0.0>,1512)
    (<0.0>,1513) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1517) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1519) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1520) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1521) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1527)
    (<0.0>,1528)
    (<0.0>,1529)
    (<0.0>,1530) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1532) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1535) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1536) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1542)
    (<0.0>,1543)
    (<0.0>,1544) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1547)
    (<0.0>,1548) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1550) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1551) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1553) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1559) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1565)
    (<0.0>,1566) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1569)
    (<0.0>,1570) tree.c:453: return &rsp->node[0];
    (<0.0>,1574) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1575) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1577) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1581) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1582) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1584) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1587) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1588) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1589) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1593) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1595) tree.c:494: }
    (<0.0>,1599) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1601) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1604) tree.c:1909: return true;
    (<0.0>,1606) tree.c:1910: }
    (<0.0>,1609) tree.c:1366: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1612) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1613) tree.c:1369: if (rnp != rnp_root)
    (<0.0>,1617) tree.c:1372: if (c_out != NULL)
    (<0.0>,1620) tree.c:1374: return ret;
    (<0.0>,1624) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1625) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1628) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1629) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,1635) tree.c:1482: return ret;
    (<0.0>,1637) tree.c:1482: return ret;
    (<0.0>,1639) tree.c:1483: }
    (<0.0>,1641) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1643) tree.c:1527: }
    (<0.0>,1647) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1648) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1649) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1650) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1656)
    (<0.0>,1657)
    (<0.0>,1658)
    (<0.0>,1659) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1661) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1664) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1665) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1671)
    (<0.0>,1672)
    (<0.0>,1673) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,1676)
    (<0.0>,1677) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1679) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1680) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1682) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,1688) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,1694)
    (<0.0>,1695) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1698)
    (<0.0>,1699) tree.c:453: return &rsp->node[0];
    (<0.0>,1703) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1704) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1706) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1710) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1711) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1713) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1716) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1717) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1718) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,1722) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,1724) tree.c:494: }
    (<0.0>,1728) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1730) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,1733) tree.c:1909: return true;
    (<0.0>,1735) tree.c:1910: }
    (<0.0>,1739) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,1740) tree.c:1937: return ret;
    (<0.0>,1744) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,1745) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1748)
    (<0.0>,1749) tree.c:453: return &rsp->node[0];
    (<0.0>,1754) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,1758)
    (<0.0>,1759)
    (<0.0>,1760) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1761) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,1764) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,1767) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1769) fake_sched.h:43: return __running_cpu;
    (<0.0>,1773) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1775) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1779) fake_sched.h:43: return __running_cpu;
    (<0.0>,1783) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1790) tree.c:2563: if (needwake)
    (<0.0>,1793) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,1796)
    (<0.0>,1797) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1798) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1800) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,1807) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,1810)
    (<0.0>,1811) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1813) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1816) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,1823) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,1826) tree_plugin.h:2720: }
    (<0.0>,1830) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1833) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1834) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1835) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1839) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1840) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1841) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1843) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,1847) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,1856) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1858) fake_sched.h:43: return __running_cpu;
    (<0.0>,1861) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1863) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1865) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,1866) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1868) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1875) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1876) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1878) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1884) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1885) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1886) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,1887) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1888) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,1892)
    (<0.0>,1893)
    (<0.0>,1894) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1895) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,1902)
    (<0.0>,1903)
    (<0.0>,1904) tree.c:1584: local_irq_save(flags);
    (<0.0>,1907) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1909) fake_sched.h:43: return __running_cpu;
    (<0.0>,1913) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1915) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1919) fake_sched.h:43: return __running_cpu;
    (<0.0>,1923) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1928) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1930) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1931) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,1932) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1934) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1935) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1937) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,1940) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1942) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1943) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1945) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,1948) tree.c:1589: local_irq_restore(flags);
    (<0.0>,1951) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1953) fake_sched.h:43: return __running_cpu;
    (<0.0>,1957) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1959) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1963) fake_sched.h:43: return __running_cpu;
    (<0.0>,1967) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1974) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1976) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,1981) tree.c:2558: local_irq_save(flags);
    (<0.0>,1984) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1986) fake_sched.h:43: return __running_cpu;
    (<0.0>,1990) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1992) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1996) fake_sched.h:43: return __running_cpu;
    (<0.0>,2000) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2005) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2006) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2012)
    (<0.0>,2013)
    (<0.0>,2014) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,2017)
    (<0.0>,2018) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2020) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2021) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2023) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2029) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,2035)
    (<0.0>,2036) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2039)
    (<0.0>,2040) tree.c:453: return &rsp->node[0];
    (<0.0>,2044) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2045) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2047) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2051) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2052) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2054) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2057) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2058) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2059) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,2063) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2066) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2069) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2072) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2073) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2076) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2078) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2081) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2084) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2087) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2088) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2090) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2093) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2097) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2099) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2101) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2104) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2107) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2110) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2111) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2113) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2116) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2120) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2122) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2124) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2127) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,2129) tree.c:494: }
    (<0.0>,2133) tree.c:2566: local_irq_restore(flags);
    (<0.0>,2136) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2138) fake_sched.h:43: return __running_cpu;
    (<0.0>,2142) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2144) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2148) fake_sched.h:43: return __running_cpu;
    (<0.0>,2152) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2158) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2161)
    (<0.0>,2162) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2164) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2167) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2174) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2177) tree_plugin.h:2720: }
    (<0.0>,2181) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2184) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2185) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2186) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2190) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2191) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2192) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2194) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,2202) fake_sched.h:43: return __running_cpu;
    (<0.0>,2206) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,2215) tree.c:624: local_irq_save(flags);
    (<0.0>,2218) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2220) fake_sched.h:43: return __running_cpu;
    (<0.0>,2224) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2226) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2230) fake_sched.h:43: return __running_cpu;
    (<0.0>,2234) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2240) fake_sched.h:43: return __running_cpu;
    (<0.0>,2244) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2245) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2247) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2248) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,2249) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2251) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2253) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,2254) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2256) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2261) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2262) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2264) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2268) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2269) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2270) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,2271) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2273) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,2281) tree_plugin.h:3141: }
    (<0.0>,2283) tree.c:634: local_irq_restore(flags);
    (<0.0>,2286) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2288) fake_sched.h:43: return __running_cpu;
    (<0.0>,2292) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2294) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2298) fake_sched.h:43: return __running_cpu;
    (<0.0>,2302) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2311) fake_sched.h:43: return __running_cpu;
    (<0.0>,2315) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,2318) tree.c:580: local_irq_save(flags);
    (<0.0>,2321) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2323) fake_sched.h:43: return __running_cpu;
    (<0.0>,2327) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2329) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2333) fake_sched.h:43: return __running_cpu;
    (<0.0>,2337) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2350) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2352) fake_sched.h:43: return __running_cpu;
    (<0.0>,2356) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2357) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2359) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2360) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,2361) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2367) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2368) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2373) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2374) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2375) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,2376) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,2380) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2382) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,2383) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2384) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,2403)
    (<0.0>,2405) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2407) fake_sched.h:43: return __running_cpu;
    (<0.0>,2411) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2414) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,2418) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2419) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2420) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2424) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2425) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2426) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2428) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2433) fake_sched.h:43: return __running_cpu;
    (<0.0>,2436) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2438) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2440) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2441) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2444) tree_plugin.h:2720: }
    (<0.0>,2447) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2450) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2451) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2452) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2456) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2457) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2458) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2460) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2465) fake_sched.h:43: return __running_cpu;
    (<0.0>,2468) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2470) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2472) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,2473) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2476) tree_plugin.h:2720: }
    (<0.0>,2479) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2482) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2483) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2484) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2488) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2489) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2490) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2492) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,2499) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2502) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2503) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2504) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2506) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2507) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,2509) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2512) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2518) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2519) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2522) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2527) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2528) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2529) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,2543) tree_plugin.h:3141: }
    (<0.0>,2545) tree.c:583: local_irq_restore(flags);
    (<0.0>,2548) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2550) fake_sched.h:43: return __running_cpu;
    (<0.0>,2554) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2556) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2560) fake_sched.h:43: return __running_cpu;
    (<0.0>,2564) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2570) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2573) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,2578) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2580) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,2589) fake_sched.h:43: return __running_cpu;
    (<0.0>,2593)
    (<0.0>,2594) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2597) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,2602) tree.c:704: local_irq_save(flags);
    (<0.0>,2605) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2607) fake_sched.h:43: return __running_cpu;
    (<0.0>,2611) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2613) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2617) fake_sched.h:43: return __running_cpu;
    (<0.0>,2621) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2634) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2636) fake_sched.h:43: return __running_cpu;
    (<0.0>,2640) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2641) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2643) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2644) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,2645) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2650) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2651) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2655) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2656) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2657) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,2658) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,2662) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2664) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,2665) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2666) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,2680)
    (<0.0>,2681) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2683) fake_sched.h:43: return __running_cpu;
    (<0.0>,2687) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,2691) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2694) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2695) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2696) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2698) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2699) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,2701) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2704) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2711) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2712) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2715) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2720) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2721) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2722) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,2727) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,2736) tree_plugin.h:3145: }
    (<0.0>,2738) tree.c:707: local_irq_restore(flags);
    (<0.0>,2741) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2743) fake_sched.h:43: return __running_cpu;
    (<0.0>,2747) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2749) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2753) fake_sched.h:43: return __running_cpu;
    (<0.0>,2757) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2764) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,2776)
    (<0.0>,2777) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2780)
    (<0.0>,2781) tree.c:453: return &rsp->node[0];
    (<0.0>,2785) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2789) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2793) fake_sync.h:92: local_irq_disable();
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2804) fake_sched.h:43: return __running_cpu;
    (<0.0>,2808) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2813) fake_sched.h:43: return __running_cpu;
    (<0.0>,2817) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,2820) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2821) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,2827) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2829) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,2832) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2834) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,2835) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2838)
    (<0.0>,2839) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2841) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2842) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2844) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2852) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2853) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2856)
    (<0.0>,2857) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2859) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2860) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2870) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2871) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,2874) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,2879)
    (<0.0>,2880) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2881) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,2882) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2883) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2885) tree.c:1012: rsp->gp_start = j;
    (<0.0>,2889) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2890) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,2891) update.c:344: if (till_stall_check < 3) {
    (<0.0>,2894) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,2898) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,2903) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,2904) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2905) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2907) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2909) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,2910) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2911) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2914) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2916) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,2920) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2922) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2924) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2926) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,2930) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,2934)
    (<0.0>,2935) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2936) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,2941) fake_sched.h:43: return __running_cpu;
    (<0.0>,2945) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,2947) fake_sched.h:43: return __running_cpu;
    (<0.0>,2951) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2957) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,2961)
    (<0.0>,2962) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2964) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,2970) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2973) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2975) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2976) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2978) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,2983) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,2987) fake_sync.h:92: local_irq_disable();
    (<0.0>,2990) fake_sched.h:43: return __running_cpu;
    (<0.0>,2994) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,2998) fake_sched.h:43: return __running_cpu;
    (<0.0>,3002) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3007) fake_sched.h:43: return __running_cpu;
    (<0.0>,3011) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3014) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3015) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,3022) fake_sched.h:43: return __running_cpu;
    (<0.0>,3025) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3027) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3029) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3030) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,3036)
    (<0.0>,3037) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3039) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3044) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3045) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3047) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3051) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3052) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3053) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,3055) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3057) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3058) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3060) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,3061) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3063) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3064) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,3066) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
  (<0>,5327) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5328) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5330) tree.c:3162: if (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) { /* outside lock */
  (<0>,5333) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5335) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5337) tree.c:3163: rdp->n_rp_gp_started++;
  (<0>,5338) tree.c:3164: return 1;
  (<0>,5340) tree.c:3176: }
  (<0>,5344) tree.c:3189: return 1;
  (<0>,5346) tree.c:3191: }
  (<0>,5353) fake_sched.h:43: return __running_cpu;
  (<0>,5357) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
  (<0>,5361) tree.c:2437: if (user)
  (<0>,5369) fake_sched.h:43: return __running_cpu;
  (<0>,5373) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
  (<0>,5375) fake_sched.h:43: return __running_cpu;
  (<0>,5379) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5385) fake_sched.h:43: return __running_cpu;
  (<0>,5389) fake_sched.h:182: if (need_softirq[get_cpu()]) {
  (<0>,5399) tree.c:2586: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,5402) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5403) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5404) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5408) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5409) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5410) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5412) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5416) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,5425) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5427) fake_sched.h:43: return __running_cpu;
  (<0>,5430) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5432) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5434) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5435) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5437) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5444) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5445) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5447) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5453) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5454) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5455) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5456) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5457) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5461)
  (<0>,5462)
  (<0>,5463) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5464) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5471)
  (<0>,5472)
  (<0>,5473) tree.c:1584: local_irq_save(flags);
  (<0>,5476) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5478) fake_sched.h:43: return __running_cpu;
  (<0>,5482) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5484) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5488) fake_sched.h:43: return __running_cpu;
  (<0>,5492) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5497) tree.c:1585: rnp = rdp->mynode;
  (<0>,5499) tree.c:1585: rnp = rdp->mynode;
  (<0>,5500) tree.c:1585: rnp = rdp->mynode;
  (<0>,5501) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5503) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5504) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5506) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5509) tree.c:1588: !raw_spin_trylock(&rnp->lock)) { /* irqs already off, so later. */
  (<0>,5514) fake_sync.h:122: preempt_disable();
  (<0>,5516) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
    (<0.0>,3067) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3069) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3070) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3072) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3077) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3078) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3080) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3081) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3083) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3087) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3088) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3089) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,3090) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3092) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3093) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3095) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,3096) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3097) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3099) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,3102) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3103) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3104) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,3110)
    (<0.0>,3111)
    (<0.0>,3112)
    (<0.0>,3113) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3115) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3116) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3118) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,3121) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3122) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3123) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3132)
    (<0.0>,3133)
    (<0.0>,3134)
    (<0.0>,3135) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3138) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3141) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3144) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3145) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,3148) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3149) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3154)
    (<0.0>,3155)
    (<0.0>,3156) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3159)
    (<0.0>,3160) tree.c:453: return &rsp->node[0];
    (<0.0>,3164) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3167) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3169) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3170) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3172) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3175) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3177) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3179) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3181) tree.c:1261: }
    (<0.0>,3183) tree.c:1450: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,3184) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3186) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3189) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3191) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3194) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3195) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3198) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3201) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3205) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3207) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3209) tree.c:1451: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,3212) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3214) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3217) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3218) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3221) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3224) tree.c:1452: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,3227) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3229) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3232) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3233) tree.c:1453: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,3238) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3240) tree.c:1462: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,3244) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3247) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3250) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3251) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3253) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3256) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3257) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3258) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3260) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3263) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3265) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3267) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3269) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3272) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3275) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3276) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3278) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3281) tree.c:1471: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,3282) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3283) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3285) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3288) tree.c:1472: rdp->nxtcompleted[i] = c;
    (<0.0>,3290) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3292) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3294) tree.c:1470: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,3297) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3298) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3307)
    (<0.0>,3308)
    (<0.0>,3309)
    (<0.0>,3310) tree.c:1289: bool ret = false;
    (<0.0>,3311) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3313) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3316)
    (<0.0>,3317) tree.c:453: return &rsp->node[0];
    (<0.0>,3321) tree.c:1290: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,3322) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3324) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3325) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3330)
    (<0.0>,3331)
    (<0.0>,3332) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3335)
    (<0.0>,3336) tree.c:453: return &rsp->node[0];
    (<0.0>,3340) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3343) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3345) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3346) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3348) tree.c:1253: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,3351) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3353) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3355) tree.c:1260: return rnp->completed + 2;
    (<0.0>,3357) tree.c:1261: }
    (<0.0>,3359) tree.c:1300: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,3360) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3361) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3362) tree.c:1302: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,3368)
    (<0.0>,3369)
    (<0.0>,3370)
    (<0.0>,3371) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,3375) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3377) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3380) tree.c:1303: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,3383) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3385) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3386) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3388) tree.c:1321: if (rnp->gpnum != rnp->completed ||
    (<0.0>,3391) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3393) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3396) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3398) tree.c:1323: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,3399) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3400) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3401) tree.c:1324: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,3407)
    (<0.0>,3408)
    (<0.0>,3409)
    (<0.0>,3410) tree.c:1270: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,3415) tree.c:1372: if (c_out != NULL)
    (<0.0>,3418) tree.c:1374: return ret;
    (<0.0>,3422) tree.c:1475: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,3423) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3426) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3427) tree.c:1478: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,3433) tree.c:1482: return ret;
    (<0.0>,3435) tree.c:1482: return ret;
    (<0.0>,3437) tree.c:1483: }
    (<0.0>,3440) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,3442) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3444) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3445) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3447) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,3450) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3452) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3453) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3455) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,3458) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3460) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,3461) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3463) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3464) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3466) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3471) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3474) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,3475) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,3477) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,3479) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,3481) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,3483) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,3484) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,3487) tree_plugin.h:1950: }
    (<0.0>,3490) tree.c:1575: return ret;
    (<0.0>,3494) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,3497) tree_plugin.h:1487: }
    (<0.0>,3501) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,3505)
    (<0.0>,3506) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3507) fake_sync.h:100: if (pthread_mutex_unlock(l))
  (<0>,5517) fake_sync.h:123: if (pthread_mutex_trylock(l)) {
  (<0>,5520) fake_sync.h:127: return 1;
  (<0>,5522) fake_sync.h:128: }
  (<0>,5528) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5529) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5530) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5536)
  (<0>,5537)
  (<0>,5538)
  (<0>,5539) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5541) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5542) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5544) tree.c:1541: if (rdp->completed == rnp->completed) {
  (<0>,5547) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5548) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5549) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5558)
  (<0>,5559)
  (<0>,5560)
  (<0>,5561) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5564) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5567) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5570) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5571) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,5574) tree.c:1434: return false;
  (<0>,5576) tree.c:1483: }
  (<0>,5579) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,5581) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5583) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5584) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5586) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
  (<0>,5589) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5591) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5592) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5594) tree.c:1562: rdp->gpnum = rnp->gpnum;
  (<0>,5597) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,5599) tree.c:1564: rdp->passed_quiesce = 0;
  (<0>,5600) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5602) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5603) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5605) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5610) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5613) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
  (<0>,5614) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
  (<0>,5616) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
  (<0>,5618) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
  (<0>,5620) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
  (<0>,5622) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
  (<0>,5623) tree.c:1573: zero_cpu_stall_ticks(rdp);
  (<0>,5626) tree_plugin.h:1950: }
  (<0>,5629) tree.c:1575: return ret;
  (<0>,5633) tree.c:1593: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,5634) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,5636) tree.c:1594: raw_spin_unlock_irqrestore(&rnp->lock, flags);
  (<0>,5640)
  (<0>,5641)
  (<0>,5642) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,5643) fake_sync.h:84: if (pthread_mutex_unlock(l))
  (<0>,5646) fake_sync.h:86: local_irq_restore(flags);
  (<0>,5649) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5651) fake_sched.h:43: return __running_cpu;
  (<0>,5655) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5657) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5661) fake_sched.h:43: return __running_cpu;
  (<0>,5665) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5672) tree.c:1595: if (needwake)
  (<0>,5676) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5678) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5681) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,5683) tree.c:2091: if (!rdp->passed_quiesce)
  (<0>,5688) tree.c:2558: local_irq_save(flags);
  (<0>,5691) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5693) fake_sched.h:43: return __running_cpu;
  (<0>,5697) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5699) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5703) fake_sched.h:43: return __running_cpu;
  (<0>,5707) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5712) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5713) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5719)
  (<0>,5720)
  (<0>,5721) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,5724)
  (<0>,5725) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5727) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5728) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5730) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5736) tree.c:481: return 0;  /* No, a grace period is already in progress. */
  (<0>,5738) tree.c:494: }
  (<0>,5742) tree.c:2566: local_irq_restore(flags);
  (<0>,5745) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5747) fake_sched.h:43: return __running_cpu;
  (<0>,5751) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5753) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5757) fake_sched.h:43: return __running_cpu;
  (<0>,5761) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5767) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,5770)
  (<0>,5771) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5773) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5776) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,5783) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,5786) tree_plugin.h:2720: }
  (<0>,5790) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5793) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5794) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5795) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5799) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5800) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5801) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5803) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,5807) tree.c:2588: __rcu_process_callbacks(rsp);
  (<0>,5816) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5818) fake_sched.h:43: return __running_cpu;
  (<0>,5821) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5823) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5825) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
  (<0>,5826) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5828) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5835) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5836) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5838) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5844) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5845) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5846) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,5847) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5848) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
  (<0>,5852)
  (<0>,5853)
  (<0>,5854) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5855) tree.c:2078: note_gp_changes(rsp, rdp);
  (<0>,5862)
  (<0>,5863)
  (<0>,5864) tree.c:1584: local_irq_save(flags);
  (<0>,5867) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5869) fake_sched.h:43: return __running_cpu;
  (<0>,5873) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5875) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5879) fake_sched.h:43: return __running_cpu;
  (<0>,5883) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5888) tree.c:1585: rnp = rdp->mynode;
  (<0>,5890) tree.c:1585: rnp = rdp->mynode;
  (<0>,5891) tree.c:1585: rnp = rdp->mynode;
  (<0>,5892) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5894) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5895) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5897) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
  (<0>,5900) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5902) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5903) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5905) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
  (<0>,5908) tree.c:1589: local_irq_restore(flags);
  (<0>,5911) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5913) fake_sched.h:43: return __running_cpu;
  (<0>,5917) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5919) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5923) fake_sched.h:43: return __running_cpu;
  (<0>,5927) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5934) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5936) tree.c:2084: if (!rdp->qs_pending)
  (<0>,5941) tree.c:2558: local_irq_save(flags);
  (<0>,5944) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5946) fake_sched.h:43: return __running_cpu;
  (<0>,5950) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5952) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5956) fake_sched.h:43: return __running_cpu;
  (<0>,5960) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5965) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5966) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,5972)
  (<0>,5973)
  (<0>,5974) tree.c:480: if (rcu_gp_in_progress(rsp))
  (<0>,5977)
  (<0>,5978) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5980) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5981) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5983) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
  (<0>,5989) tree.c:482: if (rcu_future_needs_gp(rsp))
  (<0>,5995)
  (<0>,5996) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,5999)
  (<0>,6000) tree.c:453: return &rsp->node[0];
  (<0>,6004) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6005) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6007) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6011) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6012) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6014) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6017) tree.c:465: int *fp = &rnp->need_future_gp[idx];
  (<0>,6018) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6019) tree.c:467: return ACCESS_ONCE(*fp);
  (<0>,6023) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6026) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6029) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6032) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6033) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6036) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6038) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6041) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6044) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6047) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6048) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6050) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6053) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6057) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6059) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6061) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6064) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6067) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6070) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6071) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6073) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6076) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6080) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6082) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6084) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6087) tree.c:493: return 0; /* No grace period needed. */
  (<0>,6089) tree.c:494: }
  (<0>,6093) tree.c:2566: local_irq_restore(flags);
  (<0>,6096) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6098) fake_sched.h:43: return __running_cpu;
  (<0>,6102) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6104) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6108) fake_sched.h:43: return __running_cpu;
  (<0>,6112) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6118) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,6121)
  (<0>,6122) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6124) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6127) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6134) tree.c:2574: do_nocb_deferred_wakeup(rdp);
  (<0>,6137) tree_plugin.h:2720: }
  (<0>,6141) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6144) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6145) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6146) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6150) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6151) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6152) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6154) tree.c:2587: for_each_rcu_flavor(rsp)
  (<0>,6162) fake_sched.h:43: return __running_cpu;
  (<0>,6166) fake_sched.h:184: need_softirq[get_cpu()] = 0;
  (<0>,6175) tree.c:624: local_irq_save(flags);
  (<0>,6178) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6180) fake_sched.h:43: return __running_cpu;
  (<0>,6184) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6186) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6190) fake_sched.h:43: return __running_cpu;
  (<0>,6194) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6200) fake_sched.h:43: return __running_cpu;
  (<0>,6204) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6205) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6207) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6208) tree.c:626: oldval = rdtp->dynticks_nesting;
  (<0>,6209) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6211) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6213) tree.c:627: rdtp->dynticks_nesting--;
  (<0>,6214) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6216) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6221) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6222) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6224) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6228) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6229) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6230) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
  (<0>,6231) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,6233) tree.c:629: if (rdtp->dynticks_nesting)
  (<0>,6241) tree_plugin.h:3141: }
  (<0>,6243) tree.c:634: local_irq_restore(flags);
  (<0>,6246) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6248) fake_sched.h:43: return __running_cpu;
  (<0>,6252) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6254) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6258) fake_sched.h:43: return __running_cpu;
  (<0>,6262) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,719) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3508) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,3512) fake_sched.h:43: return __running_cpu;
    (<0.0>,3516) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,3518) fake_sched.h:43: return __running_cpu;
    (<0.0>,3522) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3535) fake_sched.h:43: return __running_cpu;
    (<0.0>,3540) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,3546) fake_sched.h:43: return __running_cpu;
    (<0.0>,3551) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,3557) fake_sched.h:43: return __running_cpu;
    (<0.0>,3561) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,3573) fake_sched.h:43: return __running_cpu;
    (<0.0>,3577)
    (<0.0>,3580) tree.c:580: local_irq_save(flags);
    (<0.0>,3583)
    (<0.0>,3585) fake_sched.h:43: return __running_cpu;
    (<0.0>,3589) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3591) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3595) fake_sched.h:43: return __running_cpu;
    (<0.0>,3599) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3612)
    (<0.0>,3614) fake_sched.h:43: return __running_cpu;
    (<0.0>,3618) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3619) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3621) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3622) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,3623) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3629) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3630) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3635) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3636) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3637) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,3638) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3642) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3644) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,3645) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3646) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3665)
    (<0.0>,3667)
    (<0.0>,3669) fake_sched.h:43: return __running_cpu;
    (<0.0>,3673) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3676) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,3680) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3681) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3682) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3686) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3687) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3688) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3690) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3695) fake_sched.h:43: return __running_cpu;
    (<0.0>,3698) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3700) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3702) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3703) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3706)
    (<0.0>,3709) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3712) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3713) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3714) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3718) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3719) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3720) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3722) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3727) fake_sched.h:43: return __running_cpu;
    (<0.0>,3730) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3732) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3734) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,3735) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3738)
    (<0.0>,3741) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3744) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3745) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3746) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3750) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3751) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3752) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3754) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,3761) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3764) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3765) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3766) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3768) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3769) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,3771) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3774) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3780) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3781) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3784) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3789) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3790) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3791) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,3805)
    (<0.0>,3807) tree.c:583: local_irq_restore(flags);
    (<0.0>,3810)
    (<0.0>,3812) fake_sched.h:43: return __running_cpu;
    (<0.0>,3816) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3818) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3822) fake_sched.h:43: return __running_cpu;
    (<0.0>,3826) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3832) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3835) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3840) fake_sched.h:43: return __running_cpu;
    (<0.0>,3844)
    (<0.0>,3845) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3848) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3853) tree.c:704: local_irq_save(flags);
    (<0.0>,3856)
    (<0.0>,3858) fake_sched.h:43: return __running_cpu;
    (<0.0>,3862) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3864) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3868) fake_sched.h:43: return __running_cpu;
    (<0.0>,3872) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3885)
    (<0.0>,3887) fake_sched.h:43: return __running_cpu;
    (<0.0>,3891) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3892) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3894) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3895) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,3896) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3901) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3902) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3906) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3907) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3908) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,3909) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3913) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3915) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3916) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3917) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3931)
    (<0.0>,3932)
    (<0.0>,3934) fake_sched.h:43: return __running_cpu;
    (<0.0>,3938) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,3942) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3945) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3946) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3947) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3949) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3950) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,3952) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3955) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3962) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3963) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3966) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3971) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3972) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3973) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,3978) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,3987)
    (<0.0>,3989) tree.c:707: local_irq_restore(flags);
    (<0.0>,3992)
    (<0.0>,3994) fake_sched.h:43: return __running_cpu;
    (<0.0>,3998) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4000) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4004) fake_sched.h:43: return __running_cpu;
    (<0.0>,4008) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4023) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4025) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4027) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4028) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4030) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4035) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,4039)
    (<0.0>,4040) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4042) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,4046) tree.c:1674: return 1;
    (<0.0>,4048) tree.c:1675: }
    (<0.0>,4053) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,4054) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4055) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,4056) tree.c:1819: if (j > HZ) {
    (<0.0>,4059) tree.c:1823: ret = 0;
    (<0.0>,4061) tree.c:1825: if (!ret)
    (<0.0>,4064) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4065) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4067) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4069) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,4073) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4075) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,4079) fake_sched.h:43: return __running_cpu;
    (<0.0>,4083) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4087) fake_sched.h:43: return __running_cpu;
    (<0.0>,4091) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4096) fake_sched.h:43: return __running_cpu;
    (<0.0>,4100) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4110) tree.c:749: local_irq_save(flags);
    (<0.0>,4113)
    (<0.0>,4115) fake_sched.h:43: return __running_cpu;
    (<0.0>,4119) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4121) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4126) fake_sched.h:43: return __running_cpu;
    (<0.0>,4130) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,4131) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4133) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4134) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,4135) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4137) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4139) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,4140) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4142) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4147) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4148) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4150) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4154) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4155) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4156) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,4157) tree.c:754: if (oldval)
    (<0.0>,4165)
    (<0.0>,4167) tree.c:759: local_irq_restore(flags);
    (<0.0>,4170)
    (<0.0>,4172) fake_sched.h:43: return __running_cpu;
    (<0.0>,4176) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4178) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4186)
    (<0.0>,4191) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,4196) fake_sched.h:43: return __running_cpu;
    (<0.0>,4201) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,4209) fake_sched.h:43: return __running_cpu;
    (<0.0>,4214) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,4228) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4229) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4230) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4234) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4235) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4236) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4238) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,4242) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4244) fake_sched.h:43: return __running_cpu;
    (<0.0>,4247) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4249) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,4256)
    (<0.0>,4257)
    (<0.0>,4258) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4260) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4261) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,4262) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4264) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4266) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,4267) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4268) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,4278)
    (<0.0>,4279)
    (<0.0>,4280) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,4285) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,4288)
    (<0.0>,4291) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,4294) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4296) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4299) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4301) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,4304) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4306) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4309) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4311) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,4314) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4316) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4318) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,4319) tree.c:3140: return 1;
    (<0.0>,4321) tree.c:3176: }
    (<0.0>,4325) tree.c:3189: return 1;
    (<0.0>,4327) tree.c:3191: }
    (<0.0>,4334) fake_sched.h:43: return __running_cpu;
    (<0.0>,4338) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,4342) tree.c:2437: if (user)
    (<0.0>,4350) fake_sched.h:43: return __running_cpu;
    (<0.0>,4354) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4356) fake_sched.h:43: return __running_cpu;
    (<0.0>,4360) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4366) fake_sched.h:43: return __running_cpu;
    (<0.0>,4370) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,4380)
    (<0.0>,4383) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4384) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4385) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4389) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4390) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4391) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4393) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4397) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,4406)
    (<0.0>,4408) fake_sched.h:43: return __running_cpu;
    (<0.0>,4411) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4413) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4415) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4416) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4418) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4425) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4426) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4428) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4434) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4435) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4436) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4437) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4438) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4442)
    (<0.0>,4443)
    (<0.0>,4444) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4445) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4452)
    (<0.0>,4453)
    (<0.0>,4454) tree.c:1584: local_irq_save(flags);
    (<0.0>,4457)
    (<0.0>,4459) fake_sched.h:43: return __running_cpu;
    (<0.0>,4463) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4465) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4469) fake_sched.h:43: return __running_cpu;
    (<0.0>,4473) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4478) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4480) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4481) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4482) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4484) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4485) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4487) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4490) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4492) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4493) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4495) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4498) tree.c:1589: local_irq_restore(flags);
    (<0.0>,4501)
    (<0.0>,4503) fake_sched.h:43: return __running_cpu;
    (<0.0>,4507) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4509) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4513) fake_sched.h:43: return __running_cpu;
    (<0.0>,4517) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4524) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4526) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4529) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,4531) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,4534) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4536) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4537) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4538) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,4547)
    (<0.0>,4548)
    (<0.0>,4549)
    (<0.0>,4550) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4552) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4553) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,4554) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,4556) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,4560)
    (<0.0>,4561)
    (<0.0>,4562) fake_sync.h:76: local_irq_save(flags);
    (<0.0>,4565)
    (<0.0>,4567) fake_sched.h:43: return __running_cpu;
    (<0.0>,4571) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4573) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4577) fake_sched.h:43: return __running_cpu;
    (<0.0>,4581) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4587) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,4588) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,4594) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4596) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4601) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4603) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4604) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4606) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,4609) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4611) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4612) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4614) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,4617) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4619) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4620) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,4621) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4623) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4624) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,4628) tree.c:2052: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,4630) tree.c:2052: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,4634)
    (<0.0>,4635)
    (<0.0>,4636) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,4637) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,4640) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,4643)
    (<0.0>,4645) fake_sched.h:43: return __running_cpu;
    (<0.0>,4649) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4651) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4655) fake_sched.h:43: return __running_cpu;
    (<0.0>,4659) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4670) tree.c:2558: local_irq_save(flags);
    (<0.0>,4673)
    (<0.0>,4675) fake_sched.h:43: return __running_cpu;
    (<0.0>,4679) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4681) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4685) fake_sched.h:43: return __running_cpu;
    (<0.0>,4689) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4694) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4695) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4701)
    (<0.0>,4702)
    (<0.0>,4703) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,4706)
    (<0.0>,4707) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4709) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4710) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4712) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4718) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,4720) tree.c:494: }
    (<0.0>,4724) tree.c:2566: local_irq_restore(flags);
    (<0.0>,4727)
    (<0.0>,4729) fake_sched.h:43: return __running_cpu;
    (<0.0>,4733) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4735) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4739) fake_sched.h:43: return __running_cpu;
    (<0.0>,4743) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4749) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,4752)
    (<0.0>,4753) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4755) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4758) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,4765) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,4768)
    (<0.0>,4772) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4775) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4776) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4777) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4781) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4782) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4783) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4785) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,4789) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,4798)
    (<0.0>,4800) fake_sched.h:43: return __running_cpu;
    (<0.0>,4803) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4805) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4807) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,4808) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4810) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4817) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4818) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4820) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4826) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4827) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4828) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,4829) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4830) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,4834)
    (<0.0>,4835)
    (<0.0>,4836) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4837) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,4844)
    (<0.0>,4845)
    (<0.0>,4846) tree.c:1584: local_irq_save(flags);
    (<0.0>,4849)
    (<0.0>,4851) fake_sched.h:43: return __running_cpu;
    (<0.0>,4855) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4857) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4861) fake_sched.h:43: return __running_cpu;
    (<0.0>,4865) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4870) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4872) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4873) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,4874) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4876) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4877) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4879) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,4882) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4884) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4885) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4887) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,4890) tree.c:1589: local_irq_restore(flags);
    (<0.0>,4893)
    (<0.0>,4895) fake_sched.h:43: return __running_cpu;
    (<0.0>,4899) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4901) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,4905) fake_sched.h:43: return __running_cpu;
    (<0.0>,4909) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4916) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4918) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,4923) tree.c:2558: local_irq_save(flags);
    (<0.0>,4926)
    (<0.0>,4928) fake_sched.h:43: return __running_cpu;
    (<0.0>,4932) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4934) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4938) fake_sched.h:43: return __running_cpu;
    (<0.0>,4942) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4947) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4948) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,4954)
    (<0.0>,4955)
    (<0.0>,4956) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,4959)
    (<0.0>,4960) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4962) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4963) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4965) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,4971) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,4977)
    (<0.0>,4978) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,4981)
    (<0.0>,4982) tree.c:453: return &rsp->node[0];
    (<0.0>,4986) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,4987) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4989) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4993) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,4994) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,4996) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,4999) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,5000) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,5001) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,5005) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5008) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,5011) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5014) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5015) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,5018) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5020) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5023) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5026) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5029) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5030) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5032) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5035) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5039) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5041) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5043) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5046) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5049) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5052) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5053) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5055) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5058) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,5062) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5064) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5066) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,5069) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,5071) tree.c:494: }
    (<0.0>,5075) tree.c:2566: local_irq_restore(flags);
    (<0.0>,5078)
    (<0.0>,5080) fake_sched.h:43: return __running_cpu;
    (<0.0>,5084) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5086) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5090) fake_sched.h:43: return __running_cpu;
    (<0.0>,5094) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5100) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,5103)
    (<0.0>,5104) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5106) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5109) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5116) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5119)
    (<0.0>,5123) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5126) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5127) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5128) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5132) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5133) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5134) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5136) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,5144) fake_sched.h:43: return __running_cpu;
    (<0.0>,5148) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,5157) tree.c:624: local_irq_save(flags);
    (<0.0>,5160)
    (<0.0>,5162) fake_sched.h:43: return __running_cpu;
    (<0.0>,5166) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5168) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5172) fake_sched.h:43: return __running_cpu;
    (<0.0>,5176) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5182) fake_sched.h:43: return __running_cpu;
    (<0.0>,5186) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5187) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5189) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5190) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,5191) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5193) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5195) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,5196) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5198) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5203) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5204) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5206) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5210) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5211) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5212) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,5213) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,5215) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,5223)
    (<0.0>,5225) tree.c:634: local_irq_restore(flags);
    (<0.0>,5228)
    (<0.0>,5230) fake_sched.h:43: return __running_cpu;
    (<0.0>,5234) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5236) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5240) fake_sched.h:43: return __running_cpu;
    (<0.0>,5244) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5253) fake_sched.h:43: return __running_cpu;
    (<0.0>,5257)
    (<0.0>,5260) tree.c:580: local_irq_save(flags);
    (<0.0>,5263)
    (<0.0>,5265) fake_sched.h:43: return __running_cpu;
    (<0.0>,5269) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5271) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5275) fake_sched.h:43: return __running_cpu;
    (<0.0>,5279) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5292)
    (<0.0>,5294) fake_sched.h:43: return __running_cpu;
    (<0.0>,5298) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5299) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5301) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5302) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,5303) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5309) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5310) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5315) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5316) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5317) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,5318) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5322) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5324) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,5325) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5326) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5345)
    (<0.0>,5347)
    (<0.0>,5349) fake_sched.h:43: return __running_cpu;
    (<0.0>,5353) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5356) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,5360) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5361) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5362) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5366) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5367) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5368) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5370) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5375) fake_sched.h:43: return __running_cpu;
    (<0.0>,5378) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5380) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5382) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5383) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5386)
    (<0.0>,5389) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5392) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5393) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5394) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5398) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5399) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5400) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5402) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5407) fake_sched.h:43: return __running_cpu;
    (<0.0>,5410) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5412) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5414) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5415) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5418)
    (<0.0>,5421) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5424) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5425) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5426) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5430) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5431) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5432) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5434) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,5441) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5444) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5445) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5446) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5448) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5449) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,5451) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5454) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5460) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5461) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5464) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5469) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5470) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5471) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,5485)
    (<0.0>,5487) tree.c:583: local_irq_restore(flags);
    (<0.0>,5490)
    (<0.0>,5492) fake_sched.h:43: return __running_cpu;
    (<0.0>,5496) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5498) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5502) fake_sched.h:43: return __running_cpu;
    (<0.0>,5506) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5512) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5515) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5520) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5522) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5524) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5528) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5530) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5533) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5536)
    (<0.0>,5545) fake_sched.h:43: return __running_cpu;
    (<0.0>,5549)
    (<0.0>,5550) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5553) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5558) tree.c:704: local_irq_save(flags);
    (<0.0>,5561)
    (<0.0>,5563) fake_sched.h:43: return __running_cpu;
    (<0.0>,5567) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5569) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5573) fake_sched.h:43: return __running_cpu;
    (<0.0>,5577) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5590)
    (<0.0>,5592) fake_sched.h:43: return __running_cpu;
    (<0.0>,5596) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5597) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5599) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5600) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,5601) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5606) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5607) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5611) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5612) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5613) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,5614) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,5618) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5620) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5621) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5622) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5636)
    (<0.0>,5637)
    (<0.0>,5639) fake_sched.h:43: return __running_cpu;
    (<0.0>,5643) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,5647) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5650) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5651) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5652) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5654) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5655) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,5657) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5660) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5667) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5668) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5671) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5676) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5677) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5678) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,5683) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,5692)
    (<0.0>,5694) tree.c:707: local_irq_restore(flags);
    (<0.0>,5697)
    (<0.0>,5699) fake_sched.h:43: return __running_cpu;
    (<0.0>,5703) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5705) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5709) fake_sched.h:43: return __running_cpu;
    (<0.0>,5713) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5720) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5721) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5722) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,5723) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,5725) tree.c:1839: if (!ACCESS_ONCE(rnp->qsmask) &&
    (<0.0>,5728) tree.c:1840: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,5731)
    (<0.0>,5736) tree.c:1872: rcu_gp_cleanup(rsp);
    (<0.0>,5744)
    (<0.0>,5745) tree.c:1720: bool needgp = false;
    (<0.0>,5746) tree.c:1721: int nocb = 0;
    (<0.0>,5747) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5750)
    (<0.0>,5751) tree.c:453: return &rsp->node[0];
    (<0.0>,5755) tree.c:1723: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,5756) tree.c:1725: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,5760)
    (<0.0>,5763) fake_sched.h:43: return __running_cpu;
    (<0.0>,5767) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5771) fake_sched.h:43: return __running_cpu;
    (<0.0>,5775) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5780) fake_sched.h:43: return __running_cpu;
    (<0.0>,5784) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5787) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5788) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5794) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5795) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5797) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5799) tree.c:1727: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,5800) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5801) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5803) tree.c:1728: if (gp_duration > rsp->gp_max)
    (<0.0>,5806) tree.c:1739: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,5810)
    (<0.0>,5811) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5812) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,5817) fake_sched.h:43: return __running_cpu;
    (<0.0>,5821) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,5823) fake_sched.h:43: return __running_cpu;
    (<0.0>,5827) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5833) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5836) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5838) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5839) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5841) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5846) tree.c:1751: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,5850)
    (<0.0>,5853) fake_sched.h:43: return __running_cpu;
    (<0.0>,5857) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5861) fake_sched.h:43: return __running_cpu;
    (<0.0>,5865) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5870) fake_sched.h:43: return __running_cpu;
    (<0.0>,5874) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5877) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5878) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,5884) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5886) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5887) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5889) tree.c:1753: ACCESS_ONCE(rnp->completed) = rsp->gpnum;
    (<0.0>,5891) fake_sched.h:43: return __running_cpu;
    (<0.0>,5894) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5896) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5898) tree.c:1754: rdp = &rsp->rda[get_cpu()];
    (<0.0>,5899) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5900) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5902) tree.c:1755: if (rnp == rdp->mynode)
    (<0.0>,5905) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5906) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5907) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,5913)
    (<0.0>,5914)
    (<0.0>,5915)
    (<0.0>,5916) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5918) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5919) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5921) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,5924) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5925) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5926) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,5934)
    (<0.0>,5935)
    (<0.0>,5936)
    (<0.0>,5937) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5940) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5943) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5946) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5947) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,5950) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5952) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5955) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5957) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5958) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5960) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5963) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5967) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5969) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5972) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5973) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5976) tree.c:1511: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,5978) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5980) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5982) tree.c:1508: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,5985) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5987) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5988) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5990) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5993) tree.c:1509: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,5998) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6000) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6001) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6004) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6007) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6008) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6010) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6013) tree.c:1515: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,6015) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6017) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6019) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6020) tree.c:1514: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,6023) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,6025) tree.c:1518: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,6028) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6030) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6033) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6034) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6037) tree.c:1519: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6041) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6042) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6043) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6052)
    (<0.0>,6053)
    (<0.0>,6054)
    (<0.0>,6055) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6058) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6061) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6064) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6065) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6068) tree.c:1434: return false;
    (<0.0>,6070) tree.c:1483: }
    (<0.0>,6072) tree.c:1526: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6074) tree.c:1527: }
    (<0.0>,6077) tree.c:1549: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,6078) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6080) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6081) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6083) tree.c:1552: rdp->completed = rnp->completed;
    (<0.0>,6087) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6089) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6090) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6092) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,6095) tree.c:1575: return ret;
    (<0.0>,6099) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6103) tree.c:1756: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,6105) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6106) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6113)
    (<0.0>,6114)
    (<0.0>,6115) tree.c:1385: int c = rnp->completed;
    (<0.0>,6117) tree.c:1385: int c = rnp->completed;
    (<0.0>,6119) tree.c:1385: int c = rnp->completed;
    (<0.0>,6121) fake_sched.h:43: return __running_cpu;
    (<0.0>,6124) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6126) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6128) tree.c:1387: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,6129) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,6130) tree.c:1389: rcu_nocb_gp_cleanup(rsp, rnp);
    (<0.0>,6134)
    (<0.0>,6135)
    (<0.0>,6137) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6140) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6143) tree.c:1390: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,6144) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6148) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6151) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6152) tree.c:1391: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,6153) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6154) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6155) tree.c:1392: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,6157) tree.c:1393: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,6165)
    (<0.0>,6166)
    (<0.0>,6167)
    (<0.0>,6168)
    (<0.0>,6172) tree.c:1394: return needmore;
    (<0.0>,6174) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6176) tree.c:1758: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,6177) tree.c:1759: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,6181)
    (<0.0>,6182) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6183) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6188) fake_sched.h:43: return __running_cpu;
    (<0.0>,6192) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6194) fake_sched.h:43: return __running_cpu;
    (<0.0>,6198) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6211) fake_sched.h:43: return __running_cpu;
    (<0.0>,6216) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,6223) fake_sched.h:43: return __running_cpu;
    (<0.0>,6227) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,6239) fake_sched.h:43: return __running_cpu;
    (<0.0>,6243)
    (<0.0>,6246) tree.c:580: local_irq_save(flags);
    (<0.0>,6249)
    (<0.0>,6251) fake_sched.h:43: return __running_cpu;
    (<0.0>,6255) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6257) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6261) fake_sched.h:43: return __running_cpu;
    (<0.0>,6265) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6278)
    (<0.0>,6280) fake_sched.h:43: return __running_cpu;
    (<0.0>,6284) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6285) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6287) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6288) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,6289) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6295) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6296) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6301) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6302) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6303) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,6304) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,6308) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,6310) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,6311) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6312) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6331)
    (<0.0>,6333)
    (<0.0>,6335) fake_sched.h:43: return __running_cpu;
    (<0.0>,6339) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6342) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,6346) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6347) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6348) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6352) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6353) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6354) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6356) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6361) fake_sched.h:43: return __running_cpu;
    (<0.0>,6364) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6366) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6368) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6369) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6372)
    (<0.0>,6375) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6378) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6379) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6380) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6384) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6385) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6386) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6388) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6393) fake_sched.h:43: return __running_cpu;
    (<0.0>,6396) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6398) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6400) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6401) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6404)
    (<0.0>,6407) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6410) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6411) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6412) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6416) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6417) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6418) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6420) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,6427) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6430) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6431) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6432) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6434) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6435) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,6437) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6440) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6446) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6447) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6450) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6455) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6456) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6457) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,6471)
    (<0.0>,6473) tree.c:583: local_irq_restore(flags);
    (<0.0>,6476)
    (<0.0>,6478) fake_sched.h:43: return __running_cpu;
    (<0.0>,6482) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6484) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6488) fake_sched.h:43: return __running_cpu;
    (<0.0>,6492) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6498) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6501) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,6506) fake_sched.h:43: return __running_cpu;
    (<0.0>,6510)
    (<0.0>,6511) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6514) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,6519) tree.c:704: local_irq_save(flags);
    (<0.0>,6522)
    (<0.0>,6524) fake_sched.h:43: return __running_cpu;
    (<0.0>,6528) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6530) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6534) fake_sched.h:43: return __running_cpu;
    (<0.0>,6538) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6551)
    (<0.0>,6553) fake_sched.h:43: return __running_cpu;
    (<0.0>,6557) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6558) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6560) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6561) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,6562) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6567) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6568) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6572) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6573) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6574) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,6575) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,6579) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6581) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,6582) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6583) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,6597)
    (<0.0>,6598)
    (<0.0>,6600) fake_sched.h:43: return __running_cpu;
    (<0.0>,6604) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6608) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6611) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6612) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6613) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6615) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6616) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,6618) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6621) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6628) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6629) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6632) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6637) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6638) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6639) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,6644) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,6653)
    (<0.0>,6655) tree.c:707: local_irq_restore(flags);
    (<0.0>,6658)
    (<0.0>,6660) fake_sched.h:43: return __running_cpu;
    (<0.0>,6664) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6666) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6670) fake_sched.h:43: return __running_cpu;
    (<0.0>,6674) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6689) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6691) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6693) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6694) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6696) tree.c:1750: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,6701) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,6704)
    (<0.0>,6705) tree.c:453: return &rsp->node[0];
    (<0.0>,6709) tree.c:1762: rnp = rcu_get_root(rsp);
    (<0.0>,6710) tree.c:1763: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,6714)
    (<0.0>,6717) fake_sched.h:43: return __running_cpu;
    (<0.0>,6721) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6725) fake_sched.h:43: return __running_cpu;
    (<0.0>,6729) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6734) fake_sched.h:43: return __running_cpu;
    (<0.0>,6738) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,6741) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6742) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,6748) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,6749) tree.c:1765: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,6753)
    (<0.0>,6754)
    (<0.0>,6756) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6758) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6759) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6761) tree.c:1768: ACCESS_ONCE(rsp->completed) = rsp->gpnum;
    (<0.0>,6764) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,6766) tree.c:1770: rsp->fqs_state = RCU_GP_IDLE;
    (<0.0>,6768) fake_sched.h:43: return __running_cpu;
    (<0.0>,6771) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6773) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6775) tree.c:1771: rdp = &rsp->rda[get_cpu()];
    (<0.0>,6776) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6777) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6778) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6786)
    (<0.0>,6787)
    (<0.0>,6788)
    (<0.0>,6789) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6792) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6795) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6798) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6799) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6802) tree.c:1502: return false;
    (<0.0>,6804) tree.c:1527: }
    (<0.0>,6807) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6811) tree.c:1773: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,6812) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6815) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6816) tree.c:1774: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6822)
    (<0.0>,6823)
    (<0.0>,6824) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,6827)
    (<0.0>,6828) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6830) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6831) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6833) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,6839) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,6845)
    (<0.0>,6846) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6849)
    (<0.0>,6850) tree.c:453: return &rsp->node[0];
    (<0.0>,6854) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6855) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6857) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6861) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6862) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6864) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6867) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6868) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,6869) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,6873) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,6875) tree.c:494: }
    (<0.0>,6879) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,6881) tree.c:1775: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,6885) tree.c:1780: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,6889)
    (<0.0>,6890) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6891) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,6896) fake_sched.h:43: return __running_cpu;
    (<0.0>,6900) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6902) fake_sched.h:43: return __running_cpu;
    (<0.0>,6906) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6917) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,6919) tree.c:1802: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,6923) fake_sched.h:43: return __running_cpu;
    (<0.0>,6927) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,6931) fake_sched.h:43: return __running_cpu;
    (<0.0>,6935) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6940) fake_sched.h:43: return __running_cpu;
    (<0.0>,6944) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,6954) tree.c:749: local_irq_save(flags);
    (<0.0>,6957)
    (<0.0>,6959) fake_sched.h:43: return __running_cpu;
    (<0.0>,6963) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6965) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6970) fake_sched.h:43: return __running_cpu;
    (<0.0>,6974) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,6975) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,6977) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,6978) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,6979) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,6981) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,6983) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,6984) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6986) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6991) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6992) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6994) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6998) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,6999) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7000) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,7001) tree.c:754: if (oldval)
    (<0.0>,7009)
    (<0.0>,7011) tree.c:759: local_irq_restore(flags);
    (<0.0>,7014)
    (<0.0>,7016) fake_sched.h:43: return __running_cpu;
    (<0.0>,7020) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7022) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7030)
    (<0.0>,7035) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,7040) fake_sched.h:43: return __running_cpu;
    (<0.0>,7045) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,7053) fake_sched.h:43: return __running_cpu;
    (<0.0>,7058) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,7072) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7073) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7074) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7078) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7079) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7080) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7082) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,7086) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7088) fake_sched.h:43: return __running_cpu;
    (<0.0>,7091) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7093) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,7100)
    (<0.0>,7101)
    (<0.0>,7102) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7104) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7105) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,7106) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7108) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7110) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,7111) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,7112) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,7122)
    (<0.0>,7123)
    (<0.0>,7124) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,7129) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,7132)
    (<0.0>,7135) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,7138) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7140) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7143) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7145) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,7148) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7150) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7153) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7155) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,7158) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,7160) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,7162) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,7163) tree.c:3140: return 1;
    (<0.0>,7165) tree.c:3176: }
    (<0.0>,7169) tree.c:3189: return 1;
    (<0.0>,7171) tree.c:3191: }
    (<0.0>,7178) fake_sched.h:43: return __running_cpu;
    (<0.0>,7182) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,7186) tree.c:2437: if (user)
    (<0.0>,7194) fake_sched.h:43: return __running_cpu;
    (<0.0>,7198) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,7200) fake_sched.h:43: return __running_cpu;
    (<0.0>,7204) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7210) fake_sched.h:43: return __running_cpu;
    (<0.0>,7214) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,7224)
    (<0.0>,7227) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7228) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7229) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7233) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7234) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7235) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7237) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,7241) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,7250)
    (<0.0>,7252) fake_sched.h:43: return __running_cpu;
    (<0.0>,7255) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7257) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7259) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7260) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7262) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7269) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7270) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7272) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7278) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7279) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7280) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7281) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7282) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7286)
    (<0.0>,7287)
    (<0.0>,7288) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7289) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,7296)
    (<0.0>,7297)
    (<0.0>,7298) tree.c:1584: local_irq_save(flags);
    (<0.0>,7301)
    (<0.0>,7303) fake_sched.h:43: return __running_cpu;
    (<0.0>,7307) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7309) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7313) fake_sched.h:43: return __running_cpu;
    (<0.0>,7317) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7322) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7324) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7325) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,7326) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7328) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7329) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7331) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,7334) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7336) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7337) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7339) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,7342) tree.c:1589: local_irq_restore(flags);
    (<0.0>,7345)
    (<0.0>,7347) fake_sched.h:43: return __running_cpu;
    (<0.0>,7351) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7353) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7357) fake_sched.h:43: return __running_cpu;
    (<0.0>,7361) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7368) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7370) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,7373) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,7375) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,7378) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,7380) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,7381) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,7382) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,7391)
    (<0.0>,7392)
    (<0.0>,7393)
    (<0.0>,7394) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,7396) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,7397) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,7398) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,7400) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,7404)
    (<0.0>,7405)
    (<0.0>,7406) fake_sync.h:76: local_irq_save(flags);
    (<0.0>,7409)
    (<0.0>,7411) fake_sched.h:43: return __running_cpu;
    (<0.0>,7415) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7417) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7421) fake_sched.h:43: return __running_cpu;
    (<0.0>,7425) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7431) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,7432) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,7438) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7440) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7445) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7447) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7448) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7450) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,7453) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,7455) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,7456) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,7458) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,7461) tree.c:2046: rdp->passed_quiesce = 0;	/* need qs for new gp. */
    (<0.0>,7463) tree.c:2046: rdp->passed_quiesce = 0;	/* need qs for new gp. */
    (<0.0>,7464) tree.c:2047: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,7466) tree.c:2047: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,7470)
    (<0.0>,7471)
    (<0.0>,7472) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7473) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7476) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,7479)
    (<0.0>,7481) fake_sched.h:43: return __running_cpu;
    (<0.0>,7485) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7487) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7491) fake_sched.h:43: return __running_cpu;
    (<0.0>,7495) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7506) tree.c:2558: local_irq_save(flags);
    (<0.0>,7509)
    (<0.0>,7511) fake_sched.h:43: return __running_cpu;
    (<0.0>,7515) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7517) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7521) fake_sched.h:43: return __running_cpu;
    (<0.0>,7525) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7530) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7531) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7537)
    (<0.0>,7538)
    (<0.0>,7539) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7542)
    (<0.0>,7543) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7545) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7546) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7548) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7554) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7560)
    (<0.0>,7561) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7564)
    (<0.0>,7565) tree.c:453: return &rsp->node[0];
    (<0.0>,7569) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7570) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7572) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7576) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7577) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7579) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7582) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7583) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7584) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7588) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7590) tree.c:494: }
    (<0.0>,7594) tree.c:2560: raw_spin_lock(&rcu_get_root(rsp)->lock); /* irqs disabled. */
    (<0.0>,7597)
    (<0.0>,7598) tree.c:453: return &rsp->node[0];
    (<0.0>,7605)
    (<0.0>,7607) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7608) fake_sync.h:109: if (pthread_mutex_lock(l))
    (<0.0>,7612) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,7618)
    (<0.0>,7620) fake_sched.h:43: return __running_cpu;
    (<0.0>,7623) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7625) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7627) tree.c:1923: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,7628) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7631)
    (<0.0>,7632) tree.c:453: return &rsp->node[0];
    (<0.0>,7636) tree.c:1924: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7637) tree.c:1925: bool ret = false;
    (<0.0>,7638) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7639) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7640) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7648)
    (<0.0>,7649)
    (<0.0>,7650)
    (<0.0>,7651) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7654) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7657) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7660) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7661) tree.c:1501: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7664) tree.c:1502: return false;
    (<0.0>,7666) tree.c:1527: }
    (<0.0>,7669) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7673) tree.c:1935: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,7674) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7675) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7676) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7682)
    (<0.0>,7683)
    (<0.0>,7684)
    (<0.0>,7685) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7687) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7690) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7691) tree.c:1891: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7697)
    (<0.0>,7698)
    (<0.0>,7699) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,7702)
    (<0.0>,7703) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7705) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7706) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7708) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,7714) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,7720)
    (<0.0>,7721) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7724)
    (<0.0>,7725) tree.c:453: return &rsp->node[0];
    (<0.0>,7729) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7730) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7732) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7736) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7737) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7739) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7742) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7743) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7744) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,7748) tree.c:483: return 1;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,7750) tree.c:494: }
    (<0.0>,7754) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7756) tree.c:1900: ACCESS_ONCE(rsp->gp_flags) = RCU_GP_FLAG_INIT;
    (<0.0>,7759) tree.c:1909: return true;
    (<0.0>,7761) tree.c:1910: }
    (<0.0>,7765) tree.c:1936: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,7766) tree.c:1937: return ret;
    (<0.0>,7770) tree.c:2561: needwake = rcu_start_gp(rsp);
    (<0.0>,7771) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,7774)
    (<0.0>,7775) tree.c:453: return &rsp->node[0];
    (<0.0>,7780) tree.c:2562: raw_spin_unlock_irqrestore(&rcu_get_root(rsp)->lock, flags);
    (<0.0>,7784)
    (<0.0>,7785)
    (<0.0>,7786) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7787) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,7790) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,7793)
    (<0.0>,7795) fake_sched.h:43: return __running_cpu;
    (<0.0>,7799) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7801) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7805) fake_sched.h:43: return __running_cpu;
    (<0.0>,7809) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7816) tree.c:2563: if (needwake)
    (<0.0>,7819) tree.c:2564: rcu_gp_kthread_wake(rsp);
    (<0.0>,7822)
    (<0.0>,7823) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7824) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7826) tree.c:1406: if (current == rsp->gp_kthread ||
    (<0.0>,7833) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7836)
    (<0.0>,7837) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7839) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7842) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7845) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7848) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7855) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,7856) tree.c:2571: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,7860)
    (<0.0>,7861)
    (<0.0>,7862) tree.c:2601: if (unlikely(!ACCESS_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,7871) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,7873) tree.c:2603: if (likely(!rsp->boost)) {
    (<0.0>,7882) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,7883) tree.c:2604: rcu_do_batch(rsp, rdp);
    (<0.0>,7900)
    (<0.0>,7901)
    (<0.0>,7902) tree.c:2313: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,7905)
    (<0.0>,7906) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7908) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7911) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7914) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7917) tree.c:445: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,7924) tree.c:2325: local_irq_save(flags);
    (<0.0>,7927)
    (<0.0>,7929) fake_sched.h:43: return __running_cpu;
    (<0.0>,7933) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7935) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7939) fake_sched.h:43: return __running_cpu;
    (<0.0>,7943) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7948) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7949) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7950) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7951) tree.c:2326: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,7952) tree.c:2327: bl = rdp->blimit;
    (<0.0>,7954) tree.c:2327: bl = rdp->blimit;
    (<0.0>,7955) tree.c:2327: bl = rdp->blimit;
    (<0.0>,7958) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,7960) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,7961) tree.c:2329: list = rdp->nxtlist;
    (<0.0>,7962) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7965) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7966) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7967) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7969) tree.c:2330: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7970) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,7973) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,7974) tree.c:2331: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,7975) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7978) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7979) tree.c:2332: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7980) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7982) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,7985) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7987) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7990) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7991) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7994) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7997) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,7999) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8001) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8004) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8007) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8009) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8011) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8014) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8016) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8019) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8020) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8023) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8026) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8028) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8030) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8033) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8036) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8038) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8040) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8043) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8045) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8048) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8049) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8052) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8055) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8057) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8059) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8062) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8065) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8067) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8069) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8072) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8074) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8077) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8078) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8081) tree.c:2334: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8084) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8086) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8088) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8091) tree.c:2335: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,8094) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8096) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8098) tree.c:2333: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,8101) tree.c:2336: local_irq_restore(flags);
    (<0.0>,8104)
    (<0.0>,8106) fake_sched.h:43: return __running_cpu;
    (<0.0>,8110) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8112) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8116) fake_sched.h:43: return __running_cpu;
    (<0.0>,8120) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8125) tree.c:2339: count = count_lazy = 0;
    (<0.0>,8126) tree.c:2339: count = count_lazy = 0;
    (<0.0>,8128) tree.c:2340: while (list) {
    (<0.0>,8131) tree.c:2341: next = list->next;
    (<0.0>,8133) tree.c:2341: next = list->next;
    (<0.0>,8134) tree.c:2341: next = list->next;
    (<0.0>,8137) tree.c:2343: debug_rcu_head_unqueue(list);
    (<0.0>,8140)
    (<0.0>,8142) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8144) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8145) tree.c:2344: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,8151)
    (<0.0>,8152)
    (<0.0>,8153) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8155) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8157) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,8160) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,8163) rcu.h:118: head->func(head);
    (<0.0>,8165) rcu.h:118: head->func(head);
    (<0.0>,8166) rcu.h:118: head->func(head);
    (<0.0>,8172)
    (<0.0>,8173) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8174) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8175) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8179) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8180) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8181) update.c:215: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,8182) update.c:216: complete(&rcu->completion);
    (<0.0>,8186)
    (<0.0>,8187) fake_sync.h:248: x->done++;
    (<0.0>,8189) fake_sync.h:248: x->done++;
    (<0.0>,8191) fake_sync.h:248: x->done++;
    (<0.0>,8196) rcu.h:120: return false;
    (<0.0>,8198) rcu.h:122: }
    (<0.0>,8201) tree.c:2346: list = next;
    (<0.0>,8202) tree.c:2346: list = next;
    (<0.0>,8203) tree.c:2348: if (++count >= bl &&
    (<0.0>,8205) tree.c:2348: if (++count >= bl &&
    (<0.0>,8206) tree.c:2348: if (++count >= bl &&
    (<0.0>,8210) tree.c:2340: while (list) {
    (<0.0>,8213) tree.c:2354: local_irq_save(flags);
    (<0.0>,8216)
    (<0.0>,8218) fake_sched.h:43: return __running_cpu;
    (<0.0>,8222) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8224) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8228) fake_sched.h:43: return __running_cpu;
    (<0.0>,8232) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8239) tree.c:2360: if (list != NULL) {
    (<0.0>,8243) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8244) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8246) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8248) tree.c:2370: rdp->qlen_lazy -= count_lazy;
    (<0.0>,8249) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8251) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8252) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8254) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8256) tree.c:2371: ACCESS_ONCE(rdp->qlen) = rdp->qlen - count;
    (<0.0>,8257) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8258) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8260) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8262) tree.c:2372: rdp->n_cbs_invoked += count;
    (<0.0>,8263) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,8265) tree.c:2375: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,8268) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8270) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8273) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8275) tree.c:2379: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,8278) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8280) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8281) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8283) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8284) tree.c:2382: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,8289) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8291) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8294) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8296) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8303) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8304) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8306) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8309) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8311) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8317) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8318) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8319) tree.c:2384: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,8320) tree.c:2386: local_irq_restore(flags);
    (<0.0>,8323)
    (<0.0>,8325) fake_sched.h:43: return __running_cpu;
    (<0.0>,8329) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8331) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8335) fake_sched.h:43: return __running_cpu;
    (<0.0>,8339) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8344) tree.c:2389: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,8347)
    (<0.0>,8348) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8350) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8353) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8364) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8367)
    (<0.0>,8371) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8374) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8375) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8376) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8380) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8381) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8382) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8384) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8388) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,8397)
    (<0.0>,8399) fake_sched.h:43: return __running_cpu;
    (<0.0>,8402) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8404) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8406) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,8407) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8409) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8416) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8417) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8419) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8425) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8426) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8427) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,8428) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,8429) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,8433)
    (<0.0>,8434)
    (<0.0>,8435) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,8436) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,8443)
    (<0.0>,8444)
    (<0.0>,8445) tree.c:1584: local_irq_save(flags);
    (<0.0>,8448)
    (<0.0>,8450) fake_sched.h:43: return __running_cpu;
    (<0.0>,8454) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8456) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8460) fake_sched.h:43: return __running_cpu;
    (<0.0>,8464) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8469) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8471) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8472) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,8473) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8475) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8476) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8478) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,8481) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8483) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8484) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8486) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,8489) tree.c:1589: local_irq_restore(flags);
    (<0.0>,8492)
    (<0.0>,8494) fake_sched.h:43: return __running_cpu;
    (<0.0>,8498) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8500) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8504) fake_sched.h:43: return __running_cpu;
    (<0.0>,8508) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8515) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,8517) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,8522) tree.c:2558: local_irq_save(flags);
    (<0.0>,8525)
    (<0.0>,8527) fake_sched.h:43: return __running_cpu;
    (<0.0>,8531) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8533) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8537) fake_sched.h:43: return __running_cpu;
    (<0.0>,8541) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8546) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8547) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8553)
    (<0.0>,8554)
    (<0.0>,8555) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,8558)
    (<0.0>,8559) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8561) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8562) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8564) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,8570) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,8576)
    (<0.0>,8577) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8580)
    (<0.0>,8581) tree.c:453: return &rsp->node[0];
    (<0.0>,8585) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8586) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8588) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8592) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8593) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8595) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8598) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8599) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8600) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,8604) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8607) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8610) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8613) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8614) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,8617) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8619) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8622) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8625) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8628) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8629) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8631) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8634) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8638) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8640) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8642) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8645) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8648) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8651) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8652) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8654) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8657) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,8661) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8663) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8665) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,8668) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,8670) tree.c:494: }
    (<0.0>,8674) tree.c:2566: local_irq_restore(flags);
    (<0.0>,8677)
    (<0.0>,8679) fake_sched.h:43: return __running_cpu;
    (<0.0>,8683) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8685) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8689) fake_sched.h:43: return __running_cpu;
    (<0.0>,8693) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8699) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,8702)
    (<0.0>,8703) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8705) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8708) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,8715) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8718)
    (<0.0>,8722) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8725) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8726) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8727) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8731) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8732) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8733) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8735) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,8743) fake_sched.h:43: return __running_cpu;
    (<0.0>,8747) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,8756) tree.c:624: local_irq_save(flags);
    (<0.0>,8759)
    (<0.0>,8761) fake_sched.h:43: return __running_cpu;
    (<0.0>,8765) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8767) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8771) fake_sched.h:43: return __running_cpu;
    (<0.0>,8775) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8781) fake_sched.h:43: return __running_cpu;
    (<0.0>,8785) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8786) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8788) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8789) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,8790) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8792) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8794) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,8795) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8797) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8802) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8803) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8805) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8809) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8810) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8811) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,8812) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,8814) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,8822)
    (<0.0>,8824) tree.c:634: local_irq_restore(flags);
    (<0.0>,8827)
    (<0.0>,8829) fake_sched.h:43: return __running_cpu;
    (<0.0>,8833) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8835) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8839) fake_sched.h:43: return __running_cpu;
    (<0.0>,8843) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8852) fake_sched.h:43: return __running_cpu;
    (<0.0>,8856)
    (<0.0>,8859) tree.c:580: local_irq_save(flags);
    (<0.0>,8862)
    (<0.0>,8864) fake_sched.h:43: return __running_cpu;
    (<0.0>,8868) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8870) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8874) fake_sched.h:43: return __running_cpu;
    (<0.0>,8878) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8891)
    (<0.0>,8893) fake_sched.h:43: return __running_cpu;
    (<0.0>,8897) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8898) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8900) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8901) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,8902) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8908) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8909) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8914) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8915) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8916) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,8917) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,8921) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,8923) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,8924) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8925) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8944)
    (<0.0>,8946)
    (<0.0>,8948) fake_sched.h:43: return __running_cpu;
    (<0.0>,8952) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,8955) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,8959) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8960) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8961) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8965) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8966) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8967) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8969) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8974) fake_sched.h:43: return __running_cpu;
    (<0.0>,8977) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8979) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8981) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,8982) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8985)
    (<0.0>,8988) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8991) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8992) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8993) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8997) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8998) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,8999) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9001) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9006) fake_sched.h:43: return __running_cpu;
    (<0.0>,9009) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9011) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9013) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9014) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9017)
    (<0.0>,9020) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9023) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9024) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9025) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9029) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9030) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9031) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9033) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9040) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9043) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9044) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9045) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9047) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9048) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,9050) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9053) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9059) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9060) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9063) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9068) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9069) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9070) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,9084)
    (<0.0>,9086) tree.c:583: local_irq_restore(flags);
    (<0.0>,9089)
    (<0.0>,9091) fake_sched.h:43: return __running_cpu;
    (<0.0>,9095) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9097) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9101) fake_sched.h:43: return __running_cpu;
    (<0.0>,9105) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9111) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9114) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9119) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,9121) tree.c:1803: wait_event_interruptible(rsp->gp_wq,
    (<0.0>,9130) fake_sched.h:43: return __running_cpu;
    (<0.0>,9134)
    (<0.0>,9135) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9138) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9143) tree.c:704: local_irq_save(flags);
    (<0.0>,9146)
    (<0.0>,9148) fake_sched.h:43: return __running_cpu;
    (<0.0>,9152) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9154) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9158) fake_sched.h:43: return __running_cpu;
    (<0.0>,9162) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9175)
    (<0.0>,9177) fake_sched.h:43: return __running_cpu;
    (<0.0>,9181) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9182) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9184) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9185) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,9186) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9191) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9192) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9196) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9197) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9198) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,9199) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,9203) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9205) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9206) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9207) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9221)
    (<0.0>,9222)
    (<0.0>,9224) fake_sched.h:43: return __running_cpu;
    (<0.0>,9228) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9232) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9235) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9236) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9237) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9239) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9240) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,9242) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9245) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9252) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9253) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9256) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9261) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9262) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9263) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,9268) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,9277)
    (<0.0>,9279) tree.c:707: local_irq_restore(flags);
    (<0.0>,9282)
    (<0.0>,9284) fake_sched.h:43: return __running_cpu;
    (<0.0>,9288) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9290) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9294) fake_sched.h:43: return __running_cpu;
    (<0.0>,9298) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9305) tree.c:1807: if (rcu_gp_init(rsp))
    (<0.0>,9317)
    (<0.0>,9318) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9321)
    (<0.0>,9322) tree.c:453: return &rsp->node[0];
    (<0.0>,9326) tree.c:1605: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9330) tree.c:1608: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,9334)
    (<0.0>,9337) fake_sched.h:43: return __running_cpu;
    (<0.0>,9341) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9345) fake_sched.h:43: return __running_cpu;
    (<0.0>,9349) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9354) fake_sched.h:43: return __running_cpu;
    (<0.0>,9358) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9361) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9362) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9368) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,9370) tree.c:1610: if (!ACCESS_ONCE(rsp->gp_flags)) {
    (<0.0>,9373) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,9375) tree.c:1615: ACCESS_ONCE(rsp->gp_flags) = 0; /* Clear all flags: New grace period. */
    (<0.0>,9376) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9379)
    (<0.0>,9380) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9382) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9383) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9385) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9393) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9394) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9397)
    (<0.0>,9398) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9400) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9401) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9403) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,9410) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9411) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9412) tree.c:1617: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,9415) tree.c:1627: record_gp_stall_check_time(rsp);
    (<0.0>,9420)
    (<0.0>,9421) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,9422) tree.c:1009: unsigned long j = jiffies;
    (<0.0>,9423) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9424) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9426) tree.c:1012: rsp->gp_start = j;
    (<0.0>,9430) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,9431) update.c:338: int till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,9432) update.c:344: if (till_stall_check < 3) {
    (<0.0>,9435) update.c:347: } else if (till_stall_check > 300) {
    (<0.0>,9439) update.c:351: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,9444) tree.c:1014: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,9445) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9446) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9448) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9450) tree.c:1015: ACCESS_ONCE(rsp->jiffies_stall) = j + j1;
    (<0.0>,9451) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9452) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9455) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9457) tree.c:1016: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,9461) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9463) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9465) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9467) tree.c:1629: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,9471) tree.c:1631: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,9475)
    (<0.0>,9476) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9477) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9482) fake_sched.h:43: return __running_cpu;
    (<0.0>,9486) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9488) fake_sched.h:43: return __running_cpu;
    (<0.0>,9492) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9498) tree.c:1634: mutex_lock(&rsp->onoff_mutex);
    (<0.0>,9502)
    (<0.0>,9503) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,9505) fake_sync.h:174: if (pthread_mutex_lock(&l->lock))
    (<0.0>,9511) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9514) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9516) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9517) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9519) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9524) tree.c:1651: raw_spin_lock_irq(&rnp->lock);
    (<0.0>,9528)
    (<0.0>,9531) fake_sched.h:43: return __running_cpu;
    (<0.0>,9535) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9539) fake_sched.h:43: return __running_cpu;
    (<0.0>,9543) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9548) fake_sched.h:43: return __running_cpu;
    (<0.0>,9552) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9555) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9556) fake_sync.h:94: if (pthread_mutex_lock(l))
    (<0.0>,9563) fake_sched.h:43: return __running_cpu;
    (<0.0>,9566) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9568) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9570) tree.c:1653: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9571) tree.c:1654: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,9577)
    (<0.0>,9578) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9580) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9585) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9586) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9588) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9592) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9593) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9594) tree_plugin.h:996: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,9596) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9598) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9599) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9601) tree.c:1658: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,9602) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9604) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9605) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9607) tree.c:1660: ACCESS_ONCE(rnp->gpnum) = rsp->gpnum;
    (<0.0>,9608) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9610) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9611) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9613) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9618) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9619) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9621) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9622) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9624) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9628) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9629) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9630) tree.c:1661: WARN_ON_ONCE(rnp->completed != rsp->completed);
    (<0.0>,9631) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9633) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9634) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9636) tree.c:1662: ACCESS_ONCE(rnp->completed) = rsp->completed;
    (<0.0>,9637) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9638) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9640) tree.c:1663: if (rnp == rdp->mynode)
    (<0.0>,9643) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9644) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9645) tree.c:1664: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,9651)
    (<0.0>,9652)
    (<0.0>,9653)
    (<0.0>,9654) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9656) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9657) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9659) tree.c:1541: if (rdp->completed == rnp->completed) {
    (<0.0>,9662) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9663) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9664) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9673)
    (<0.0>,9674)
    (<0.0>,9675)
    (<0.0>,9676) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9679) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9682) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9685) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9686) tree.c:1433: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9689) tree.c:1434: return false;
    (<0.0>,9691) tree.c:1483: }
    (<0.0>,9694) tree.c:1544: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,9696) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9698) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9699) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9701) tree.c:1556: if (rdp->gpnum != rnp->gpnum) {
    (<0.0>,9704) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9706) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9707) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9709) tree.c:1562: rdp->gpnum = rnp->gpnum;
    (<0.0>,9712) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,9714) tree.c:1564: rdp->passed_quiesce = 0;
    (<0.0>,9715) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9717) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9718) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9720) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9725) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9728) tree.c:1568: rdp->qs_pending = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,9729) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,9731) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,9733) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,9735) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,9737) tree.c:1571: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,9738) tree.c:1573: zero_cpu_stall_ticks(rdp);
    (<0.0>,9741)
    (<0.0>,9744) tree.c:1575: return ret;
    (<0.0>,9748) tree.c:1665: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,9751)
    (<0.0>,9755) tree.c:1669: raw_spin_unlock_irq(&rnp->lock);
    (<0.0>,9759)
    (<0.0>,9760) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9761) fake_sync.h:100: if (pthread_mutex_unlock(l))
    (<0.0>,9766) fake_sched.h:43: return __running_cpu;
    (<0.0>,9770) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9772) fake_sched.h:43: return __running_cpu;
    (<0.0>,9776) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9789) fake_sched.h:43: return __running_cpu;
    (<0.0>,9794) tree.c:192: if (!rcu_sched_data[get_cpu()].passed_quiesce) {
    (<0.0>,9800) fake_sched.h:43: return __running_cpu;
    (<0.0>,9805) tree.c:196: rcu_sched_data[get_cpu()].passed_quiesce = 1;
    (<0.0>,9811) fake_sched.h:43: return __running_cpu;
    (<0.0>,9815) tree.c:284: if (unlikely(rcu_sched_qs_mask[get_cpu()]))
    (<0.0>,9827) fake_sched.h:43: return __running_cpu;
    (<0.0>,9831)
    (<0.0>,9834) tree.c:580: local_irq_save(flags);
    (<0.0>,9837)
    (<0.0>,9839) fake_sched.h:43: return __running_cpu;
    (<0.0>,9843) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9845) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9849) fake_sched.h:43: return __running_cpu;
    (<0.0>,9853) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9866)
    (<0.0>,9868) fake_sched.h:43: return __running_cpu;
    (<0.0>,9872) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9873) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9875) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9876) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,9877) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9883) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9884) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9889) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9890) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9891) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,9892) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,9896) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9898) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,9899) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9900) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9919)
    (<0.0>,9921)
    (<0.0>,9923) fake_sched.h:43: return __running_cpu;
    (<0.0>,9927) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,9930) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,9934) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9935) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9936) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9940) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9941) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9942) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9944) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9949) fake_sched.h:43: return __running_cpu;
    (<0.0>,9952) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9954) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9956) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9957) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9960)
    (<0.0>,9963) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9966) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9967) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9968) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9972) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9973) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9974) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9976) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9981) fake_sched.h:43: return __running_cpu;
    (<0.0>,9984) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9986) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9988) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,9989) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9992)
    (<0.0>,9995) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9998) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,9999) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10000) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10004) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10005) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10006) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10008) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,10015) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10018) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10019) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10020) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10022) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10023) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,10025) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10028) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10034) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10035) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10038) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10043) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10044) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10045) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,10059)
    (<0.0>,10061) tree.c:583: local_irq_restore(flags);
    (<0.0>,10064)
    (<0.0>,10066) fake_sched.h:43: return __running_cpu;
    (<0.0>,10070) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10072) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10076) fake_sched.h:43: return __running_cpu;
    (<0.0>,10080) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10086) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10089) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10094) fake_sched.h:43: return __running_cpu;
    (<0.0>,10098)
    (<0.0>,10099) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10102) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,10107) tree.c:704: local_irq_save(flags);
    (<0.0>,10110)
    (<0.0>,10112) fake_sched.h:43: return __running_cpu;
    (<0.0>,10116) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10118) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10122) fake_sched.h:43: return __running_cpu;
    (<0.0>,10126) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10139)
    (<0.0>,10141) fake_sched.h:43: return __running_cpu;
    (<0.0>,10145) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10146) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10148) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10149) tree.c:679: oldval = rdtp->dynticks_nesting;
    (<0.0>,10150) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10155) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10156) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10160) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10161) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10162) tree.c:680: WARN_ON_ONCE(oldval < 0);
    (<0.0>,10163) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,10167) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10169) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,10170) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10171) tree.c:685: rcu_eqs_exit_common(oldval, user);
    (<0.0>,10185)
    (<0.0>,10186)
    (<0.0>,10188) fake_sched.h:43: return __running_cpu;
    (<0.0>,10192) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10196) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10199) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10200) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10201) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10203) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10204) tree.c:650: atomic_inc(&rdtp->dynticks);
    (<0.0>,10206) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10209) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10216) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10217) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10220) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10225) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10226) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10227) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
    (<0.0>,10232) tree.c:656: if (!user && !is_idle_task(current)) {
    (<0.0>,10241)
    (<0.0>,10243) tree.c:707: local_irq_restore(flags);
    (<0.0>,10246)
    (<0.0>,10248) fake_sched.h:43: return __running_cpu;
    (<0.0>,10252) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10254) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10258) fake_sched.h:43: return __running_cpu;
    (<0.0>,10262) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10277) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10279) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10281) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10282) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10284) tree.c:1650: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,10289) tree.c:1673: mutex_unlock(&rsp->onoff_mutex);
    (<0.0>,10293)
    (<0.0>,10294) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,10296) fake_sync.h:180: if (pthread_mutex_unlock(&l->lock))
    (<0.0>,10300) tree.c:1674: return 1;
    (<0.0>,10302) tree.c:1675: }
    (<0.0>,10307) tree.c:1817: fqs_state = RCU_SAVE_DYNTICK;
    (<0.0>,10308) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,10309) tree.c:1818: j = jiffies_till_first_fqs;
    (<0.0>,10310) tree.c:1819: if (j > HZ) {
    (<0.0>,10313) tree.c:1823: ret = 0;
    (<0.0>,10315) tree.c:1825: if (!ret)
    (<0.0>,10318) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10319) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10321) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10323) tree.c:1826: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,10327) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,10329) tree.c:1830: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,10333) fake_sched.h:43: return __running_cpu;
    (<0.0>,10337) fake_sched.h:148: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,10341) fake_sched.h:43: return __running_cpu;
    (<0.0>,10345) fake_sched.h:149: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10350) fake_sched.h:43: return __running_cpu;
    (<0.0>,10354) fake_sched.h:152: local_irq_depth[get_cpu()] = 1;
    (<0.0>,10364) tree.c:749: local_irq_save(flags);
    (<0.0>,10367)
    (<0.0>,10369) fake_sched.h:43: return __running_cpu;
    (<0.0>,10373) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10375) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10380) fake_sched.h:43: return __running_cpu;
    (<0.0>,10384) tree.c:750: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,10385) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10387) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10388) tree.c:751: oldval = rdtp->dynticks_nesting;
    (<0.0>,10389) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10391) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10393) tree.c:752: rdtp->dynticks_nesting++;
    (<0.0>,10394) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10396) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10401) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10402) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10404) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10408) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10409) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10410) tree.c:753: WARN_ON_ONCE(rdtp->dynticks_nesting == 0);
    (<0.0>,10411) tree.c:754: if (oldval)
    (<0.0>,10419)
    (<0.0>,10421) tree.c:759: local_irq_restore(flags);
    (<0.0>,10424)
    (<0.0>,10426) fake_sched.h:43: return __running_cpu;
    (<0.0>,10430) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10432) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10440)
    (<0.0>,10445) tree.c:2406: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,10450) fake_sched.h:43: return __running_cpu;
    (<0.0>,10455) tree.c:887: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,10463) fake_sched.h:43: return __running_cpu;
    (<0.0>,10468) tree.c:202: if (!rcu_bh_data[get_cpu()].passed_quiesce) {
    (<0.0>,10482) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10483) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10484) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10488) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10489) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10490) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10492) tree.c:3187: for_each_rcu_flavor(rsp)
    (<0.0>,10496) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10498) fake_sched.h:43: return __running_cpu;
    (<0.0>,10501) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10503) tree.c:3188: if (__rcu_pending(rsp, &rsp->rda[get_cpu()]))
    (<0.0>,10510)
    (<0.0>,10511)
    (<0.0>,10512) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10514) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10515) tree.c:3123: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10516) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10518) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10520) tree.c:3125: rdp->n_rcu_pending++;
    (<0.0>,10521) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10522) tree.c:3128: check_cpu_stall(rsp, rdp);
    (<0.0>,10532)
    (<0.0>,10533)
    (<0.0>,10534) tree.c:1156: if (rcu_cpu_stall_suppress || !rcu_gp_in_progress(rsp))
    (<0.0>,10539) tree.c:3131: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,10542)
    (<0.0>,10545) tree.c:3135: if (rcu_scheduler_fully_active &&
    (<0.0>,10548) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10550) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10553) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10555) tree.c:3136: rdp->qs_pending && !rdp->passed_quiesce) {
    (<0.0>,10558) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10560) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10563) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10565) tree.c:3138: } else if (rdp->qs_pending && rdp->passed_quiesce) {
    (<0.0>,10568) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10570) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10572) tree.c:3139: rdp->n_rp_report_qs++;
    (<0.0>,10573) tree.c:3140: return 1;
    (<0.0>,10575) tree.c:3176: }
    (<0.0>,10579) tree.c:3189: return 1;
    (<0.0>,10581) tree.c:3191: }
    (<0.0>,10588) fake_sched.h:43: return __running_cpu;
    (<0.0>,10592) tree.c:2613: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,10596) tree.c:2437: if (user)
    (<0.0>,10604) fake_sched.h:43: return __running_cpu;
    (<0.0>,10608) fake_sched.h:157: local_irq_depth[get_cpu()] = 0;
    (<0.0>,10610) fake_sched.h:43: return __running_cpu;
    (<0.0>,10614) fake_sched.h:158: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10620) fake_sched.h:43: return __running_cpu;
    (<0.0>,10624) fake_sched.h:182: if (need_softirq[get_cpu()]) {
    (<0.0>,10634)
    (<0.0>,10637) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10638) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10639) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10643) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10644) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10645) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10647) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,10651) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,10660)
    (<0.0>,10662) fake_sched.h:43: return __running_cpu;
    (<0.0>,10665) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10667) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10669) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,10670) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10672) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10679) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10680) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10682) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10688) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10689) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10690) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10691) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10692) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10696)
    (<0.0>,10697)
    (<0.0>,10698) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,10699) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,10706)
    (<0.0>,10707)
    (<0.0>,10708) tree.c:1584: local_irq_save(flags);
    (<0.0>,10711)
    (<0.0>,10713) fake_sched.h:43: return __running_cpu;
    (<0.0>,10717) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10719) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10723) fake_sched.h:43: return __running_cpu;
    (<0.0>,10727) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10732) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,10734) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,10735) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,10736) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,10738) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,10739) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,10741) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,10744) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,10746) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,10747) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,10749) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,10752) tree.c:1589: local_irq_restore(flags);
    (<0.0>,10755)
    (<0.0>,10757) fake_sched.h:43: return __running_cpu;
    (<0.0>,10761) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10763) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10767) fake_sched.h:43: return __running_cpu;
    (<0.0>,10771) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10778) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,10780) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,10783) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,10785) tree.c:2091: if (!rdp->passed_quiesce)
    (<0.0>,10788) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10790) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10791) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10792) tree.c:2098: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,10801)
    (<0.0>,10802)
    (<0.0>,10803)
    (<0.0>,10804) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,10806) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,10807) tree.c:2034: rnp = rdp->mynode;
    (<0.0>,10808) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,10810) tree.c:2035: raw_spin_lock_irqsave(&rnp->lock, flags);
    (<0.0>,10814)
    (<0.0>,10815)
    (<0.0>,10816) fake_sync.h:76: local_irq_save(flags);
    (<0.0>,10819)
    (<0.0>,10821) fake_sched.h:43: return __running_cpu;
    (<0.0>,10825) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10827) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10831) fake_sched.h:43: return __running_cpu;
    (<0.0>,10835) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10841) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,10842) fake_sync.h:78: if (pthread_mutex_lock(l))
    (<0.0>,10848) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10850) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10855) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10857) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10858) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10860) tree.c:2037: if (rdp->passed_quiesce == 0 || rdp->gpnum != rnp->gpnum ||
    (<0.0>,10863) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,10865) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,10866) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,10868) tree.c:2038: rnp->completed == rnp->gpnum) {
    (<0.0>,10871) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,10873) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,10874) tree.c:2050: mask = rdp->grpmask;
    (<0.0>,10875) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,10877) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,10878) tree.c:2051: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,10882) tree.c:2052: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,10884) tree.c:2052: raw_spin_unlock_irqrestore(&rnp->lock, flags);
    (<0.0>,10888)
    (<0.0>,10889)
    (<0.0>,10890) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,10891) fake_sync.h:84: if (pthread_mutex_unlock(l))
    (<0.0>,10894) fake_sync.h:86: local_irq_restore(flags);
    (<0.0>,10897)
    (<0.0>,10899) fake_sched.h:43: return __running_cpu;
    (<0.0>,10903) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10905) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10909) fake_sched.h:43: return __running_cpu;
    (<0.0>,10913) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10924) tree.c:2558: local_irq_save(flags);
    (<0.0>,10927)
    (<0.0>,10929) fake_sched.h:43: return __running_cpu;
    (<0.0>,10933) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10935) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10939) fake_sched.h:43: return __running_cpu;
    (<0.0>,10943) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10948) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10949) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10955)
    (<0.0>,10956)
    (<0.0>,10957) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,10960)
    (<0.0>,10961) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10963) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10964) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10966) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,10972) tree.c:481: return 0;  /* No, a grace period is already in progress. */
    (<0.0>,10974) tree.c:494: }
    (<0.0>,10978) tree.c:2566: local_irq_restore(flags);
    (<0.0>,10981)
    (<0.0>,10983) fake_sched.h:43: return __running_cpu;
    (<0.0>,10987) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10989) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10993) fake_sched.h:43: return __running_cpu;
    (<0.0>,10997) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11003) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11006)
    (<0.0>,11007) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11009) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11012) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11019) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11022)
    (<0.0>,11026) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11029) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11030) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11031) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11035) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11036) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11037) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11039) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11043) tree.c:2588: __rcu_process_callbacks(rsp);
    (<0.0>,11052)
    (<0.0>,11054) fake_sched.h:43: return __running_cpu;
    (<0.0>,11057) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11059) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11061) tree.c:2550: struct rcu_data *rdp = &rsp->rda[get_cpu()];
    (<0.0>,11062) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11064) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11071) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11072) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11074) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11080) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11081) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11082) tree.c:2552: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11083) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11084) tree.c:2555: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11088)
    (<0.0>,11089)
    (<0.0>,11090) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,11091) tree.c:2078: note_gp_changes(rsp, rdp);
    (<0.0>,11098)
    (<0.0>,11099)
    (<0.0>,11100) tree.c:1584: local_irq_save(flags);
    (<0.0>,11103)
    (<0.0>,11105) fake_sched.h:43: return __running_cpu;
    (<0.0>,11109) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11111) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11115) fake_sched.h:43: return __running_cpu;
    (<0.0>,11119) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11124) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11126) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11127) tree.c:1585: rnp = rdp->mynode;
    (<0.0>,11128) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11130) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11131) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11133) tree.c:1586: if ((rdp->gpnum == ACCESS_ONCE(rnp->gpnum) &&
    (<0.0>,11136) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11138) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11139) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11141) tree.c:1587: rdp->completed == ACCESS_ONCE(rnp->completed)) || /* w/out lock. */
    (<0.0>,11144) tree.c:1589: local_irq_restore(flags);
    (<0.0>,11147)
    (<0.0>,11149) fake_sched.h:43: return __running_cpu;
    (<0.0>,11153) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11155) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11159) fake_sched.h:43: return __running_cpu;
    (<0.0>,11163) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11170) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11172) tree.c:2084: if (!rdp->qs_pending)
    (<0.0>,11177) tree.c:2558: local_irq_save(flags);
    (<0.0>,11180)
    (<0.0>,11182) fake_sched.h:43: return __running_cpu;
    (<0.0>,11186) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11188) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11192) fake_sched.h:43: return __running_cpu;
    (<0.0>,11196) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11201) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11202) tree.c:2559: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11208)
    (<0.0>,11209)
    (<0.0>,11210) tree.c:480: if (rcu_gp_in_progress(rsp))
    (<0.0>,11213)
    (<0.0>,11214) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11216) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11217) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11219) tree.c:178: return ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);
    (<0.0>,11225) tree.c:482: if (rcu_future_needs_gp(rsp))
    (<0.0>,11231)
    (<0.0>,11232) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11235)
    (<0.0>,11236) tree.c:453: return &rsp->node[0];
    (<0.0>,11240) tree.c:463: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11241) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11243) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11247) tree.c:464: int idx = (ACCESS_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11248) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11250) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11253) tree.c:465: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11254) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11255) tree.c:467: return ACCESS_ONCE(*fp);
    (<0.0>,11259) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11262) tree.c:484: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11265) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11268) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11269) tree.c:486: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11272) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11274) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11277) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11280) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11283) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11284) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11286) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11289) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11293) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11295) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11297) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11300) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11303) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11306) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11307) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11309) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11312) tree.c:489: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11316) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11318) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11320) tree.c:488: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11323) tree.c:493: return 0; /* No grace period needed. */
    (<0.0>,11325) tree.c:494: }
    (<0.0>,11329) tree.c:2566: local_irq_restore(flags);
    (<0.0>,11332)
    (<0.0>,11334) fake_sched.h:43: return __running_cpu;
    (<0.0>,11338) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11340) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11344) fake_sched.h:43: return __running_cpu;
    (<0.0>,11348) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11354) tree.c:2570: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11357)
    (<0.0>,11358) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11360) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11363) tree.c:444: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11370) tree.c:2574: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11373)
    (<0.0>,11377) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11380) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11381) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11382) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11386) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11387) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11388) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11390) tree.c:2587: for_each_rcu_flavor(rsp)
    (<0.0>,11398) fake_sched.h:43: return __running_cpu;
    (<0.0>,11402) fake_sched.h:184: need_softirq[get_cpu()] = 0;
    (<0.0>,11411) tree.c:624: local_irq_save(flags);
    (<0.0>,11414)
    (<0.0>,11416) fake_sched.h:43: return __running_cpu;
    (<0.0>,11420) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11422) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11426) fake_sched.h:43: return __running_cpu;
    (<0.0>,11430) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11436) fake_sched.h:43: return __running_cpu;
    (<0.0>,11440) tree.c:625: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11441) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11443) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11444) tree.c:626: oldval = rdtp->dynticks_nesting;
    (<0.0>,11445) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11447) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11449) tree.c:627: rdtp->dynticks_nesting--;
    (<0.0>,11450) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11452) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11457) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11458) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11460) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11464) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11465) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11466) tree.c:628: WARN_ON_ONCE(rdtp->dynticks_nesting < 0);
    (<0.0>,11467) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,11469) tree.c:629: if (rdtp->dynticks_nesting)
    (<0.0>,11477)
    (<0.0>,11479) tree.c:634: local_irq_restore(flags);
    (<0.0>,11482)
    (<0.0>,11484) fake_sched.h:43: return __running_cpu;
    (<0.0>,11488) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11490) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11494) fake_sched.h:43: return __running_cpu;
    (<0.0>,11498) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11507) fake_sched.h:43: return __running_cpu;
    (<0.0>,11511)
    (<0.0>,11514) tree.c:580: local_irq_save(flags);
    (<0.0>,11517)
    (<0.0>,11519) fake_sched.h:43: return __running_cpu;
    (<0.0>,11523) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11525) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11529) fake_sched.h:43: return __running_cpu;
    (<0.0>,11533) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11546)
    (<0.0>,11548) fake_sched.h:43: return __running_cpu;
    (<0.0>,11552) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11553) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11555) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11556) tree.c:554: oldval = rdtp->dynticks_nesting;
    (<0.0>,11557) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11563) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11564) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11569) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11570) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11571) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
    (<0.0>,11572) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,11576) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11578) tree.c:557: rdtp->dynticks_nesting = 0;
    (<0.0>,11579) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11580) tree.c:558: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11599)
    (<0.0>,11601)
    (<0.0>,11603) fake_sched.h:43: return __running_cpu;
    (<0.0>,11607) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
    (<0.0>,11610) tree.c:510: if (!user && !is_idle_task(current)) {
    (<0.0>,11614) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11615) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11616) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11620) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11621) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11622) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11624) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11629) fake_sched.h:43: return __running_cpu;
    (<0.0>,11632) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11634) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11636) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11637) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11640)
    (<0.0>,11643) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11646) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11647) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11648) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11652) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11653) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11654) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11656) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11661) fake_sched.h:43: return __running_cpu;
    (<0.0>,11664) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11666) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11668) tree.c:521: rdp = &rsp->rda[get_cpu()];
    (<0.0>,11669) tree.c:522: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11672)
    (<0.0>,11675) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11678) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11679) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11680) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11684) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11685) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11686) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11688) tree.c:520: for_each_rcu_flavor(rsp) {
    (<0.0>,11695) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11698) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11699) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11700) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11702) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11703) tree.c:527: atomic_inc(&rdtp->dynticks);
    (<0.0>,11705) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11708) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11714) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11715) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11718) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11723) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11724) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11725) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
    (<0.0>,11739)
    (<0.0>,11741) tree.c:583: local_irq_restore(flags);
    (<0.0>,11744)
    (<0.0>,11746) fake_sched.h:43: return __running_cpu;
    (<0.0>,11750) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11752) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11756) fake_sched.h:43: return __running_cpu;
    (<0.0>,11760) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11766) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11769) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11774) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11776) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11778) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11782) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11784) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11791) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11793) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11795) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11799) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11801) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11808) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11810) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11812) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11816) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11818) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11825) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11827) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11829) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11833) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11835) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11842) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11844) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11846) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11850) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,11852) tree.c:1831: ret = wait_event_interruptible_timeout(rsp->gp_wq,
      (<0.1>,720) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,723) fake_sync.h:241: while (!x->done)
      (<0.1>,725) fake_sync.h:241: while (!x->done)
      (<0.1>,732) fake_sched.h:43: return __running_cpu;
      (<0.1>,736)
      (<0.1>,737) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,740) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,745) tree.c:704: local_irq_save(flags);
      (<0.1>,748)
      (<0.1>,750) fake_sched.h:43: return __running_cpu;
      (<0.1>,754) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,756) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,760) fake_sched.h:43: return __running_cpu;
      (<0.1>,764) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,777)
      (<0.1>,779) fake_sched.h:43: return __running_cpu;
      (<0.1>,783) tree.c:678: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,784) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,786) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,787) tree.c:679: oldval = rdtp->dynticks_nesting;
      (<0.1>,788) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,793) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,794) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,798) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,799) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,800) tree.c:680: WARN_ON_ONCE(oldval < 0);
      (<0.1>,801) tree.c:681: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,805) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,807) tree.c:684: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,808) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,809) tree.c:685: rcu_eqs_exit_common(oldval, user);
      (<0.1>,823)
      (<0.1>,824)
      (<0.1>,826) fake_sched.h:43: return __running_cpu;
      (<0.1>,830) tree.c:646: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,834) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,837) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,838) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,839) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,841) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,842) tree.c:650: atomic_inc(&rdtp->dynticks);
      (<0.1>,844) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,847) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,854) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,855) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,858) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,863) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,864) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,865) tree.c:653: WARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));
      (<0.1>,870) tree.c:656: if (!user && !is_idle_task(current)) {
      (<0.1>,879)
      (<0.1>,881) tree.c:707: local_irq_restore(flags);
      (<0.1>,884)
      (<0.1>,886) fake_sched.h:43: return __running_cpu;
      (<0.1>,890) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,892) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,896) fake_sched.h:43: return __running_cpu;
      (<0.1>,900) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,911)
      (<0.1>,917) litmus.c:91: y = 1;
  (<0>,6270) litmus.c:69: r_y = y;
  (<0>,6271) litmus.c:69: r_y = y;
  (<0>,6282) fake_sched.h:43: return __running_cpu;
  (<0>,6286)
  (<0>,6289) tree.c:580: local_irq_save(flags);
  (<0>,6292)
  (<0>,6294) fake_sched.h:43: return __running_cpu;
  (<0>,6298) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6300) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6304) fake_sched.h:43: return __running_cpu;
  (<0>,6308) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6321)
  (<0>,6323) fake_sched.h:43: return __running_cpu;
  (<0>,6327) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6328) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6330) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6331) tree.c:554: oldval = rdtp->dynticks_nesting;
  (<0>,6332) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6338) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6339) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6344) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6345) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6346) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
  (<0>,6347) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,6351) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,6353) tree.c:557: rdtp->dynticks_nesting = 0;
  (<0>,6354) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,6355) tree.c:558: rcu_eqs_enter_common(oldval, user);
  (<0>,6374)
  (<0>,6376)
  (<0>,6378) fake_sched.h:43: return __running_cpu;
  (<0>,6382) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
  (<0>,6385) tree.c:510: if (!user && !is_idle_task(current)) {
  (<0>,6389) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6390) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6391) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6395) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6396) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6397) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6399) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6404) fake_sched.h:43: return __running_cpu;
  (<0>,6407) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6409) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6411) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6412) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,6415)
  (<0>,6418) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6421) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6422) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6423) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6427) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6428) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6429) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6431) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6436) fake_sched.h:43: return __running_cpu;
  (<0>,6439) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6441) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6443) tree.c:521: rdp = &rsp->rda[get_cpu()];
  (<0>,6444) tree.c:522: do_nocb_deferred_wakeup(rdp);
  (<0>,6447)
  (<0>,6450) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6453) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6454) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6455) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6459) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6460) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6461) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6463) tree.c:520: for_each_rcu_flavor(rsp) {
  (<0>,6470) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6473) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6474) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6475) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6477) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6478) tree.c:527: atomic_inc(&rdtp->dynticks);
  (<0>,6480) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6483) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6489) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6490) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6493) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6498) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6499) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6500) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
  (<0>,6514)
  (<0>,6516) tree.c:583: local_irq_restore(flags);
  (<0>,6519)
  (<0>,6521) fake_sched.h:43: return __running_cpu;
  (<0>,6525) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6527) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6531) fake_sched.h:43: return __running_cpu;
  (<0>,6535) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6541) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6544) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6549) litmus.c:138: if (pthread_join(tu, NULL))
      (<0.1>,918) litmus.c:93: fake_release_cpu(get_cpu());
      (<0.1>,919) fake_sched.h:43: return __running_cpu;
      (<0.1>,923)
      (<0.1>,926) tree.c:580: local_irq_save(flags);
      (<0.1>,929)
      (<0.1>,931) fake_sched.h:43: return __running_cpu;
      (<0.1>,935) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,937) fake_sched.h:132: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,941) fake_sched.h:43: return __running_cpu;
      (<0.1>,945) fake_sched.h:133: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,958)
      (<0.1>,960) fake_sched.h:43: return __running_cpu;
      (<0.1>,964) tree.c:553: rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,965) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,967) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,968) tree.c:554: oldval = rdtp->dynticks_nesting;
      (<0.1>,969) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,975) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,976) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,981) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,982) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,983) tree.c:555: WARN_ON_ONCE((oldval & DYNTICK_TASK_NEST_MASK) == 0);
      (<0.1>,984) tree.c:556: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,988) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,990) tree.c:557: rdtp->dynticks_nesting = 0;
      (<0.1>,991) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,992) tree.c:558: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1011)
      (<0.1>,1013)
      (<0.1>,1015) fake_sched.h:43: return __running_cpu;
      (<0.1>,1019) tree.c:507: struct rcu_dynticks *rdtp = &rcu_dynticks[get_cpu()];
      (<0.1>,1022) tree.c:510: if (!user && !is_idle_task(current)) {
      (<0.1>,1026) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1027) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1028) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1033) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1034) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1041) fake_sched.h:43: return __running_cpu;
      (<0.1>,1044) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1046) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1048) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1049) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1052)
      (<0.1>,1055) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1058) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1059) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1060) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1065) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1066) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1073) fake_sched.h:43: return __running_cpu;
      (<0.1>,1076) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1078) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1080) tree.c:521: rdp = &rsp->rda[get_cpu()];
      (<0.1>,1081) tree.c:522: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1084)
      (<0.1>,1087) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1090) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1091) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1092) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1097) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1098) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:520: for_each_rcu_flavor(rsp) {
      (<0.1>,1107) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1110) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1111) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1112) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:527: atomic_inc(&rdtp->dynticks);
      (<0.1>,1117) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1120) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1126) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1127) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1130) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1135) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1136) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1137) tree.c:529: WARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);
      (<0.1>,1151)
      (<0.1>,1153) tree.c:583: local_irq_restore(flags);
      (<0.1>,1156)
      (<0.1>,1158) fake_sched.h:43: return __running_cpu;
      (<0.1>,1162) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1164) fake_sched.h:140: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1168) fake_sched.h:43: return __running_cpu;
      (<0.1>,1172) fake_sched.h:141: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1178) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1181) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,6550) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,6553) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,6556) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,6560) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,6561) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,6562) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
             Error: Assertion violation at (<0>,6565): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpijsc97fe/tmpiie64soq.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DFORCE_FAILURE_5 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpijsc97fe/tmphw66nvme.ll /tmp/tmpijsc97fe/tmpiie64soq.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=-1 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpijsc97fe/tmphw66nvme.ll
Total wall-clock time: 0.92 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 588 (also 20 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp2mlmvo7x/tmp9ns01bs0.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DLIVENESS_CHECK_1 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp2mlmvo7x/tmpnm2yuwca.ll /tmp/tmp2mlmvo7x/tmp9ns01bs0.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp2mlmvo7x/tmpnm2yuwca.ll
Total wall-clock time: 12.79 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 588 (also 20 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpzt1zpv71/tmpq8q8pm04.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DLIVENESS_CHECK_2 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpzt1zpv71/tmpsck9245w.ll /tmp/tmpzt1zpv71/tmpq8q8pm04.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpzt1zpv71/tmpsck9245w.ll
Total wall-clock time: 12.76 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v3.19 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 668 (also 20 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp9t02d3c4/tmprheajd2h.ll -S -emit-llvm -g -I v3.19 -std=gnu99 -DLIVENESS_CHECK_3 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp9t02d3c4/tmp2tve756r.ll /tmp/tmp9t02d3c4/tmprheajd2h.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp9t02d3c4/tmp2tve756r.ll
Total wall-clock time: 18.79 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 28980 (also 16 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpgshm5dym/tmp5hm0fq1d.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpgshm5dym/tmpmd3w49om.ll /tmp/tmpgshm5dym/tmp5hm0fq1d.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpgshm5dym/tmpmd3w49om.ll
Total wall-clock time: 1739.91 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 28 (also 5 sleepset blocked)

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,75) tree_plugin.h:731: pr_info("Hierarchical RCU implementation.\n");
  (<0>,79) tree_plugin.h:76: if (rcu_fanout_exact)
  (<0>,82) tree_plugin.h:87: if (rcu_fanout_leaf != RCU_FANOUT_LEAF)
  (<0>,94) tree.c:4147: ulong d;
  (<0>,95) tree.c:4159: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,98) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,99) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,101) tree.c:4161: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,104) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,105) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,107) tree.c:4165: if (rcu_fanout_leaf == RCU_FANOUT_LEAF &&
  (<0>,120)
  (<0>,121) tree.c:4056: static void __init rcu_init_one(struct rcu_state *rsp)
  (<0>,122) tree.c:4067: int i;
  (<0>,125) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,128) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,130) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,131) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,134) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,137) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,141) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,143) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,145) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,147) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,151) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,153) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,154) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,165)
  (<0>,166) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,167) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,170) tree.c:4041: int ccur;
  (<0>,171) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,173) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,175) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,178) tree.c:4046: ccur = levelcnt[i];
  (<0>,180) tree.c:4046: ccur = levelcnt[i];
  (<0>,182) tree.c:4046: ccur = levelcnt[i];
  (<0>,183) tree.c:4046: ccur = levelcnt[i];
  (<0>,184) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,185) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,188) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,190) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,192) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,194) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,195) tree.c:4048: cprv = ccur;
  (<0>,196) tree.c:4048: cprv = ccur;
  (<0>,198) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,200) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,202) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,207) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,208) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,210) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,211) tree.c:4085: fl_mask <<= 1;
  (<0>,215) tree.c:4085: fl_mask <<= 1;
  (<0>,216) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,218) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,220) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,223) tree.c:4090: cpustride *= levelspread[i];
  (<0>,226) tree.c:4090: cpustride *= levelspread[i];
  (<0>,227) tree.c:4090: cpustride *= levelspread[i];
  (<0>,229) tree.c:4090: cpustride *= levelspread[i];
  (<0>,230) tree.c:4091: rnp = rsp->level[i];
  (<0>,232) tree.c:4091: rnp = rsp->level[i];
  (<0>,235) tree.c:4091: rnp = rsp->level[i];
  (<0>,236) tree.c:4091: rnp = rsp->level[i];
  (<0>,237) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,240) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,243) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,246) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,250)
  (<0>,251) fake_sync.h:75: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,252) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,258) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,262)
  (<0>,263) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,264) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,270) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,273) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,275) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,276) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,278) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,279) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,281) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,282) tree.c:4101: rnp->qsmask = 0;
  (<0>,284) tree.c:4101: rnp->qsmask = 0;
  (<0>,285) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,287) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,288) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,291) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,293) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,294) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,296) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,299) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,301) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,302) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,304) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,307) tree.c:4107: if (i == 0) {
  (<0>,310) tree.c:4108: rnp->grpnum = 0;
  (<0>,312) tree.c:4108: rnp->grpnum = 0;
  (<0>,313) tree.c:4109: rnp->grpmask = 0;
  (<0>,315) tree.c:4109: rnp->grpmask = 0;
  (<0>,316) tree.c:4110: rnp->parent = NULL;
  (<0>,318) tree.c:4110: rnp->parent = NULL;
  (<0>,320) tree.c:4117: rnp->level = i;
  (<0>,322) tree.c:4117: rnp->level = i;
  (<0>,324) tree.c:4117: rnp->level = i;
  (<0>,325) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,329)
  (<0>,330) fake_defs.h:358: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,331) fake_defs.h:360: list->next = list;
  (<0>,333) fake_defs.h:360: list->next = list;
  (<0>,334) fake_defs.h:361: list->prev = list;
  (<0>,335) fake_defs.h:361: list->prev = list;
  (<0>,337) fake_defs.h:361: list->prev = list;
  (<0>,339) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,342) tree_plugin.h:2431: }
  (<0>,352) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,356)
  (<0>,357) fake_sync.h:141: void spin_lock_init(raw_spinlock_t *l)
  (<0>,358) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,363) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,365) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,366) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,368) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,370) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,371) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,374) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,378) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,380) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,382) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,389) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,392) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,395) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,396) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,397) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,399) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,400) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,407)
  (<0>,408) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,409) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,411) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,412) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,415) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,417) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,418) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,421) fake_defs.h:635: if (offset & mask)
  (<0>,422) fake_defs.h:635: if (offset & mask)
  (<0>,426) fake_defs.h:636: return cpu;
  (<0>,427) fake_defs.h:636: return cpu;
  (<0>,429) fake_defs.h:639: }
  (<0>,431) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,432) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,436) tree.c:4132: while (i > rnp->grphi)
  (<0>,437) tree.c:4132: while (i > rnp->grphi)
  (<0>,439) tree.c:4132: while (i > rnp->grphi)
  (<0>,442) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,443) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,445) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,447) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,450) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,451) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,452) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,475)
  (<0>,476) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,477) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,479) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,481) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,483) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,484) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,487)
  (<0>,488) tree.c:623: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,492) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,496) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,497) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,498) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,500) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,504)
  (<0>,505) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,506) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,509) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,511) fake_sched.h:43: return __running_cpu;
  (<0>,515) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,517) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,521) fake_sched.h:43: return __running_cpu;
  (<0>,525) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,531) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,532) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,539) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,540) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,542) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,544) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,548) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,550) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,551) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,554) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,556) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,557) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,559) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,561) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,566) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,567) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,569) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,571) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,575) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,576) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,577) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,578) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,579) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,581) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,584) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,585) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,586) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,591) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,592) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,593) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,595) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,598) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,599) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,600) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,604) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,605) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,606) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,607) tree.c:3776: rdp->cpu = cpu;
  (<0>,608) tree.c:3776: rdp->cpu = cpu;
  (<0>,610) tree.c:3776: rdp->cpu = cpu;
  (<0>,611) tree.c:3777: rdp->rsp = rsp;
  (<0>,612) tree.c:3777: rdp->rsp = rsp;
  (<0>,614) tree.c:3777: rdp->rsp = rsp;
  (<0>,615) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,618) tree_plugin.h:2448: }
  (<0>,623) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,624) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,625) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,627) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,631)
  (<0>,632) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,633) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,634) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,637) fake_sync.h:93: local_irq_restore(flags);
  (<0>,640) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,642) fake_sched.h:43: return __running_cpu;
  (<0>,646) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,648) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,652) fake_sched.h:43: return __running_cpu;
  (<0>,656) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,666) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,667) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,674)
  (<0>,675)
  (<0>,676) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,678) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,679) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,682) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,684) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,685) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,688) fake_defs.h:635: if (offset & mask)
  (<0>,689) fake_defs.h:635: if (offset & mask)
  (<0>,693) fake_defs.h:636: return cpu;
  (<0>,694) fake_defs.h:636: return cpu;
  (<0>,696) fake_defs.h:639: }
  (<0>,698) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,699) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,703) tree.c:4132: while (i > rnp->grphi)
  (<0>,704) tree.c:4132: while (i > rnp->grphi)
  (<0>,706) tree.c:4132: while (i > rnp->grphi)
  (<0>,709) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,710) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,712) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,714) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,717) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,718) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,719) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,742)
  (<0>,743)
  (<0>,744) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,746) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,748) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,750) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,751) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,754)
  (<0>,755) tree.c:625: return &rsp->node[0];
  (<0>,759) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,763) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,764) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,765) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,767) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,771)
  (<0>,772)
  (<0>,773) fake_sync.h:83: local_irq_save(flags);
  (<0>,776) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,778) fake_sched.h:43: return __running_cpu;
  (<0>,782) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,784) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,788) fake_sched.h:43: return __running_cpu;
  (<0>,792) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,798) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,799) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,806) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,807) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,809) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,811) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,815) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,817) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,818) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,821) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,823) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,824) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,826) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,828) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,833) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,834) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,836) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,838) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,842) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,843) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,844) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,845) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,846) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,848) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,851) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,852) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,853) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,858) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,859) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,860) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,862) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,865) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,866) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,867) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,871) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,872) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,873) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,874) tree.c:3776: rdp->cpu = cpu;
  (<0>,875) tree.c:3776: rdp->cpu = cpu;
  (<0>,877) tree.c:3776: rdp->cpu = cpu;
  (<0>,878) tree.c:3777: rdp->rsp = rsp;
  (<0>,879) tree.c:3777: rdp->rsp = rsp;
  (<0>,881) tree.c:3777: rdp->rsp = rsp;
  (<0>,882) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,885) tree_plugin.h:2448: }
  (<0>,890) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,891) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,892) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,894) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,898)
  (<0>,899)
  (<0>,900) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,901) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,904) fake_sync.h:93: local_irq_restore(flags);
  (<0>,907) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,909) fake_sched.h:43: return __running_cpu;
  (<0>,913) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,915) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,919) fake_sched.h:43: return __running_cpu;
  (<0>,923) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,933) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,934) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,941)
  (<0>,942)
  (<0>,943) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,945) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,946) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,949) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,951) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,952) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,955) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,957) fake_defs.h:639: }
  (<0>,959) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,960) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,963) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,968)
  (<0>,969) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,970) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,971) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,972) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,974) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,979)
  (<0>,980) fake_defs.h:364: static inline void __list_add(struct list_head *new,
  (<0>,981) fake_defs.h:365: struct list_head *prev,
  (<0>,982) fake_defs.h:366: struct list_head *next)
  (<0>,983) fake_defs.h:368: next->prev = new;
  (<0>,985) fake_defs.h:368: next->prev = new;
  (<0>,986) fake_defs.h:369: new->next = next;
  (<0>,987) fake_defs.h:369: new->next = next;
  (<0>,989) fake_defs.h:369: new->next = next;
  (<0>,990) fake_defs.h:370: new->prev = prev;
  (<0>,991) fake_defs.h:370: new->prev = prev;
  (<0>,993) fake_defs.h:370: new->prev = prev;
  (<0>,994) fake_defs.h:371: prev->next = new;
  (<0>,995) fake_defs.h:371: prev->next = new;
  (<0>,997) fake_defs.h:371: prev->next = new;
  (<0>,1009)
  (<0>,1010) tree.c:4066: int cpustride = 1;
  (<0>,1011) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1014) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1017) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1019) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1020) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1023) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1026) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1027) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1030) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1032) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1034) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1036) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1037) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1040) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1042) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1043) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1054)
  (<0>,1055)
  (<0>,1056) tree.c:4036: if (rcu_fanout_exact) {
  (<0>,1059) tree.c:4044: cprv = nr_cpu_ids;
  (<0>,1060) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1062) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1064) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1067) tree.c:4046: ccur = levelcnt[i];
  (<0>,1069) tree.c:4046: ccur = levelcnt[i];
  (<0>,1071) tree.c:4046: ccur = levelcnt[i];
  (<0>,1072) tree.c:4046: ccur = levelcnt[i];
  (<0>,1073) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1074) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1077) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1079) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1081) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1083) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1084) tree.c:4048: cprv = ccur;
  (<0>,1085) tree.c:4048: cprv = ccur;
  (<0>,1087) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1089) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1091) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1096) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1097) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1099) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1100) tree.c:4085: fl_mask <<= 1;
  (<0>,1104) tree.c:4085: fl_mask <<= 1;
  (<0>,1105) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1107) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1109) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1112) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1115) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1116) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1118) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1119) tree.c:4091: rnp = rsp->level[i];
  (<0>,1121) tree.c:4091: rnp = rsp->level[i];
  (<0>,1124) tree.c:4091: rnp = rsp->level[i];
  (<0>,1125) tree.c:4091: rnp = rsp->level[i];
  (<0>,1126) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1128) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1129) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1132) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1135) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,1139)
  (<0>,1140) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1141) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1147) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1151)
  (<0>,1152) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1153) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1159) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1161) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1162) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1164) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1165) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1167) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1168) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1170) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1171) tree.c:4101: rnp->qsmask = 0;
  (<0>,1173) tree.c:4101: rnp->qsmask = 0;
  (<0>,1174) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1176) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1177) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1178) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1180) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1182) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1183) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1185) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1188) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1190) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1191) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1193) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1196) tree.c:4107: if (i == 0) {
  (<0>,1199) tree.c:4108: rnp->grpnum = 0;
  (<0>,1201) tree.c:4108: rnp->grpnum = 0;
  (<0>,1202) tree.c:4109: rnp->grpmask = 0;
  (<0>,1204) tree.c:4109: rnp->grpmask = 0;
  (<0>,1205) tree.c:4110: rnp->parent = NULL;
  (<0>,1207) tree.c:4110: rnp->parent = NULL;
  (<0>,1209) tree.c:4117: rnp->level = i;
  (<0>,1211) tree.c:4117: rnp->level = i;
  (<0>,1213) tree.c:4117: rnp->level = i;
  (<0>,1214) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1218)
  (<0>,1219) fake_defs.h:360: list->next = list;
  (<0>,1220) fake_defs.h:360: list->next = list;
  (<0>,1222) fake_defs.h:360: list->next = list;
  (<0>,1223) fake_defs.h:361: list->prev = list;
  (<0>,1224) fake_defs.h:361: list->prev = list;
  (<0>,1226) fake_defs.h:361: list->prev = list;
  (<0>,1228) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,1231) tree_plugin.h:2431: }
  (<0>,1241) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,1245)
  (<0>,1246) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1247) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1252) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1254) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1255) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1257) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1259) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1260) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1263) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1267) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1269) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1271) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1278) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1281) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1284) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1285) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1286) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1288) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1289) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1296)
  (<0>,1297)
  (<0>,1298) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1300) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1301) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1304) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1306) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1307) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1310) fake_defs.h:635: if (offset & mask)
  (<0>,1311) fake_defs.h:635: if (offset & mask)
  (<0>,1315) fake_defs.h:636: return cpu;
  (<0>,1316) fake_defs.h:636: return cpu;
  (<0>,1318) fake_defs.h:639: }
  (<0>,1320) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1321) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1325) tree.c:4132: while (i > rnp->grphi)
  (<0>,1326) tree.c:4132: while (i > rnp->grphi)
  (<0>,1328) tree.c:4132: while (i > rnp->grphi)
  (<0>,1331) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1332) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1334) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1336) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1339) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1340) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1341) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1364)
  (<0>,1365)
  (<0>,1366) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1368) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1370) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1372) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1373) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1376)
  (<0>,1377) tree.c:625: return &rsp->node[0];
  (<0>,1381) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1385) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1386) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1387) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1389) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1393)
  (<0>,1394)
  (<0>,1395) fake_sync.h:83: local_irq_save(flags);
  (<0>,1398) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1400) fake_sched.h:43: return __running_cpu;
  (<0>,1404) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1406) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1410) fake_sched.h:43: return __running_cpu;
  (<0>,1414) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1420) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1421) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1428) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1429) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1431) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1433) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1437) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1439) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1440) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1443) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1445) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1446) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1448) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1450) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1455) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1456) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1458) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1460) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1464) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1465) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1466) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1467) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1468) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1470) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1473) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1474) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1475) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1480) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1481) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1482) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1484) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1487) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1488) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1489) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1493) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1494) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1495) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1496) tree.c:3776: rdp->cpu = cpu;
  (<0>,1497) tree.c:3776: rdp->cpu = cpu;
  (<0>,1499) tree.c:3776: rdp->cpu = cpu;
  (<0>,1500) tree.c:3777: rdp->rsp = rsp;
  (<0>,1501) tree.c:3777: rdp->rsp = rsp;
  (<0>,1503) tree.c:3777: rdp->rsp = rsp;
  (<0>,1504) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1507) tree_plugin.h:2448: }
  (<0>,1512) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1513) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1514) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1516) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1520)
  (<0>,1521)
  (<0>,1522) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1523) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1526) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1529) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1531) fake_sched.h:43: return __running_cpu;
  (<0>,1535) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1541) fake_sched.h:43: return __running_cpu;
  (<0>,1545) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1555) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1556) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1563)
  (<0>,1564)
  (<0>,1565) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1567) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1568) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1571) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1573) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1574) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1577) fake_defs.h:635: if (offset & mask)
  (<0>,1578) fake_defs.h:635: if (offset & mask)
  (<0>,1582) fake_defs.h:636: return cpu;
  (<0>,1583) fake_defs.h:636: return cpu;
  (<0>,1585) fake_defs.h:639: }
  (<0>,1587) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1588) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1592) tree.c:4132: while (i > rnp->grphi)
  (<0>,1593) tree.c:4132: while (i > rnp->grphi)
  (<0>,1595) tree.c:4132: while (i > rnp->grphi)
  (<0>,1598) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1599) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1601) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1603) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1606) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1607) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1608) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1631)
  (<0>,1632)
  (<0>,1633) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1635) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1637) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1639) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1640) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1643)
  (<0>,1644) tree.c:625: return &rsp->node[0];
  (<0>,1648) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1652) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1653) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1654) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1656) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1660)
  (<0>,1661)
  (<0>,1662) fake_sync.h:83: local_irq_save(flags);
  (<0>,1665) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1667) fake_sched.h:43: return __running_cpu;
  (<0>,1671) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1673) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1677) fake_sched.h:43: return __running_cpu;
  (<0>,1681) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1687) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1688) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1695) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1696) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1698) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1700) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1704) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1706) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1707) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1710) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1712) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1713) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1715) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1717) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1722) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1723) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1725) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1727) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1731) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1732) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1733) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1734) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1735) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1737) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1740) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1741) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1742) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1747) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1748) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1749) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1751) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1754) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1755) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1756) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1760) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1761) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1762) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1763) tree.c:3776: rdp->cpu = cpu;
  (<0>,1764) tree.c:3776: rdp->cpu = cpu;
  (<0>,1766) tree.c:3776: rdp->cpu = cpu;
  (<0>,1767) tree.c:3777: rdp->rsp = rsp;
  (<0>,1768) tree.c:3777: rdp->rsp = rsp;
  (<0>,1770) tree.c:3777: rdp->rsp = rsp;
  (<0>,1771) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1774) tree_plugin.h:2448: }
  (<0>,1779) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1780) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1781) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1783) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1787)
  (<0>,1788)
  (<0>,1789) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1790) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1793) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1796) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1798) fake_sched.h:43: return __running_cpu;
  (<0>,1802) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1804) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1808) fake_sched.h:43: return __running_cpu;
  (<0>,1812) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1822) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1823) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1830)
  (<0>,1831)
  (<0>,1832) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1834) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1835) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1838) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1840) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1841) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1844) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,1846) fake_defs.h:639: }
  (<0>,1848) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1849) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1852) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1857)
  (<0>,1858)
  (<0>,1859) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1860) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1861) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1863) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1868)
  (<0>,1869)
  (<0>,1870)
  (<0>,1871) fake_defs.h:368: next->prev = new;
  (<0>,1872) fake_defs.h:368: next->prev = new;
  (<0>,1874) fake_defs.h:368: next->prev = new;
  (<0>,1875) fake_defs.h:369: new->next = next;
  (<0>,1876) fake_defs.h:369: new->next = next;
  (<0>,1878) fake_defs.h:369: new->next = next;
  (<0>,1879) fake_defs.h:370: new->prev = prev;
  (<0>,1880) fake_defs.h:370: new->prev = prev;
  (<0>,1882) fake_defs.h:370: new->prev = prev;
  (<0>,1883) fake_defs.h:371: prev->next = new;
  (<0>,1884) fake_defs.h:371: prev->next = new;
  (<0>,1886) fake_defs.h:371: prev->next = new;
  (<0>,1890) tree.c:4251: if (dump_tree)
  (<0>,1899) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1901) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1902) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1909)
  (<0>,1910)
  (<0>,1911) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1913) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1914) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1917) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1919) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1920) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1923) fake_defs.h:635: if (offset & mask)
  (<0>,1924) fake_defs.h:635: if (offset & mask)
  (<0>,1928) fake_defs.h:636: return cpu;
  (<0>,1929) fake_defs.h:636: return cpu;
  (<0>,1931) fake_defs.h:639: }
  (<0>,1933) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1934) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1937) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,1945)
  (<0>,1946) tree.c:3829: int rcutree_prepare_cpu(unsigned int cpu)
  (<0>,1947) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1948) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1952) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1953) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1954) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1956) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1960) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1961) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1987)
  (<0>,1988) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1989) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1991) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1993) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1995) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1996) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1999)
  (<0>,2000) tree.c:625: return &rsp->node[0];
  (<0>,2004) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2008) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2009) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2010) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2012) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2016)
  (<0>,2017)
  (<0>,2018) fake_sync.h:83: local_irq_save(flags);
  (<0>,2021) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2023) fake_sched.h:43: return __running_cpu;
  (<0>,2027) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2029) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2033) fake_sched.h:43: return __running_cpu;
  (<0>,2037) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2043) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2044) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2051) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2053) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2054) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2056) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2057) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2059) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2060) tree.c:3800: rdp->blimit = blimit;
  (<0>,2061) tree.c:3800: rdp->blimit = blimit;
  (<0>,2063) tree.c:3800: rdp->blimit = blimit;
  (<0>,2064) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2066) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2069) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2072)
  (<0>,2073) tree.c:1551: static void init_callback_list(struct rcu_data *rdp)
  (<0>,2076) tree_plugin.h:2469: return false;
  (<0>,2079) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2083)
  (<0>,2084) tree.c:1539: static void init_default_callback_list(struct rcu_data *rdp)
  (<0>,2086) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2087) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2089) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2092) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2094) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2096) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2099) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2101) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2103) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2105) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2108) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2110) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2112) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2115) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2117) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2119) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2121) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2124) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2126) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2128) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2131) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2133) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2135) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2137) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2140) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2142) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2144) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2147) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2149) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2151) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2153) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2160) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2162) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2164) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2165) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2167) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2170) tree_plugin.h:2902: }
  (<0>,2172) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2173) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2175) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2178) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2179) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2180) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2183) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2185) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2188) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2189) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2192)
  (<0>,2193) tree.h:721: static inline void raw_spin_unlock_rcu_node(struct rcu_node *rnp)
  (<0>,2197)
  (<0>,2198) fake_sync.h:120: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2199) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2205) tree.c:3814: rnp = rdp->mynode;
  (<0>,2207) tree.c:3814: rnp = rdp->mynode;
  (<0>,2208) tree.c:3814: rnp = rdp->mynode;
  (<0>,2209) tree.c:3815: mask = rdp->grpmask;
  (<0>,2211) tree.c:3815: mask = rdp->grpmask;
  (<0>,2212) tree.c:3815: mask = rdp->grpmask;
  (<0>,2213) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2216)
  (<0>,2217) tree.h:715: static inline void raw_spin_lock_rcu_node(struct rcu_node *rnp)
  (<0>,2221) fake_sync.h:115: preempt_disable();
  (<0>,2223) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2224) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2231) tree.c:3817: if (!rdp->beenonline)
  (<0>,2233) tree.c:3817: if (!rdp->beenonline)
  (<0>,2237) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2242) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2243) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2244) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2245) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2247) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2249) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2250) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2252) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2255) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2256) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2257) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2259) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2260) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2265) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2266) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2267) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2268) fake_defs.h:237: switch (size) {
  (<0>,2270) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2272) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2273) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2275) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2278) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2279) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2280) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2282) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2284) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2285) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2287) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2288) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2290) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2291) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2293) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2294) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2296) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2297) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2301) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2302) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2305) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2306) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2308) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2309) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2311) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2317) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2318) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2319) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2321) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2325)
  (<0>,2326)
  (<0>,2327) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2328) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2331) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2334) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2336) fake_sched.h:43: return __running_cpu;
  (<0>,2340) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2342) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2346) fake_sched.h:43: return __running_cpu;
  (<0>,2350) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2360) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2363) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2364) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2365) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2369) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2370) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2371) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2373) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2377) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2378) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2404)
  (<0>,2405)
  (<0>,2406) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2408) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2410) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2412) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2413) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2416)
  (<0>,2417) tree.c:625: return &rsp->node[0];
  (<0>,2421) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2425) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2426) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2427) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2429) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2433)
  (<0>,2434)
  (<0>,2435) fake_sync.h:83: local_irq_save(flags);
  (<0>,2438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2440) fake_sched.h:43: return __running_cpu;
  (<0>,2444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2446) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2450) fake_sched.h:43: return __running_cpu;
  (<0>,2454) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2460) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2461) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2468) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2470) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2471) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2473) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2474) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2476) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2477) tree.c:3800: rdp->blimit = blimit;
  (<0>,2478) tree.c:3800: rdp->blimit = blimit;
  (<0>,2480) tree.c:3800: rdp->blimit = blimit;
  (<0>,2481) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2483) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2486) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2489)
  (<0>,2490) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,2493) tree_plugin.h:2469: return false;
  (<0>,2496) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2500)
  (<0>,2501) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2503) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2504) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2506) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2509) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2511) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2513) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2516) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2518) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2520) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2522) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2525) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2527) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2529) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2532) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2534) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2536) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2538) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2541) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2543) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2545) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2548) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2550) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2552) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2554) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2557) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2559) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2561) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2564) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2566) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2568) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2570) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2577) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2579) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2581) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2582) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2584) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2587) tree_plugin.h:2902: }
  (<0>,2589) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2590) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2592) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2595) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2596) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2597) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2600) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2602) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2605) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2606) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2609)
  (<0>,2610) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2614)
  (<0>,2615) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2616) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2622) tree.c:3814: rnp = rdp->mynode;
  (<0>,2624) tree.c:3814: rnp = rdp->mynode;
  (<0>,2625) tree.c:3814: rnp = rdp->mynode;
  (<0>,2626) tree.c:3815: mask = rdp->grpmask;
  (<0>,2628) tree.c:3815: mask = rdp->grpmask;
  (<0>,2629) tree.c:3815: mask = rdp->grpmask;
  (<0>,2630) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2633)
  (<0>,2634) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2638) fake_sync.h:115: preempt_disable();
  (<0>,2640) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2641) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2648) tree.c:3817: if (!rdp->beenonline)
  (<0>,2650) tree.c:3817: if (!rdp->beenonline)
  (<0>,2654) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2659) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2660) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2661) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2662) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2664) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2666) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2667) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2669) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2672) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2673) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2674) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2676) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2677) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2682) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2683) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2684) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2685) fake_defs.h:237: switch (size) {
  (<0>,2687) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2689) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2690) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2692) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2695) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2696) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2697) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2699) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2701) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2702) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2704) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2705) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2707) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2708) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2710) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2711) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2713) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2714) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2718) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2719) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2722) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2723) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2725) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2726) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2728) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2734) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2735) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2736) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2738) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2742)
  (<0>,2743)
  (<0>,2744) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2745) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2748) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2751) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2753) fake_sched.h:43: return __running_cpu;
  (<0>,2757) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2759) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2763) fake_sched.h:43: return __running_cpu;
  (<0>,2767) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2777) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2780) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2781) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2782) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2786) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2787) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2788) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2790) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2794) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,2797) tree_plugin.h:1243: }
  (<0>,2799) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2802) tree_plugin.h:2461: }
  (<0>,2805) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,2823)
  (<0>,2824) tree.c:3890: void rcu_cpu_starting(unsigned int cpu)
  (<0>,2825) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2826) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2830) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2831) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2832) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2834) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2839) fake_sched.h:43: return __running_cpu;
  (<0>,2842) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2844) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2846) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2847) tree.c:3900: rnp = rdp->mynode;
  (<0>,2849) tree.c:3900: rnp = rdp->mynode;
  (<0>,2850) tree.c:3900: rnp = rdp->mynode;
  (<0>,2851) tree.c:3901: mask = rdp->grpmask;
  (<0>,2853) tree.c:3901: mask = rdp->grpmask;
  (<0>,2854) tree.c:3901: mask = rdp->grpmask;
  (<0>,2858) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2859) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2860) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2862) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2866)
  (<0>,2867)
  (<0>,2868) fake_sync.h:83: local_irq_save(flags);
  (<0>,2871) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2873) fake_sched.h:43: return __running_cpu;
  (<0>,2877) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2879) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2883) fake_sched.h:43: return __running_cpu;
  (<0>,2887) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2893) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2894) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2901) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2902) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2904) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2906) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2907) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2908) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2910) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2912) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2916) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2917) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2918) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2920) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2924)
  (<0>,2925)
  (<0>,2926) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2927) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2930) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2933) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2935) fake_sched.h:43: return __running_cpu;
  (<0>,2939) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2941) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2945) fake_sched.h:43: return __running_cpu;
  (<0>,2949) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2958) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2961) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2962) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2963) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2967) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2968) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2969) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2971) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2976) fake_sched.h:43: return __running_cpu;
  (<0>,2979) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2981) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2983) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2984) tree.c:3900: rnp = rdp->mynode;
  (<0>,2986) tree.c:3900: rnp = rdp->mynode;
  (<0>,2987) tree.c:3900: rnp = rdp->mynode;
  (<0>,2988) tree.c:3901: mask = rdp->grpmask;
  (<0>,2990) tree.c:3901: mask = rdp->grpmask;
  (<0>,2991) tree.c:3901: mask = rdp->grpmask;
  (<0>,2995) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2996) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2997) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2999) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3003)
  (<0>,3004)
  (<0>,3005) fake_sync.h:83: local_irq_save(flags);
  (<0>,3008) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3010) fake_sched.h:43: return __running_cpu;
  (<0>,3014) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3016) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3020) fake_sched.h:43: return __running_cpu;
  (<0>,3024) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3030) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3031) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3038) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3039) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3041) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3043) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3044) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3045) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3047) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3049) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3053) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3054) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3055) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3057) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3061)
  (<0>,3062)
  (<0>,3063) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3064) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3067) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3070) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3072) fake_sched.h:43: return __running_cpu;
  (<0>,3076) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3078) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3082) fake_sched.h:43: return __running_cpu;
  (<0>,3086) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3095) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3098) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3099) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3100) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3104) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3105) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3106) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3108) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3114) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3115) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3122)
  (<0>,3123)
  (<0>,3124) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3126) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3127) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3130) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3132) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3133) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3136) fake_defs.h:635: if (offset & mask)
  (<0>,3137) fake_defs.h:635: if (offset & mask)
  (<0>,3141) fake_defs.h:636: return cpu;
  (<0>,3142) fake_defs.h:636: return cpu;
  (<0>,3144) fake_defs.h:639: }
  (<0>,3146) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3147) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3150) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,3158)
  (<0>,3159) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3160) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3161) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3165) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3166) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3167) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3169) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3173) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3174) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3200)
  (<0>,3201)
  (<0>,3202) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3204) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3206) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3208) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3209) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3212)
  (<0>,3213) tree.c:625: return &rsp->node[0];
  (<0>,3217) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3221) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3222) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3223) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3225) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3229)
  (<0>,3230)
  (<0>,3231) fake_sync.h:83: local_irq_save(flags);
  (<0>,3234) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3236) fake_sched.h:43: return __running_cpu;
  (<0>,3240) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3242) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3246) fake_sched.h:43: return __running_cpu;
  (<0>,3250) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3256) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3257) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3264) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3266) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3267) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3269) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3270) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3272) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3273) tree.c:3800: rdp->blimit = blimit;
  (<0>,3274) tree.c:3800: rdp->blimit = blimit;
  (<0>,3276) tree.c:3800: rdp->blimit = blimit;
  (<0>,3277) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3279) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3282) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3285)
  (<0>,3286) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3289) tree_plugin.h:2469: return false;
  (<0>,3292) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3296)
  (<0>,3297) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3299) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3300) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3302) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3305) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3307) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3309) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3312) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3314) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3316) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3318) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3321) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3323) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3325) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3328) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3330) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3332) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3334) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3337) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3339) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3341) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3344) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3346) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3348) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3350) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3353) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3355) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3357) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3360) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3362) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3364) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3366) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3373) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3375) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3377) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3378) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3380) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3383) tree_plugin.h:2902: }
  (<0>,3385) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3386) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3388) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3391) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3392) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3393) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3396) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3398) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3401) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3402) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3405)
  (<0>,3406) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3410)
  (<0>,3411) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3412) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3418) tree.c:3814: rnp = rdp->mynode;
  (<0>,3420) tree.c:3814: rnp = rdp->mynode;
  (<0>,3421) tree.c:3814: rnp = rdp->mynode;
  (<0>,3422) tree.c:3815: mask = rdp->grpmask;
  (<0>,3424) tree.c:3815: mask = rdp->grpmask;
  (<0>,3425) tree.c:3815: mask = rdp->grpmask;
  (<0>,3426) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3429)
  (<0>,3430) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3434) fake_sync.h:115: preempt_disable();
  (<0>,3436) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3437) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3444) tree.c:3817: if (!rdp->beenonline)
  (<0>,3446) tree.c:3817: if (!rdp->beenonline)
  (<0>,3450) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3455) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3456) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3457) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3458) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3460) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3462) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3463) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3465) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3468) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3469) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3470) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3472) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3473) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3478) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3479) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3480) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3481) fake_defs.h:237: switch (size) {
  (<0>,3483) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3485) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3486) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3488) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3491) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3492) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3493) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3495) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3497) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3498) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3500) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3501) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3503) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3504) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3506) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3507) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3509) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3510) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3514) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3515) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3518) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3519) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3521) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3522) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3524) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3530) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3531) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3532) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3534) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3538)
  (<0>,3539)
  (<0>,3540) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3541) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3544) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3547) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3549) fake_sched.h:43: return __running_cpu;
  (<0>,3553) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3555) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3559) fake_sched.h:43: return __running_cpu;
  (<0>,3563) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3573) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3576) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3577) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3578) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3582) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3583) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3584) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3586) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3590) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3591) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3617)
  (<0>,3618)
  (<0>,3619) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3621) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3623) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3625) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3626) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3629)
  (<0>,3630) tree.c:625: return &rsp->node[0];
  (<0>,3634) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3638) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3639) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3640) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3642) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3646)
  (<0>,3647)
  (<0>,3648) fake_sync.h:83: local_irq_save(flags);
  (<0>,3651) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3653) fake_sched.h:43: return __running_cpu;
  (<0>,3657) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3659) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3663) fake_sched.h:43: return __running_cpu;
  (<0>,3667) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3673) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3674) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3681) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3683) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3684) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3686) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3687) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3689) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3690) tree.c:3800: rdp->blimit = blimit;
  (<0>,3691) tree.c:3800: rdp->blimit = blimit;
  (<0>,3693) tree.c:3800: rdp->blimit = blimit;
  (<0>,3694) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3696) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3699) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3702)
  (<0>,3703) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3706) tree_plugin.h:2469: return false;
  (<0>,3709) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3713)
  (<0>,3714) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3716) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3717) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3719) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3722) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3724) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3726) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3729) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3731) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3733) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3735) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3738) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3740) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3742) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3745) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3747) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3749) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3751) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3754) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3756) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3758) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3761) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3763) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3765) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3767) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3770) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3772) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3774) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3777) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3779) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3781) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3783) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3790) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3792) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3794) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3795) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3797) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3800) tree_plugin.h:2902: }
  (<0>,3802) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3803) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3805) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3808) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3809) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3810) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3813) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3815) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3818) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3819) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3822)
  (<0>,3823) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3827)
  (<0>,3828) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3829) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3835) tree.c:3814: rnp = rdp->mynode;
  (<0>,3837) tree.c:3814: rnp = rdp->mynode;
  (<0>,3838) tree.c:3814: rnp = rdp->mynode;
  (<0>,3839) tree.c:3815: mask = rdp->grpmask;
  (<0>,3841) tree.c:3815: mask = rdp->grpmask;
  (<0>,3842) tree.c:3815: mask = rdp->grpmask;
  (<0>,3843) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3846)
  (<0>,3847) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3851) fake_sync.h:115: preempt_disable();
  (<0>,3853) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3854) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3861) tree.c:3817: if (!rdp->beenonline)
  (<0>,3863) tree.c:3817: if (!rdp->beenonline)
  (<0>,3867) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3872) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3873) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3874) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3875) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3877) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3879) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3880) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3882) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3885) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3886) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3887) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3889) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3890) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3895) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3896) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3897) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3898) fake_defs.h:237: switch (size) {
  (<0>,3900) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3902) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3903) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3905) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3908) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3909) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3910) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3912) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3914) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3915) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3917) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3918) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3920) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3921) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3923) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3924) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3926) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3927) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3931) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3932) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3935) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3936) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3938) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3939) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3941) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3947) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3948) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3949) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3951) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3955)
  (<0>,3956)
  (<0>,3957) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3958) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3961) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3964) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3966) fake_sched.h:43: return __running_cpu;
  (<0>,3970) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3972) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3976) fake_sched.h:43: return __running_cpu;
  (<0>,3980) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3990) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3993) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3994) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3995) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3999) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4000) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4001) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4003) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4007) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,4010) tree_plugin.h:1243: }
  (<0>,4012) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,4015) tree_plugin.h:2461: }
  (<0>,4018) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,4036)
  (<0>,4037) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4038) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4039) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4043) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4044) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4045) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4047) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4052) fake_sched.h:43: return __running_cpu;
  (<0>,4055) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4057) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4059) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4060) tree.c:3900: rnp = rdp->mynode;
  (<0>,4062) tree.c:3900: rnp = rdp->mynode;
  (<0>,4063) tree.c:3900: rnp = rdp->mynode;
  (<0>,4064) tree.c:3901: mask = rdp->grpmask;
  (<0>,4066) tree.c:3901: mask = rdp->grpmask;
  (<0>,4067) tree.c:3901: mask = rdp->grpmask;
  (<0>,4071) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4072) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4073) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4075) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4079)
  (<0>,4080)
  (<0>,4081) fake_sync.h:83: local_irq_save(flags);
  (<0>,4084) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4086) fake_sched.h:43: return __running_cpu;
  (<0>,4090) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4092) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4096) fake_sched.h:43: return __running_cpu;
  (<0>,4100) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4106) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4107) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4114) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4115) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4117) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4119) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4120) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4121) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4123) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4125) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4129) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4130) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4131) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4133) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4137)
  (<0>,4138)
  (<0>,4139) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4140) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4143) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4146) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4148) fake_sched.h:43: return __running_cpu;
  (<0>,4152) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4154) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4158) fake_sched.h:43: return __running_cpu;
  (<0>,4162) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4171) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4175) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4180) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4181) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4182) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4184) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4189) fake_sched.h:43: return __running_cpu;
  (<0>,4192) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4194) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4196) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4197) tree.c:3900: rnp = rdp->mynode;
  (<0>,4199) tree.c:3900: rnp = rdp->mynode;
  (<0>,4200) tree.c:3900: rnp = rdp->mynode;
  (<0>,4201) tree.c:3901: mask = rdp->grpmask;
  (<0>,4203) tree.c:3901: mask = rdp->grpmask;
  (<0>,4204) tree.c:3901: mask = rdp->grpmask;
  (<0>,4208) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4209) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4210) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4212) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4216)
  (<0>,4217)
  (<0>,4218) fake_sync.h:83: local_irq_save(flags);
  (<0>,4221) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4223) fake_sched.h:43: return __running_cpu;
  (<0>,4227) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4229) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4233) fake_sched.h:43: return __running_cpu;
  (<0>,4237) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4243) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4244) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4251) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4252) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4254) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4256) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4257) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4258) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4260) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4262) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4266) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4267) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4268) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4270) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4274)
  (<0>,4275)
  (<0>,4276) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4277) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4280) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4283) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4285) fake_sched.h:43: return __running_cpu;
  (<0>,4289) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4291) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4295) fake_sched.h:43: return __running_cpu;
  (<0>,4299) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4308) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4313) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4317) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4319) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4321) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4327) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4328) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4335)
  (<0>,4336)
  (<0>,4337) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4339) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4340) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4343) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4345) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4346) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4349) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,4351) fake_defs.h:639: }
  (<0>,4353) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4354) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4358) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4360) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4363) litmus.c:123: set_cpu(i);
  (<0>,4366)
  (<0>,4367) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,4368) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4370) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4388)
  (<0>,4389) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4390) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4391) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4395) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4396) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4397) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4399) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4404) fake_sched.h:43: return __running_cpu;
  (<0>,4407) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4409) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4411) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4412) tree.c:3900: rnp = rdp->mynode;
  (<0>,4414) tree.c:3900: rnp = rdp->mynode;
  (<0>,4415) tree.c:3900: rnp = rdp->mynode;
  (<0>,4416) tree.c:3901: mask = rdp->grpmask;
  (<0>,4418) tree.c:3901: mask = rdp->grpmask;
  (<0>,4419) tree.c:3901: mask = rdp->grpmask;
  (<0>,4423) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4424) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4425) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4427) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4431)
  (<0>,4432)
  (<0>,4433) fake_sync.h:83: local_irq_save(flags);
  (<0>,4436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4438) fake_sched.h:43: return __running_cpu;
  (<0>,4442) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4448) fake_sched.h:43: return __running_cpu;
  (<0>,4452) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4458) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4459) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4466) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4467) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4469) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4471) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4472) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4473) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4475) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4477) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4481) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4482) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4483) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4485) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4489)
  (<0>,4490)
  (<0>,4491) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4492) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4495) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4498) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4500) fake_sched.h:43: return __running_cpu;
  (<0>,4504) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4506) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4510) fake_sched.h:43: return __running_cpu;
  (<0>,4514) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4523) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4526) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4527) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4528) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4532) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4533) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4534) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4541) fake_sched.h:43: return __running_cpu;
  (<0>,4544) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4546) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4548) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4549) tree.c:3900: rnp = rdp->mynode;
  (<0>,4551) tree.c:3900: rnp = rdp->mynode;
  (<0>,4552) tree.c:3900: rnp = rdp->mynode;
  (<0>,4553) tree.c:3901: mask = rdp->grpmask;
  (<0>,4555) tree.c:3901: mask = rdp->grpmask;
  (<0>,4556) tree.c:3901: mask = rdp->grpmask;
  (<0>,4560) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4561) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4562) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4564) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4568)
  (<0>,4569)
  (<0>,4570) fake_sync.h:83: local_irq_save(flags);
  (<0>,4573) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4575) fake_sched.h:43: return __running_cpu;
  (<0>,4579) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4581) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4585) fake_sched.h:43: return __running_cpu;
  (<0>,4589) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4595) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4596) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4603) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4604) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4606) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4608) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4609) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4610) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4612) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4614) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4618) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4619) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4620) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4622) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4626)
  (<0>,4627)
  (<0>,4628) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4629) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4632) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4635) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4637) fake_sched.h:43: return __running_cpu;
  (<0>,4641) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4643) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4647) fake_sched.h:43: return __running_cpu;
  (<0>,4651) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4660) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4663) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4664) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4665) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4669) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4670) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4671) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4673) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4680) tree.c:753: unsigned long flags;
  (<0>,4683) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4685) fake_sched.h:43: return __running_cpu;
  (<0>,4689) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4691) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4695) fake_sched.h:43: return __running_cpu;
  (<0>,4699) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4711) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4713) fake_sched.h:43: return __running_cpu;
  (<0>,4717) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4718) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4720) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4721) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4722) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4723) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4724) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4725) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4726) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4730) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4732) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4733) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4734) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4750)
  (<0>,4752) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4754) fake_sched.h:43: return __running_cpu;
  (<0>,4758) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4761) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4762) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4763) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4767) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4768) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4769) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4771) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4779) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4781) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4783) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4784) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4787) tree_plugin.h:2457: }
  (<0>,4790) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4793) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4794) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4795) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4799) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4800) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4801) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4803) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4808) fake_sched.h:43: return __running_cpu;
  (<0>,4811) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4813) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4815) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4816) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4819) tree_plugin.h:2457: }
  (<0>,4822) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4825) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4826) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4827) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4831) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4832) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4833) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4835) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4842) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4845) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4846) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4847) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4849) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4850) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4852) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4853) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4854) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4855) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4869) tree_plugin.h:2879: }
  (<0>,4871) tree.c:758: local_irq_restore(flags);
  (<0>,4874) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4876) fake_sched.h:43: return __running_cpu;
  (<0>,4880) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4882) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4886) fake_sched.h:43: return __running_cpu;
  (<0>,4890) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4897) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4899) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4901) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4904) litmus.c:123: set_cpu(i);
  (<0>,4907)
  (<0>,4908) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4909) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4911) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4929)
  (<0>,4930) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4931) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4932) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4936) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4937) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4938) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4940) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4948) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4950) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4952) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4953) tree.c:3900: rnp = rdp->mynode;
  (<0>,4955) tree.c:3900: rnp = rdp->mynode;
  (<0>,4956) tree.c:3900: rnp = rdp->mynode;
  (<0>,4957) tree.c:3901: mask = rdp->grpmask;
  (<0>,4959) tree.c:3901: mask = rdp->grpmask;
  (<0>,4960) tree.c:3901: mask = rdp->grpmask;
  (<0>,4964) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4965) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4966) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4968) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4972)
  (<0>,4973)
  (<0>,4974) fake_sync.h:83: local_irq_save(flags);
  (<0>,4977) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:43: return __running_cpu;
  (<0>,4983) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4985) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4989) fake_sched.h:43: return __running_cpu;
  (<0>,4993) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4999) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5000) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5007) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5008) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5010) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5012) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5013) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5014) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5016) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5018) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5022) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5023) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5024) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5026) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5030)
  (<0>,5031)
  (<0>,5032) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5033) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5036) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5039) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5041) fake_sched.h:43: return __running_cpu;
  (<0>,5045) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5047) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5051) fake_sched.h:43: return __running_cpu;
  (<0>,5055) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5064) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5067) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5068) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5069) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5073) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5074) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5075) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5077) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5082) fake_sched.h:43: return __running_cpu;
  (<0>,5085) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5087) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5089) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5090) tree.c:3900: rnp = rdp->mynode;
  (<0>,5092) tree.c:3900: rnp = rdp->mynode;
  (<0>,5093) tree.c:3900: rnp = rdp->mynode;
  (<0>,5094) tree.c:3901: mask = rdp->grpmask;
  (<0>,5096) tree.c:3901: mask = rdp->grpmask;
  (<0>,5097) tree.c:3901: mask = rdp->grpmask;
  (<0>,5101) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5102) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5103) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5105) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5109)
  (<0>,5110)
  (<0>,5111) fake_sync.h:83: local_irq_save(flags);
  (<0>,5114) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5116) fake_sched.h:43: return __running_cpu;
  (<0>,5120) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5122) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5126) fake_sched.h:43: return __running_cpu;
  (<0>,5130) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5136) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5137) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5144) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5145) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5147) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5149) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5150) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5151) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5153) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5155) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5159) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5160) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5161) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5163) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5167)
  (<0>,5168)
  (<0>,5169) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5170) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5173) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5176) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5178) fake_sched.h:43: return __running_cpu;
  (<0>,5182) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5184) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5188) fake_sched.h:43: return __running_cpu;
  (<0>,5192) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5201) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5204) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5205) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5206) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5210) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5211) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5212) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5214) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5221) tree.c:755: local_irq_save(flags);
  (<0>,5224) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5226) fake_sched.h:43: return __running_cpu;
  (<0>,5230) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5232) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5236) fake_sched.h:43: return __running_cpu;
  (<0>,5240) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5252) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5254) fake_sched.h:43: return __running_cpu;
  (<0>,5258) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5259) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5261) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5262) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5263) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5264) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5265) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5266) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5267) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,5271) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5273) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5274) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5275) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5291)
  (<0>,5293) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5295) fake_sched.h:43: return __running_cpu;
  (<0>,5299) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5302) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5303) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5304) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5308) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5309) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5310) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5312) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5317) fake_sched.h:43: return __running_cpu;
  (<0>,5320) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5322) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5324) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5325) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5328) tree_plugin.h:2457: }
  (<0>,5331) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5334) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5335) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5336) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5340) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5341) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5342) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5344) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5349) fake_sched.h:43: return __running_cpu;
  (<0>,5352) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5354) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5356) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5357) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5360) tree_plugin.h:2457: }
  (<0>,5363) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5366) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5367) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5368) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5372) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5373) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5374) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5376) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5383) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5386) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5387) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5388) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5390) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5391) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5393) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5394) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5395) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5396) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5410) tree_plugin.h:2879: }
  (<0>,5412) tree.c:758: local_irq_restore(flags);
  (<0>,5415) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5417) fake_sched.h:43: return __running_cpu;
  (<0>,5421) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5423) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5427) fake_sched.h:43: return __running_cpu;
  (<0>,5431) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5438) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5440) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5442) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5462) tree.c:3971: unsigned long flags;
  (<0>,5463) tree.c:3972: int kthread_prio_in = kthread_prio;
  (<0>,5464) tree.c:3973: struct rcu_node *rnp;
  (<0>,5467) tree.c:3983: else if (kthread_prio > 99)
  (<0>,5471) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5472) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5475) tree.c:3989: rcu_scheduler_fully_active = 1;
  (<0>,5476) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5477) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5478) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5482) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5483) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5484) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5486) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5490) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5492) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5494) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5503)
  (<0>,5504) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5505) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5506) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5507) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5508) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5511)
  (<0>,5512)
  (<0>,5519)
  (<0>,5520)
  (<0>,5527)
  (<0>,5528)
  (<0>,5535)
  (<0>,5536)
  (<0>,5543)
  (<0>,5544)
  (<0>,5551)
  (<0>,5552)
  (<0>,5559)
  (<0>,5560)
  (<0>,5567)
  (<0>,5568)
  (<0>,5575)
  (<0>,5576)
  (<0>,5583)
  (<0>,5584) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5593) fake_defs.h:868: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,5599) fake_defs.h:870: task = malloc(sizeof(*task));
  (<0>,5600) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5602) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5604) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5605) fake_defs.h:872: task->tid = t;
  (<0>,5606) fake_defs.h:872: task->tid = t;
  (<0>,5608) fake_defs.h:872: task->tid = t;
  (<0>,5609) fake_defs.h:873: return task;
  (<0>,5610) fake_defs.h:873: return task;
  (<0>,5612) fake_defs.h:876: }
  (<0>,5614) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5615) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5618)
  (<0>,5619) tree.c:625: return &rsp->node[0];
  (<0>,5623) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5627) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5628) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5629) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5631) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5635)
  (<0>,5636)
  (<0>,5637) fake_sync.h:83: local_irq_save(flags);
  (<0>,5640) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5642) fake_sched.h:43: return __running_cpu;
  (<0>,5646) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5648) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5652) fake_sched.h:43: return __running_cpu;
  (<0>,5656) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5662) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5663) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5670) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5671) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5673) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5674) tree.c:3996: if (kthread_prio) {
  (<0>,5680) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5681) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5682) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5684) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5688)
  (<0>,5689)
  (<0>,5690) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5691) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5694) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5697) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5699) fake_sched.h:43: return __running_cpu;
  (<0>,5703) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5705) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5709) fake_sched.h:43: return __running_cpu;
  (<0>,5713) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5724) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5727) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5728) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5729) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5733) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5734) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5735) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5737) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5741) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5743) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5745) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5754)
  (<0>,5755)
  (<0>,5756)
  (<0>,5757)
  (<0>,5758) fake_defs.h:865: struct task_struct *task = NULL;
  (<0>,5759) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5762)
  (<0>,5763)
  (<0>,5770)
  (<0>,5771)
  (<0>,5778)
  (<0>,5779)
  (<0>,5786)
  (<0>,5787)
  (<0>,5794)
  (<0>,5795) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5808) fake_defs.h:875: return NULL;
  (<0>,5810) fake_defs.h:876: }
  (<0>,5812) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5813) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5816)
  (<0>,5817) tree.c:625: return &rsp->node[0];
  (<0>,5821) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5825) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5826) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5827) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5829) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5833)
  (<0>,5834)
  (<0>,5835) fake_sync.h:83: local_irq_save(flags);
  (<0>,5838) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5840) fake_sched.h:43: return __running_cpu;
  (<0>,5844) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5846) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5850) fake_sched.h:43: return __running_cpu;
  (<0>,5854) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5860) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5861) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5868) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5869) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5871) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5872) tree.c:3996: if (kthread_prio) {
  (<0>,5878) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5879) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5880) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5882) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5886)
  (<0>,5887)
  (<0>,5888) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5889) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5892) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5895) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5897) fake_sched.h:43: return __running_cpu;
  (<0>,5901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5903) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5907) fake_sched.h:43: return __running_cpu;
  (<0>,5911) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5922) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5925) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5926) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5927) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5931) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5932) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5933) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5935) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5949)
  (<0>,5950) litmus.c:51: void *thread_reader(void *arg)
  (<0>,5953)
  (<0>,5954) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5955) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5958) fake_sched.h:43: return __running_cpu;
  (<0>,5962)
  (<0>,5963) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,5966) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,5971) tree.c:890: unsigned long flags;
  (<0>,5974) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5976) fake_sched.h:43: return __running_cpu;
  (<0>,5980) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5982) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5986) fake_sched.h:43: return __running_cpu;
  (<0>,5990) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6002) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6004) fake_sched.h:43: return __running_cpu;
  (<0>,6008) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6009) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6011) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6012) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6013) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6014) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6015) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6016) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6017) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,6021) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6023) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6024) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6025) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6036)
  (<0>,6037) tree.c:830: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,6039) fake_sched.h:43: return __running_cpu;
  (<0>,6043) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6047) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6050) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6051) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6052) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6054) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6055) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6057) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6058) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6059) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6060) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6070) tree_plugin.h:2883: }
  (<0>,6072) tree.c:895: local_irq_restore(flags);
  (<0>,6075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6077) fake_sched.h:43: return __running_cpu;
  (<0>,6081) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6083) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6087) fake_sched.h:43: return __running_cpu;
  (<0>,6091) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6106) litmus.c:57: r_x = x;
  (<0>,6107) litmus.c:57: r_x = x;
  (<0>,6111) fake_sched.h:43: return __running_cpu;
  (<0>,6115) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
  (<0>,6119) fake_sched.h:43: return __running_cpu;
  (<0>,6123) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6128) fake_sched.h:43: return __running_cpu;
  (<0>,6132) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
  (<0>,6143) fake_sched.h:43: return __running_cpu;
  (<0>,6147) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6148) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6150) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6151) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6152) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6154) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6156) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6157) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6158) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6159) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6160) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6161) tree.c:942: if (oldval)
  (<0>,6169) tree_plugin.h:2883: }
  (<0>,6175) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,6184) tree_plugin.h:1669: struct rcu_state *rsp;
  (<0>,6185) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6186) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6190) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6191) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6192) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6194) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6199) fake_sched.h:43: return __running_cpu;
  (<0>,6202) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6204) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6207) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6209) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6211) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6214) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6215) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6216) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6220) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6221) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6222) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6224) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6229) fake_sched.h:43: return __running_cpu;
  (<0>,6232) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6234) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6237) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6239) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6241) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6244) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6245) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6246) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6250) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6251) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6252) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6254) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6259) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,6264) fake_sched.h:43: return __running_cpu;
  (<0>,6269) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,6277) fake_sched.h:43: return __running_cpu;
  (<0>,6283) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
  (<0>,6289) fake_sched.h:43: return __running_cpu;
  (<0>,6296) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
  (<0>,6309) tree.c:3557: struct rcu_state *rsp;
  (<0>,6310) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6311) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6315) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6316) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6317) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6319) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6323) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6325) fake_sched.h:43: return __running_cpu;
  (<0>,6328) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6330) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6352)
  (<0>,6353) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6354) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6356) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6357) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6358) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6360) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6362) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6363) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6364) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6399)
  (<0>,6400) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6401) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6404) tree.c:1469: !rcu_gp_in_progress(rsp))
  (<0>,6417)
  (<0>,6418) tree.c:237: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,6423) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6424) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6425) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6426) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6428) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6430) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6431) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6433) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6436) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6437) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6438) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6439) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6444) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6445) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6446) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6447) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6449) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6451) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6452) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6454) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6457) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6458) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6459) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6467) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
  (<0>,6470) tree_plugin.h:2923: return false;
  (<0>,6473) tree.c:3503: if (rcu_scheduler_fully_active &&
  (<0>,6476) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6478) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6481) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6483) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6487) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,6490)
  (<0>,6491) tree.c:614: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,6493) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6496) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6503) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6504) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6515)
  (<0>,6516) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6517) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6530)
  (<0>,6531) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6536) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6537) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6538) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6539) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6541) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6543) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6544) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6546) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6549) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6550) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6551) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6552) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6557) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6560) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6562) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6564) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6565) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6567) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6570) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6571) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6572) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6578) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,6594)
  (<0>,6595) tree.c:633: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,6598)
  (<0>,6599) tree.c:625: return &rsp->node[0];
  (<0>,6603) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6604) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6609) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6610) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6611) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6612) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6614) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6616) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6617) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6619) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6622) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6623) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6624) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6628) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6629) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6631) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6634) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6635) tree.c:639: return READ_ONCE(*fp);
  (<0>,6639) tree.c:639: return READ_ONCE(*fp);
  (<0>,6640) tree.c:639: return READ_ONCE(*fp);
  (<0>,6641) tree.c:639: return READ_ONCE(*fp);
  (<0>,6642) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6644) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6646) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6647) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6649) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6652) tree.c:639: return READ_ONCE(*fp);
  (<0>,6653) tree.c:639: return READ_ONCE(*fp);
  (<0>,6654) tree.c:639: return READ_ONCE(*fp);
  (<0>,6658) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6661) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6664) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6667) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6668) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6671) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6673) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6676) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6679) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6682) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6683) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6685) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6688) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6692) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6694) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6696) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6699) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6702) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6705) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6706) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6708) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6711) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6715) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6717) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6719) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6722) tree.c:665: return false; /* No grace period needed. */
  (<0>,6724) tree.c:666: }
  (<0>,6727) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6732) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6733) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6734) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6735) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6737) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6739) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6740) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6742) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6745) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6746) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6747) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6748) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6750) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6753) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6758) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6759) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6760) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6761) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6763) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,1)
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:892: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,55) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,57) fake_sched.h:43: return __running_cpu;
      (<0.1>,61) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,62) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,64) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,67) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,68) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,69) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,70) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,74) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,76) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,77) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,78) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,89)
      (<0.1>,90) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,92) fake_sched.h:43: return __running_cpu;
      (<0.1>,96) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,100) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,103) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,104) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,105) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,107) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,110) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,111) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,112) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,113) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,123) tree_plugin.h:2883: }
      (<0.1>,125) tree.c:895: local_irq_restore(flags);
      (<0.1>,128) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,130) fake_sched.h:43: return __running_cpu;
      (<0.1>,134) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,140) fake_sched.h:43: return __running_cpu;
      (<0.1>,144) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,151) litmus.c:87: x = 1;
      (<0.1>,163) tree.c:3255: ret = num_online_cpus() <= 1;
      (<0.1>,165) tree.c:3257: return ret;
      (<0.1>,172) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,175) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,176) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,177) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,178) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,181) update.c:148: rcu_scheduler_active == RCU_SCHEDULER_INIT;
      (<0.1>,198)
      (<0.1>,199)
      (<0.1>,200)
      (<0.1>,201)
      (<0.1>,202) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,204) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,205) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,208) update.c:352: if (checktiny &&
      (<0.1>,211) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,213) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,218) rcupdate.h:476: }
      (<0.1>,220) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,222) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,227)
      (<0.1>,228) fake_sync.h:272: x->done = 0;
      (<0.1>,230) fake_sync.h:272: x->done = 0;
      (<0.1>,234) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,236) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,238) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,239) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,241) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,247)
      (<0.1>,248)
      (<0.1>,249) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,250) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,281)
      (<0.1>,282)
      (<0.1>,283)
      (<0.1>,284)
      (<0.1>,286)
      (<0.1>,287) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,294) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,295) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,301) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,302) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,303) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,304) tree.c:3147: if (debug_rcu_head_queue(head)) {
      (<0.1>,307) rcu.h:92: return 0;
      (<0.1>,311) tree.c:3153: head->func = func;
      (<0.1>,312) tree.c:3153: head->func = func;
      (<0.1>,315) tree.c:3153: head->func = func;
      (<0.1>,316) tree.c:3154: head->next = NULL;
      (<0.1>,318) tree.c:3154: head->next = NULL;
      (<0.1>,319) tree.c:3162: local_irq_save(flags);
      (<0.1>,322) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,324) fake_sched.h:43: return __running_cpu;
      (<0.1>,328) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,330) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,334) fake_sched.h:43: return __running_cpu;
      (<0.1>,338) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,344) fake_sched.h:43: return __running_cpu;
      (<0.1>,347) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,349) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,351) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,352) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,355) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,363) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,367) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,369) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,371) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,372) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,377) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,378) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,379) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,380) fake_defs.h:237: switch (size) {
      (<0.1>,382) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,384) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,385) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,387) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,390) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,391) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,392) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,393) tree.c:3189: if (lazy)
      (<0.1>,400) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,401) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,404) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,405) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,406) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,408) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,411) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,412) tree.c:3197: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,419) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,420) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,421) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,422) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,430)
      (<0.1>,431)
      (<0.1>,432)
      (<0.1>,433) tree.c:3086: if (!rcu_is_watching())
      (<0.1>,440) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,442) fake_sched.h:43: return __running_cpu;
      (<0.1>,448) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,449) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,450) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,455) tree.c:1060: ret = __rcu_is_watching();
      (<0.1>,457) tree.c:1062: return ret;
      (<0.1>,461) tree.c:3090: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,464) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,466) fake_sched.h:43: return __running_cpu;
      (<0.1>,470) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,480) tree.c:3205: local_irq_restore(flags);
      (<0.1>,483) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,485) fake_sched.h:43: return __running_cpu;
      (<0.1>,489) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,491) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,495) fake_sched.h:43: return __running_cpu;
      (<0.1>,499) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,508) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,510) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,512) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,513) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,516) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,518) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,519) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,522) update.c:365: if (checktiny &&
      (<0.1>,525) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,527) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,532) fake_sync.h:278: might_sleep();
      (<0.1>,536) fake_sched.h:43: return __running_cpu;
      (<0.1>,540) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,543) tree.c:755: local_irq_save(flags);
      (<0.1>,546) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,548) fake_sched.h:43: return __running_cpu;
      (<0.1>,552) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,554) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,558) fake_sched.h:43: return __running_cpu;
      (<0.1>,562) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,574) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,576) fake_sched.h:43: return __running_cpu;
      (<0.1>,580) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,581) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,583) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,584) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,585) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,586) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,587) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,588) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,589) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,593) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,595) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,596) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,597) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,613)
      (<0.1>,615) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,617) fake_sched.h:43: return __running_cpu;
      (<0.1>,621) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,624) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,625) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,626) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,630) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,631) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,632) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,639) fake_sched.h:43: return __running_cpu;
      (<0.1>,642) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,644) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,646) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,647) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,650) tree_plugin.h:2457: }
      (<0.1>,653) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,656) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,657) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,658) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,662) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,663) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,664) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,666) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,671) fake_sched.h:43: return __running_cpu;
      (<0.1>,674) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,676) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,678) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,679) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,682) tree_plugin.h:2457: }
      (<0.1>,685) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,688) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,689) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,690) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,694) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,695) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,696) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,698) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,705) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,708) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,709) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,710) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,712) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,713) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,715) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,716) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,717) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,718) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,732) tree_plugin.h:2879: }
      (<0.1>,734) tree.c:758: local_irq_restore(flags);
      (<0.1>,737) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,739) fake_sched.h:43: return __running_cpu;
      (<0.1>,743) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,745) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,749) fake_sched.h:43: return __running_cpu;
      (<0.1>,753) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,759) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,762) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:892: local_irq_save(flags);
    (<0.0>,35) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,63) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,65) fake_sched.h:43: return __running_cpu;
    (<0.0>,69) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,70) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,72) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,75) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,76) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,77) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,78) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,82) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,84) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,85) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,86) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,97)
    (<0.0>,98) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,100) fake_sched.h:43: return __running_cpu;
    (<0.0>,104) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,111) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,112) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,113) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,115) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,116) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,118) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,119) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,120) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,121) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,131) tree_plugin.h:2883: }
    (<0.0>,133) tree.c:895: local_irq_restore(flags);
    (<0.0>,136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,138) fake_sched.h:43: return __running_cpu;
    (<0.0>,142) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,144) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,148) fake_sched.h:43: return __running_cpu;
    (<0.0>,152) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,159) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,207)
    (<0.0>,208) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,211) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,214)
    (<0.0>,215) tree.c:625: return &rsp->node[0];
    (<0.0>,219) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,227) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,233) fake_sched.h:43: return __running_cpu;
    (<0.0>,237) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,241) fake_sched.h:43: return __running_cpu;
    (<0.0>,245) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,250) fake_sched.h:43: return __running_cpu;
    (<0.0>,254) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,270) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,272) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,273) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,274) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,276) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,278) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,279) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,280) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,281) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,282) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,283) tree.c:942: if (oldval)
    (<0.0>,291) tree_plugin.h:2883: }
    (<0.0>,297) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,306) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,307) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,308) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,312) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,313) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,314) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,316) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,321) fake_sched.h:43: return __running_cpu;
    (<0.0>,324) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,326) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,329) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,331) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,333) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,336) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,337) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,338) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,342) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,343) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,344) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,346) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,351) fake_sched.h:43: return __running_cpu;
    (<0.0>,354) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,356) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,359) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,361) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,363) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,366) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,367) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,368) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,372) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,373) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,374) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,376) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,381) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,386) fake_sched.h:43: return __running_cpu;
    (<0.0>,391) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,399) fake_sched.h:43: return __running_cpu;
    (<0.0>,405) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,411) fake_sched.h:43: return __running_cpu;
    (<0.0>,418) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,431) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,432) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,433) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,437) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,438) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,439) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,441) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,445) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,447) fake_sched.h:43: return __running_cpu;
    (<0.0>,450) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,452) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,474)
    (<0.0>,475)
    (<0.0>,476) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,478) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,479) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,480) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,482) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,484) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,485) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,486) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,521)
    (<0.0>,522)
    (<0.0>,523) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,526) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,539)
    (<0.0>,540) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,545) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,546) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,547) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,548) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,550) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,552) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,553) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,555) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,560) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,561) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,566) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,567) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,569) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,571) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,573) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,574) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,576) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,579) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,580) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,581) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,589) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,592) tree_plugin.h:2923: return false;
    (<0.0>,595) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,598) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,600) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,603) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,605) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,609) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,612)
    (<0.0>,613) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,615) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,618) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,625) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,626) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,637)
    (<0.0>,638)
    (<0.0>,639) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,652)
    (<0.0>,653) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,658) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,659) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,660) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,661) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,663) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,665) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,666) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,668) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,671) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,672) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,673) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,674) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,679) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,680) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,681) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,682) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,684) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,686) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,692) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,693) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,694) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,700) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,716)
    (<0.0>,717) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,720)
    (<0.0>,721) tree.c:625: return &rsp->node[0];
    (<0.0>,725) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,726) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,731) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,732) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,733) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,734) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,736) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,739) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,744) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,745) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,746) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,750) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,751) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,753) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,756) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,757) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,761) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,762) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,763) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,774) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,775) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,776) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,780) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,783) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,786) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,789) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,790) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,793) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,795) tree.c:666: }
    (<0.0>,798) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,800) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,802) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,803) tree.c:3523: return 1;
    (<0.0>,805) tree.c:3548: }
    (<0.0>,809) tree.c:3561: return 1;
    (<0.0>,811) tree.c:3563: }
    (<0.0>,818) fake_sched.h:43: return __running_cpu;
    (<0.0>,822) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,826) tree.c:2891: if (user)
    (<0.0>,834) fake_sched.h:43: return __running_cpu;
    (<0.0>,838) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,840) fake_sched.h:43: return __running_cpu;
    (<0.0>,844) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,850) fake_sched.h:43: return __running_cpu;
    (<0.0>,854) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,864) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,867) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,868) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,869) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,873) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,874) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,875) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,877) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,881) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,893) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,895) fake_sched.h:43: return __running_cpu;
    (<0.0>,898) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,900) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,902) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,903) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,905) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,912) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,913) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,915) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,921) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,922) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,923) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,924) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,925) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,929)
    (<0.0>,930)
    (<0.0>,931) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,932) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,957)
    (<0.0>,958)
    (<0.0>,959) tree.c:1905: local_irq_save(flags);
    (<0.0>,962) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,964) fake_sched.h:43: return __running_cpu;
    (<0.0>,968) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,970) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,974) fake_sched.h:43: return __running_cpu;
    (<0.0>,978) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,983) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,985) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,986) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,987) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,989) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,990) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,995) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,996) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,997) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,998) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1000) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1002) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1003) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1005) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1008) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1009) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1010) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1013) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1015) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1016) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1021) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1022) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1023) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1024) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1026) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1028) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1029) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1031) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1034) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1035) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1036) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1039) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1043) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1044) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1045) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1046) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1048) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1050) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1054) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1057) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1058) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1066) tree.c:1911: local_irq_restore(flags);
    (<0.0>,1069) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1071) fake_sched.h:43: return __running_cpu;
    (<0.0>,1075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1077) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1081) fake_sched.h:43: return __running_cpu;
    (<0.0>,1085) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1092) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1094) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1099) tree.c:3016: local_irq_save(flags);
    (<0.0>,1102) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1104) fake_sched.h:43: return __running_cpu;
    (<0.0>,1108) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1110) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1114) fake_sched.h:43: return __running_cpu;
    (<0.0>,1118) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1123) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1124) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1135)
    (<0.0>,1136)
    (<0.0>,1137) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,1150)
    (<0.0>,1151) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1156) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1157) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1158) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1161) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1163) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1164) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1166) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1169) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1170) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1171) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1172) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1177) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1178) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1179) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1180) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1182) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1184) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1185) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1187) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1190) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1191) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1192) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1198) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,1214)
    (<0.0>,1215) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1218)
    (<0.0>,1219) tree.c:625: return &rsp->node[0];
    (<0.0>,1223) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1224) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1229) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1230) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1231) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1232) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1234) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1236) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1237) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1239) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1242) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1243) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1244) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1248) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1249) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1251) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1254) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1255) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1259) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1260) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1261) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1262) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1264) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1266) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1267) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1269) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1272) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1273) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1274) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1278) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1281) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1284) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1287) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1288) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1291) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,1293) tree.c:666: }
    (<0.0>,1296) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,1299)
    (<0.0>,1300) tree.c:625: return &rsp->node[0];
    (<0.0>,1306)
    (<0.0>,1307) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,1311) fake_sync.h:115: preempt_disable();
    (<0.0>,1313) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1314) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1321) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,1327) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1329) fake_sched.h:43: return __running_cpu;
    (<0.0>,1332) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1334) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1336) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1337) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1340)
    (<0.0>,1341) tree.c:625: return &rsp->node[0];
    (<0.0>,1345) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1346) tree.c:2334: bool ret = false;
    (<0.0>,1347) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1348) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1349) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1357)
    (<0.0>,1358)
    (<0.0>,1359)
    (<0.0>,1360) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1363) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1366) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1369) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1370) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1373) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1375) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1378) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1380) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1381) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1383) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1386) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1391) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1393) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1394) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1397) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1399) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1402) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1404) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1407) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1408) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1411) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1414) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1416) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1419) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1420) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1422) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1425) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1426) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1428) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1431) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1432) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1434) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1437) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1439) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1441) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1442) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1444) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1446) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1449) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1451) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1454) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1455) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1458) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1461) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1463) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1466) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1467) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1469) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1472) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1473) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1475) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1478) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1479) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1481) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1484) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1486) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1488) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1489) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1491) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1493) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1496) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1497) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1498) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1507)
    (<0.0>,1508)
    (<0.0>,1509)
    (<0.0>,1510) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1513) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1516) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1519) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1520) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1523) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1524) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1529)
    (<0.0>,1530)
    (<0.0>,1531) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1534)
    (<0.0>,1535) tree.c:625: return &rsp->node[0];
    (<0.0>,1539) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1542) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1544) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1545) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1547) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1550) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1552) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1554) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1556) tree.c:1585: }
    (<0.0>,1558) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1559) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1561) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1564) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1566) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1569) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1570) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1573) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1576) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1580) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1582) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1584) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1587) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1589) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1592) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1593) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1596) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1599) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1603) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1605) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1607) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1610) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1612) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1616) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1619) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1622) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1623) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1625) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1628) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1629) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1630) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1632) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1635) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1637) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1639) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1641) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1644) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1647) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1648) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1650) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1653) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1654) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1655) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1657) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1660) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1662) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1664) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1666) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1669) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1672) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1673) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1675) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1678) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1679) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1680) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1682) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1685) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1687) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1689) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1691) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1694) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1695) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1714)
    (<0.0>,1715)
    (<0.0>,1716)
    (<0.0>,1717) tree.c:1613: bool ret = false;
    (<0.0>,1718) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1720) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1723)
    (<0.0>,1724) tree.c:625: return &rsp->node[0];
    (<0.0>,1728) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1729) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1731) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1732) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1737)
    (<0.0>,1738)
    (<0.0>,1739) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1742)
    (<0.0>,1743) tree.c:625: return &rsp->node[0];
    (<0.0>,1747) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1750) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1752) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1753) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1755) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1758) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1760) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1762) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1764) tree.c:1585: }
    (<0.0>,1766) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1767) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1768) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1769) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1775)
    (<0.0>,1776)
    (<0.0>,1777)
    (<0.0>,1778) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1782) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1784) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1787) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1790) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1792) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1793) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1795) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1798) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1803) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1804) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1805) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1806) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1808) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1810) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1811) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1813) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1816) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1817) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1818) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1819) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1824) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1825) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1826) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1834) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1837) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1838) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1839) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1842) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1843) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1846) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1848) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1849) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1854)
    (<0.0>,1855)
    (<0.0>,1856) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1859)
    (<0.0>,1860) tree.c:625: return &rsp->node[0];
    (<0.0>,1864) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1867) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1869) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1870) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1872) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1875) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1877) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1879) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1881) tree.c:1585: }
    (<0.0>,1883) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1884) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1886) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1889) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1890) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1892) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1895) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1899) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1900) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1902) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1905) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1908) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1910) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1912) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1915) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1916) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1918) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1921) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1926) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1928) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1930) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1933) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1934) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1936) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1939) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1944) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1946) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1948) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1951) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1953) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1956) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1959) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1961) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1964) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1966) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1967) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1969) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1970) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1972) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1975) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1976) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1977) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1983)
    (<0.0>,1984)
    (<0.0>,1985)
    (<0.0>,1986) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1990) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1992) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1993) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1994) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2005)
    (<0.0>,2006)
    (<0.0>,2007)
    (<0.0>,2008) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2010) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2013) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2014) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2025)
    (<0.0>,2026)
    (<0.0>,2027) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2040)
    (<0.0>,2041) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2046) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2047) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2048) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2053) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2054) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2056) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2059) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2060) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2061) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2062) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2067) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2068) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2069) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2070) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2072) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2074) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2075) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2077) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2080) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2081) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2082) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2088) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2104)
    (<0.0>,2105) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2108)
    (<0.0>,2109) tree.c:625: return &rsp->node[0];
    (<0.0>,2113) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2114) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2119) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2120) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2121) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2122) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2124) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2126) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2127) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2129) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2132) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2133) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2134) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2138) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2139) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2141) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2144) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2145) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2149) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2150) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2151) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2152) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2154) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2156) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2157) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2162) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2163) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2164) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2168) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2170) tree.c:666: }
    (<0.0>,2175) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2180) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2181) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2182) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2183) fake_defs.h:237: switch (size) {
    (<0.0>,2185) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2187) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2188) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2190) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2193) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2194) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2195) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2198) tree.c:2318: return true;
    (<0.0>,2200) tree.c:2319: }
    (<0.0>,2203) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2206) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2207) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2211) tree.c:1689: if (c_out != NULL)
    (<0.0>,2214) tree.c:1691: return ret;
    (<0.0>,2218) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,2219) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2222) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2223) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2229) tree.c:1798: return ret;
    (<0.0>,2231) tree.c:1798: return ret;
    (<0.0>,2233) tree.c:1799: }
    (<0.0>,2235) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,2237) tree.c:1843: }
    (<0.0>,2241) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,2242) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2243) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2244) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2255)
    (<0.0>,2256)
    (<0.0>,2257)
    (<0.0>,2258) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2260) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2263) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2264) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2275)
    (<0.0>,2276)
    (<0.0>,2277) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2290)
    (<0.0>,2291) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2296) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2297) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2298) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2299) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2301) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2303) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2304) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2306) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2309) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2310) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2311) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2312) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2317) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2318) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2320) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2322) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2324) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2325) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2327) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2330) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2331) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2332) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2338) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2354)
    (<0.0>,2355) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2358)
    (<0.0>,2359) tree.c:625: return &rsp->node[0];
    (<0.0>,2363) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2364) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2369) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2370) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2371) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2372) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2374) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2376) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2377) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2379) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2382) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2383) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2384) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2388) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2389) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2391) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2394) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2395) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2399) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2400) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2401) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2407) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2409) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2412) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2413) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2414) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2418) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2420) tree.c:666: }
    (<0.0>,2425) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2430) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2431) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2432) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2433) fake_defs.h:237: switch (size) {
    (<0.0>,2435) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2437) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2438) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2440) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2443) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2444) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2445) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2448) tree.c:2318: return true;
    (<0.0>,2450) tree.c:2319: }
    (<0.0>,2454) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2455) tree.c:2346: return ret;
    (<0.0>,2459) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,2463) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2464) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2465) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2468)
    (<0.0>,2469) tree.c:625: return &rsp->node[0];
    (<0.0>,2474) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2478)
    (<0.0>,2479)
    (<0.0>,2480) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2481) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2484) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,2487) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2489) fake_sched.h:43: return __running_cpu;
    (<0.0>,2493) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2495) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2499) fake_sched.h:43: return __running_cpu;
    (<0.0>,2503) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2511) tree.c:3021: if (needwake)
    (<0.0>,2514) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,2522)
    (<0.0>,2523) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2524) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2526) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2533) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2536)
    (<0.0>,2537) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2539) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2542) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2549) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2552) tree_plugin.h:2457: }
    (<0.0>,2556) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2559) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2560) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2561) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2565) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2566) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2567) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2569) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2573) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,2585) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2587) fake_sched.h:43: return __running_cpu;
    (<0.0>,2590) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2592) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2594) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2595) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2597) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2604) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2605) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2607) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2613) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2614) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2615) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2616) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2617) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2621)
    (<0.0>,2622)
    (<0.0>,2623) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2624) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2649)
    (<0.0>,2650)
    (<0.0>,2651) tree.c:1905: local_irq_save(flags);
    (<0.0>,2654) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2656) fake_sched.h:43: return __running_cpu;
    (<0.0>,2660) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2662) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2666) fake_sched.h:43: return __running_cpu;
    (<0.0>,2670) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2675) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2677) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2678) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2679) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2681) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2682) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2687) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2688) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2689) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2690) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2695) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2697) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2700) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2701) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2702) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2705) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2707) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2708) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2713) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2714) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2715) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2716) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2718) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2720) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2721) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2723) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2726) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2727) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2728) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2731) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2735) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2736) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2737) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2740) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2742) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2743) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2746) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2749) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2750) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2758) tree.c:1911: local_irq_restore(flags);
    (<0.0>,2761) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2763) fake_sched.h:43: return __running_cpu;
    (<0.0>,2767) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2769) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2773) fake_sched.h:43: return __running_cpu;
    (<0.0>,2777) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2784) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2786) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2791) tree.c:3016: local_irq_save(flags);
    (<0.0>,2794) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2802) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2806) fake_sched.h:43: return __running_cpu;
    (<0.0>,2810) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2815) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2816) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2827)
    (<0.0>,2828)
    (<0.0>,2829) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2842)
    (<0.0>,2843) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2849) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2850) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2855) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2858) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2863) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2864) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2870) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2871) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2872) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2877) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2879) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2882) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2883) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2890) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2906)
    (<0.0>,2907) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2910)
    (<0.0>,2911) tree.c:625: return &rsp->node[0];
    (<0.0>,2915) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2916) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2921) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2922) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2923) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2924) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2926) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2929) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2934) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2935) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2936) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2940) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2941) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2943) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2946) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2947) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2951) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2952) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2953) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2954) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2956) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2959) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2964) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2965) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2966) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2970) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2973) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2976) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2979) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2980) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2983) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2985) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2988) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2991) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2994) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2995) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2997) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3000) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3004) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3006) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3008) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3011) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3014) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3017) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3018) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3020) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3023) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3027) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3029) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3031) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3034) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,3036) tree.c:666: }
    (<0.0>,3039) tree.c:3024: local_irq_restore(flags);
    (<0.0>,3042) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3044) fake_sched.h:43: return __running_cpu;
    (<0.0>,3048) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3050) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3054) fake_sched.h:43: return __running_cpu;
    (<0.0>,3058) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3064) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,3067)
    (<0.0>,3068) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3070) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3073) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3080) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3083) tree_plugin.h:2457: }
    (<0.0>,3087) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3090) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3091) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3092) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3096) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3097) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3098) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3100) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3108) fake_sched.h:43: return __running_cpu;
    (<0.0>,3112) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,3122) fake_sched.h:43: return __running_cpu;
    (<0.0>,3126) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3127) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3129) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3130) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3131) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3133) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3135) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3136) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3137) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3138) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3139) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3140) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3142) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3150) tree_plugin.h:2879: }
    (<0.0>,3156) fake_sched.h:43: return __running_cpu;
    (<0.0>,3160) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,3163) tree.c:755: local_irq_save(flags);
    (<0.0>,3166) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3168) fake_sched.h:43: return __running_cpu;
    (<0.0>,3172) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3174) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3178) fake_sched.h:43: return __running_cpu;
    (<0.0>,3182) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3194) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3196) fake_sched.h:43: return __running_cpu;
    (<0.0>,3200) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3201) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3203) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3204) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3205) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3206) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3207) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3208) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3209) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3213) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3215) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3216) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3217) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3233)
    (<0.0>,3235) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3237) fake_sched.h:43: return __running_cpu;
    (<0.0>,3241) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3244) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3245) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3246) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3250) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3251) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3252) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3254) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3259) fake_sched.h:43: return __running_cpu;
    (<0.0>,3262) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3264) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3266) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3267) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3270) tree_plugin.h:2457: }
    (<0.0>,3273) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3276) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3277) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3278) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3282) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3283) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3284) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3286) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3291) fake_sched.h:43: return __running_cpu;
    (<0.0>,3294) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3296) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3298) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3299) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3302) tree_plugin.h:2457: }
    (<0.0>,3305) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3308) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3309) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3310) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3314) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3315) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3316) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3318) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3325) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3328) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3329) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3330) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3332) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3333) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3335) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3336) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3337) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3338) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3352) tree_plugin.h:2879: }
    (<0.0>,3354) tree.c:758: local_irq_restore(flags);
    (<0.0>,3357) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3359) fake_sched.h:43: return __running_cpu;
    (<0.0>,3363) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3365) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3369) fake_sched.h:43: return __running_cpu;
    (<0.0>,3373) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3379) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3382) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3387) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3392) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3393) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3394) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3395) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3397) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3399) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3400) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3405) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3406) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3407) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3414) fake_sched.h:43: return __running_cpu;
    (<0.0>,3418)
    (<0.0>,3419) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3422) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3427) tree.c:892: local_irq_save(flags);
    (<0.0>,3430) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3432) fake_sched.h:43: return __running_cpu;
    (<0.0>,3436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3442) fake_sched.h:43: return __running_cpu;
    (<0.0>,3446) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3458) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3460) fake_sched.h:43: return __running_cpu;
    (<0.0>,3464) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3465) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3467) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3468) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3469) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3470) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3471) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3472) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3473) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3477) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3479) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3480) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3481) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3492)
    (<0.0>,3493) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3495) fake_sched.h:43: return __running_cpu;
    (<0.0>,3499) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3503) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3506) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3507) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3508) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3510) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3511) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3513) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3514) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3515) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3516) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3526) tree_plugin.h:2883: }
    (<0.0>,3528) tree.c:895: local_irq_restore(flags);
    (<0.0>,3531) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3533) fake_sched.h:43: return __running_cpu;
    (<0.0>,3537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3539) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3543) fake_sched.h:43: return __running_cpu;
    (<0.0>,3547) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3554) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3556) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3557) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,3602)
    (<0.0>,3603) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3606)
    (<0.0>,3607) tree.c:625: return &rsp->node[0];
    (<0.0>,3611) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3613) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3614) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3615) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3620) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3621) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3622) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3623) fake_defs.h:237: switch (size) {
    (<0.0>,3625) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3627) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3628) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3630) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3633) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3634) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3635) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3636) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,3639)
    (<0.0>,3640) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,3644) fake_sync.h:99: local_irq_disable();
    (<0.0>,3647) fake_sched.h:43: return __running_cpu;
    (<0.0>,3651) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,3655) fake_sched.h:43: return __running_cpu;
    (<0.0>,3659) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3664) fake_sched.h:43: return __running_cpu;
    (<0.0>,3668) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3671) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3672) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3679) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3684) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3685) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3686) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3691) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3697) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3698) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3699) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3710)
    (<0.0>,3716)
    (<0.0>,3721) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3726) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3727) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3728) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3729) fake_defs.h:237: switch (size) {
    (<0.0>,3731) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3733) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3734) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3736) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3739) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3740) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3741) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3742) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3755)
    (<0.0>,3756) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3761) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3762) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3763) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3774) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3775) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3776) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3777) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3782) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3783) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3784) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3785) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3787) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3789) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3790) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3792) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3795) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3796) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3797) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3805) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3806) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3819)
    (<0.0>,3820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3825) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3826) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3827) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3830) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3833) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3835) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3838) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3839) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3840) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3841) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3846) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3847) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3849) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3854) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3859) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3860) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3868) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3869) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3870) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3873) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,3888)
    (<0.0>,3889) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3890) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3891) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3892) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3894) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3915) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3916) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3917) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3918) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3920) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3922) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3923) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3925) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3928) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3929) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3930) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3931) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3932) update.c:466: if (till_stall_check < 3) {
    (<0.0>,3935) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,3939) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,3944) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,3946) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3947) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3949) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3950) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3955) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3956) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3957) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3958) fake_defs.h:237: switch (size) {
    (<0.0>,3960) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3962) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3963) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3965) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3968) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3969) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3970) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3971) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3972) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3975) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3977) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3978) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3983) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3984) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3985) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3986) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3988) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3990) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3991) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3993) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3996) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3997) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3998) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3999) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4001) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4005) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4007) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4009) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4010) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4012) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4013) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4014) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4018) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4021)
    (<0.0>,4022) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4026)
    (<0.0>,4027) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4028) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4033) fake_sched.h:43: return __running_cpu;
    (<0.0>,4037) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4039) fake_sched.h:43: return __running_cpu;
    (<0.0>,4043) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4050) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4053) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4056) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4057) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4059) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4060) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4062) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4067) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4068) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4072)
    (<0.0>,4073)
    (<0.0>,4074) tree.c:1922: if (delay > 0 &&
    (<0.0>,4078) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4081)
    (<0.0>,4082) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4086) fake_sync.h:99: local_irq_disable();
    (<0.0>,4089) fake_sched.h:43: return __running_cpu;
    (<0.0>,4093) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4097) fake_sched.h:43: return __running_cpu;
    (<0.0>,4101) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4106) fake_sched.h:43: return __running_cpu;
    (<0.0>,4110) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4113) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4114) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4121) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4123) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4124) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4126) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4129) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4131) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4132) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4133) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4135) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4136) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4138) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4139) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4143) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4145) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4151) tree.c:1983: if (!oldmask) /* First online CPU for this rcu_node. */
    (<0.0>,4154) tree.c:1984: rcu_init_new_rnp(rnp);
    (<0.0>,4159)
    (<0.0>,4160) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4161) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4163) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4165) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4166) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4167) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4169) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4170) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4171) tree.c:3752: if (rnp == NULL)
    (<0.0>,4177) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4179) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4182) tree.c:2007: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4185)
    (<0.0>,4186) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4190)
    (<0.0>,4191) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4192) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4197) fake_sched.h:43: return __running_cpu;
    (<0.0>,4201) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4203) fake_sched.h:43: return __running_cpu;
    (<0.0>,4207) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4215) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4217) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4219) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4220) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4222) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4227) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4230) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4232) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4233) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4235) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4240) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4241) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4245)
    (<0.0>,4246)
    (<0.0>,4247) tree.c:1922: if (delay > 0 &&
    (<0.0>,4251) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4254)
    (<0.0>,4255) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4259) fake_sync.h:99: local_irq_disable();
    (<0.0>,4262) fake_sched.h:43: return __running_cpu;
    (<0.0>,4266) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4270) fake_sched.h:43: return __running_cpu;
    (<0.0>,4274) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4279) fake_sched.h:43: return __running_cpu;
    (<0.0>,4283) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4286) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4287) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4295) fake_sched.h:43: return __running_cpu;
    (<0.0>,4298) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4300) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4302) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4303) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,4309)
    (<0.0>,4310) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4312) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4317) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4318) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4320) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4324) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4325) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4326) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4328) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4330) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4331) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4333) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4335) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4337) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4338) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4339) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4344) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4345) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4346) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4347) fake_defs.h:237: switch (size) {
    (<0.0>,4349) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4351) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4352) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4354) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
  (<0>,6765) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6766) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6768) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6771) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6772) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6773) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6774) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6776) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6779) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6781) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6783) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6784) tree.c:3536: return 1;
  (<0>,6786) tree.c:3548: }
  (<0>,6790) tree.c:3561: return 1;
  (<0>,6792) tree.c:3563: }
  (<0>,6799) fake_sched.h:43: return __running_cpu;
  (<0>,6803) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
  (<0>,6807) tree.c:2891: if (user)
  (<0>,6815) fake_sched.h:43: return __running_cpu;
  (<0>,6819) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
  (<0>,6821) fake_sched.h:43: return __running_cpu;
  (<0>,6825) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6831) fake_sched.h:43: return __running_cpu;
  (<0>,6835) fake_sched.h:183: if (need_softirq[get_cpu()]) {
  (<0>,6845) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,6848) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6849) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6850) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6854) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6855) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6856) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6858) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6862) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,6874) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6876) fake_sched.h:43: return __running_cpu;
  (<0>,6879) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6881) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6883) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6884) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6886) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6893) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6894) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6896) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6902) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6903) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6904) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6905) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6906) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6910)
  (<0>,6911)
  (<0>,6912) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,6913) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,6938)
  (<0>,6939)
  (<0>,6940) tree.c:1905: local_irq_save(flags);
  (<0>,6943) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6945) fake_sched.h:43: return __running_cpu;
  (<0>,6949) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6951) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6955) fake_sched.h:43: return __running_cpu;
  (<0>,6959) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6964) tree.c:1906: rnp = rdp->mynode;
  (<0>,6966) tree.c:1906: rnp = rdp->mynode;
  (<0>,6967) tree.c:1906: rnp = rdp->mynode;
  (<0>,6968) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6970) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6971) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6976) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6977) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6978) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6979) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6981) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6983) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6984) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6986) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6989) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6990) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6991) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6994) tree.c:1910: !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */
  (<0>,6998)
  (<0>,6999) tree.h:752: bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,7004) fake_sync.h:129: preempt_disable();
  (<0>,7006) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
    (<0.0>,4355) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4357) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4358) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4359) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4360) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4362) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4363) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4365) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4370) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4371) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4373) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4374) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4376) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4380) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4381) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4382) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4385) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4386) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4388) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4391) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4392) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4393) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4415)
    (<0.0>,4416)
    (<0.0>,4417)
    (<0.0>,4418) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4420) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4421) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4423) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4426) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4430) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4431) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4432) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4433) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4435) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4436) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4437) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4438) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4441) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4444) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4445) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4453) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4454) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4455) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4464)
    (<0.0>,4465)
    (<0.0>,4466)
    (<0.0>,4467) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4470) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4473) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4476) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4477) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4480) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4481) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4486)
    (<0.0>,4487)
    (<0.0>,4488) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4491)
    (<0.0>,4492) tree.c:625: return &rsp->node[0];
    (<0.0>,4496) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4499) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4501) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4502) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4504) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4507) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4509) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4511) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4513) tree.c:1585: }
    (<0.0>,4515) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4516) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4518) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4521) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4523) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4526) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4527) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4530) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4533) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4537) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4539) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4541) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4544) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4546) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4549) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4550) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4553) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4556) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4559) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4561) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4564) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4565) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4570) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4572) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4576) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4579) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4582) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4583) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4585) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4588) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4589) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4590) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4592) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4595) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4597) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4599) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4601) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4604) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4607) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4608) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4610) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4613) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4614) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4615) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4617) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4620) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4622) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4624) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4626) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4629) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4630) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4649)
    (<0.0>,4650)
    (<0.0>,4651)
    (<0.0>,4652) tree.c:1613: bool ret = false;
    (<0.0>,4653) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4655) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4658)
    (<0.0>,4659) tree.c:625: return &rsp->node[0];
    (<0.0>,4663) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4664) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4666) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4667) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4672)
    (<0.0>,4673)
    (<0.0>,4674) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4677)
    (<0.0>,4678) tree.c:625: return &rsp->node[0];
    (<0.0>,4682) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4685) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4687) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4688) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4690) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4693) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4695) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4697) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4699) tree.c:1585: }
    (<0.0>,4701) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4702) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4703) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4704) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4710)
    (<0.0>,4711)
    (<0.0>,4712)
    (<0.0>,4713) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4717) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4719) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4722) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4725) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4727) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4728) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4730) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4733) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4735) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4738) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4740) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4741) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4742) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4743) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4749)
    (<0.0>,4750)
    (<0.0>,4751)
    (<0.0>,4752) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4757) tree.c:1689: if (c_out != NULL)
    (<0.0>,4760) tree.c:1691: return ret;
    (<0.0>,4764) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4765) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4768) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4769) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4775) tree.c:1798: return ret;
    (<0.0>,4777) tree.c:1798: return ret;
    (<0.0>,4779) tree.c:1799: }
    (<0.0>,4782) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4784) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4786) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4787) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4789) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4792) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4794) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4795) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4797) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4800) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4802) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4803) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4805) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4811) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4812) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4815) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4819) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4821) fake_sched.h:43: return __running_cpu;
    (<0.0>,4825) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4826) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4828) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4829) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4831) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4834) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4835) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,4838)
    (<0.0>,4839) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4841) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4842) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4844) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4854)
    (<0.0>,4859) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4863) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4864) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4865) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4866) fake_defs.h:237: switch (size) {
    (<0.0>,4868) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4869) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4870) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4871) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4874) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4877) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4878) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4881) tree.c:1896: return ret;
    (<0.0>,4885) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,4888) tree_plugin.h:1231: }
    (<0.0>,4892) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4895)
    (<0.0>,4896) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4900)
    (<0.0>,4901) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4902) fake_sync.h:107: if (pthread_mutex_unlock(l))
  (<0>,7007) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
  (<0>,7010) fake_sync.h:134: return 1;
  (<0>,7012) fake_sync.h:135: }
  (<0>,7016) tree.h:752: bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,7017) tree.h:754: if (locked)
  (<0>,7023) tree.h:756: return locked;
  (<0>,7027) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7028) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7029) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7051)
  (<0>,7052)
  (<0>,7053)
  (<0>,7054) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7056) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7057) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7059) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7062) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7066) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7067) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7068) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7069) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7071) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7072) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7073) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7074) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7077) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7080) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7081) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7089) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7090) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7091) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7100)
  (<0>,7101)
  (<0>,7102)
  (<0>,7103) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7106) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7109) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7112) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7113) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7116) tree.c:1750: return false;
  (<0>,7118) tree.c:1799: }
  (<0>,7121) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7123) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7125) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7126) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7128) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7131) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7133) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7134) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7136) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7139) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7141) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7142) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7144) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7150) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7151) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7154) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7158) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7160) fake_sched.h:43: return __running_cpu;
  (<0>,7164) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7165) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7167) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7168) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7170) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7173) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7174) tree.c:1893: zero_cpu_stall_ticks(rdp);
  (<0>,7177)
  (<0>,7178) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,7180) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,7181) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,7183) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,7193)
  (<0>,7198) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7202) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7203) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7204) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7205) fake_defs.h:237: switch (size) {
  (<0>,7207) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7208) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7209) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7210) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7213) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7216) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7217) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7220) tree.c:1896: return ret;
  (<0>,7224) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7228) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7229) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7230) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7232) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7236)
  (<0>,7237)
  (<0>,7238) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,7239) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,7242) fake_sync.h:93: local_irq_restore(flags);
  (<0>,7245) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7247) fake_sched.h:43: return __running_cpu;
  (<0>,7251) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7253) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7257) fake_sched.h:43: return __running_cpu;
  (<0>,7261) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7269) tree.c:1916: if (needwake)
  (<0>,7273) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7275) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7278) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,7282) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,7286) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7288) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7290) fake_sched.h:43: return __running_cpu;
  (<0>,7294) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7299) tree.c:3016: local_irq_save(flags);
  (<0>,7302) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7304) fake_sched.h:43: return __running_cpu;
  (<0>,7308) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7310) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7314) fake_sched.h:43: return __running_cpu;
  (<0>,7318) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7323) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7324) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7335)
  (<0>,7336)
  (<0>,7337) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7350)
  (<0>,7351) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7356) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7357) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7358) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7359) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7361) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7363) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7364) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7366) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7369) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7370) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7371) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7372) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7377) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7378) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7379) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7380) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7382) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7384) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7385) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7387) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7390) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7391) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7392) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7398) tree.c:653: return false;  /* No, a grace period is already in progress. */
  (<0>,7400) tree.c:666: }
  (<0>,7403) tree.c:3024: local_irq_restore(flags);
  (<0>,7406) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7408) fake_sched.h:43: return __running_cpu;
  (<0>,7412) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7414) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7418) fake_sched.h:43: return __running_cpu;
  (<0>,7422) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7428) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7431)
  (<0>,7432) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7434) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7437) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7444) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,7447) tree_plugin.h:2457: }
  (<0>,7451) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7454) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7455) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7456) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7460) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7461) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7462) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7464) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7468) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,7480) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7482) fake_sched.h:43: return __running_cpu;
  (<0>,7485) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7487) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7489) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7490) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7492) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7499) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7500) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7502) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7508) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7509) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7510) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7511) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7512) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7516)
  (<0>,7517)
  (<0>,7518) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,7519) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,7544)
  (<0>,7545)
  (<0>,7546) tree.c:1905: local_irq_save(flags);
  (<0>,7549) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7551) fake_sched.h:43: return __running_cpu;
  (<0>,7555) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7557) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7561) fake_sched.h:43: return __running_cpu;
  (<0>,7565) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7570) tree.c:1906: rnp = rdp->mynode;
  (<0>,7572) tree.c:1906: rnp = rdp->mynode;
  (<0>,7573) tree.c:1906: rnp = rdp->mynode;
  (<0>,7574) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7576) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7577) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7582) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7583) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7584) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7585) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7587) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7589) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7590) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7592) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7595) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7596) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7597) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7600) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7602) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7603) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7608) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7609) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7610) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7611) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7613) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7615) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7616) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7618) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7621) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7622) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7623) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7626) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7630) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7631) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7632) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7633) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7635) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7636) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7637) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7638) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7641) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7644) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7645) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7653) tree.c:1911: local_irq_restore(flags);
  (<0>,7656) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7658) fake_sched.h:43: return __running_cpu;
  (<0>,7662) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7664) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7668) fake_sched.h:43: return __running_cpu;
  (<0>,7672) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7679) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7681) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7686) tree.c:3016: local_irq_save(flags);
  (<0>,7689) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7691) fake_sched.h:43: return __running_cpu;
  (<0>,7695) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7697) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7701) fake_sched.h:43: return __running_cpu;
  (<0>,7705) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7710) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7711) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7722)
  (<0>,7723)
  (<0>,7724) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7737)
  (<0>,7738) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7743) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7744) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7745) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7746) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7748) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7750) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7751) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7753) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7756) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7757) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7758) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7759) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7764) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7765) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7766) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7767) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7769) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7771) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7772) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7774) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7777) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7778) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7779) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7785) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,7801)
  (<0>,7802) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7805)
  (<0>,7806) tree.c:625: return &rsp->node[0];
  (<0>,7810) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7811) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7816) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7817) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7818) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7819) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7821) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7823) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7824) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7826) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7829) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7830) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7831) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7835) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7836) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7838) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7841) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7842) tree.c:639: return READ_ONCE(*fp);
  (<0>,7846) tree.c:639: return READ_ONCE(*fp);
  (<0>,7847) tree.c:639: return READ_ONCE(*fp);
  (<0>,7848) tree.c:639: return READ_ONCE(*fp);
  (<0>,7849) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7851) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7853) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7854) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7856) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7859) tree.c:639: return READ_ONCE(*fp);
  (<0>,7860) tree.c:639: return READ_ONCE(*fp);
  (<0>,7861) tree.c:639: return READ_ONCE(*fp);
  (<0>,7865) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7868) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7871) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7874) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7875) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7878) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7880) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7883) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7886) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7889) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7890) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7892) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7895) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7899) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7901) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7903) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7906) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7909) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7912) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7913) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7915) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7918) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7922) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7924) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7926) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7929) tree.c:665: return false; /* No grace period needed. */
  (<0>,7931) tree.c:666: }
  (<0>,7934) tree.c:3024: local_irq_restore(flags);
  (<0>,7937) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7939) fake_sched.h:43: return __running_cpu;
  (<0>,7943) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7945) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7949) fake_sched.h:43: return __running_cpu;
  (<0>,7953) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7959) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7962)
  (<0>,7963) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7965) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7968) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7975) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,7978) tree_plugin.h:2457: }
  (<0>,7982) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7985) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7986) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7987) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7991) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7992) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7993) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7995) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8003) fake_sched.h:43: return __running_cpu;
  (<0>,8007) fake_sched.h:185: need_softirq[get_cpu()] = 0;
  (<0>,8017) fake_sched.h:43: return __running_cpu;
  (<0>,8021) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8022) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8024) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8025) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8026) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8028) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8030) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8031) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8032) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8033) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8034) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8035) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,8037) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,8045) tree_plugin.h:2879: }
  (<0>,8057) fake_sched.h:43: return __running_cpu;
  (<0>,8063) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
  (<0>,8069) fake_sched.h:43: return __running_cpu;
  (<0>,8076) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
  (<0>,8078) fake_sched.h:43: return __running_cpu;
  (<0>,8085) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
  (<0>,8093) fake_sched.h:43: return __running_cpu;
  (<0>,8097) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
  (<0>,8110) fake_sched.h:43: return __running_cpu;
  (<0>,8114) fake_sched.h:96: rcu_idle_enter();
  (<0>,8117) tree.c:755: local_irq_save(flags);
  (<0>,8120) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8122) fake_sched.h:43: return __running_cpu;
  (<0>,8126) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8128) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8132) fake_sched.h:43: return __running_cpu;
  (<0>,8136) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8148) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8150) fake_sched.h:43: return __running_cpu;
  (<0>,8154) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8155) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8157) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8158) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8159) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8160) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8161) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8162) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8163) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,8167) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,8169) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,8170) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,8171) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,8187)
  (<0>,8189) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8191) fake_sched.h:43: return __running_cpu;
  (<0>,8195) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8198) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8199) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8200) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8204) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8205) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8206) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8208) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8213) fake_sched.h:43: return __running_cpu;
  (<0>,8216) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8218) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8220) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8221) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,8224) tree_plugin.h:2457: }
  (<0>,8227) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8230) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8231) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8232) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8236) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8237) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8238) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8240) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8245) fake_sched.h:43: return __running_cpu;
  (<0>,8248) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8250) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8252) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8253) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,8256) tree_plugin.h:2457: }
  (<0>,8259) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8262) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8263) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8264) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8268) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8269) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8270) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8272) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8279) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8282) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8283) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8284) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8286) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8287) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8289) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8290) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8291) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8292) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8306) tree_plugin.h:2879: }
  (<0>,8308) tree.c:758: local_irq_restore(flags);
  (<0>,8311) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8313) fake_sched.h:43: return __running_cpu;
  (<0>,8317) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8319) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8323) fake_sched.h:43: return __running_cpu;
  (<0>,8327) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8333) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8336) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8341) fake_sched.h:43: return __running_cpu;
  (<0>,8345)
  (<0>,8346) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,8349) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,8354) tree.c:892: local_irq_save(flags);
  (<0>,8357) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8359) fake_sched.h:43: return __running_cpu;
  (<0>,8363) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8365) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8369) fake_sched.h:43: return __running_cpu;
  (<0>,8373) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8385) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8387) fake_sched.h:43: return __running_cpu;
  (<0>,8391) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8392) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,8394) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,8395) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,8396) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8397) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8398) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8399) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8400) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,8404) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,8406) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,8407) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,8408) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,8419)
  (<0>,8420) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8422) fake_sched.h:43: return __running_cpu;
  (<0>,8426) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8430) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8433) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8434) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8435) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8437) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8438) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8440) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8441) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8442) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8443) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8453) tree_plugin.h:2883: }
  (<0>,8455) tree.c:895: local_irq_restore(flags);
  (<0>,8458) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8460) fake_sched.h:43: return __running_cpu;
  (<0>,8464) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8466) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8470) fake_sched.h:43: return __running_cpu;
  (<0>,8474) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8485) fake_sched.h:43: return __running_cpu;
  (<0>,8489) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
  (<0>,8493) fake_sched.h:43: return __running_cpu;
  (<0>,8497) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8502) fake_sched.h:43: return __running_cpu;
  (<0>,8506) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
  (<0>,8517) fake_sched.h:43: return __running_cpu;
  (<0>,8521) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8522) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,8524) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,8525) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,8526) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,8528) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,8530) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,8531) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8532) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8533) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8534) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8535) tree.c:942: if (oldval)
  (<0>,8543) tree_plugin.h:2883: }
  (<0>,8549) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,8558) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8559) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8560) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8564) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8565) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8566) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8568) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8573) fake_sched.h:43: return __running_cpu;
  (<0>,8576) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8578) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8581) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8583) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8585) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8588) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8589) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8590) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8594) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8595) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8596) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8598) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8603) fake_sched.h:43: return __running_cpu;
  (<0>,8606) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8608) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8611) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8613) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8615) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8618) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8619) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8620) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8624) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8625) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8626) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8628) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8633) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,8638) fake_sched.h:43: return __running_cpu;
  (<0>,8643) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,8651) fake_sched.h:43: return __running_cpu;
  (<0>,8657) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
  (<0>,8671) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8672) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8673) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8677) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8678) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8679) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8681) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8685) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8687) fake_sched.h:43: return __running_cpu;
  (<0>,8690) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8692) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8714)
  (<0>,8715)
  (<0>,8716) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8718) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8719) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8720) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8722) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8724) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8725) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,8726) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,8761)
  (<0>,8762)
  (<0>,8763) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
  (<0>,8766) tree.c:1469: !rcu_gp_in_progress(rsp))
  (<0>,8779)
  (<0>,8780) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8785) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8786) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8787) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8788) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8790) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8792) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8793) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8795) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8798) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8799) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8800) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8801) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8806) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8807) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8808) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8809) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8811) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8813) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8814) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8816) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8819) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8821) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8827) tree.c:1471: rcu_stall_kick_kthreads(rsp);
  (<0>,8852)
  (<0>,8853) tree.c:1310: if (!rcu_kick_kthreads)
  (<0>,8858) tree.c:1472: j = jiffies;
  (<0>,8859) tree.c:1472: j = jiffies;
  (<0>,8860) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8865) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8866) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8867) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8868) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8870) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8872) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8873) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8875) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8878) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8879) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8880) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8881) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
  (<0>,8883) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8888) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8889) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8890) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8891) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8893) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8895) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8896) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8898) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8901) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8902) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8903) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8904) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
  (<0>,8906) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8911) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8912) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8913) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8914) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8916) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8918) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8919) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8921) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8924) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8925) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8926) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8927) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
  (<0>,8929) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8934) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8935) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8936) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8937) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8939) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8941) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8942) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8944) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8947) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8948) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8949) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8950) tree.c:1497: completed = READ_ONCE(rsp->completed);
  (<0>,8951) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
  (<0>,8952) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
  (<0>,8956) tree.c:1499: ULONG_CMP_LT(j, js) ||
  (<0>,8957) tree.c:1499: ULONG_CMP_LT(j, js) ||
  (<0>,8963) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
  (<0>,8966) tree_plugin.h:2923: return false;
  (<0>,8969) tree.c:3503: if (rcu_scheduler_fully_active &&
  (<0>,8972) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8974) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8977) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8981) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8985) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,8987) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,8990) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
  (<0>,8994) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
  (<0>,8997) tree.c:3510: rdp->n_rp_report_qs++;
  (<0>,8999) tree.c:3510: rdp->n_rp_report_qs++;
  (<0>,9001) tree.c:3510: rdp->n_rp_report_qs++;
  (<0>,9002) tree.c:3511: return 1;
  (<0>,9004) tree.c:3548: }
  (<0>,9008) tree.c:3561: return 1;
  (<0>,9010) tree.c:3563: }
  (<0>,9017) fake_sched.h:43: return __running_cpu;
  (<0>,9021) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
  (<0>,9025) tree.c:2891: if (user)
  (<0>,9033) fake_sched.h:43: return __running_cpu;
  (<0>,9037) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
  (<0>,9039) fake_sched.h:43: return __running_cpu;
  (<0>,9043) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9049) fake_sched.h:43: return __running_cpu;
  (<0>,9053) fake_sched.h:183: if (need_softirq[get_cpu()]) {
  (<0>,9063) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,9066) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9067) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9068) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9072) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9073) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9074) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9076) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9080) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,9092) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9094) fake_sched.h:43: return __running_cpu;
  (<0>,9097) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9099) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9101) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9102) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9104) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9111) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9112) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9114) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9120) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9121) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9122) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9123) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,9124) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,9128)
  (<0>,9129)
  (<0>,9130) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,9131) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,9156)
  (<0>,9157)
  (<0>,9158) tree.c:1905: local_irq_save(flags);
  (<0>,9161) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9163) fake_sched.h:43: return __running_cpu;
  (<0>,9167) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9169) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9173) fake_sched.h:43: return __running_cpu;
  (<0>,9177) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9182) tree.c:1906: rnp = rdp->mynode;
  (<0>,9184) tree.c:1906: rnp = rdp->mynode;
  (<0>,9185) tree.c:1906: rnp = rdp->mynode;
  (<0>,9186) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9188) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9189) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9194) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9195) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9196) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9197) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9199) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9201) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9202) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9204) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9207) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9208) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9209) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9212) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9214) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9215) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9220) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9221) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9222) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9223) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9225) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9227) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9228) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9230) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9233) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9234) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9235) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9238) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9242) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9243) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9244) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9245) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9247) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9248) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9249) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9250) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9253) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9256) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9257) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9265) tree.c:1911: local_irq_restore(flags);
  (<0>,9268) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9270) fake_sched.h:43: return __running_cpu;
  (<0>,9274) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9276) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9280) fake_sched.h:43: return __running_cpu;
  (<0>,9284) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9291) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,9293) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,9296) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,9300) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,9304) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,9306) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,9307) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,9308) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
  (<0>,9326)
  (<0>,9327)
  (<0>,9328)
  (<0>,9329) tree.c:2486: rnp = rdp->mynode;
  (<0>,9331) tree.c:2486: rnp = rdp->mynode;
  (<0>,9332) tree.c:2486: rnp = rdp->mynode;
  (<0>,9336) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,9337) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,9338) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,9340) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,9344)
  (<0>,9345)
  (<0>,9346) fake_sync.h:83: local_irq_save(flags);
  (<0>,9349) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9351) fake_sched.h:43: return __running_cpu;
  (<0>,9355) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9357) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9361) fake_sched.h:43: return __running_cpu;
  (<0>,9365) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9371) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,9372) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,9379) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
  (<0>,9383) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
  (<0>,9387) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9389) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9390) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9392) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9395) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9397) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9398) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9400) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
  (<0>,9403) tree.c:2491: rdp->gpwrap) {
  (<0>,9405) tree.c:2491: rdp->gpwrap) {
  (<0>,9408) tree.c:2504: mask = rdp->grpmask;
  (<0>,9410) tree.c:2504: mask = rdp->grpmask;
  (<0>,9411) tree.c:2504: mask = rdp->grpmask;
  (<0>,9412) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
  (<0>,9414) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
  (<0>,9415) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
  (<0>,9419) tree.c:2508: rdp->core_needs_qs = false;
  (<0>,9421) tree.c:2508: rdp->core_needs_qs = false;
  (<0>,9422) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,9423) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,9424) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,9433)
  (<0>,9434)
  (<0>,9435)
  (<0>,9436) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9439) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9442) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9445) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9446) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9449) tree.c:1750: return false;
  (<0>,9451) tree.c:1799: }
  (<0>,9454) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,9455) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9456) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9457) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9458) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9460) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9461) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
  (<0>,9485)
  (<0>,9486)
  (<0>,9487)
  (<0>,9488)
  (<0>,9489)
  (<0>,9490) tree.c:2385: unsigned long oldmask = 0;
  (<0>,9492) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9494) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9495) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9499) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9501) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9502) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
  (<0>,9505) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9510) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9511) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9515) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9516) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9517) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
  (<0>,9518) tree.c:2400: rnp->qsmask &= ~mask;
  (<0>,9520) tree.c:2400: rnp->qsmask &= ~mask;
  (<0>,9522) tree.c:2400: rnp->qsmask &= ~mask;
  (<0>,9524) tree.c:2400: rnp->qsmask &= ~mask;
  (<0>,9527) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,9529) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
  (<0>,9535) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,9536) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,9537) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,9539) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,9543)
  (<0>,9544)
  (<0>,9545) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,9546) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,9549) fake_sync.h:93: local_irq_restore(flags);
  (<0>,9552) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9554) fake_sched.h:43: return __running_cpu;
  (<0>,9558) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9560) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9564) fake_sched.h:43: return __running_cpu;
  (<0>,9568) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9578) tree.c:2518: if (needwake)
  (<0>,9585) tree.c:3016: local_irq_save(flags);
  (<0>,9588) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9590) fake_sched.h:43: return __running_cpu;
  (<0>,9594) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9596) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9600) fake_sched.h:43: return __running_cpu;
  (<0>,9604) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9609) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9610) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9621)
  (<0>,9622)
  (<0>,9623) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,9636)
  (<0>,9637) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9642) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9643) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9644) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9645) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9647) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9649) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9650) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9652) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9655) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9656) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9657) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9658) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9663) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9664) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9665) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9666) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9668) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9670) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9671) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9673) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9676) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9677) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9678) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9684) tree.c:653: return false;  /* No, a grace period is already in progress. */
  (<0>,9686) tree.c:666: }
  (<0>,9689) tree.c:3024: local_irq_restore(flags);
  (<0>,9692) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9694) fake_sched.h:43: return __running_cpu;
  (<0>,9698) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9700) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9704) fake_sched.h:43: return __running_cpu;
  (<0>,9708) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9714) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,9717)
  (<0>,9718) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9720) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9723) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9730) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,9733) tree_plugin.h:2457: }
  (<0>,9737) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9740) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9741) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9742) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9746) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9747) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9748) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9750) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9754) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,9766) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9768) fake_sched.h:43: return __running_cpu;
  (<0>,9771) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9773) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9775) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9776) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9778) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9785) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9786) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9788) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9794) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9795) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9796) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9797) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,9798) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,9802)
  (<0>,9803)
  (<0>,9804) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,9805) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,9830)
  (<0>,9831)
  (<0>,9832) tree.c:1905: local_irq_save(flags);
  (<0>,9835) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9837) fake_sched.h:43: return __running_cpu;
  (<0>,9841) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9843) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9847) fake_sched.h:43: return __running_cpu;
  (<0>,9851) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9856) tree.c:1906: rnp = rdp->mynode;
  (<0>,9858) tree.c:1906: rnp = rdp->mynode;
  (<0>,9859) tree.c:1906: rnp = rdp->mynode;
  (<0>,9860) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9862) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9863) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9868) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9869) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9870) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9871) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9873) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9875) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9876) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9878) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9881) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9882) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9883) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9886) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9888) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9889) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9894) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9895) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9896) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9897) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9899) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9901) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9902) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9904) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9907) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9908) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9909) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,9912) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9916) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9917) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9918) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9919) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9921) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9922) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9923) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9924) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9927) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9930) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9931) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,9939) tree.c:1911: local_irq_restore(flags);
  (<0>,9942) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9944) fake_sched.h:43: return __running_cpu;
  (<0>,9948) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9950) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9954) fake_sched.h:43: return __running_cpu;
  (<0>,9958) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9965) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,9967) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,9972) tree.c:3016: local_irq_save(flags);
  (<0>,9975) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9977) fake_sched.h:43: return __running_cpu;
  (<0>,9981) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9983) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9987) fake_sched.h:43: return __running_cpu;
  (<0>,9991) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9996) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9997) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,10008)
  (<0>,10009)
  (<0>,10010) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,10023)
  (<0>,10024) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10029) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10030) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10031) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10032) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10034) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10036) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10037) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10039) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10042) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10043) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10044) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10045) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10050) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10051) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10052) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10053) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10055) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10057) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10058) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10060) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10063) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10064) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10065) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10071) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,10087)
  (<0>,10088) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,10091)
  (<0>,10092) tree.c:625: return &rsp->node[0];
  (<0>,10096) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,10097) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10102) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10103) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10104) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10105) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10107) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10109) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10110) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10112) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10115) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10116) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10117) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10121) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10122) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,10124) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,10127) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,10128) tree.c:639: return READ_ONCE(*fp);
  (<0>,10132) tree.c:639: return READ_ONCE(*fp);
  (<0>,10133) tree.c:639: return READ_ONCE(*fp);
  (<0>,10134) tree.c:639: return READ_ONCE(*fp);
  (<0>,10135) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10137) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10139) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10140) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10142) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10145) tree.c:639: return READ_ONCE(*fp);
  (<0>,10146) tree.c:639: return READ_ONCE(*fp);
  (<0>,10147) tree.c:639: return READ_ONCE(*fp);
  (<0>,10151) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,10154) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,10157) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,10160) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,10161) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,10164) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10166) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10169) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10172) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10175) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10176) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10178) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10181) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10185) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10187) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10189) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10192) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10195) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10198) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10199) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10201) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10204) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10208) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10210) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10212) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10215) tree.c:665: return false; /* No grace period needed. */
  (<0>,10217) tree.c:666: }
  (<0>,10220) tree.c:3024: local_irq_restore(flags);
  (<0>,10223) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10225) fake_sched.h:43: return __running_cpu;
  (<0>,10229) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10231) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10235) fake_sched.h:43: return __running_cpu;
  (<0>,10239) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,10245) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,10248)
  (<0>,10249) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10251) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10254) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10261) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,10264) tree_plugin.h:2457: }
  (<0>,10268) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10271) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10272) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10273) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10277) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10278) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10279) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10281) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10289) fake_sched.h:43: return __running_cpu;
  (<0>,10293) fake_sched.h:185: need_softirq[get_cpu()] = 0;
  (<0>,10303) fake_sched.h:43: return __running_cpu;
  (<0>,10307) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,10308) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,10310) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,10311) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,10312) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,10314) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,10316) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,10317) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10318) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10319) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10320) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10321) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,10323) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,10331) tree_plugin.h:2879: }
      (<0.1>,763) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,4903) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4907) fake_sched.h:43: return __running_cpu;
    (<0.0>,4911) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4913) fake_sched.h:43: return __running_cpu;
    (<0.0>,4917) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4932) fake_sched.h:43: return __running_cpu;
    (<0.0>,4938) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,4944) fake_sched.h:43: return __running_cpu;
    (<0.0>,4951) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,4953) fake_sched.h:43: return __running_cpu;
    (<0.0>,4960) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
    (<0.0>,4968) fake_sched.h:43: return __running_cpu;
    (<0.0>,4972) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,4985) fake_sched.h:43: return __running_cpu;
    (<0.0>,4989)
    (<0.0>,4992) tree.c:755: local_irq_save(flags);
    (<0.0>,4995)
    (<0.0>,4997) fake_sched.h:43: return __running_cpu;
    (<0.0>,5001) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5003) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5007) fake_sched.h:43: return __running_cpu;
    (<0.0>,5011) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5023)
    (<0.0>,5025) fake_sched.h:43: return __running_cpu;
    (<0.0>,5029) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5030) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5032) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5033) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5034) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5035) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5036) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5037) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5038) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5042) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5044) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5045) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5046) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5062)
    (<0.0>,5064)
    (<0.0>,5066) fake_sched.h:43: return __running_cpu;
    (<0.0>,5070) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5073) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5074) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5075) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5079) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5080) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5081) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5083) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5088) fake_sched.h:43: return __running_cpu;
    (<0.0>,5091) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5093) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5095) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5096) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5099)
    (<0.0>,5102) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5105) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5106) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5107) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5111) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5112) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5113) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5115) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5120) fake_sched.h:43: return __running_cpu;
    (<0.0>,5123) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5125) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5127) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5128) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5131)
    (<0.0>,5134) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5137) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5138) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5139) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5143) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5144) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5145) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5147) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5154) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5157) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5158) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5159) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5161) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5162) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5164) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5165) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5166) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5167) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5181)
    (<0.0>,5183) tree.c:758: local_irq_restore(flags);
    (<0.0>,5186)
    (<0.0>,5188) fake_sched.h:43: return __running_cpu;
    (<0.0>,5192) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5194) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5198) fake_sched.h:43: return __running_cpu;
    (<0.0>,5202) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5208) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5211) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5216) fake_sched.h:43: return __running_cpu;
    (<0.0>,5220)
    (<0.0>,5221) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5224) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5229) tree.c:892: local_irq_save(flags);
    (<0.0>,5232)
    (<0.0>,5234) fake_sched.h:43: return __running_cpu;
    (<0.0>,5238) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5240) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5244) fake_sched.h:43: return __running_cpu;
    (<0.0>,5248) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5260)
    (<0.0>,5262) fake_sched.h:43: return __running_cpu;
    (<0.0>,5266) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5267) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5269) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5270) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5271) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5272) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5273) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5274) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5275) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,5279) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5281) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5282) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5283) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5294)
    (<0.0>,5295)
    (<0.0>,5297) fake_sched.h:43: return __running_cpu;
    (<0.0>,5301) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5305) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5308) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5309) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5310) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5312) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5313) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5315) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5316) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5317) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5318) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5328)
    (<0.0>,5330) tree.c:895: local_irq_restore(flags);
    (<0.0>,5333)
    (<0.0>,5335) fake_sched.h:43: return __running_cpu;
    (<0.0>,5339) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5341) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5345) fake_sched.h:43: return __running_cpu;
    (<0.0>,5349) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5363) fake_sched.h:43: return __running_cpu;
    (<0.0>,5367) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,5376) fake_sched.h:43: return __running_cpu;
    (<0.0>,5383) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,5392) fake_sched.h:43: return __running_cpu;
    (<0.0>,5396) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5398) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5404) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5405) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5406) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5411) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5412) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5413) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5414) fake_defs.h:237: switch (size) {
    (<0.0>,5416) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5418) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5419) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5421) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5424) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5425) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5426) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5428) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5430) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5432) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5433) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5435) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5440) tree.c:2046: return true;
    (<0.0>,5442) tree.c:2047: }
    (<0.0>,5446) tree.c:2214: first_gp_fqs = true;
    (<0.0>,5447) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5448) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5449) tree.c:2216: if (j > HZ) {
    (<0.0>,5452) tree.c:2220: ret = 0;
    (<0.0>,5454) tree.c:2222: if (!ret) {
    (<0.0>,5457) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5458) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5460) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5462) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5464) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5465) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5468) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5469) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5474) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5475) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5476) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5477) fake_defs.h:237: switch (size) {
    (<0.0>,5479) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5481) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5482) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5484) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5487) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5488) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5489) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5493) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5495) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5499) fake_sched.h:43: return __running_cpu;
    (<0.0>,5503) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5507) fake_sched.h:43: return __running_cpu;
    (<0.0>,5511) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5516) fake_sched.h:43: return __running_cpu;
    (<0.0>,5520) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5531) fake_sched.h:43: return __running_cpu;
    (<0.0>,5535) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5536) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5538) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5539) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5540) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5542) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5544) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5545) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5546) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5547) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5548) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5549) tree.c:942: if (oldval)
    (<0.0>,5557)
    (<0.0>,5563)
    (<0.0>,5572) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5573) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5574) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5578) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5579) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5580) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5582) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5587) fake_sched.h:43: return __running_cpu;
    (<0.0>,5590) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5592) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5595) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5597) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5599) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5602) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5603) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5604) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5608) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5609) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5610) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5612) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5617) fake_sched.h:43: return __running_cpu;
    (<0.0>,5620) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5622) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5625) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5627) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5629) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5632) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5633) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5634) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5638) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5639) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5640) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5642) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5647) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,5652) fake_sched.h:43: return __running_cpu;
    (<0.0>,5657) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,5665) fake_sched.h:43: return __running_cpu;
    (<0.0>,5671) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,5685) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5686) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5687) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5691) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5692) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5693) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5695) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5699) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5701) fake_sched.h:43: return __running_cpu;
    (<0.0>,5704) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5706) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5728)
    (<0.0>,5729)
    (<0.0>,5730) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5732) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5733) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5734) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5736) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5738) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5739) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5740) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5775)
    (<0.0>,5776)
    (<0.0>,5777) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,5780) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,5793)
    (<0.0>,5794) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5799) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5800) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5801) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5802) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5804) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5806) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5807) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5809) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5812) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5813) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5814) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5815) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5821) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5822) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5823) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5825) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5830) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5833) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5834) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5835) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5841) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,5866)
    (<0.0>,5867) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,5872) tree.c:1472: j = jiffies;
    (<0.0>,5873) tree.c:1472: j = jiffies;
    (<0.0>,5874) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5879) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5880) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5881) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5882) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5884) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5886) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5887) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5889) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5892) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5893) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5894) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5895) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5897) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5902) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5903) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5904) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5905) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5907) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5909) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5910) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5912) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5915) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5916) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5917) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5918) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5920) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5925) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5926) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5927) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5930) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5932) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5933) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5935) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5938) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5939) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5940) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5941) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5943) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5948) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5949) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5950) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5951) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5953) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5955) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5956) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5961) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5962) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5963) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5964) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5965) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5966) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5970) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5971) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5977) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,5980)
    (<0.0>,5983) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,5986) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5988) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5991) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5995) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5999) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6001) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6004) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,6008) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,6011) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6013) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6015) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6016) tree.c:3511: return 1;
    (<0.0>,6018) tree.c:3548: }
    (<0.0>,6022) tree.c:3561: return 1;
    (<0.0>,6024) tree.c:3563: }
    (<0.0>,6031) fake_sched.h:43: return __running_cpu;
    (<0.0>,6035) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,6039) tree.c:2891: if (user)
    (<0.0>,6047) fake_sched.h:43: return __running_cpu;
    (<0.0>,6051) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6053) fake_sched.h:43: return __running_cpu;
    (<0.0>,6057) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6063) fake_sched.h:43: return __running_cpu;
    (<0.0>,6067) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,6077)
    (<0.0>,6080) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6081) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6082) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6086) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6087) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6088) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6090) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6094) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,6106)
    (<0.0>,6108) fake_sched.h:43: return __running_cpu;
    (<0.0>,6111) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6113) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6115) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6116) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6118) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6125) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6126) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6128) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6134) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6135) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6136) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6137) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6138) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6142)
    (<0.0>,6143)
    (<0.0>,6144) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6145) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6170)
    (<0.0>,6171)
    (<0.0>,6172) tree.c:1905: local_irq_save(flags);
    (<0.0>,6175)
    (<0.0>,6177) fake_sched.h:43: return __running_cpu;
    (<0.0>,6181) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6183) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6187) fake_sched.h:43: return __running_cpu;
    (<0.0>,6191) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6196) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6198) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6199) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6200) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6202) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6203) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6208) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6209) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6210) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6211) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6213) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6215) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6216) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6218) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6221) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6222) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6223) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6226) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6228) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6229) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6234) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6235) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6236) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6237) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6239) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6241) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6242) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6244) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6247) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6248) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6249) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6252) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6256) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6257) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6258) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6259) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6261) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6262) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6263) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6264) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6267) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6270) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6271) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6279) tree.c:1911: local_irq_restore(flags);
    (<0.0>,6282)
    (<0.0>,6284) fake_sched.h:43: return __running_cpu;
    (<0.0>,6288) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6290) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6294) fake_sched.h:43: return __running_cpu;
    (<0.0>,6298) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6305) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6307) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6310) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,6314) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,6318) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6320) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6321) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6322) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6340)
    (<0.0>,6341)
    (<0.0>,6342)
    (<0.0>,6343) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6345) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6346) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6350) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6351) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6352) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6354) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6358)
    (<0.0>,6359)
    (<0.0>,6360) fake_sync.h:83: local_irq_save(flags);
    (<0.0>,6363)
    (<0.0>,6365) fake_sched.h:43: return __running_cpu;
    (<0.0>,6369) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6371) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6375) fake_sched.h:43: return __running_cpu;
    (<0.0>,6379) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6385) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,6386) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,6393) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,6397) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,6401) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6403) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6404) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6406) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6409) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6411) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6412) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6414) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6417) tree.c:2491: rdp->gpwrap) {
    (<0.0>,6419) tree.c:2491: rdp->gpwrap) {
    (<0.0>,6422) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6424) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6425) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6426) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6428) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6429) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6433) tree.c:2508: rdp->core_needs_qs = false;
    (<0.0>,6435) tree.c:2508: rdp->core_needs_qs = false;
    (<0.0>,6436) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6437) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6438) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6447)
    (<0.0>,6448)
    (<0.0>,6449)
    (<0.0>,6450) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6453) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6456) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6459) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6460) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,6463) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,6464) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,6469)
    (<0.0>,6470)
    (<0.0>,6471) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6474)
    (<0.0>,6475) tree.c:625: return &rsp->node[0];
    (<0.0>,6479) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6482) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6484) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6485) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6487) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6490) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6492) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6494) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6496) tree.c:1585: }
    (<0.0>,6498) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,6499) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,6501) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,6504) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6506) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6509) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6510) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6513) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6516) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6520) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,6522) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,6524) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,6527) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6529) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6532) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6533) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6536) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6539) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,6542) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,6544) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,6547) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,6548) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,6553) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,6555) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,6559) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6562) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6565) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6566) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6568) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6571) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6572) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6573) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6575) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6578) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6580) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6582) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6584) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6587) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6590) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6591) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6593) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6596) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,6597) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6598) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6600) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6603) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,6605) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6607) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6609) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,6612) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,6613) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,6632)
    (<0.0>,6633)
    (<0.0>,6634)
    (<0.0>,6635) tree.c:1613: bool ret = false;
    (<0.0>,6636) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,6638) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,6641)
    (<0.0>,6642) tree.c:625: return &rsp->node[0];
    (<0.0>,6646) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,6647) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,6649) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,6650) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,6655)
    (<0.0>,6656)
    (<0.0>,6657) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6660)
    (<0.0>,6661) tree.c:625: return &rsp->node[0];
    (<0.0>,6665) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6668) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6670) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6671) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6673) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,6676) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6678) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6680) tree.c:1584: return rnp->completed + 2;
    (<0.0>,6682) tree.c:1585: }
    (<0.0>,6684) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,6685) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,6686) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,6687) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,6693)
    (<0.0>,6694)
    (<0.0>,6695)
    (<0.0>,6696)
    (<0.0>,6700) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,6702) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,6705) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,6708) tree.c:1623: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,6709) tree.c:1623: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,6710) tree.c:1623: trace_rcu_future_gp(rnp, rdp, c, TPS("Prestartleaf"));
    (<0.0>,6716)
    (<0.0>,6717)
    (<0.0>,6718)
    (<0.0>,6719)
    (<0.0>,6724) tree.c:1689: if (c_out != NULL)
    (<0.0>,6727) tree.c:1691: return ret;
    (<0.0>,6731) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,6732) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,6735) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,6736) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,6742) tree.c:1798: return ret;
    (<0.0>,6744) tree.c:1798: return ret;
    (<0.0>,6746) tree.c:1799: }
    (<0.0>,6749) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,6750) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6751) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6752) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6753) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6755) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6756) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,6780)
    (<0.0>,6781)
    (<0.0>,6782)
    (<0.0>,6783)
    (<0.0>,6784)
    (<0.0>,6785) tree.c:2385: unsigned long oldmask = 0;
    (<0.0>,6787) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6789) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6790) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6794) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6796) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6797) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,6800) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6805) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6806) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6810) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6811) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6812) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,6813) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,6815) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,6817) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,6819) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,6822) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,6824) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,6827) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,6830)
    (<0.0>,6834) tree.c:2413: mask = rnp->grpmask;
    (<0.0>,6836) tree.c:2413: mask = rnp->grpmask;
    (<0.0>,6837) tree.c:2413: mask = rnp->grpmask;
    (<0.0>,6838) tree.c:2414: if (rnp->parent == NULL) {
    (<0.0>,6840) tree.c:2414: if (rnp->parent == NULL) {
    (<0.0>,6844) tree.c:2432: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
    (<0.0>,6845) tree.c:2432: rcu_report_qs_rsp(rsp, flags); /* releases rnp->lock. */
    (<0.0>,6865)
    (<0.0>,6866)
    (<0.0>,6867) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,6880)
    (<0.0>,6881) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6886) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6887) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6888) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6889) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6891) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6893) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6894) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6896) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6899) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6900) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6901) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6902) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6907) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6908) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6909) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6910) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6912) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6914) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6915) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6917) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6920) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6921) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6922) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6931) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,6932) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,6945)
    (<0.0>,6946) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6951) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6952) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6953) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6954) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6956) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6959) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6964) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6965) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6966) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6967) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6972) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6973) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6974) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6975) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6977) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6979) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6980) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6982) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6985) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6986) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6987) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6994) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,6995) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,6996) tree.c:2361: WARN_ON_ONCE(!rcu_gp_in_progress(rsp));
    (<0.0>,6998) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7003) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7004) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7005) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7006) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7008) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7010) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7011) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7013) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7016) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7017) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7018) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7022) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7023) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7028) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7029) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7030) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7031) fake_defs.h:237: switch (size) {
    (<0.0>,7033) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,7035) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,7036) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,7038) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,7041) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7042) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7043) tree.c:2362: WRITE_ONCE(rsp->gp_flags, READ_ONCE(rsp->gp_flags) | RCU_GP_FLAG_FQS);
    (<0.0>,7047) tree.c:2363: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,7048) tree.c:2363: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,7049) tree.c:2363: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,7052)
    (<0.0>,7053) tree.c:625: return &rsp->node[0];
    (<0.0>,7058) tree.c:2363: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,7062)
    (<0.0>,7063)
    (<0.0>,7064) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,7065) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,7068) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,7071)
    (<0.0>,7073) fake_sched.h:43: return __running_cpu;
    (<0.0>,7077) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7079) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7083) fake_sched.h:43: return __running_cpu;
    (<0.0>,7087) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7095) tree.c:2364: rcu_gp_kthread_wake(rsp);
    (<0.0>,7103)
    (<0.0>,7104) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,7105) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,7107) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,7115) tree.c:2518: if (needwake)
    (<0.0>,7122) tree.c:3016: local_irq_save(flags);
    (<0.0>,7125)
    (<0.0>,7127) fake_sched.h:43: return __running_cpu;
    (<0.0>,7131) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7133) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7137) fake_sched.h:43: return __running_cpu;
    (<0.0>,7141) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7146) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7147) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7158)
    (<0.0>,7159)
    (<0.0>,7160) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,7173)
    (<0.0>,7174) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7179) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7180) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7181) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7182) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7184) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7186) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7187) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7189) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7192) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7193) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7194) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7195) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7200) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7201) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7202) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7203) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7205) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7207) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7208) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7210) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7213) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7214) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7215) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7221) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,7223) tree.c:666: }
    (<0.0>,7226) tree.c:3024: local_irq_restore(flags);
    (<0.0>,7229)
    (<0.0>,7231) fake_sched.h:43: return __running_cpu;
    (<0.0>,7235) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7237) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7241) fake_sched.h:43: return __running_cpu;
    (<0.0>,7245) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7251) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7254)
    (<0.0>,7255) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7257) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7260) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7267) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7270)
    (<0.0>,7274) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7277) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7278) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7279) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7283) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7284) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7285) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7287) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7291) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,7303)
    (<0.0>,7305) fake_sched.h:43: return __running_cpu;
    (<0.0>,7308) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,7310) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,7312) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,7313) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7315) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7322) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7323) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7325) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7331) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7332) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7333) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,7334) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7335) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,7339)
    (<0.0>,7340)
    (<0.0>,7341) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,7342) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,7367)
    (<0.0>,7368)
    (<0.0>,7369) tree.c:1905: local_irq_save(flags);
    (<0.0>,7372)
    (<0.0>,7374) fake_sched.h:43: return __running_cpu;
    (<0.0>,7378) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7380) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7384) fake_sched.h:43: return __running_cpu;
    (<0.0>,7388) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7393) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,7395) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,7396) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,7397) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7399) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7400) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7405) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7406) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7407) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7408) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7410) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7412) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7413) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7415) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7418) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7419) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7420) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,7423) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7425) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7426) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7431) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7432) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7433) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7434) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7436) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7438) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7439) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7441) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7444) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7445) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7446) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,7449) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7453) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7454) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7455) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7456) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7458) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7459) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7460) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7461) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7464) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7467) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7468) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,7476) tree.c:1911: local_irq_restore(flags);
    (<0.0>,7479)
    (<0.0>,7481) fake_sched.h:43: return __running_cpu;
    (<0.0>,7485) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7487) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7491) fake_sched.h:43: return __running_cpu;
    (<0.0>,7495) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7502) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,7504) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,7509) tree.c:3016: local_irq_save(flags);
    (<0.0>,7512)
    (<0.0>,7514) fake_sched.h:43: return __running_cpu;
    (<0.0>,7518) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7520) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7524) fake_sched.h:43: return __running_cpu;
    (<0.0>,7528) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7533) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7534) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,7545)
    (<0.0>,7546)
    (<0.0>,7547) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,7560)
    (<0.0>,7561) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7566) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7567) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7569) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7571) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7573) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7574) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7576) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7579) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7580) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7581) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7582) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7587) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7588) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7589) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7590) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7592) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7594) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7595) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7597) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7600) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7601) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7602) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,7608) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,7624)
    (<0.0>,7625) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7628)
    (<0.0>,7629) tree.c:625: return &rsp->node[0];
    (<0.0>,7633) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7634) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7639) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7640) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7641) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7642) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7644) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7646) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7647) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7649) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7652) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7653) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7654) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7658) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7659) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7661) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7664) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7665) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7669) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7670) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7671) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7672) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7674) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7676) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7677) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7679) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7682) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7683) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7684) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7688) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7691) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7694) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7697) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7698) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7701) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7703) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7706) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7709) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7712) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7713) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7715) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7718) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7722) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7724) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7726) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7729) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7732) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7735) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7736) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7738) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7741) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7745) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7747) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7749) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7752) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,7754) tree.c:666: }
    (<0.0>,7757) tree.c:3024: local_irq_restore(flags);
    (<0.0>,7760)
    (<0.0>,7762) fake_sched.h:43: return __running_cpu;
    (<0.0>,7766) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7768) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7772) fake_sched.h:43: return __running_cpu;
    (<0.0>,7776) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7782) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7785)
    (<0.0>,7786) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7788) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7791) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7798) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7801)
    (<0.0>,7805) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7808) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7809) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7810) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7814) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7815) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7816) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7818) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7826) fake_sched.h:43: return __running_cpu;
    (<0.0>,7830) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,7840) fake_sched.h:43: return __running_cpu;
    (<0.0>,7844) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7845) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7847) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7848) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7849) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7851) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7853) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7854) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7855) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7856) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7857) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7858) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,7860) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,7868)
    (<0.0>,7874) fake_sched.h:43: return __running_cpu;
    (<0.0>,7878)
    (<0.0>,7881) tree.c:755: local_irq_save(flags);
    (<0.0>,7884)
    (<0.0>,7886) fake_sched.h:43: return __running_cpu;
    (<0.0>,7890) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7892) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7896) fake_sched.h:43: return __running_cpu;
    (<0.0>,7900) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7912)
    (<0.0>,7914) fake_sched.h:43: return __running_cpu;
    (<0.0>,7918) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7919) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7921) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7922) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7923) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7924) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7925) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7926) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7927) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,7931) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,7933) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,7934) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,7935) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,7951)
    (<0.0>,7953)
    (<0.0>,7955) fake_sched.h:43: return __running_cpu;
    (<0.0>,7959) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7962) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7963) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7964) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7968) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7969) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7970) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7972) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7977) fake_sched.h:43: return __running_cpu;
    (<0.0>,7980) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7982) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7984) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7985) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7988)
    (<0.0>,7991) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7994) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7995) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7996) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8000) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8001) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8002) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8004) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8009) fake_sched.h:43: return __running_cpu;
    (<0.0>,8012) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8014) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8016) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8017) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8020)
    (<0.0>,8023) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8026) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8027) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8028) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8032) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8033) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8034) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8036) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8043) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8046) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8047) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8048) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8050) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8051) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8053) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8054) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8055) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8056) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8070)
    (<0.0>,8072) tree.c:758: local_irq_restore(flags);
    (<0.0>,8075)
    (<0.0>,8077) fake_sched.h:43: return __running_cpu;
    (<0.0>,8081) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8083) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8087) fake_sched.h:43: return __running_cpu;
    (<0.0>,8091) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8097) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8100) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8105) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8121)
    (<0.0>,8122)
    (<0.0>,8123) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8126)
    (<0.0>,8127) tree.c:625: return &rsp->node[0];
    (<0.0>,8131) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8132) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8137) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8138) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8139) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8140) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8142) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8144) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8145) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8147) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8150) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8151) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8152) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8154) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8155) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,8156) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,8157) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,8161) tree.c:2060: return true;
    (<0.0>,8163) tree.c:2067: }
    (<0.0>,8168) fake_sched.h:43: return __running_cpu;
    (<0.0>,8172)
    (<0.0>,8173) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8176) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8181) tree.c:892: local_irq_save(flags);
    (<0.0>,8184)
    (<0.0>,8186) fake_sched.h:43: return __running_cpu;
    (<0.0>,8190) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8192) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8196) fake_sched.h:43: return __running_cpu;
    (<0.0>,8200) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8212)
    (<0.0>,8214) fake_sched.h:43: return __running_cpu;
    (<0.0>,8218) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8219) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8221) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8222) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8223) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8224) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8225) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8226) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8227) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,8231) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8233) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8234) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8235) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8246)
    (<0.0>,8247)
    (<0.0>,8249) fake_sched.h:43: return __running_cpu;
    (<0.0>,8253) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8257) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8260) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8261) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8262) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8264) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8265) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8267) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8268) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8269) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8270) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8280)
    (<0.0>,8282) tree.c:895: local_irq_restore(flags);
    (<0.0>,8285)
    (<0.0>,8287) fake_sched.h:43: return __running_cpu;
    (<0.0>,8291) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8293) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8297) fake_sched.h:43: return __running_cpu;
    (<0.0>,8301) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8308) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8309) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8310) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,8311) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,8313) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,8314) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8319) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8320) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8321) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8322) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8324) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8326) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8327) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8329) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8332) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8333) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8334) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,8337) tree.c:2237: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,8340)
    (<0.0>,8345) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,8347) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,8348) tree.c:2280: rcu_gp_cleanup(rsp);
    (<0.0>,8388)
    (<0.0>,8389) tree.c:2109: bool needgp = false;
    (<0.0>,8390) tree.c:2110: int nocb = 0;
    (<0.0>,8391) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8394)
    (<0.0>,8395) tree.c:625: return &rsp->node[0];
    (<0.0>,8399) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8401) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8402) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8403) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8408) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8409) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8410) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8411) fake_defs.h:237: switch (size) {
    (<0.0>,8413) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8415) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8416) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8418) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8421) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8422) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8423) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8424) tree.c:2116: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,8427)
    (<0.0>,8428) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8432)
    (<0.0>,8435) fake_sched.h:43: return __running_cpu;
    (<0.0>,8439) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8443) fake_sched.h:43: return __running_cpu;
    (<0.0>,8447) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8452) fake_sched.h:43: return __running_cpu;
    (<0.0>,8456) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8459) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8460) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8467) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8468) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8470) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8472) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,8473) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,8474) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,8476) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,8479) tree.c:2129: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,8482)
    (<0.0>,8483) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8487)
    (<0.0>,8488) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8489) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8494) fake_sched.h:43: return __running_cpu;
    (<0.0>,8498) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,8500) fake_sched.h:43: return __running_cpu;
    (<0.0>,8504) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8511) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8514) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8516) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8517) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8519) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8524) tree.c:2141: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,8527)
    (<0.0>,8528) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8532)
    (<0.0>,8535) fake_sched.h:43: return __running_cpu;
    (<0.0>,8539) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8543) fake_sched.h:43: return __running_cpu;
    (<0.0>,8547) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8552) fake_sched.h:43: return __running_cpu;
    (<0.0>,8556) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8559) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8560) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8567) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8570)
    (<0.0>,8576) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8577) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8580)
    (<0.0>,8585) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8586) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8587) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,8588) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8590) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8595) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8596) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8598) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8602) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8603) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8604) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8606) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8608) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8609) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8610) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8615) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8616) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8617) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8618) fake_defs.h:237: switch (size) {
    (<0.0>,8620) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8622) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8623) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8625) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8628) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8629) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8630) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8632) fake_sched.h:43: return __running_cpu;
    (<0.0>,8635) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8637) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8639) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8640) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8641) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8643) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8646) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8647) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8648) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8670)
    (<0.0>,8671)
    (<0.0>,8672)
    (<0.0>,8673) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8675) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8676) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8678) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8681) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8682) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8683) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8691)
    (<0.0>,8692)
    (<0.0>,8693)
    (<0.0>,8694) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8697) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8700) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8703) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8704) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8707) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8709) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8712) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8714) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8715) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8717) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8720) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8724) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8726) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8729) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8730) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8733) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8735) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8737) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8739) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8742) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8744) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8745) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8747) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8750) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8755) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8757) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8758) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8761) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8764) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8765) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8767) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8770) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8772) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8774) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8776) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8777) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8780) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,8782) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,8785) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8787) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8790) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8791) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8794) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8798) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8799) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8800) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8809)
    (<0.0>,8810)
    (<0.0>,8811)
    (<0.0>,8812) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8815) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8818) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8821) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8822) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8825) tree.c:1750: return false;
    (<0.0>,8827) tree.c:1799: }
    (<0.0>,8829) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8831) tree.c:1843: }
    (<0.0>,8834) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8835) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8837) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8838) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8840) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8844) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8846) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8847) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8849) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8852) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8856) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8857) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8858) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8859) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8861) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8862) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8863) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8864) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8867) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8870) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8871) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8879) tree.c:1896: return ret;
    (<0.0>,8883) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8887) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8889) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8890) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8897)
    (<0.0>,8898)
    (<0.0>,8899) tree.c:1702: int c = rnp->completed;
    (<0.0>,8901) tree.c:1702: int c = rnp->completed;
    (<0.0>,8903) tree.c:1702: int c = rnp->completed;
    (<0.0>,8905) fake_sched.h:43: return __running_cpu;
    (<0.0>,8908) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8910) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8912) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8913) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,8916) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,8919) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,8920) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8924) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8927) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8928) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8929) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,8930) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,8931) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,8933) tree.c:1709: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,8941)
    (<0.0>,8942)
    (<0.0>,8943)
    (<0.0>,8944)
    (<0.0>,8948) tree.c:1710: return needmore;
    (<0.0>,8950) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8952) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8953) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,8956)
    (<0.0>,8958) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,8959) tree.c:2151: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,8962)
    (<0.0>,8963) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8967)
    (<0.0>,8968) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8969) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8974) fake_sched.h:43: return __running_cpu;
    (<0.0>,8978) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,8980) fake_sched.h:43: return __running_cpu;
    (<0.0>,8984) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8991) tree.c:2152: rcu_nocb_gp_cleanup(sq);
    (<0.0>,8994)
    (<0.0>,9004) fake_sched.h:43: return __running_cpu;
    (<0.0>,9010) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,9018) fake_sched.h:43: return __running_cpu;
    (<0.0>,9022) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,9035) fake_sched.h:43: return __running_cpu;
    (<0.0>,9039)
    (<0.0>,9042) tree.c:755: local_irq_save(flags);
    (<0.0>,9045)
    (<0.0>,9047) fake_sched.h:43: return __running_cpu;
    (<0.0>,9051) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9053) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9057) fake_sched.h:43: return __running_cpu;
    (<0.0>,9061) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9073)
    (<0.0>,9075) fake_sched.h:43: return __running_cpu;
    (<0.0>,9079) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9080) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,9082) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,9083) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,9084) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9085) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9086) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9087) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9088) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,9092) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,9094) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,9095) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9096) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,9112)
    (<0.0>,9114)
    (<0.0>,9116) fake_sched.h:43: return __running_cpu;
    (<0.0>,9120) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9123) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9124) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9125) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9129) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9130) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9131) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9133) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9138) fake_sched.h:43: return __running_cpu;
    (<0.0>,9141) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9143) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9145) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9146) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9149)
    (<0.0>,9152) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9155) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9156) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9157) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9161) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9162) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9163) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9165) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9170) fake_sched.h:43: return __running_cpu;
    (<0.0>,9173) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9175) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9177) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9178) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,9181)
    (<0.0>,9184) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9187) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9188) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9189) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9193) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9194) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9195) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9197) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,9204) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9207) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9208) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9209) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9211) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9212) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,9214) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9215) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9216) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9217) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9231)
    (<0.0>,9233) tree.c:758: local_irq_restore(flags);
    (<0.0>,9236)
    (<0.0>,9238) fake_sched.h:43: return __running_cpu;
    (<0.0>,9242) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9244) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9248) fake_sched.h:43: return __running_cpu;
    (<0.0>,9252) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9258) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9261) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,9266) fake_sched.h:43: return __running_cpu;
    (<0.0>,9270)
    (<0.0>,9271) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9274) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,9279) tree.c:892: local_irq_save(flags);
    (<0.0>,9282)
    (<0.0>,9284) fake_sched.h:43: return __running_cpu;
    (<0.0>,9288) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9290) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9294) fake_sched.h:43: return __running_cpu;
    (<0.0>,9298) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9310)
    (<0.0>,9312) fake_sched.h:43: return __running_cpu;
    (<0.0>,9316) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9317) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,9319) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,9320) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,9321) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9322) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9323) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9324) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,9325) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,9329) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9331) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,9332) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9333) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,9344)
    (<0.0>,9345)
    (<0.0>,9347) fake_sched.h:43: return __running_cpu;
    (<0.0>,9351) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9355) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9358) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9359) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9360) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9362) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9363) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,9365) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9366) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9367) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9368) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9378)
    (<0.0>,9380) tree.c:895: local_irq_restore(flags);
    (<0.0>,9383)
    (<0.0>,9385) fake_sched.h:43: return __running_cpu;
    (<0.0>,9389) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9391) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9395) fake_sched.h:43: return __running_cpu;
    (<0.0>,9399) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9413) fake_sched.h:43: return __running_cpu;
    (<0.0>,9417) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,9426) fake_sched.h:43: return __running_cpu;
    (<0.0>,9433) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,9442) fake_sched.h:43: return __running_cpu;
    (<0.0>,9446) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,9448) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,9454) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9455) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9456) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9461) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9462) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9463) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9464) fake_defs.h:237: switch (size) {
    (<0.0>,9466) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9468) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9469) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9471) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9474) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9475) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9476) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,9477) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,9478) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,9482)
    (<0.0>,9483)
    (<0.0>,9484) tree.c:1922: if (delay > 0 &&
    (<0.0>,9489) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9491) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9493) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9494) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9496) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,9501) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,9504)
    (<0.0>,9505) tree.c:625: return &rsp->node[0];
    (<0.0>,9509) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,9510) tree.c:2158: raw_spin_lock_irq_rcu_node(rnp); /* Order GP before ->completed update. */
    (<0.0>,9513)
    (<0.0>,9514) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,9518)
    (<0.0>,9521) fake_sched.h:43: return __running_cpu;
    (<0.0>,9525) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9529) fake_sched.h:43: return __running_cpu;
    (<0.0>,9533) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9538) fake_sched.h:43: return __running_cpu;
    (<0.0>,9542) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9545) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,9546) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,9553) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,9554) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,9558)
    (<0.0>,9559)
    (<0.0>,9562) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9564) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9565) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9566) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9571) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9572) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9573) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9574) fake_defs.h:237: switch (size) {
    (<0.0>,9576) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9578) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9579) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9581) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,9584) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9585) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9586) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,9589) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,9591) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,9593) fake_sched.h:43: return __running_cpu;
    (<0.0>,9596) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9598) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9600) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9601) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9602) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9603) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9611)
    (<0.0>,9612)
    (<0.0>,9613)
    (<0.0>,9614) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9617) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9620) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9623) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9624) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9627) tree.c:1818: return false;
    (<0.0>,9629) tree.c:1843: }
    (<0.0>,9632) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9636) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9637) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9640) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9641) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9652)
    (<0.0>,9653)
    (<0.0>,9654) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,9667)
    (<0.0>,9668) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9673) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9674) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9675) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9676) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9678) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9680) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9681) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9683) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9686) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9687) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9688) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9689) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9694) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9695) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9696) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9697) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9699) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9701) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9702) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9704) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9707) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9708) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9709) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9715) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,9731)
    (<0.0>,9732) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9735)
    (<0.0>,9736) tree.c:625: return &rsp->node[0];
    (<0.0>,9740) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9741) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9746) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9747) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9748) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9749) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9751) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9753) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9754) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9756) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9759) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9760) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9761) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9765) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9766) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9768) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9771) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9772) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9776) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9777) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9778) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9779) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9781) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9783) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9784) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9786) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9789) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9790) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9791) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9795) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,9797) tree.c:666: }
    (<0.0>,9802) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9807) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9808) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9809) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9810) fake_defs.h:237: switch (size) {
    (<0.0>,9812) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9814) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9815) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9817) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9820) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9821) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9822) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9826) tree.c:2174: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,9829)
    (<0.0>,9830) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,9834)
    (<0.0>,9835) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,9836) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,9841) fake_sched.h:43: return __running_cpu;
    (<0.0>,9845) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9847) fake_sched.h:43: return __running_cpu;
    (<0.0>,9851) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9859) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,9861) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,9866) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,9868) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,9872) fake_sched.h:43: return __running_cpu;
    (<0.0>,9876) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9880) fake_sched.h:43: return __running_cpu;
    (<0.0>,9884) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9889) fake_sched.h:43: return __running_cpu;
    (<0.0>,9893) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9904) fake_sched.h:43: return __running_cpu;
    (<0.0>,9908) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9909) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,9911) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,9912) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,9913) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,9915) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,9917) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,9918) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9919) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9920) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9921) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9922) tree.c:942: if (oldval)
    (<0.0>,9930)
    (<0.0>,9936)
    (<0.0>,9945) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9946) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9947) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9951) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9952) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9953) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9955) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9960) fake_sched.h:43: return __running_cpu;
    (<0.0>,9963) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9965) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9968) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9970) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9972) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9975) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9976) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9977) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9981) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9982) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9983) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9985) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9990) fake_sched.h:43: return __running_cpu;
    (<0.0>,9993) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9995) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9998) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,10000) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,10002) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10005) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10006) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10007) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10011) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10012) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10013) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10015) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,10020) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,10025) fake_sched.h:43: return __running_cpu;
    (<0.0>,10030) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,10038) fake_sched.h:43: return __running_cpu;
    (<0.0>,10044) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,10058) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10059) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10060) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10064) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10065) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10066) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10068) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,10072) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,10074) fake_sched.h:43: return __running_cpu;
    (<0.0>,10077) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,10079) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,10101)
    (<0.0>,10102)
    (<0.0>,10103) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10105) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10106) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,10107) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,10109) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,10111) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,10112) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,10113) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,10148)
    (<0.0>,10149)
    (<0.0>,10150) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,10153) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,10166)
    (<0.0>,10167) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10172) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10173) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10174) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10175) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10177) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10179) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10180) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10182) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10185) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10186) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10187) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10188) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10193) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10194) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10195) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10196) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10198) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10200) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10201) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10203) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10206) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10207) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10208) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10216) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,10219)
    (<0.0>,10222) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,10225) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,10227) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,10230) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,10232) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,10236) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,10239)
    (<0.0>,10240) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10242) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10245) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10248) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10251) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10258) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,10260) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,10262) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,10263) tree.c:3517: return 1;
    (<0.0>,10265) tree.c:3548: }
    (<0.0>,10269) tree.c:3561: return 1;
    (<0.0>,10271) tree.c:3563: }
    (<0.0>,10278) fake_sched.h:43: return __running_cpu;
    (<0.0>,10282) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,10286) tree.c:2891: if (user)
    (<0.0>,10294) fake_sched.h:43: return __running_cpu;
    (<0.0>,10298) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,10300) fake_sched.h:43: return __running_cpu;
    (<0.0>,10304) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10310) fake_sched.h:43: return __running_cpu;
    (<0.0>,10314) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,10324)
    (<0.0>,10327) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10328) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10329) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10333) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10334) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10335) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10337) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10341) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,10353)
    (<0.0>,10355) fake_sched.h:43: return __running_cpu;
    (<0.0>,10358) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10360) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10362) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10363) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10365) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10372) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10373) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10375) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10381) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10382) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10383) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10384) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10385) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10389)
    (<0.0>,10390)
    (<0.0>,10391) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,10392) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,10417)
    (<0.0>,10418)
    (<0.0>,10419) tree.c:1905: local_irq_save(flags);
    (<0.0>,10422)
    (<0.0>,10424) fake_sched.h:43: return __running_cpu;
    (<0.0>,10428) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10430) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10434) fake_sched.h:43: return __running_cpu;
    (<0.0>,10438) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10443) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10445) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10446) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10447) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10449) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10450) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10455) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10456) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10457) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10458) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10460) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10462) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10463) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10465) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10468) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10469) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10470) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10473) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10475) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10476) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10481) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10482) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10483) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10484) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10486) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10488) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10489) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10491) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10494) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10495) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10496) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10499) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10503) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10504) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10505) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10506) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10508) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10509) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10510) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10511) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10514) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10517) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10518) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10526) tree.c:1911: local_irq_restore(flags);
    (<0.0>,10529)
    (<0.0>,10531) fake_sched.h:43: return __running_cpu;
    (<0.0>,10535) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10541) fake_sched.h:43: return __running_cpu;
    (<0.0>,10545) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10552) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,10554) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,10559) tree.c:3016: local_irq_save(flags);
    (<0.0>,10562)
    (<0.0>,10564) fake_sched.h:43: return __running_cpu;
    (<0.0>,10568) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10570) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10574) fake_sched.h:43: return __running_cpu;
    (<0.0>,10578) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10583) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10584) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10595)
    (<0.0>,10596)
    (<0.0>,10597) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,10610)
    (<0.0>,10611) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10616) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10617) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10618) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10619) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10621) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10623) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10624) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10626) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10629) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10630) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10631) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10632) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10637) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10638) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10639) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10640) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10642) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10644) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10645) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10647) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10650) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10651) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10652) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10658) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,10674)
    (<0.0>,10675) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10678)
    (<0.0>,10679) tree.c:625: return &rsp->node[0];
    (<0.0>,10683) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10684) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10689) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10690) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10691) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10696) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10697) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10699) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10702) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10703) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10704) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10708) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10709) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10711) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10714) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10715) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10719) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10720) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10721) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10722) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10724) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10726) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10727) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10729) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10732) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10733) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10734) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10738) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,10740) tree.c:666: }
    (<0.0>,10743) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,10746)
    (<0.0>,10747) tree.c:625: return &rsp->node[0];
    (<0.0>,10753)
    (<0.0>,10754) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,10758)
    (<0.0>,10760) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,10761) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,10768) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,10774)
    (<0.0>,10776) fake_sched.h:43: return __running_cpu;
    (<0.0>,10779) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10781) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10783) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10784) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10787)
    (<0.0>,10788) tree.c:625: return &rsp->node[0];
    (<0.0>,10792) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10793) tree.c:2334: bool ret = false;
    (<0.0>,10794) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10795) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10796) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10804)
    (<0.0>,10805)
    (<0.0>,10806)
    (<0.0>,10807) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10810) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10813) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10816) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10817) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10820) tree.c:1818: return false;
    (<0.0>,10822) tree.c:1843: }
    (<0.0>,10825) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10829) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10830) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10831) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10832) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10843)
    (<0.0>,10844)
    (<0.0>,10845)
    (<0.0>,10846) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10848) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10851) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10852) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10863)
    (<0.0>,10864)
    (<0.0>,10865) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,10878)
    (<0.0>,10879) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10885) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10886) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10887) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10889) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10891) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10892) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10894) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10897) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10898) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10899) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10900) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10905) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10906) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10907) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10908) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10910) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10912) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10913) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10915) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10918) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10919) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10920) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10926) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,10942)
    (<0.0>,10943) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10946)
    (<0.0>,10947) tree.c:625: return &rsp->node[0];
    (<0.0>,10951) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10952) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10957) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10958) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10959) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10960) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10962) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10964) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10965) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10967) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10970) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10971) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10972) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10976) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10977) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10979) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10982) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10983) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10987) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10988) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10989) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10990) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10992) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10994) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10995) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10997) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11000) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11001) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11002) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11006) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,11008) tree.c:666: }
    (<0.0>,11013) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11018) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11019) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11020) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11021) fake_defs.h:237: switch (size) {
    (<0.0>,11023) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11025) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11026) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11028) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11031) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11032) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11033) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,11036) tree.c:2318: return true;
    (<0.0>,11038) tree.c:2319: }
    (<0.0>,11042) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,11043) tree.c:2346: return ret;
    (<0.0>,11047) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,11051) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,11052) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,11053) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,11056)
    (<0.0>,11057) tree.c:625: return &rsp->node[0];
    (<0.0>,11062) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,11066)
    (<0.0>,11067)
    (<0.0>,11068) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,11069) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,11072) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,11075)
    (<0.0>,11077) fake_sched.h:43: return __running_cpu;
    (<0.0>,11081) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11083) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11087) fake_sched.h:43: return __running_cpu;
    (<0.0>,11091) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11099) tree.c:3021: if (needwake)
    (<0.0>,11102) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,11110)
    (<0.0>,11111) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,11112) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,11114) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,11121) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11124)
    (<0.0>,11125) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11127) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11130) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11133) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11136) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11143) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,11144) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,11153)
    (<0.0>,11154)
    (<0.0>,11157) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11158) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11159) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11160) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11162) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11164) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11165) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11167) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11170) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11171) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11172) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,11181) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,11183) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,11192) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,11193) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,11215)
    (<0.0>,11216)
    (<0.0>,11217) tree.c:2767: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,11220)
    (<0.0>,11221) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11223) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11226) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11229) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11232) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,11239) tree.c:2779: local_irq_save(flags);
    (<0.0>,11242)
    (<0.0>,11244) fake_sched.h:43: return __running_cpu;
    (<0.0>,11248) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11250) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11254) fake_sched.h:43: return __running_cpu;
    (<0.0>,11258) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11263) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11264) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11265) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11266) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,11267) tree.c:2781: bl = rdp->blimit;
    (<0.0>,11269) tree.c:2781: bl = rdp->blimit;
    (<0.0>,11270) tree.c:2781: bl = rdp->blimit;
    (<0.0>,11273) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,11275) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,11276) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,11277) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11280) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11281) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11282) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11284) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11285) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,11288) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,11289) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,11290) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11293) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11294) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,11295) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11297) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11300) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11302) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11305) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11306) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11309) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11312) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11314) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11316) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11319) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11322) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11324) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11326) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11329) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11331) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11334) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11335) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11338) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11341) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11343) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11345) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11348) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11351) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11353) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11355) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11358) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11360) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11363) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11364) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11367) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11370) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11372) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11374) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11377) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11380) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11382) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11384) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11387) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11389) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11392) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11393) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11396) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,11399) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11401) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11403) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11406) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,11409) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11411) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11413) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,11416) tree.c:2790: local_irq_restore(flags);
    (<0.0>,11419)
    (<0.0>,11421) fake_sched.h:43: return __running_cpu;
    (<0.0>,11425) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11427) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11431) fake_sched.h:43: return __running_cpu;
    (<0.0>,11435) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11440) tree.c:2793: count = count_lazy = 0;
    (<0.0>,11441) tree.c:2793: count = count_lazy = 0;
    (<0.0>,11443) tree.c:2794: while (list) {
    (<0.0>,11446) tree.c:2795: next = list->next;
    (<0.0>,11448) tree.c:2795: next = list->next;
    (<0.0>,11449) tree.c:2795: next = list->next;
    (<0.0>,11452) tree.c:2797: debug_rcu_head_unqueue(list);
    (<0.0>,11455)
    (<0.0>,11457) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11459) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11460) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11466)
    (<0.0>,11467)
    (<0.0>,11468) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11471) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11473) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11476) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,11479) rcu.h:118: head->func(head);
    (<0.0>,11482) rcu.h:118: head->func(head);
    (<0.0>,11483) rcu.h:118: head->func(head);
    (<0.0>,11489)
    (<0.0>,11490) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11491) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11492) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11496) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11497) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11498) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11499) update.c:341: complete(&rcu->completion);
    (<0.0>,11503)
    (<0.0>,11504) fake_sync.h:288: x->done++;
    (<0.0>,11506) fake_sync.h:288: x->done++;
    (<0.0>,11508) fake_sync.h:288: x->done++;
    (<0.0>,11513) rcu.h:120: return false;
    (<0.0>,11515) rcu.h:122: }
    (<0.0>,11518) tree.c:2800: list = next;
    (<0.0>,11519) tree.c:2800: list = next;
    (<0.0>,11520) tree.c:2802: if (++count >= bl &&
    (<0.0>,11522) tree.c:2802: if (++count >= bl &&
    (<0.0>,11523) tree.c:2802: if (++count >= bl &&
    (<0.0>,11527) tree.c:2794: while (list) {
    (<0.0>,11530) tree.c:2808: local_irq_save(flags);
    (<0.0>,11533)
    (<0.0>,11535) fake_sched.h:43: return __running_cpu;
    (<0.0>,11539) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11541) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11545) fake_sched.h:43: return __running_cpu;
    (<0.0>,11549) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11556) tree.c:2814: if (list != NULL) {
    (<0.0>,11560) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11561) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11563) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11565) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11567) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11569) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11570) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11572) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11573) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11578) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11579) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11580) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11581) fake_defs.h:237: switch (size) {
    (<0.0>,11583) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11585) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11586) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11588) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11591) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11592) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11593) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11594) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11595) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11597) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11599) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11600) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,11602) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,11605) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11607) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11610) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11612) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11615) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11617) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11618) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11620) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11621) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11626) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11628) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11631) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11633) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11640) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11641) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11643) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11646) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11648) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11654) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11655) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11656) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11657) tree.c:2840: local_irq_restore(flags);
    (<0.0>,11660)
    (<0.0>,11662) fake_sched.h:43: return __running_cpu;
    (<0.0>,11666) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11668) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11672) fake_sched.h:43: return __running_cpu;
    (<0.0>,11676) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11681) tree.c:2843: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11684)
    (<0.0>,11685) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11687) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11690) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11701) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11704)
    (<0.0>,11708) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11711) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11712) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11713) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11717) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11718) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11719) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11721) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11725) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,11737)
    (<0.0>,11739) fake_sched.h:43: return __running_cpu;
    (<0.0>,11742) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11744) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11746) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11747) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11749) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11756) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11757) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11759) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11765) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11766) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11767) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11768) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11769) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11773)
    (<0.0>,11774)
    (<0.0>,11775) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,11776) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,11801)
    (<0.0>,11802)
    (<0.0>,11803) tree.c:1905: local_irq_save(flags);
    (<0.0>,11806)
    (<0.0>,11808) fake_sched.h:43: return __running_cpu;
    (<0.0>,11812) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11814) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11818) fake_sched.h:43: return __running_cpu;
    (<0.0>,11822) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11827) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11829) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11830) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11831) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11833) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11834) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11839) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11840) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11841) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11842) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11844) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11846) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11847) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11849) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11852) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11853) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11854) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11857) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11859) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11860) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11865) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11866) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11867) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11868) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11870) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11872) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11873) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11875) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11878) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11879) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11880) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11883) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11887) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11888) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11889) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11890) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11892) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11893) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11894) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11895) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11898) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11901) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11902) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11910) tree.c:1911: local_irq_restore(flags);
    (<0.0>,11913)
    (<0.0>,11915) fake_sched.h:43: return __running_cpu;
    (<0.0>,11919) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11921) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11925) fake_sched.h:43: return __running_cpu;
    (<0.0>,11929) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11936) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,11938) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,11943) tree.c:3016: local_irq_save(flags);
    (<0.0>,11946)
    (<0.0>,11948) fake_sched.h:43: return __running_cpu;
    (<0.0>,11952) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11954) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11958) fake_sched.h:43: return __running_cpu;
    (<0.0>,11962) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11967) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11968) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11979)
    (<0.0>,11980)
    (<0.0>,11981) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,11994)
    (<0.0>,11995) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12000) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12001) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12002) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12003) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12005) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12007) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12008) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12010) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12013) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12014) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12015) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12016) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12021) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12022) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12023) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12024) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12026) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12028) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12029) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12031) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12034) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12035) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12036) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12042) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,12058)
    (<0.0>,12059) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12062)
    (<0.0>,12063) tree.c:625: return &rsp->node[0];
    (<0.0>,12067) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12068) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12073) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12074) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12075) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12076) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12078) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12080) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12081) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12083) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12086) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12087) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12088) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12092) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,12093) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12095) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12098) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,12099) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12103) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12104) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12105) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12106) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12108) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12110) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12111) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12113) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12116) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12117) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12118) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,12122) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12125) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,12128) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12131) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12132) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,12135) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12137) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12140) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12143) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12146) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12147) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12149) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12152) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12156) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12158) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12160) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12163) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12166) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12169) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12170) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12172) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12175) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,12179) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12181) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12183) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,12186) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,12188) tree.c:666: }
    (<0.0>,12191) tree.c:3024: local_irq_restore(flags);
    (<0.0>,12194)
    (<0.0>,12196) fake_sched.h:43: return __running_cpu;
    (<0.0>,12200) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12202) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12206) fake_sched.h:43: return __running_cpu;
    (<0.0>,12210) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12216) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,12219)
    (<0.0>,12220) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12222) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12225) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,12232) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12235)
    (<0.0>,12239) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12242) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12243) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12244) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12248) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12249) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12250) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12252) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,12260) fake_sched.h:43: return __running_cpu;
    (<0.0>,12264) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,12274) fake_sched.h:43: return __running_cpu;
    (<0.0>,12278) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12279) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,12281) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,12282) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,12283) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,12285) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,12287) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,12288) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12289) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12290) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12291) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12292) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,12294) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,12302)
    (<0.0>,12308) fake_sched.h:43: return __running_cpu;
    (<0.0>,12312)
    (<0.0>,12315) tree.c:755: local_irq_save(flags);
    (<0.0>,12318)
    (<0.0>,12320) fake_sched.h:43: return __running_cpu;
    (<0.0>,12324) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12326) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12330) fake_sched.h:43: return __running_cpu;
    (<0.0>,12334) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12346)
    (<0.0>,12348) fake_sched.h:43: return __running_cpu;
    (<0.0>,12352) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12353) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12355) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12356) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12357) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12358) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12359) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12360) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12361) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,12365) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,12367) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,12368) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12369) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12385)
    (<0.0>,12387)
    (<0.0>,12389) fake_sched.h:43: return __running_cpu;
    (<0.0>,12393) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12396) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12397) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12398) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12402) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12403) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12404) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12406) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12411) fake_sched.h:43: return __running_cpu;
    (<0.0>,12414) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12416) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12418) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12419) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12422)
    (<0.0>,12425) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12428) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12429) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12430) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12434) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12435) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12436) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12438) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12443) fake_sched.h:43: return __running_cpu;
    (<0.0>,12446) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12448) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12450) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12451) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12454)
    (<0.0>,12457) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12460) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12461) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12462) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12466) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12467) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12468) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12470) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12477) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12480) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12481) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12482) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12484) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12485) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12487) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12488) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12489) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12490) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12504)
    (<0.0>,12506) tree.c:758: local_irq_restore(flags);
    (<0.0>,12509)
    (<0.0>,12511) fake_sched.h:43: return __running_cpu;
    (<0.0>,12515) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12517) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12521) fake_sched.h:43: return __running_cpu;
    (<0.0>,12525) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12531) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12534) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12539) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12544) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12545) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12546) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12547) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12549) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12551) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12552) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12554) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12557) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12558) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12559) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12566) fake_sched.h:43: return __running_cpu;
    (<0.0>,12570)
    (<0.0>,12571) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12574) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12579) tree.c:892: local_irq_save(flags);
    (<0.0>,12582)
    (<0.0>,12584) fake_sched.h:43: return __running_cpu;
    (<0.0>,12588) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12590) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12594) fake_sched.h:43: return __running_cpu;
    (<0.0>,12598) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12610)
    (<0.0>,12612) fake_sched.h:43: return __running_cpu;
    (<0.0>,12616) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12617) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12619) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12620) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12621) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12622) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12623) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12624) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12625) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,12629) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,12631) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,12632) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,12633) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,12644)
    (<0.0>,12645)
    (<0.0>,12647) fake_sched.h:43: return __running_cpu;
    (<0.0>,12651) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12655) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12658) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12659) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12660) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12662) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12663) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12665) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12666) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12667) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12668) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12678)
    (<0.0>,12680) tree.c:895: local_irq_restore(flags);
    (<0.0>,12683)
    (<0.0>,12685) fake_sched.h:43: return __running_cpu;
    (<0.0>,12689) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12691) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12695) fake_sched.h:43: return __running_cpu;
    (<0.0>,12699) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12706) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,12708) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,12709) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,12754)
    (<0.0>,12755) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12758)
    (<0.0>,12759) tree.c:625: return &rsp->node[0];
    (<0.0>,12763) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12765) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12766) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12767) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12772) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12773) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12774) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12775) fake_defs.h:237: switch (size) {
    (<0.0>,12777) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12779) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12780) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12782) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12785) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12786) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12787) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12788) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,12791)
    (<0.0>,12792) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12796)
    (<0.0>,12799) fake_sched.h:43: return __running_cpu;
    (<0.0>,12803) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12807) fake_sched.h:43: return __running_cpu;
    (<0.0>,12811) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12816) fake_sched.h:43: return __running_cpu;
    (<0.0>,12820) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12823) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12824) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12831) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12836) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12837) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12838) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12839) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12841) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12843) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12844) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12846) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12849) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12850) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12851) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12862)
    (<0.0>,12868)
    (<0.0>,12873) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12878) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12879) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12880) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12881) fake_defs.h:237: switch (size) {
    (<0.0>,12883) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12885) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12886) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12888) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12891) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12892) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12893) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12894) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12907)
    (<0.0>,12908) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12913) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12914) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12915) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12916) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12918) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12920) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12921) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12923) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12926) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12927) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12928) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12929) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12934) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12935) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12936) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12937) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12939) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12941) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12944) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12947) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12948) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12949) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12957) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12958) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12971)
    (<0.0>,12972) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12977) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12978) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12979) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12980) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12982) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12984) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12985) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12987) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12990) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12991) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12992) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12993) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12998) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12999) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13000) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13001) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13003) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13005) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13006) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13008) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13011) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13012) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13013) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13020) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,13021) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,13022) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,13025) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,13040)
    (<0.0>,13041) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,13042) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,13043) tree.c:1240: rsp->gp_start = j;
    (<0.0>,13044) tree.c:1240: rsp->gp_start = j;
    (<0.0>,13046) tree.c:1240: rsp->gp_start = j;
    (<0.0>,13067) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13068) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13069) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13070) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13072) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13074) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13075) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13077) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13080) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13081) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13082) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13083) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,13084) update.c:466: if (till_stall_check < 3) {
    (<0.0>,13087) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,13091) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,13096) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,13098) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13099) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13101) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13102) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13107) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13108) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13109) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13110) fake_defs.h:237: switch (size) {
    (<0.0>,13112) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13114) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13115) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13117) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13120) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13121) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13122) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,13123) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13124) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13127) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13129) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,13130) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13135) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13136) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13137) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13138) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13140) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13142) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13143) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13145) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13148) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13149) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13150) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13151) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13153) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,13157) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13159) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13161) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13162) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13164) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13165) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13166) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,13170) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,13173)
    (<0.0>,13174) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13178)
    (<0.0>,13179) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13180) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13185) fake_sched.h:43: return __running_cpu;
    (<0.0>,13189) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,13191) fake_sched.h:43: return __running_cpu;
    (<0.0>,13195) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13202) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13205) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13208) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13209) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13211) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13212) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13214) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13219) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,13220) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,13224)
    (<0.0>,13225)
    (<0.0>,13226) tree.c:1922: if (delay > 0 &&
    (<0.0>,13230) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,13233)
    (<0.0>,13234) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13238)
    (<0.0>,13241) fake_sched.h:43: return __running_cpu;
    (<0.0>,13245) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,13249) fake_sched.h:43: return __running_cpu;
    (<0.0>,13253) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13258) fake_sched.h:43: return __running_cpu;
    (<0.0>,13262) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,13265) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13266) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13273) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13275) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13276) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13278) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,13281) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,13283) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,13286) tree.c:1973: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,13289)
    (<0.0>,13290) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13294)
    (<0.0>,13295) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13296) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13301) fake_sched.h:43: return __running_cpu;
    (<0.0>,13305) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,13307) fake_sched.h:43: return __running_cpu;
    (<0.0>,13311) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13319) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13321) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13323) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13324) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13326) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,13331) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13334) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13336) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13337) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13339) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13344) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,13345) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,13349)
    (<0.0>,13350)
    (<0.0>,13351) tree.c:1922: if (delay > 0 &&
    (<0.0>,13355) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,13358)
    (<0.0>,13359) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13363)
    (<0.0>,13366) fake_sched.h:43: return __running_cpu;
    (<0.0>,13370) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,13374) fake_sched.h:43: return __running_cpu;
    (<0.0>,13378) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13383) fake_sched.h:43: return __running_cpu;
    (<0.0>,13387) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,13390) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13391) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,13399) fake_sched.h:43: return __running_cpu;
    (<0.0>,13402) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13404) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13406) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13407) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,13413)
    (<0.0>,13414) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13416) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13421) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13422) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13424) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13428) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13429) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13430) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,13432) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13434) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13435) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13437) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,13439) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13441) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13442) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13443) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13448) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13449) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13450) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13451) fake_defs.h:237: switch (size) {
    (<0.0>,13453) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13455) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13456) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13458) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13461) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13462) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13463) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13464) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13466) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13467) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13469) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13474) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13475) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13477) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13478) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13480) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13484) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13485) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13486) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13489) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13490) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13492) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13495) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13496) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13497) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13519)
    (<0.0>,13520)
    (<0.0>,13521)
    (<0.0>,13522) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13524) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13525) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13527) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13530) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13534) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13535) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13536) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13537) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13539) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13540) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13541) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13542) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13545) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13548) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13549) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13557) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13558) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13559) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13568)
    (<0.0>,13569)
    (<0.0>,13570)
    (<0.0>,13571) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13574) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13577) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13580) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13581) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13584) tree.c:1750: return false;
    (<0.0>,13586) tree.c:1799: }
    (<0.0>,13589) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13591) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13593) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13594) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13596) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13599) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13601) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13602) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13604) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13607) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13609) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13610) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13612) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13618) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13619) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13622) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13626) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13628) fake_sched.h:43: return __running_cpu;
    (<0.0>,13632) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13633) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13635) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13636) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13638) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13641) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13642) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,13645)
    (<0.0>,13646) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,13648) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,13649) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,13651) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,13661)
    (<0.0>,13666) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13670) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13671) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13672) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13673) fake_defs.h:237: switch (size) {
    (<0.0>,13675) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13676) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13677) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13678) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13681) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13684) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13685) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13688) tree.c:1896: return ret;
    (<0.0>,13692) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,13695)
    (<0.0>,13699) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,13702)
    (<0.0>,13703) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13707)
    (<0.0>,13708) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13709) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13714) fake_sched.h:43: return __running_cpu;
    (<0.0>,13718) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,13720) fake_sched.h:43: return __running_cpu;
    (<0.0>,13724) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13739) fake_sched.h:43: return __running_cpu;
    (<0.0>,13745) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,13751) fake_sched.h:43: return __running_cpu;
    (<0.0>,13758) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,13760) fake_sched.h:43: return __running_cpu;
    (<0.0>,13767) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
    (<0.0>,13775) fake_sched.h:43: return __running_cpu;
    (<0.0>,13779) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,13792) fake_sched.h:43: return __running_cpu;
    (<0.0>,13796)
    (<0.0>,13799) tree.c:755: local_irq_save(flags);
    (<0.0>,13802)
    (<0.0>,13804) fake_sched.h:43: return __running_cpu;
    (<0.0>,13808) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13810) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13814) fake_sched.h:43: return __running_cpu;
    (<0.0>,13818) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13830)
    (<0.0>,13832) fake_sched.h:43: return __running_cpu;
    (<0.0>,13836) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13837) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13839) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13840) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13841) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13842) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13843) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13844) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13845) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,13849) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,13851) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,13852) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,13853) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,13869)
    (<0.0>,13871)
    (<0.0>,13873) fake_sched.h:43: return __running_cpu;
    (<0.0>,13877) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13880) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13881) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13882) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13886) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13887) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13888) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13890) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13895) fake_sched.h:43: return __running_cpu;
    (<0.0>,13898) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13900) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13902) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13903) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,13906)
    (<0.0>,13909) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13912) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13913) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13914) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13918) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13919) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13920) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13922) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13927) fake_sched.h:43: return __running_cpu;
    (<0.0>,13930) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13932) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13934) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13935) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,13938)
    (<0.0>,13941) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13944) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13945) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13946) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13950) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13951) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13952) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13954) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13961) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13964) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13965) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13966) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13968) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13969) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13971) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13972) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13973) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13974) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13988)
    (<0.0>,13990) tree.c:758: local_irq_restore(flags);
    (<0.0>,13993)
    (<0.0>,13995) fake_sched.h:43: return __running_cpu;
    (<0.0>,13999) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14001) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14005) fake_sched.h:43: return __running_cpu;
    (<0.0>,14009) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14015) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,14018) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,14023) fake_sched.h:43: return __running_cpu;
    (<0.0>,14027)
    (<0.0>,14028) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,14031) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,14036) tree.c:892: local_irq_save(flags);
    (<0.0>,14039)
    (<0.0>,14041) fake_sched.h:43: return __running_cpu;
    (<0.0>,14045) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14047) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14051) fake_sched.h:43: return __running_cpu;
    (<0.0>,14055) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14067)
    (<0.0>,14069) fake_sched.h:43: return __running_cpu;
    (<0.0>,14073) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14074) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14076) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14077) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14078) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14079) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14080) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14081) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14082) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,14086) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,14088) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,14089) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,14090) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,14101)
    (<0.0>,14102)
    (<0.0>,14104) fake_sched.h:43: return __running_cpu;
    (<0.0>,14108) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14112) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14115) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14116) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14117) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14119) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14120) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14122) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14123) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14124) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14125) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14135)
    (<0.0>,14137) tree.c:895: local_irq_restore(flags);
    (<0.0>,14140)
    (<0.0>,14142) fake_sched.h:43: return __running_cpu;
    (<0.0>,14146) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14148) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14152) fake_sched.h:43: return __running_cpu;
    (<0.0>,14156) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14170) fake_sched.h:43: return __running_cpu;
    (<0.0>,14174) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,14183) fake_sched.h:43: return __running_cpu;
    (<0.0>,14190) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,14199) fake_sched.h:43: return __running_cpu;
    (<0.0>,14203) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,14205) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,14211) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14212) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14213) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14218) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14219) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14220) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14221) fake_defs.h:237: switch (size) {
    (<0.0>,14223) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14225) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14226) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14228) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14231) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14232) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14233) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,14235) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14237) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14239) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14240) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14242) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,14247) tree.c:2046: return true;
    (<0.0>,14249) tree.c:2047: }
    (<0.0>,14253) tree.c:2214: first_gp_fqs = true;
    (<0.0>,14254) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,14255) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,14256) tree.c:2216: if (j > HZ) {
    (<0.0>,14259) tree.c:2220: ret = 0;
    (<0.0>,14261) tree.c:2222: if (!ret) {
    (<0.0>,14264) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14265) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14267) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14269) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,14271) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14272) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14275) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14276) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14281) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14282) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14283) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14284) fake_defs.h:237: switch (size) {
    (<0.0>,14286) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14288) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14289) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14291) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,14294) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14295) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14296) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,14300) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,14302) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,14306) fake_sched.h:43: return __running_cpu;
    (<0.0>,14310) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,14314) fake_sched.h:43: return __running_cpu;
    (<0.0>,14318) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14323) fake_sched.h:43: return __running_cpu;
    (<0.0>,14327) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,14338) fake_sched.h:43: return __running_cpu;
    (<0.0>,14342) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14343) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,14345) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,14346) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,14347) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,14349) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,14351) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,14352) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14353) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14354) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14355) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14356) tree.c:942: if (oldval)
    (<0.0>,14364)
    (<0.0>,14370)
    (<0.0>,14379) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14380) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14381) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14385) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14386) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14387) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14389) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14394) fake_sched.h:43: return __running_cpu;
    (<0.0>,14397) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14399) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14402) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14404) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14406) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14409) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14410) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14411) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14415) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14416) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14417) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14419) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14424) fake_sched.h:43: return __running_cpu;
    (<0.0>,14427) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14429) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14432) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14434) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14436) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14439) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14440) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14441) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14445) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14446) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14447) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14449) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14454) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,14459) fake_sched.h:43: return __running_cpu;
    (<0.0>,14464) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,14472) fake_sched.h:43: return __running_cpu;
    (<0.0>,14478) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,14492) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14493) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14494) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14498) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14499) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14500) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14502) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14506) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14508) fake_sched.h:43: return __running_cpu;
    (<0.0>,14511) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14513) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14535)
    (<0.0>,14536)
    (<0.0>,14537) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14539) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14540) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14541) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14543) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14545) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14546) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,14547) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,14582)
    (<0.0>,14583)
    (<0.0>,14584) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,14587) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,14600)
    (<0.0>,14601) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14606) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14607) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14608) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14609) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14611) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14613) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14614) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14616) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14619) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14620) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14621) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14622) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14627) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14628) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14629) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14630) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14632) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14634) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14635) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14637) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14640) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14641) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14642) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14648) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,14673)
    (<0.0>,14674) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,14679) tree.c:1472: j = jiffies;
    (<0.0>,14680) tree.c:1472: j = jiffies;
    (<0.0>,14681) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14686) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14687) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14688) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14691) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14693) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14696) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14699) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14700) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14701) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14702) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14704) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14709) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14710) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14711) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14712) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14714) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14716) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14717) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14719) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14722) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14723) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14724) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14725) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14727) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14732) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14733) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14734) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14735) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14737) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14739) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14740) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14742) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14745) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14746) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14747) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14748) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14750) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14755) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14756) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14757) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14758) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14760) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14762) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14763) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14765) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14768) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14769) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14770) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14771) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14772) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,14773) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,14777) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,14778) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,14784) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,14787)
    (<0.0>,14790) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,14793) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14795) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14798) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14802) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14806) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14808) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14811) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,14815) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,14818) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14820) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14822) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14823) tree.c:3511: return 1;
    (<0.0>,14825) tree.c:3548: }
    (<0.0>,14829) tree.c:3561: return 1;
    (<0.0>,14831) tree.c:3563: }
    (<0.0>,14838) fake_sched.h:43: return __running_cpu;
    (<0.0>,14842) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,14846) tree.c:2891: if (user)
    (<0.0>,14854) fake_sched.h:43: return __running_cpu;
    (<0.0>,14858) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,14860) fake_sched.h:43: return __running_cpu;
    (<0.0>,14864) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14870) fake_sched.h:43: return __running_cpu;
    (<0.0>,14874) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,14884)
    (<0.0>,14887) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14888) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14889) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14893) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14894) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14895) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14897) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14901) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,14913)
    (<0.0>,14915) fake_sched.h:43: return __running_cpu;
    (<0.0>,14918) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,14920) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,14922) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,14923) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14925) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14932) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14933) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14935) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14941) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14942) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14943) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14944) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,14945) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,14949)
    (<0.0>,14950)
    (<0.0>,14951) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,14952) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,14977)
    (<0.0>,14978)
    (<0.0>,14979) tree.c:1905: local_irq_save(flags);
    (<0.0>,14982)
    (<0.0>,14984) fake_sched.h:43: return __running_cpu;
    (<0.0>,14988) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14990) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14994) fake_sched.h:43: return __running_cpu;
    (<0.0>,14998) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15003) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15005) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15006) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15007) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15009) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15010) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15015) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15016) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15017) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15018) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15020) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15022) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15023) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15025) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15028) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15029) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15030) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15033) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15035) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15036) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15041) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15042) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15043) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15044) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15046) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15048) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15054) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15055) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15056) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15059) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15063) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15064) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15065) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15066) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15068) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15069) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15070) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15071) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15074) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15077) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15078) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15086) tree.c:1911: local_irq_restore(flags);
    (<0.0>,15089)
    (<0.0>,15091) fake_sched.h:43: return __running_cpu;
    (<0.0>,15095) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15097) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15101) fake_sched.h:43: return __running_cpu;
    (<0.0>,15105) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15112) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15114) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15117) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,15121) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,15125) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15127) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15128) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15129) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,15147)
    (<0.0>,15148)
    (<0.0>,15149)
    (<0.0>,15150) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,15152) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,15153) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,15157) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15158) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15159) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15161) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,15165)
    (<0.0>,15166)
    (<0.0>,15167) fake_sync.h:83: local_irq_save(flags);
    (<0.0>,15170)
    (<0.0>,15172) fake_sched.h:43: return __running_cpu;
    (<0.0>,15176) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15178) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15182) fake_sched.h:43: return __running_cpu;
    (<0.0>,15186) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15192) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,15193) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,15200) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,15204) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,15208) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15210) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15211) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15213) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15216) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15218) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15219) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15221) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,15224) tree.c:2491: rdp->gpwrap) {
    (<0.0>,15226) tree.c:2491: rdp->gpwrap) {
    (<0.0>,15229) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,15231) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,15232) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,15233) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,15235) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,15236) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,15240) tree.c:2508: rdp->core_needs_qs = false;
    (<0.0>,15242) tree.c:2508: rdp->core_needs_qs = false;
    (<0.0>,15243) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,15244) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,15245) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,15254)
    (<0.0>,15255)
    (<0.0>,15256)
    (<0.0>,15257) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15260) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15263) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15266) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15267) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15270) tree.c:1750: return false;
    (<0.0>,15272) tree.c:1799: }
    (<0.0>,15275) tree.c:2514: needwake = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,15276) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15277) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15278) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15279) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15281) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15282) tree.c:2516: rcu_report_qs_rnp(mask, rsp, rnp, rnp->gpnum, flags);
    (<0.0>,15306)
    (<0.0>,15307)
    (<0.0>,15308)
    (<0.0>,15309)
    (<0.0>,15310)
    (<0.0>,15311) tree.c:2385: unsigned long oldmask = 0;
    (<0.0>,15313) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15315) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15316) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15320) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15322) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15323) tree.c:2390: if (!(rnp->qsmask & mask) || rnp->gpnum != gps) {
    (<0.0>,15326) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15331) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15332) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15336) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15337) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15338) tree.c:2399: WARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */
    (<0.0>,15339) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,15341) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,15343) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,15345) tree.c:2400: rnp->qsmask &= ~mask;
    (<0.0>,15348) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,15350) tree.c:2406: if (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {
    (<0.0>,15356) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15357) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15358) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15360) tree.c:2409: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,15364)
    (<0.0>,15365)
    (<0.0>,15366) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,15367) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,15370) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,15373)
    (<0.0>,15375) fake_sched.h:43: return __running_cpu;
    (<0.0>,15379) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15381) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15385) fake_sched.h:43: return __running_cpu;
    (<0.0>,15389) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15399) tree.c:2518: if (needwake)
    (<0.0>,15406) tree.c:3016: local_irq_save(flags);
    (<0.0>,15409)
    (<0.0>,15411) fake_sched.h:43: return __running_cpu;
    (<0.0>,15415) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15417) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15421) fake_sched.h:43: return __running_cpu;
    (<0.0>,15425) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15430) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15431) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15442)
    (<0.0>,15443)
    (<0.0>,15444) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,15457)
    (<0.0>,15458) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15463) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15464) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15465) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15466) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15468) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15470) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15471) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15473) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15476) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15477) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15478) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15479) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15484) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15485) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15486) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15487) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15489) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15491) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15492) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15494) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15497) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15498) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15499) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15505) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,15507) tree.c:666: }
    (<0.0>,15510) tree.c:3024: local_irq_restore(flags);
    (<0.0>,15513)
    (<0.0>,15515) fake_sched.h:43: return __running_cpu;
    (<0.0>,15519) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15521) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15525) fake_sched.h:43: return __running_cpu;
    (<0.0>,15529) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15535) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,15538)
    (<0.0>,15539) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15541) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15544) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15551) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15554)
    (<0.0>,15558) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15561) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15562) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15563) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15567) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15568) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15569) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15571) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15575) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,15587)
    (<0.0>,15589) fake_sched.h:43: return __running_cpu;
    (<0.0>,15592) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15594) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15596) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15597) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15599) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15606) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15607) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15609) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15615) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15616) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15617) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15618) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,15619) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,15623)
    (<0.0>,15624)
    (<0.0>,15625) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,15626) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,15651)
    (<0.0>,15652)
    (<0.0>,15653) tree.c:1905: local_irq_save(flags);
    (<0.0>,15656)
    (<0.0>,15658) fake_sched.h:43: return __running_cpu;
    (<0.0>,15662) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15664) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15668) fake_sched.h:43: return __running_cpu;
    (<0.0>,15672) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15677) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15679) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15680) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15681) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15683) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15684) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15689) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15690) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15691) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15696) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15697) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15699) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15702) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15703) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15704) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15707) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15709) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15710) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15715) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15716) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15717) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15718) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15720) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15722) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15723) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15725) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15728) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15729) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15730) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15733) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15737) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15738) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15739) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15740) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15742) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15743) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15744) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15745) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15748) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15751) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15752) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15760) tree.c:1911: local_irq_restore(flags);
    (<0.0>,15763)
    (<0.0>,15765) fake_sched.h:43: return __running_cpu;
    (<0.0>,15769) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15771) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15775) fake_sched.h:43: return __running_cpu;
    (<0.0>,15779) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15786) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15788) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15793) tree.c:3016: local_irq_save(flags);
    (<0.0>,15796)
    (<0.0>,15798) fake_sched.h:43: return __running_cpu;
    (<0.0>,15802) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15804) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15808) fake_sched.h:43: return __running_cpu;
    (<0.0>,15812) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15817) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15818) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15829)
    (<0.0>,15830)
    (<0.0>,15831) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,15844)
    (<0.0>,15845) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15850) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15851) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15852) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15855) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15857) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15858) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15860) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15863) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15864) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15865) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15866) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15871) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15872) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15873) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15878) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15879) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15881) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15885) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15886) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15892) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,15908)
    (<0.0>,15909) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15912)
    (<0.0>,15913) tree.c:625: return &rsp->node[0];
    (<0.0>,15917) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15918) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15923) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15924) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15925) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15926) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15930) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15933) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15936) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15937) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15938) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15942) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15943) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,15945) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,15948) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,15949) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15953) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15954) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15955) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15956) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15960) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15963) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15966) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15967) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15968) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15972) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,15975) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,15978) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,15981) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,15982) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,15985) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15987) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15990) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15993) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15996) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15997) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15999) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16002) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16006) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16008) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16010) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16013) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16016) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16019) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16020) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16022) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16025) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16029) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16031) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16033) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16036) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,16038) tree.c:666: }
    (<0.0>,16041) tree.c:3024: local_irq_restore(flags);
    (<0.0>,16044)
    (<0.0>,16046) fake_sched.h:43: return __running_cpu;
    (<0.0>,16050) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,16052) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,16056) fake_sched.h:43: return __running_cpu;
    (<0.0>,16060) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,16066) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,16069)
    (<0.0>,16070) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16072) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16075) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16082) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,16085)
    (<0.0>,16089) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16092) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16093) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16094) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16098) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16099) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16100) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16102) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,16110) fake_sched.h:43: return __running_cpu;
    (<0.0>,16114) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,16124) fake_sched.h:43: return __running_cpu;
    (<0.0>,16128) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,16129) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,16131) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,16132) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,16133) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,16135) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,16137) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,16138) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16139) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16140) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16141) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16142) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,16144) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,16152)
    (<0.0>,16158) fake_sched.h:43: return __running_cpu;
    (<0.0>,16162)
    (<0.0>,16165) tree.c:755: local_irq_save(flags);
    (<0.0>,16168)
    (<0.0>,16170) fake_sched.h:43: return __running_cpu;
    (<0.0>,16174) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,16176) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,16180) fake_sched.h:43: return __running_cpu;
    (<0.0>,16184) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,16196)
    (<0.0>,16198) fake_sched.h:43: return __running_cpu;
    (<0.0>,16202) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,16203) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,16205) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,16206) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,16207) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16208) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16209) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16210) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16211) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,16215) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,16217) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,16218) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,16219) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,16235)
    (<0.0>,16237)
    (<0.0>,16239) fake_sched.h:43: return __running_cpu;
    (<0.0>,16243) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,16246) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16247) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16248) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16252) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16253) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16254) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16256) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16261) fake_sched.h:43: return __running_cpu;
    (<0.0>,16264) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16266) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16268) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16269) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,16272)
    (<0.0>,16275) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16278) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16279) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16280) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16284) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16285) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16286) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16288) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16293) fake_sched.h:43: return __running_cpu;
    (<0.0>,16296) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16298) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16300) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16301) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,16304)
    (<0.0>,16307) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16310) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16311) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16312) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16316) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16317) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16318) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16320) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,16327) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16330) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16331) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16332) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16334) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16335) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,16337) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16338) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16339) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16340) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16354)
    (<0.0>,16356) tree.c:758: local_irq_restore(flags);
    (<0.0>,16359)
    (<0.0>,16361) fake_sched.h:43: return __running_cpu;
    (<0.0>,16365) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,16367) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,16371) fake_sched.h:43: return __running_cpu;
    (<0.0>,16375) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,16381) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,16384) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,16389) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16405)
    (<0.0>,16406)
    (<0.0>,16407) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16410)
    (<0.0>,16411) tree.c:625: return &rsp->node[0];
    (<0.0>,16415) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16416) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16421) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16422) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16423) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16424) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16426) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16428) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16429) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16431) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16434) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16435) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16436) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16438) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16439) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16440) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16441) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16445) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16450) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16451) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16452) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16453) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16455) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16457) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16458) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16460) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16463) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16464) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16465) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16468) tree.c:2066: return false;
    (<0.0>,16470) tree.c:2067: }
    (<0.0>,16475) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16491)
    (<0.0>,16492)
    (<0.0>,16493) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16496)
    (<0.0>,16497) tree.c:625: return &rsp->node[0];
    (<0.0>,16501) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16502) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16507) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16508) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16509) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16510) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16512) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16514) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16515) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16517) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16520) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16521) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16522) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16524) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16525) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16526) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16527) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16531) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16536) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16537) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16538) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16539) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16541) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16543) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16544) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16546) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16549) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16550) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16551) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16554) tree.c:2066: return false;
    (<0.0>,16556) tree.c:2067: }
    (<0.0>,16561) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16577)
    (<0.0>,16578)
    (<0.0>,16579) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16582)
    (<0.0>,16583) tree.c:625: return &rsp->node[0];
    (<0.0>,16587) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16588) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16593) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16594) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16595) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16596) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16598) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16600) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16601) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16603) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16606) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16607) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16608) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16610) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16611) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16612) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16613) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16617) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16622) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16623) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16624) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16625) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16627) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16629) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16630) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16632) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16635) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16636) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16637) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16640) tree.c:2066: return false;
    (<0.0>,16642) tree.c:2067: }
    (<0.0>,16647) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16663)
    (<0.0>,16664)
    (<0.0>,16665) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16668)
    (<0.0>,16669) tree.c:625: return &rsp->node[0];
    (<0.0>,16673) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16674) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16679) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16680) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16681) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16682) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16684) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16686) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16692) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16693) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16694) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16696) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16697) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16698) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16699) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16703) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16708) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16709) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16710) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16711) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16713) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16715) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16716) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16718) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16721) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16722) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16723) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16726) tree.c:2066: return false;
    (<0.0>,16728) tree.c:2067: }
    (<0.0>,16733) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16749)
    (<0.0>,16750)
    (<0.0>,16751) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16754)
    (<0.0>,16755) tree.c:625: return &rsp->node[0];
    (<0.0>,16759) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16760) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16765) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16766) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16767) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16770) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16772) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16773) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16775) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16778) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16779) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16780) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16782) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16783) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16784) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16785) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16789) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16794) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16795) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16796) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16797) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16799) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16801) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16802) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16804) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16807) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16808) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16809) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16812) tree.c:2066: return false;
    (<0.0>,16814) tree.c:2067: }
      (<0.1>,764) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,767) fake_sync.h:281: while (!x->done)
      (<0.1>,769) fake_sync.h:281: while (!x->done)
      (<0.1>,776) fake_sched.h:43: return __running_cpu;
      (<0.1>,780)
      (<0.1>,781) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,784) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,789) tree.c:892: local_irq_save(flags);
      (<0.1>,792)
      (<0.1>,794) fake_sched.h:43: return __running_cpu;
      (<0.1>,798) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,804) fake_sched.h:43: return __running_cpu;
      (<0.1>,808) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,820)
      (<0.1>,822) fake_sched.h:43: return __running_cpu;
      (<0.1>,826) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,827) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,829) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,830) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,831) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,832) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,833) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,834) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,835) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,839) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,841) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,842) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,843) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,854)
      (<0.1>,855)
      (<0.1>,857) fake_sched.h:43: return __running_cpu;
      (<0.1>,861) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,865) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,868) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,869) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,870) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,872) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,873) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,875) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,876) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,877) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,878) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,888)
      (<0.1>,890) tree.c:895: local_irq_restore(flags);
      (<0.1>,893)
      (<0.1>,895) fake_sched.h:43: return __running_cpu;
      (<0.1>,899) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,905) fake_sched.h:43: return __running_cpu;
      (<0.1>,909) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,917) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,919) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,924)
      (<0.1>,927) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,929) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,931) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,932) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,940) litmus.c:91: y = 1;
  (<0>,10336) litmus.c:69: r_y = y;
  (<0>,10337) litmus.c:69: r_y = y;
  (<0>,10347) fake_sched.h:43: return __running_cpu;
  (<0>,10351)
  (<0>,10354) tree.c:755: local_irq_save(flags);
  (<0>,10357)
  (<0>,10359) fake_sched.h:43: return __running_cpu;
  (<0>,10363) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10365) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10369) fake_sched.h:43: return __running_cpu;
  (<0>,10373) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,10385)
  (<0>,10387) fake_sched.h:43: return __running_cpu;
  (<0>,10391) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,10392) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,10394) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,10395) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,10396) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10397) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10398) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10399) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10400) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,10404) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,10406) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,10407) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,10408) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,10424)
  (<0>,10426)
  (<0>,10428) fake_sched.h:43: return __running_cpu;
  (<0>,10432) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,10435) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10436) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10437) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10441) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10442) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10443) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10445) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10450) fake_sched.h:43: return __running_cpu;
  (<0>,10453) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10455) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10457) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10458) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,10461)
  (<0>,10464) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10467) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10468) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10469) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10473) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10474) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10475) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10477) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10482) fake_sched.h:43: return __running_cpu;
  (<0>,10485) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10487) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10489) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10490) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,10493)
  (<0>,10496) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10499) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10500) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10501) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10505) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10506) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10507) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10509) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10516) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10519) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10520) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10521) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10523) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10524) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10526) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10527) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10528) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10529) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10543)
  (<0>,10545) tree.c:758: local_irq_restore(flags);
  (<0>,10548)
  (<0>,10550) fake_sched.h:43: return __running_cpu;
  (<0>,10554) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10556) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10560) fake_sched.h:43: return __running_cpu;
  (<0>,10564) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,10570) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,10573) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,10578) litmus.c:138: if (pthread_join(tu, NULL))
      (<0.1>,941) litmus.c:93: fake_release_cpu(get_cpu());
      (<0.1>,942) fake_sched.h:43: return __running_cpu;
      (<0.1>,946)
      (<0.1>,949) tree.c:755: local_irq_save(flags);
      (<0.1>,952)
      (<0.1>,954) fake_sched.h:43: return __running_cpu;
      (<0.1>,958) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,960) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,964) fake_sched.h:43: return __running_cpu;
      (<0.1>,968) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,980)
      (<0.1>,982) fake_sched.h:43: return __running_cpu;
      (<0.1>,986) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,987) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,989) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,990) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,991) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,992) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,993) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,994) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,995) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,999) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1001) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1002) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1003) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1019)
      (<0.1>,1021)
      (<0.1>,1023) fake_sched.h:43: return __running_cpu;
      (<0.1>,1027) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,1030) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1031) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1037) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1038) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1040) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1045) fake_sched.h:43: return __running_cpu;
      (<0.1>,1048) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1050) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1052) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1053) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1056)
      (<0.1>,1059) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1062) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1063) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1069) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1070) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1072) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1077) fake_sched.h:43: return __running_cpu;
      (<0.1>,1080) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1082) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1084) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1085) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1088)
      (<0.1>,1091) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1094) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1095) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1101) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1102) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1104) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1111) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1116) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1118) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1119) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1121) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1122) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1123) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1124) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1138)
      (<0.1>,1140) tree.c:758: local_irq_restore(flags);
      (<0.1>,1143)
      (<0.1>,1145) fake_sched.h:43: return __running_cpu;
      (<0.1>,1149) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1151) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1155) fake_sched.h:43: return __running_cpu;
      (<0.1>,1159) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1165) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1168) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,10579) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,10582) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10585) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10589) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10590) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10591) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
              Error: Assertion violation at (<0>,10594): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp1jzbvqwn/tmpc40lr8i0.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DFORCE_FAILURE_1 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp1jzbvqwn/tmpmj6psfxy.ll /tmp/tmp1jzbvqwn/tmpc40lr8i0.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=-1 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp1jzbvqwn/tmpmj6psfxy.ll
Total wall-clock time: 2.42 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 8108 (also 6 sleepset blocked)

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,75) tree_plugin.h:731: pr_info("Hierarchical RCU implementation.\n");
  (<0>,79) tree_plugin.h:76: if (rcu_fanout_exact)
  (<0>,82) tree_plugin.h:87: if (rcu_fanout_leaf != RCU_FANOUT_LEAF)
  (<0>,94) tree.c:4147: ulong d;
  (<0>,95) tree.c:4159: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,98) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,99) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,101) tree.c:4161: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,104) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,105) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,107) tree.c:4165: if (rcu_fanout_leaf == RCU_FANOUT_LEAF &&
  (<0>,120)
  (<0>,121) tree.c:4056: static void __init rcu_init_one(struct rcu_state *rsp)
  (<0>,122) tree.c:4067: int i;
  (<0>,125) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,128) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,130) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,131) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,134) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,137) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,141) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,143) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,145) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,147) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,151) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,153) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,154) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,165)
  (<0>,166) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,167) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,170) tree.c:4041: int ccur;
  (<0>,171) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,173) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,175) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,178) tree.c:4046: ccur = levelcnt[i];
  (<0>,180) tree.c:4046: ccur = levelcnt[i];
  (<0>,182) tree.c:4046: ccur = levelcnt[i];
  (<0>,183) tree.c:4046: ccur = levelcnt[i];
  (<0>,184) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,185) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,188) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,190) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,192) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,194) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,195) tree.c:4048: cprv = ccur;
  (<0>,196) tree.c:4048: cprv = ccur;
  (<0>,198) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,200) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,202) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,207) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,208) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,210) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,211) tree.c:4085: fl_mask <<= 1;
  (<0>,215) tree.c:4085: fl_mask <<= 1;
  (<0>,216) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,218) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,220) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,223) tree.c:4090: cpustride *= levelspread[i];
  (<0>,226) tree.c:4090: cpustride *= levelspread[i];
  (<0>,227) tree.c:4090: cpustride *= levelspread[i];
  (<0>,229) tree.c:4090: cpustride *= levelspread[i];
  (<0>,230) tree.c:4091: rnp = rsp->level[i];
  (<0>,232) tree.c:4091: rnp = rsp->level[i];
  (<0>,235) tree.c:4091: rnp = rsp->level[i];
  (<0>,236) tree.c:4091: rnp = rsp->level[i];
  (<0>,237) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,240) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,243) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,246) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,250)
  (<0>,251) fake_sync.h:75: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,252) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,258) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,262)
  (<0>,263) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,264) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,270) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,273) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,275) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,276) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,278) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,279) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,281) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,282) tree.c:4101: rnp->qsmask = 0;
  (<0>,284) tree.c:4101: rnp->qsmask = 0;
  (<0>,285) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,287) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,288) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,291) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,293) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,294) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,296) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,299) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,301) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,302) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,304) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,307) tree.c:4107: if (i == 0) {
  (<0>,310) tree.c:4108: rnp->grpnum = 0;
  (<0>,312) tree.c:4108: rnp->grpnum = 0;
  (<0>,313) tree.c:4109: rnp->grpmask = 0;
  (<0>,315) tree.c:4109: rnp->grpmask = 0;
  (<0>,316) tree.c:4110: rnp->parent = NULL;
  (<0>,318) tree.c:4110: rnp->parent = NULL;
  (<0>,320) tree.c:4117: rnp->level = i;
  (<0>,322) tree.c:4117: rnp->level = i;
  (<0>,324) tree.c:4117: rnp->level = i;
  (<0>,325) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,329)
  (<0>,330) fake_defs.h:358: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,331) fake_defs.h:360: list->next = list;
  (<0>,333) fake_defs.h:360: list->next = list;
  (<0>,334) fake_defs.h:361: list->prev = list;
  (<0>,335) fake_defs.h:361: list->prev = list;
  (<0>,337) fake_defs.h:361: list->prev = list;
  (<0>,339) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,342) tree_plugin.h:2431: }
  (<0>,352) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,356)
  (<0>,357) fake_sync.h:141: void spin_lock_init(raw_spinlock_t *l)
  (<0>,358) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,363) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,365) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,366) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,368) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,370) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,371) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,374) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,378) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,380) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,382) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,389) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,392) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,395) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,396) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,397) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,399) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,400) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,407)
  (<0>,408) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,409) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,411) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,412) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,415) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,417) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,418) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,421) fake_defs.h:635: if (offset & mask)
  (<0>,422) fake_defs.h:635: if (offset & mask)
  (<0>,426) fake_defs.h:636: return cpu;
  (<0>,427) fake_defs.h:636: return cpu;
  (<0>,429) fake_defs.h:639: }
  (<0>,431) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,432) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,436) tree.c:4132: while (i > rnp->grphi)
  (<0>,437) tree.c:4132: while (i > rnp->grphi)
  (<0>,439) tree.c:4132: while (i > rnp->grphi)
  (<0>,442) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,443) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,445) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,447) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,450) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,451) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,452) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,475)
  (<0>,476) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,477) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,479) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,481) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,483) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,484) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,487)
  (<0>,488) tree.c:623: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,492) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,496) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,497) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,498) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,500) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,504)
  (<0>,505) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,506) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,509) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,511) fake_sched.h:43: return __running_cpu;
  (<0>,515) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,517) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,521) fake_sched.h:43: return __running_cpu;
  (<0>,525) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,531) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,532) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,539) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,540) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,542) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,544) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,548) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,550) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,551) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,554) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,556) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,557) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,559) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,561) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,566) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,567) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,569) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,571) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,575) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,576) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,577) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,578) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,579) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,581) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,584) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,585) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,586) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,591) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,592) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,593) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,595) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,598) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,599) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,600) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,604) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,605) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,606) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,607) tree.c:3776: rdp->cpu = cpu;
  (<0>,608) tree.c:3776: rdp->cpu = cpu;
  (<0>,610) tree.c:3776: rdp->cpu = cpu;
  (<0>,611) tree.c:3777: rdp->rsp = rsp;
  (<0>,612) tree.c:3777: rdp->rsp = rsp;
  (<0>,614) tree.c:3777: rdp->rsp = rsp;
  (<0>,615) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,618) tree_plugin.h:2448: }
  (<0>,623) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,624) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,625) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,627) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,631)
  (<0>,632) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,633) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,634) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,637) fake_sync.h:93: local_irq_restore(flags);
  (<0>,640) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,642) fake_sched.h:43: return __running_cpu;
  (<0>,646) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,648) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,652) fake_sched.h:43: return __running_cpu;
  (<0>,656) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,666) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,667) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,674)
  (<0>,675)
  (<0>,676) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,678) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,679) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,682) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,684) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,685) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,688) fake_defs.h:635: if (offset & mask)
  (<0>,689) fake_defs.h:635: if (offset & mask)
  (<0>,693) fake_defs.h:636: return cpu;
  (<0>,694) fake_defs.h:636: return cpu;
  (<0>,696) fake_defs.h:639: }
  (<0>,698) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,699) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,703) tree.c:4132: while (i > rnp->grphi)
  (<0>,704) tree.c:4132: while (i > rnp->grphi)
  (<0>,706) tree.c:4132: while (i > rnp->grphi)
  (<0>,709) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,710) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,712) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,714) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,717) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,718) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,719) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,742)
  (<0>,743)
  (<0>,744) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,746) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,748) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,750) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,751) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,754)
  (<0>,755) tree.c:625: return &rsp->node[0];
  (<0>,759) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,763) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,764) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,765) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,767) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,771)
  (<0>,772)
  (<0>,773) fake_sync.h:83: local_irq_save(flags);
  (<0>,776) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,778) fake_sched.h:43: return __running_cpu;
  (<0>,782) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,784) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,788) fake_sched.h:43: return __running_cpu;
  (<0>,792) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,798) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,799) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,806) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,807) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,809) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,811) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,815) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,817) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,818) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,821) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,823) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,824) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,826) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,828) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,833) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,834) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,836) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,838) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,842) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,843) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,844) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,845) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,846) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,848) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,851) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,852) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,853) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,858) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,859) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,860) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,862) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,865) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,866) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,867) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,871) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,872) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,873) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,874) tree.c:3776: rdp->cpu = cpu;
  (<0>,875) tree.c:3776: rdp->cpu = cpu;
  (<0>,877) tree.c:3776: rdp->cpu = cpu;
  (<0>,878) tree.c:3777: rdp->rsp = rsp;
  (<0>,879) tree.c:3777: rdp->rsp = rsp;
  (<0>,881) tree.c:3777: rdp->rsp = rsp;
  (<0>,882) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,885) tree_plugin.h:2448: }
  (<0>,890) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,891) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,892) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,894) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,898)
  (<0>,899)
  (<0>,900) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,901) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,904) fake_sync.h:93: local_irq_restore(flags);
  (<0>,907) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,909) fake_sched.h:43: return __running_cpu;
  (<0>,913) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,915) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,919) fake_sched.h:43: return __running_cpu;
  (<0>,923) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,933) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,934) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,941)
  (<0>,942)
  (<0>,943) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,945) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,946) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,949) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,951) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,952) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,955) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,957) fake_defs.h:639: }
  (<0>,959) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,960) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,963) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,968)
  (<0>,969) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,970) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,971) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,972) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,974) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,979)
  (<0>,980) fake_defs.h:364: static inline void __list_add(struct list_head *new,
  (<0>,981) fake_defs.h:365: struct list_head *prev,
  (<0>,982) fake_defs.h:366: struct list_head *next)
  (<0>,983) fake_defs.h:368: next->prev = new;
  (<0>,985) fake_defs.h:368: next->prev = new;
  (<0>,986) fake_defs.h:369: new->next = next;
  (<0>,987) fake_defs.h:369: new->next = next;
  (<0>,989) fake_defs.h:369: new->next = next;
  (<0>,990) fake_defs.h:370: new->prev = prev;
  (<0>,991) fake_defs.h:370: new->prev = prev;
  (<0>,993) fake_defs.h:370: new->prev = prev;
  (<0>,994) fake_defs.h:371: prev->next = new;
  (<0>,995) fake_defs.h:371: prev->next = new;
  (<0>,997) fake_defs.h:371: prev->next = new;
  (<0>,1009)
  (<0>,1010) tree.c:4066: int cpustride = 1;
  (<0>,1011) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1014) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1017) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1019) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1020) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1023) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1026) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1027) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1030) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1032) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1034) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1036) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1037) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1040) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1042) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1043) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1054)
  (<0>,1055)
  (<0>,1056) tree.c:4036: if (rcu_fanout_exact) {
  (<0>,1059) tree.c:4044: cprv = nr_cpu_ids;
  (<0>,1060) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1062) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1064) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1067) tree.c:4046: ccur = levelcnt[i];
  (<0>,1069) tree.c:4046: ccur = levelcnt[i];
  (<0>,1071) tree.c:4046: ccur = levelcnt[i];
  (<0>,1072) tree.c:4046: ccur = levelcnt[i];
  (<0>,1073) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1074) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1077) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1079) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1081) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1083) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1084) tree.c:4048: cprv = ccur;
  (<0>,1085) tree.c:4048: cprv = ccur;
  (<0>,1087) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1089) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1091) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1096) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1097) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1099) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1100) tree.c:4085: fl_mask <<= 1;
  (<0>,1104) tree.c:4085: fl_mask <<= 1;
  (<0>,1105) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1107) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1109) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1112) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1115) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1116) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1118) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1119) tree.c:4091: rnp = rsp->level[i];
  (<0>,1121) tree.c:4091: rnp = rsp->level[i];
  (<0>,1124) tree.c:4091: rnp = rsp->level[i];
  (<0>,1125) tree.c:4091: rnp = rsp->level[i];
  (<0>,1126) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1128) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1129) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1132) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1135) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,1139)
  (<0>,1140) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1141) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1147) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1151)
  (<0>,1152) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1153) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1159) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1161) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1162) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1164) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1165) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1167) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1168) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1170) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1171) tree.c:4101: rnp->qsmask = 0;
  (<0>,1173) tree.c:4101: rnp->qsmask = 0;
  (<0>,1174) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1176) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1177) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1178) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1180) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1182) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1183) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1185) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1188) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1190) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1191) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1193) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1196) tree.c:4107: if (i == 0) {
  (<0>,1199) tree.c:4108: rnp->grpnum = 0;
  (<0>,1201) tree.c:4108: rnp->grpnum = 0;
  (<0>,1202) tree.c:4109: rnp->grpmask = 0;
  (<0>,1204) tree.c:4109: rnp->grpmask = 0;
  (<0>,1205) tree.c:4110: rnp->parent = NULL;
  (<0>,1207) tree.c:4110: rnp->parent = NULL;
  (<0>,1209) tree.c:4117: rnp->level = i;
  (<0>,1211) tree.c:4117: rnp->level = i;
  (<0>,1213) tree.c:4117: rnp->level = i;
  (<0>,1214) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1218)
  (<0>,1219) fake_defs.h:360: list->next = list;
  (<0>,1220) fake_defs.h:360: list->next = list;
  (<0>,1222) fake_defs.h:360: list->next = list;
  (<0>,1223) fake_defs.h:361: list->prev = list;
  (<0>,1224) fake_defs.h:361: list->prev = list;
  (<0>,1226) fake_defs.h:361: list->prev = list;
  (<0>,1228) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,1231) tree_plugin.h:2431: }
  (<0>,1241) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,1245)
  (<0>,1246) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1247) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1252) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1254) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1255) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1257) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1259) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1260) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1263) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1267) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1269) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1271) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1278) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1281) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1284) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1285) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1286) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1288) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1289) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1296)
  (<0>,1297)
  (<0>,1298) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1300) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1301) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1304) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1306) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1307) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1310) fake_defs.h:635: if (offset & mask)
  (<0>,1311) fake_defs.h:635: if (offset & mask)
  (<0>,1315) fake_defs.h:636: return cpu;
  (<0>,1316) fake_defs.h:636: return cpu;
  (<0>,1318) fake_defs.h:639: }
  (<0>,1320) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1321) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1325) tree.c:4132: while (i > rnp->grphi)
  (<0>,1326) tree.c:4132: while (i > rnp->grphi)
  (<0>,1328) tree.c:4132: while (i > rnp->grphi)
  (<0>,1331) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1332) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1334) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1336) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1339) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1340) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1341) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1364)
  (<0>,1365)
  (<0>,1366) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1368) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1370) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1372) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1373) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1376)
  (<0>,1377) tree.c:625: return &rsp->node[0];
  (<0>,1381) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1385) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1386) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1387) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1389) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1393)
  (<0>,1394)
  (<0>,1395) fake_sync.h:83: local_irq_save(flags);
  (<0>,1398) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1400) fake_sched.h:43: return __running_cpu;
  (<0>,1404) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1406) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1410) fake_sched.h:43: return __running_cpu;
  (<0>,1414) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1420) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1421) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1428) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1429) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1431) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1433) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1437) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1439) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1440) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1443) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1445) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1446) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1448) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1450) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1455) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1456) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1458) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1460) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1464) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1465) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1466) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1467) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1468) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1470) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1473) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1474) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1475) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1480) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1481) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1482) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1484) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1487) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1488) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1489) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1493) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1494) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1495) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1496) tree.c:3776: rdp->cpu = cpu;
  (<0>,1497) tree.c:3776: rdp->cpu = cpu;
  (<0>,1499) tree.c:3776: rdp->cpu = cpu;
  (<0>,1500) tree.c:3777: rdp->rsp = rsp;
  (<0>,1501) tree.c:3777: rdp->rsp = rsp;
  (<0>,1503) tree.c:3777: rdp->rsp = rsp;
  (<0>,1504) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1507) tree_plugin.h:2448: }
  (<0>,1512) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1513) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1514) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1516) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1520)
  (<0>,1521)
  (<0>,1522) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1523) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1526) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1529) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1531) fake_sched.h:43: return __running_cpu;
  (<0>,1535) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1541) fake_sched.h:43: return __running_cpu;
  (<0>,1545) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1555) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1556) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1563)
  (<0>,1564)
  (<0>,1565) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1567) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1568) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1571) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1573) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1574) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1577) fake_defs.h:635: if (offset & mask)
  (<0>,1578) fake_defs.h:635: if (offset & mask)
  (<0>,1582) fake_defs.h:636: return cpu;
  (<0>,1583) fake_defs.h:636: return cpu;
  (<0>,1585) fake_defs.h:639: }
  (<0>,1587) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1588) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1592) tree.c:4132: while (i > rnp->grphi)
  (<0>,1593) tree.c:4132: while (i > rnp->grphi)
  (<0>,1595) tree.c:4132: while (i > rnp->grphi)
  (<0>,1598) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1599) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1601) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1603) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1606) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1607) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1608) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1631)
  (<0>,1632)
  (<0>,1633) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1635) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1637) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1639) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1640) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1643)
  (<0>,1644) tree.c:625: return &rsp->node[0];
  (<0>,1648) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1652) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1653) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1654) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1656) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1660)
  (<0>,1661)
  (<0>,1662) fake_sync.h:83: local_irq_save(flags);
  (<0>,1665) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1667) fake_sched.h:43: return __running_cpu;
  (<0>,1671) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1673) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1677) fake_sched.h:43: return __running_cpu;
  (<0>,1681) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1687) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1688) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1695) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1696) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1698) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1700) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1704) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1706) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1707) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1710) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1712) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1713) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1715) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1717) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1722) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1723) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1725) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1727) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1731) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1732) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1733) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1734) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1735) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1737) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1740) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1741) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1742) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1747) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1748) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1749) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1751) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1754) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1755) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1756) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1760) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1761) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1762) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1763) tree.c:3776: rdp->cpu = cpu;
  (<0>,1764) tree.c:3776: rdp->cpu = cpu;
  (<0>,1766) tree.c:3776: rdp->cpu = cpu;
  (<0>,1767) tree.c:3777: rdp->rsp = rsp;
  (<0>,1768) tree.c:3777: rdp->rsp = rsp;
  (<0>,1770) tree.c:3777: rdp->rsp = rsp;
  (<0>,1771) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1774) tree_plugin.h:2448: }
  (<0>,1779) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1780) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1781) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1783) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1787)
  (<0>,1788)
  (<0>,1789) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1790) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1793) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1796) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1798) fake_sched.h:43: return __running_cpu;
  (<0>,1802) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1804) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1808) fake_sched.h:43: return __running_cpu;
  (<0>,1812) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1822) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1823) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1830)
  (<0>,1831)
  (<0>,1832) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1834) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1835) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1838) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1840) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1841) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1844) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,1846) fake_defs.h:639: }
  (<0>,1848) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1849) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1852) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1857)
  (<0>,1858)
  (<0>,1859) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1860) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1861) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1863) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1868)
  (<0>,1869)
  (<0>,1870)
  (<0>,1871) fake_defs.h:368: next->prev = new;
  (<0>,1872) fake_defs.h:368: next->prev = new;
  (<0>,1874) fake_defs.h:368: next->prev = new;
  (<0>,1875) fake_defs.h:369: new->next = next;
  (<0>,1876) fake_defs.h:369: new->next = next;
  (<0>,1878) fake_defs.h:369: new->next = next;
  (<0>,1879) fake_defs.h:370: new->prev = prev;
  (<0>,1880) fake_defs.h:370: new->prev = prev;
  (<0>,1882) fake_defs.h:370: new->prev = prev;
  (<0>,1883) fake_defs.h:371: prev->next = new;
  (<0>,1884) fake_defs.h:371: prev->next = new;
  (<0>,1886) fake_defs.h:371: prev->next = new;
  (<0>,1890) tree.c:4251: if (dump_tree)
  (<0>,1899) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1901) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1902) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1909)
  (<0>,1910)
  (<0>,1911) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1913) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1914) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1917) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1919) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1920) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1923) fake_defs.h:635: if (offset & mask)
  (<0>,1924) fake_defs.h:635: if (offset & mask)
  (<0>,1928) fake_defs.h:636: return cpu;
  (<0>,1929) fake_defs.h:636: return cpu;
  (<0>,1931) fake_defs.h:639: }
  (<0>,1933) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1934) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1937) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,1945)
  (<0>,1946) tree.c:3829: int rcutree_prepare_cpu(unsigned int cpu)
  (<0>,1947) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1948) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1952) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1953) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1954) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1956) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1960) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1961) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1987)
  (<0>,1988) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1989) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1991) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1993) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1995) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1996) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1999)
  (<0>,2000) tree.c:625: return &rsp->node[0];
  (<0>,2004) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2008) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2009) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2010) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2012) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2016)
  (<0>,2017)
  (<0>,2018) fake_sync.h:83: local_irq_save(flags);
  (<0>,2021) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2023) fake_sched.h:43: return __running_cpu;
  (<0>,2027) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2029) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2033) fake_sched.h:43: return __running_cpu;
  (<0>,2037) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2043) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2044) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2051) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2053) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2054) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2056) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2057) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2059) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2060) tree.c:3800: rdp->blimit = blimit;
  (<0>,2061) tree.c:3800: rdp->blimit = blimit;
  (<0>,2063) tree.c:3800: rdp->blimit = blimit;
  (<0>,2064) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2066) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2069) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2072)
  (<0>,2073) tree.c:1551: static void init_callback_list(struct rcu_data *rdp)
  (<0>,2076) tree_plugin.h:2469: return false;
  (<0>,2079) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2083)
  (<0>,2084) tree.c:1539: static void init_default_callback_list(struct rcu_data *rdp)
  (<0>,2086) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2087) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2089) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2092) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2094) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2096) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2099) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2101) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2103) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2105) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2108) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2110) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2112) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2115) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2117) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2119) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2121) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2124) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2126) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2128) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2131) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2133) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2135) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2137) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2140) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2142) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2144) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2147) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2149) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2151) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2153) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2160) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2162) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2164) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2165) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2167) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2170) tree_plugin.h:2902: }
  (<0>,2172) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2173) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2175) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2178) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2179) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2180) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2183) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2185) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2188) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2189) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2192)
  (<0>,2193) tree.h:721: static inline void raw_spin_unlock_rcu_node(struct rcu_node *rnp)
  (<0>,2197)
  (<0>,2198) fake_sync.h:120: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2199) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2205) tree.c:3814: rnp = rdp->mynode;
  (<0>,2207) tree.c:3814: rnp = rdp->mynode;
  (<0>,2208) tree.c:3814: rnp = rdp->mynode;
  (<0>,2209) tree.c:3815: mask = rdp->grpmask;
  (<0>,2211) tree.c:3815: mask = rdp->grpmask;
  (<0>,2212) tree.c:3815: mask = rdp->grpmask;
  (<0>,2213) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2216)
  (<0>,2217) tree.h:715: static inline void raw_spin_lock_rcu_node(struct rcu_node *rnp)
  (<0>,2221) fake_sync.h:115: preempt_disable();
  (<0>,2223) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2224) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2231) tree.c:3817: if (!rdp->beenonline)
  (<0>,2233) tree.c:3817: if (!rdp->beenonline)
  (<0>,2237) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2242) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2243) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2244) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2245) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2247) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2249) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2250) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2252) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2255) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2256) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2257) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2259) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2260) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2265) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2266) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2267) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2268) fake_defs.h:237: switch (size) {
  (<0>,2270) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2272) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2273) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2275) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2278) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2279) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2280) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2282) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2284) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2285) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2287) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2288) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2290) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2291) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2293) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2294) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2296) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2297) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2301) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2302) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2305) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2306) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2308) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2309) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2311) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2317) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2318) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2319) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2321) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2325)
  (<0>,2326)
  (<0>,2327) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2328) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2331) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2334) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2336) fake_sched.h:43: return __running_cpu;
  (<0>,2340) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2342) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2346) fake_sched.h:43: return __running_cpu;
  (<0>,2350) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2360) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2363) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2364) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2365) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2369) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2370) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2371) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2373) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2377) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2378) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2404)
  (<0>,2405)
  (<0>,2406) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2408) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2410) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2412) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2413) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2416)
  (<0>,2417) tree.c:625: return &rsp->node[0];
  (<0>,2421) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2425) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2426) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2427) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2429) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2433)
  (<0>,2434)
  (<0>,2435) fake_sync.h:83: local_irq_save(flags);
  (<0>,2438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2440) fake_sched.h:43: return __running_cpu;
  (<0>,2444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2446) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2450) fake_sched.h:43: return __running_cpu;
  (<0>,2454) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2460) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2461) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2468) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2470) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2471) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2473) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2474) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2476) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2477) tree.c:3800: rdp->blimit = blimit;
  (<0>,2478) tree.c:3800: rdp->blimit = blimit;
  (<0>,2480) tree.c:3800: rdp->blimit = blimit;
  (<0>,2481) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2483) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2486) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2489)
  (<0>,2490) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,2493) tree_plugin.h:2469: return false;
  (<0>,2496) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2500)
  (<0>,2501) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2503) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2504) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2506) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2509) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2511) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2513) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2516) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2518) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2520) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2522) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2525) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2527) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2529) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2532) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2534) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2536) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2538) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2541) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2543) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2545) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2548) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2550) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2552) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2554) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2557) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2559) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2561) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2564) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2566) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2568) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2570) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2577) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2579) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2581) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2582) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2584) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2587) tree_plugin.h:2902: }
  (<0>,2589) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2590) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2592) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2595) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2596) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2597) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2600) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2602) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2605) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2606) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2609)
  (<0>,2610) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2614)
  (<0>,2615) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2616) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2622) tree.c:3814: rnp = rdp->mynode;
  (<0>,2624) tree.c:3814: rnp = rdp->mynode;
  (<0>,2625) tree.c:3814: rnp = rdp->mynode;
  (<0>,2626) tree.c:3815: mask = rdp->grpmask;
  (<0>,2628) tree.c:3815: mask = rdp->grpmask;
  (<0>,2629) tree.c:3815: mask = rdp->grpmask;
  (<0>,2630) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2633)
  (<0>,2634) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2638) fake_sync.h:115: preempt_disable();
  (<0>,2640) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2641) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2648) tree.c:3817: if (!rdp->beenonline)
  (<0>,2650) tree.c:3817: if (!rdp->beenonline)
  (<0>,2654) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2659) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2660) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2661) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2662) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2664) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2666) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2667) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2669) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2672) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2673) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2674) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2676) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2677) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2682) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2683) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2684) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2685) fake_defs.h:237: switch (size) {
  (<0>,2687) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2689) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2690) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2692) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2695) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2696) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2697) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2699) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2701) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2702) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2704) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2705) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2707) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2708) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2710) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2711) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2713) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2714) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2718) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2719) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2722) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2723) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2725) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2726) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2728) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2734) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2735) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2736) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2738) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2742)
  (<0>,2743)
  (<0>,2744) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2745) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2748) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2751) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2753) fake_sched.h:43: return __running_cpu;
  (<0>,2757) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2759) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2763) fake_sched.h:43: return __running_cpu;
  (<0>,2767) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2777) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2780) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2781) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2782) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2786) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2787) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2788) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2790) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2794) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,2797) tree_plugin.h:1243: }
  (<0>,2799) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2802) tree_plugin.h:2461: }
  (<0>,2805) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,2823)
  (<0>,2824) tree.c:3890: void rcu_cpu_starting(unsigned int cpu)
  (<0>,2825) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2826) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2830) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2831) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2832) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2834) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2839) fake_sched.h:43: return __running_cpu;
  (<0>,2842) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2844) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2846) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2847) tree.c:3900: rnp = rdp->mynode;
  (<0>,2849) tree.c:3900: rnp = rdp->mynode;
  (<0>,2850) tree.c:3900: rnp = rdp->mynode;
  (<0>,2851) tree.c:3901: mask = rdp->grpmask;
  (<0>,2853) tree.c:3901: mask = rdp->grpmask;
  (<0>,2854) tree.c:3901: mask = rdp->grpmask;
  (<0>,2858) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2859) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2860) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2862) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2866)
  (<0>,2867)
  (<0>,2868) fake_sync.h:83: local_irq_save(flags);
  (<0>,2871) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2873) fake_sched.h:43: return __running_cpu;
  (<0>,2877) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2879) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2883) fake_sched.h:43: return __running_cpu;
  (<0>,2887) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2893) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2894) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2901) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2902) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2904) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2906) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2907) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2908) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2910) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2912) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2916) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2917) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2918) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2920) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2924)
  (<0>,2925)
  (<0>,2926) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2927) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2930) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2933) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2935) fake_sched.h:43: return __running_cpu;
  (<0>,2939) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2941) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2945) fake_sched.h:43: return __running_cpu;
  (<0>,2949) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2958) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2961) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2962) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2963) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2967) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2968) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2969) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2971) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2976) fake_sched.h:43: return __running_cpu;
  (<0>,2979) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2981) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2983) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2984) tree.c:3900: rnp = rdp->mynode;
  (<0>,2986) tree.c:3900: rnp = rdp->mynode;
  (<0>,2987) tree.c:3900: rnp = rdp->mynode;
  (<0>,2988) tree.c:3901: mask = rdp->grpmask;
  (<0>,2990) tree.c:3901: mask = rdp->grpmask;
  (<0>,2991) tree.c:3901: mask = rdp->grpmask;
  (<0>,2995) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2996) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2997) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2999) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3003)
  (<0>,3004)
  (<0>,3005) fake_sync.h:83: local_irq_save(flags);
  (<0>,3008) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3010) fake_sched.h:43: return __running_cpu;
  (<0>,3014) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3016) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3020) fake_sched.h:43: return __running_cpu;
  (<0>,3024) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3030) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3031) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3038) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3039) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3041) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3043) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3044) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3045) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3047) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3049) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3053) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3054) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3055) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3057) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3061)
  (<0>,3062)
  (<0>,3063) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3064) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3067) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3070) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3072) fake_sched.h:43: return __running_cpu;
  (<0>,3076) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3078) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3082) fake_sched.h:43: return __running_cpu;
  (<0>,3086) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3095) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3098) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3099) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3100) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3104) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3105) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3106) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3108) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3114) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3115) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3122)
  (<0>,3123)
  (<0>,3124) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3126) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3127) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3130) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3132) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3133) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3136) fake_defs.h:635: if (offset & mask)
  (<0>,3137) fake_defs.h:635: if (offset & mask)
  (<0>,3141) fake_defs.h:636: return cpu;
  (<0>,3142) fake_defs.h:636: return cpu;
  (<0>,3144) fake_defs.h:639: }
  (<0>,3146) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3147) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3150) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,3158)
  (<0>,3159) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3160) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3161) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3165) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3166) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3167) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3169) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3173) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3174) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3200)
  (<0>,3201)
  (<0>,3202) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3204) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3206) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3208) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3209) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3212)
  (<0>,3213) tree.c:625: return &rsp->node[0];
  (<0>,3217) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3221) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3222) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3223) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3225) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3229)
  (<0>,3230)
  (<0>,3231) fake_sync.h:83: local_irq_save(flags);
  (<0>,3234) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3236) fake_sched.h:43: return __running_cpu;
  (<0>,3240) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3242) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3246) fake_sched.h:43: return __running_cpu;
  (<0>,3250) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3256) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3257) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3264) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3266) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3267) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3269) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3270) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3272) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3273) tree.c:3800: rdp->blimit = blimit;
  (<0>,3274) tree.c:3800: rdp->blimit = blimit;
  (<0>,3276) tree.c:3800: rdp->blimit = blimit;
  (<0>,3277) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3279) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3282) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3285)
  (<0>,3286) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3289) tree_plugin.h:2469: return false;
  (<0>,3292) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3296)
  (<0>,3297) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3299) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3300) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3302) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3305) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3307) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3309) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3312) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3314) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3316) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3318) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3321) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3323) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3325) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3328) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3330) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3332) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3334) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3337) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3339) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3341) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3344) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3346) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3348) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3350) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3353) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3355) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3357) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3360) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3362) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3364) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3366) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3373) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3375) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3377) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3378) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3380) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3383) tree_plugin.h:2902: }
  (<0>,3385) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3386) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3388) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3391) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3392) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3393) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3396) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3398) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3401) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3402) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3405)
  (<0>,3406) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3410)
  (<0>,3411) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3412) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3418) tree.c:3814: rnp = rdp->mynode;
  (<0>,3420) tree.c:3814: rnp = rdp->mynode;
  (<0>,3421) tree.c:3814: rnp = rdp->mynode;
  (<0>,3422) tree.c:3815: mask = rdp->grpmask;
  (<0>,3424) tree.c:3815: mask = rdp->grpmask;
  (<0>,3425) tree.c:3815: mask = rdp->grpmask;
  (<0>,3426) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3429)
  (<0>,3430) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3434) fake_sync.h:115: preempt_disable();
  (<0>,3436) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3437) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3444) tree.c:3817: if (!rdp->beenonline)
  (<0>,3446) tree.c:3817: if (!rdp->beenonline)
  (<0>,3450) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3455) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3456) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3457) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3458) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3460) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3462) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3463) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3465) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3468) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3469) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3470) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3472) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3473) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3478) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3479) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3480) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3481) fake_defs.h:237: switch (size) {
  (<0>,3483) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3485) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3486) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3488) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3491) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3492) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3493) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3495) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3497) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3498) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3500) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3501) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3503) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3504) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3506) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3507) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3509) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3510) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3514) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3515) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3518) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3519) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3521) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3522) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3524) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3530) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3531) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3532) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3534) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3538)
  (<0>,3539)
  (<0>,3540) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3541) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3544) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3547) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3549) fake_sched.h:43: return __running_cpu;
  (<0>,3553) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3555) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3559) fake_sched.h:43: return __running_cpu;
  (<0>,3563) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3573) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3576) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3577) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3578) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3582) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3583) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3584) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3586) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3590) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3591) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3617)
  (<0>,3618)
  (<0>,3619) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3621) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3623) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3625) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3626) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3629)
  (<0>,3630) tree.c:625: return &rsp->node[0];
  (<0>,3634) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3638) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3639) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3640) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3642) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3646)
  (<0>,3647)
  (<0>,3648) fake_sync.h:83: local_irq_save(flags);
  (<0>,3651) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3653) fake_sched.h:43: return __running_cpu;
  (<0>,3657) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3659) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3663) fake_sched.h:43: return __running_cpu;
  (<0>,3667) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3673) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3674) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3681) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3683) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3684) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3686) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3687) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3689) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3690) tree.c:3800: rdp->blimit = blimit;
  (<0>,3691) tree.c:3800: rdp->blimit = blimit;
  (<0>,3693) tree.c:3800: rdp->blimit = blimit;
  (<0>,3694) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3696) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3699) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3702)
  (<0>,3703) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3706) tree_plugin.h:2469: return false;
  (<0>,3709) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3713)
  (<0>,3714) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3716) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3717) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3719) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3722) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3724) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3726) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3729) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3731) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3733) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3735) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3738) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3740) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3742) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3745) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3747) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3749) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3751) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3754) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3756) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3758) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3761) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3763) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3765) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3767) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3770) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3772) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3774) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3777) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3779) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3781) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3783) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3790) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3792) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3794) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3795) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3797) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3800) tree_plugin.h:2902: }
  (<0>,3802) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3803) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3805) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3808) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3809) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3810) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3813) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3815) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3818) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3819) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3822)
  (<0>,3823) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3827)
  (<0>,3828) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3829) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3835) tree.c:3814: rnp = rdp->mynode;
  (<0>,3837) tree.c:3814: rnp = rdp->mynode;
  (<0>,3838) tree.c:3814: rnp = rdp->mynode;
  (<0>,3839) tree.c:3815: mask = rdp->grpmask;
  (<0>,3841) tree.c:3815: mask = rdp->grpmask;
  (<0>,3842) tree.c:3815: mask = rdp->grpmask;
  (<0>,3843) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3846)
  (<0>,3847) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3851) fake_sync.h:115: preempt_disable();
  (<0>,3853) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3854) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3861) tree.c:3817: if (!rdp->beenonline)
  (<0>,3863) tree.c:3817: if (!rdp->beenonline)
  (<0>,3867) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3872) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3873) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3874) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3875) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3877) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3879) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3880) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3882) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3885) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3886) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3887) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3889) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3890) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3895) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3896) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3897) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3898) fake_defs.h:237: switch (size) {
  (<0>,3900) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3902) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3903) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3905) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3908) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3909) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3910) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3912) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3914) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3915) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3917) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3918) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3920) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3921) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3923) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3924) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3926) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3927) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3931) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3932) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3935) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3936) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3938) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3939) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3941) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3947) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3948) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3949) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3951) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3955)
  (<0>,3956)
  (<0>,3957) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3958) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3961) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3964) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3966) fake_sched.h:43: return __running_cpu;
  (<0>,3970) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3972) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3976) fake_sched.h:43: return __running_cpu;
  (<0>,3980) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3990) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3993) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3994) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3995) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3999) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4000) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4001) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4003) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4007) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,4010) tree_plugin.h:1243: }
  (<0>,4012) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,4015) tree_plugin.h:2461: }
  (<0>,4018) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,4036)
  (<0>,4037) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4038) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4039) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4043) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4044) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4045) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4047) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4052) fake_sched.h:43: return __running_cpu;
  (<0>,4055) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4057) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4059) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4060) tree.c:3900: rnp = rdp->mynode;
  (<0>,4062) tree.c:3900: rnp = rdp->mynode;
  (<0>,4063) tree.c:3900: rnp = rdp->mynode;
  (<0>,4064) tree.c:3901: mask = rdp->grpmask;
  (<0>,4066) tree.c:3901: mask = rdp->grpmask;
  (<0>,4067) tree.c:3901: mask = rdp->grpmask;
  (<0>,4071) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4072) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4073) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4075) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4079)
  (<0>,4080)
  (<0>,4081) fake_sync.h:83: local_irq_save(flags);
  (<0>,4084) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4086) fake_sched.h:43: return __running_cpu;
  (<0>,4090) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4092) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4096) fake_sched.h:43: return __running_cpu;
  (<0>,4100) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4106) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4107) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4114) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4115) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4117) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4119) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4120) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4121) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4123) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4125) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4129) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4130) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4131) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4133) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4137)
  (<0>,4138)
  (<0>,4139) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4140) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4143) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4146) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4148) fake_sched.h:43: return __running_cpu;
  (<0>,4152) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4154) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4158) fake_sched.h:43: return __running_cpu;
  (<0>,4162) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4171) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4175) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4180) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4181) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4182) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4184) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4189) fake_sched.h:43: return __running_cpu;
  (<0>,4192) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4194) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4196) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4197) tree.c:3900: rnp = rdp->mynode;
  (<0>,4199) tree.c:3900: rnp = rdp->mynode;
  (<0>,4200) tree.c:3900: rnp = rdp->mynode;
  (<0>,4201) tree.c:3901: mask = rdp->grpmask;
  (<0>,4203) tree.c:3901: mask = rdp->grpmask;
  (<0>,4204) tree.c:3901: mask = rdp->grpmask;
  (<0>,4208) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4209) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4210) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4212) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4216)
  (<0>,4217)
  (<0>,4218) fake_sync.h:83: local_irq_save(flags);
  (<0>,4221) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4223) fake_sched.h:43: return __running_cpu;
  (<0>,4227) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4229) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4233) fake_sched.h:43: return __running_cpu;
  (<0>,4237) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4243) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4244) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4251) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4252) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4254) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4256) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4257) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4258) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4260) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4262) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4266) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4267) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4268) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4270) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4274)
  (<0>,4275)
  (<0>,4276) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4277) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4280) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4283) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4285) fake_sched.h:43: return __running_cpu;
  (<0>,4289) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4291) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4295) fake_sched.h:43: return __running_cpu;
  (<0>,4299) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4308) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4313) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4317) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4319) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4321) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4327) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4328) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4335)
  (<0>,4336)
  (<0>,4337) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4339) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4340) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4343) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4345) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4346) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4349) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,4351) fake_defs.h:639: }
  (<0>,4353) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4354) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4358) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4360) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4363) litmus.c:123: set_cpu(i);
  (<0>,4366)
  (<0>,4367) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,4368) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4370) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4388)
  (<0>,4389) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4390) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4391) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4395) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4396) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4397) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4399) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4404) fake_sched.h:43: return __running_cpu;
  (<0>,4407) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4409) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4411) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4412) tree.c:3900: rnp = rdp->mynode;
  (<0>,4414) tree.c:3900: rnp = rdp->mynode;
  (<0>,4415) tree.c:3900: rnp = rdp->mynode;
  (<0>,4416) tree.c:3901: mask = rdp->grpmask;
  (<0>,4418) tree.c:3901: mask = rdp->grpmask;
  (<0>,4419) tree.c:3901: mask = rdp->grpmask;
  (<0>,4423) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4424) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4425) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4427) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4431)
  (<0>,4432)
  (<0>,4433) fake_sync.h:83: local_irq_save(flags);
  (<0>,4436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4438) fake_sched.h:43: return __running_cpu;
  (<0>,4442) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4448) fake_sched.h:43: return __running_cpu;
  (<0>,4452) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4458) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4459) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4466) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4467) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4469) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4471) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4472) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4473) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4475) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4477) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4481) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4482) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4483) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4485) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4489)
  (<0>,4490)
  (<0>,4491) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4492) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4495) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4498) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4500) fake_sched.h:43: return __running_cpu;
  (<0>,4504) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4506) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4510) fake_sched.h:43: return __running_cpu;
  (<0>,4514) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4523) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4526) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4527) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4528) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4532) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4533) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4534) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4541) fake_sched.h:43: return __running_cpu;
  (<0>,4544) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4546) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4548) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4549) tree.c:3900: rnp = rdp->mynode;
  (<0>,4551) tree.c:3900: rnp = rdp->mynode;
  (<0>,4552) tree.c:3900: rnp = rdp->mynode;
  (<0>,4553) tree.c:3901: mask = rdp->grpmask;
  (<0>,4555) tree.c:3901: mask = rdp->grpmask;
  (<0>,4556) tree.c:3901: mask = rdp->grpmask;
  (<0>,4560) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4561) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4562) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4564) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4568)
  (<0>,4569)
  (<0>,4570) fake_sync.h:83: local_irq_save(flags);
  (<0>,4573) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4575) fake_sched.h:43: return __running_cpu;
  (<0>,4579) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4581) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4585) fake_sched.h:43: return __running_cpu;
  (<0>,4589) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4595) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4596) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4603) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4604) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4606) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4608) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4609) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4610) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4612) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4614) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4618) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4619) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4620) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4622) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4626)
  (<0>,4627)
  (<0>,4628) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4629) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4632) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4635) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4637) fake_sched.h:43: return __running_cpu;
  (<0>,4641) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4643) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4647) fake_sched.h:43: return __running_cpu;
  (<0>,4651) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4660) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4663) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4664) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4665) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4669) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4670) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4671) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4673) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4680) tree.c:753: unsigned long flags;
  (<0>,4683) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4685) fake_sched.h:43: return __running_cpu;
  (<0>,4689) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4691) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4695) fake_sched.h:43: return __running_cpu;
  (<0>,4699) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4711) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4713) fake_sched.h:43: return __running_cpu;
  (<0>,4717) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4718) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4720) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4721) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4722) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4723) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4724) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4725) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4726) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4730) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4732) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4733) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4734) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4750) tree.c:675: static void rcu_eqs_enter_common(long long oldval, bool user)
  (<0>,4752) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4754) fake_sched.h:43: return __running_cpu;
  (<0>,4758) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4761) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4762) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4763) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4767) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4768) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4769) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4771) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4779) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4781) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4783) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4784) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4787) tree_plugin.h:2457: }
  (<0>,4790) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4793) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4794) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4795) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4799) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4800) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4801) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4803) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4808) fake_sched.h:43: return __running_cpu;
  (<0>,4811) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4813) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4815) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4816) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4819) tree_plugin.h:2457: }
  (<0>,4822) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4825) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4826) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4827) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4831) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4832) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4833) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4835) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4842) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4845) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4846) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4847) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4849) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4850) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4852) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4853) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4854) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4855) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4869) tree_plugin.h:2879: }
  (<0>,4871) tree.c:758: local_irq_restore(flags);
  (<0>,4874) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4876) fake_sched.h:43: return __running_cpu;
  (<0>,4880) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4882) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4886) fake_sched.h:43: return __running_cpu;
  (<0>,4890) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4897) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4899) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4901) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4904) litmus.c:123: set_cpu(i);
  (<0>,4907)
  (<0>,4908) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4909) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4911) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4929)
  (<0>,4930) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4931) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4932) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4936) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4937) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4938) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4940) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4948) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4950) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4952) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4953) tree.c:3900: rnp = rdp->mynode;
  (<0>,4955) tree.c:3900: rnp = rdp->mynode;
  (<0>,4956) tree.c:3900: rnp = rdp->mynode;
  (<0>,4957) tree.c:3901: mask = rdp->grpmask;
  (<0>,4959) tree.c:3901: mask = rdp->grpmask;
  (<0>,4960) tree.c:3901: mask = rdp->grpmask;
  (<0>,4964) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4965) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4966) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4968) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4972)
  (<0>,4973)
  (<0>,4974) fake_sync.h:83: local_irq_save(flags);
  (<0>,4977) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:43: return __running_cpu;
  (<0>,4983) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4985) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4989) fake_sched.h:43: return __running_cpu;
  (<0>,4993) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4999) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5000) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5007) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5008) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5010) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5012) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5013) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5014) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5016) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5018) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5022) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5023) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5024) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5026) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5030)
  (<0>,5031)
  (<0>,5032) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5033) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5036) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5039) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5041) fake_sched.h:43: return __running_cpu;
  (<0>,5045) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5047) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5051) fake_sched.h:43: return __running_cpu;
  (<0>,5055) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5064) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5067) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5068) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5069) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5073) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5074) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5075) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5077) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5082) fake_sched.h:43: return __running_cpu;
  (<0>,5085) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5087) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5089) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5090) tree.c:3900: rnp = rdp->mynode;
  (<0>,5092) tree.c:3900: rnp = rdp->mynode;
  (<0>,5093) tree.c:3900: rnp = rdp->mynode;
  (<0>,5094) tree.c:3901: mask = rdp->grpmask;
  (<0>,5096) tree.c:3901: mask = rdp->grpmask;
  (<0>,5097) tree.c:3901: mask = rdp->grpmask;
  (<0>,5101) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5102) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5103) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5105) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5109)
  (<0>,5110)
  (<0>,5111) fake_sync.h:83: local_irq_save(flags);
  (<0>,5114) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5116) fake_sched.h:43: return __running_cpu;
  (<0>,5120) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5122) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5126) fake_sched.h:43: return __running_cpu;
  (<0>,5130) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5136) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5137) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5144) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5145) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5147) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5149) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5150) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5151) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5153) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5155) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5159) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5160) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5161) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5163) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5167)
  (<0>,5168)
  (<0>,5169) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5170) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5173) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5176) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5178) fake_sched.h:43: return __running_cpu;
  (<0>,5182) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5184) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5188) fake_sched.h:43: return __running_cpu;
  (<0>,5192) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5201) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5204) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5205) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5206) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5210) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5211) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5212) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5214) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5221) tree.c:755: local_irq_save(flags);
  (<0>,5224) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5226) fake_sched.h:43: return __running_cpu;
  (<0>,5230) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5232) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5236) fake_sched.h:43: return __running_cpu;
  (<0>,5240) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5252) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5254) fake_sched.h:43: return __running_cpu;
  (<0>,5258) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5259) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5261) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5262) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5263) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5264) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5265) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5266) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5267) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,5271) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5273) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5274) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5275) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5291)
  (<0>,5293) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5295) fake_sched.h:43: return __running_cpu;
  (<0>,5299) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5302) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5303) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5304) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5308) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5309) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5310) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5312) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5317) fake_sched.h:43: return __running_cpu;
  (<0>,5320) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5322) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5324) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5325) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5328) tree_plugin.h:2457: }
  (<0>,5331) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5334) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5335) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5336) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5340) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5341) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5342) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5344) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5349) fake_sched.h:43: return __running_cpu;
  (<0>,5352) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5354) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5356) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5357) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5360) tree_plugin.h:2457: }
  (<0>,5363) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5366) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5367) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5368) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5372) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5373) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5374) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5376) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5383) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5386) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5387) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5388) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5390) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5391) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5393) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5394) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5395) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5396) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5410) tree_plugin.h:2879: }
  (<0>,5412) tree.c:758: local_irq_restore(flags);
  (<0>,5415) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5417) fake_sched.h:43: return __running_cpu;
  (<0>,5421) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5423) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5427) fake_sched.h:43: return __running_cpu;
  (<0>,5431) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5438) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5440) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5442) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5462) tree.c:3971: unsigned long flags;
  (<0>,5463) tree.c:3972: int kthread_prio_in = kthread_prio;
  (<0>,5464) tree.c:3973: struct rcu_node *rnp;
  (<0>,5467) tree.c:3983: else if (kthread_prio > 99)
  (<0>,5471) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5472) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5475) tree.c:3989: rcu_scheduler_fully_active = 1;
  (<0>,5476) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5477) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5478) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5482) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5483) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5484) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5486) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5490) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5492) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5494) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5503)
  (<0>,5504) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5505) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5506) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5507) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5508) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5511)
  (<0>,5512)
  (<0>,5519)
  (<0>,5520)
  (<0>,5527)
  (<0>,5528)
  (<0>,5535)
  (<0>,5536)
  (<0>,5543)
  (<0>,5544)
  (<0>,5551)
  (<0>,5552)
  (<0>,5559)
  (<0>,5560)
  (<0>,5567)
  (<0>,5568)
  (<0>,5575)
  (<0>,5576)
  (<0>,5583)
  (<0>,5584) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5593) fake_defs.h:868: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,5599) fake_defs.h:870: task = malloc(sizeof(*task));
  (<0>,5600) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5602) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5604) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5605) fake_defs.h:872: task->tid = t;
  (<0>,5606) fake_defs.h:872: task->tid = t;
  (<0>,5608) fake_defs.h:872: task->tid = t;
  (<0>,5609) fake_defs.h:873: return task;
  (<0>,5610) fake_defs.h:873: return task;
  (<0>,5612) fake_defs.h:876: }
  (<0>,5614) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5615) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5618)
  (<0>,5619) tree.c:625: return &rsp->node[0];
  (<0>,5623) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5627) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5628) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5629) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5631) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5635)
  (<0>,5636)
  (<0>,5637) fake_sync.h:83: local_irq_save(flags);
  (<0>,5640) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5642) fake_sched.h:43: return __running_cpu;
  (<0>,5646) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5648) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5652) fake_sched.h:43: return __running_cpu;
  (<0>,5656) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5662) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5663) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5670) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5671) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5673) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5674) tree.c:3996: if (kthread_prio) {
  (<0>,5680) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5681) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5682) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5684) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5688)
  (<0>,5689)
  (<0>,5690) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5691) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5694) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5697) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5699) fake_sched.h:43: return __running_cpu;
  (<0>,5703) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5705) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5709) fake_sched.h:43: return __running_cpu;
  (<0>,5713) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5724) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5727) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5728) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5729) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5733) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5734) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5735) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5737) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5741) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5743) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5745) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5754)
  (<0>,5755)
  (<0>,5756)
  (<0>,5757)
  (<0>,5758) fake_defs.h:865: struct task_struct *task = NULL;
  (<0>,5759) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5762)
  (<0>,5763)
  (<0>,5770)
  (<0>,5771)
  (<0>,5778)
  (<0>,5779)
  (<0>,5786)
  (<0>,5787)
  (<0>,5794)
  (<0>,5795) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5808) fake_defs.h:875: return NULL;
  (<0>,5810) fake_defs.h:876: }
  (<0>,5812) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5813) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5816)
  (<0>,5817) tree.c:625: return &rsp->node[0];
  (<0>,5821) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5825) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5826) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5827) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5829) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5833)
  (<0>,5834)
  (<0>,5835) fake_sync.h:83: local_irq_save(flags);
  (<0>,5838) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5840) fake_sched.h:43: return __running_cpu;
  (<0>,5844) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5846) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5850) fake_sched.h:43: return __running_cpu;
  (<0>,5854) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5860) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5861) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5868) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5869) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5871) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5872) tree.c:3996: if (kthread_prio) {
  (<0>,5878) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5879) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5880) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5882) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5886)
  (<0>,5887)
  (<0>,5888) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5889) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5892) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5895) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5897) fake_sched.h:43: return __running_cpu;
  (<0>,5901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5903) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5907) fake_sched.h:43: return __running_cpu;
  (<0>,5911) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5922) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5925) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5926) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5927) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5931) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5932) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5933) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5935) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5949)
  (<0>,5950) litmus.c:51: void *thread_reader(void *arg)
  (<0>,5953)
  (<0>,5954) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5955) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5958) fake_sched.h:43: return __running_cpu;
  (<0>,5962)
  (<0>,5963) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,5966) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,5971) tree.c:890: unsigned long flags;
  (<0>,5974) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5976) fake_sched.h:43: return __running_cpu;
  (<0>,5980) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5982) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5986) fake_sched.h:43: return __running_cpu;
  (<0>,5990) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6002) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6004) fake_sched.h:43: return __running_cpu;
  (<0>,6008) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6009) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6011) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6012) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6013) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6014) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6015) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6016) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6017) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,6021) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6023) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6024) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6025) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6036)
  (<0>,6037) tree.c:830: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,6039) fake_sched.h:43: return __running_cpu;
  (<0>,6043) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6047) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6050) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6051) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6052) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6054) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6055) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6057) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6058) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6059) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6060) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6070) tree_plugin.h:2883: }
  (<0>,6072) tree.c:895: local_irq_restore(flags);
  (<0>,6075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6077) fake_sched.h:43: return __running_cpu;
  (<0>,6081) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6083) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6087) fake_sched.h:43: return __running_cpu;
  (<0>,6091) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6106) litmus.c:57: r_x = x;
  (<0>,6107) litmus.c:57: r_x = x;
  (<0>,6111) fake_sched.h:43: return __running_cpu;
  (<0>,6115) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
  (<0>,6119) fake_sched.h:43: return __running_cpu;
  (<0>,6123) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6128) fake_sched.h:43: return __running_cpu;
  (<0>,6132) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
  (<0>,6143) fake_sched.h:43: return __running_cpu;
  (<0>,6147) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6148) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6150) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6151) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6152) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6154) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6156) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6157) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6158) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6159) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6160) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6161) tree.c:942: if (oldval)
  (<0>,6169) tree_plugin.h:2883: }
  (<0>,6175) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,6184) tree_plugin.h:1669: struct rcu_state *rsp;
  (<0>,6185) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6186) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6190) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6191) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6192) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6194) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6199) fake_sched.h:43: return __running_cpu;
  (<0>,6202) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6204) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6207) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6209) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6211) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6214) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6215) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6216) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6220) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6221) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6222) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6224) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6229) fake_sched.h:43: return __running_cpu;
  (<0>,6232) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6234) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6237) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6239) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6241) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6244) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6245) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6246) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6250) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6251) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6252) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6254) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6259) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,6264) fake_sched.h:43: return __running_cpu;
  (<0>,6269) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,6277) fake_sched.h:43: return __running_cpu;
  (<0>,6283) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
  (<0>,6289) fake_sched.h:43: return __running_cpu;
  (<0>,6296) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
  (<0>,6309) tree.c:3557: struct rcu_state *rsp;
  (<0>,6310) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6311) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6315) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6316) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6317) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6319) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6323) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6325) fake_sched.h:43: return __running_cpu;
  (<0>,6328) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6330) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6352)
  (<0>,6353) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6354) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6356) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6357) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6358) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6360) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6362) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6363) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6364) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6399)
  (<0>,6400) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6401) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6404) tree.c:1469: !rcu_gp_in_progress(rsp))
  (<0>,6417)
  (<0>,6418) tree.c:237: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,6423) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6424) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6425) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6426) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6428) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6430) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6431) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6433) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6436) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6437) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6438) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6439) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6444) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6445) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6446) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6447) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6449) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6451) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6452) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6454) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6457) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6458) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6459) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6467) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
  (<0>,6470) tree_plugin.h:2923: return false;
  (<0>,6473) tree.c:3503: if (rcu_scheduler_fully_active &&
  (<0>,6476) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6478) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6481) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6483) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6487) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,6490)
  (<0>,6491) tree.c:614: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,6493) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6496) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6503) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6504) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6515)
  (<0>,6516) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6517) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6530)
  (<0>,6531) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6536) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6537) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6538) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6539) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6541) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6543) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6544) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6546) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6549) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6550) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6551) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6552) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6557) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6560) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6562) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6564) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6565) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6567) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6570) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6571) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6572) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6578) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,6594)
  (<0>,6595) tree.c:633: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,6598)
  (<0>,6599) tree.c:625: return &rsp->node[0];
  (<0>,6603) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6604) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6609) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6610) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6611) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6612) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6614) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6616) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6617) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6619) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6622) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6623) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6624) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6628) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6629) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6631) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6634) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6635) tree.c:639: return READ_ONCE(*fp);
  (<0>,6639) tree.c:639: return READ_ONCE(*fp);
  (<0>,6640) tree.c:639: return READ_ONCE(*fp);
  (<0>,6641) tree.c:639: return READ_ONCE(*fp);
  (<0>,6642) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6644) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,1)
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:892: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,55) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,57) fake_sched.h:43: return __running_cpu;
      (<0.1>,61) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,62) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,64) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,67) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,68) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,69) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,70) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,74) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,76) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,77) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,78) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,89)
      (<0.1>,90) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,92) fake_sched.h:43: return __running_cpu;
      (<0.1>,96) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,100) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,103) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,104) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,105) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,107) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,110) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,111) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,112) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,113) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,123) tree_plugin.h:2883: }
      (<0.1>,125) tree.c:895: local_irq_restore(flags);
      (<0.1>,128) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,130) fake_sched.h:43: return __running_cpu;
      (<0.1>,134) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,140) fake_sched.h:43: return __running_cpu;
      (<0.1>,144) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,151) litmus.c:87: x = 1;
      (<0.1>,163) tree.c:3255: ret = num_online_cpus() <= 1;
      (<0.1>,165) tree.c:3257: return ret;
      (<0.1>,172) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,175) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,176) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,177) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,178) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,181) update.c:148: rcu_scheduler_active == RCU_SCHEDULER_INIT;
      (<0.1>,198)
      (<0.1>,199) update.c:345: void __wait_rcu_gp(bool checktiny, int n, call_rcu_func_t *crcu_array,
      (<0.1>,200) update.c:345: void __wait_rcu_gp(bool checktiny, int n, call_rcu_func_t *crcu_array,
      (<0.1>,201) update.c:345: void __wait_rcu_gp(bool checktiny, int n, call_rcu_func_t *crcu_array,
      (<0.1>,202) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,204) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,205) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,208) update.c:352: if (checktiny &&
      (<0.1>,211) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,213) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,218) rcupdate.h:476: }
      (<0.1>,220) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,222) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,227)
      (<0.1>,228) fake_sync.h:272: x->done = 0;
      (<0.1>,230) fake_sync.h:272: x->done = 0;
      (<0.1>,234) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,236) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,238) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,239) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,241) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,247)
      (<0.1>,248) tree.c:3211: void call_rcu_sched(struct rcu_head *head, rcu_callback_t func)
      (<0.1>,249) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,250) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,281)
      (<0.1>,282) tree.c:3140: __call_rcu(struct rcu_head *head, rcu_callback_t func,
      (<0.1>,283) tree.c:3140: __call_rcu(struct rcu_head *head, rcu_callback_t func,
      (<0.1>,284) tree.c:3141: struct rcu_state *rsp, int cpu, bool lazy)
      (<0.1>,286)
      (<0.1>,287) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,294) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,295) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,301) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,302) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,303) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,304) tree.c:3147: if (debug_rcu_head_queue(head)) {
      (<0.1>,307) rcu.h:92: return 0;
      (<0.1>,311) tree.c:3153: head->func = func;
      (<0.1>,312) tree.c:3153: head->func = func;
      (<0.1>,315) tree.c:3153: head->func = func;
      (<0.1>,316) tree.c:3154: head->next = NULL;
      (<0.1>,318) tree.c:3154: head->next = NULL;
      (<0.1>,319) tree.c:3162: local_irq_save(flags);
      (<0.1>,322) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,324) fake_sched.h:43: return __running_cpu;
      (<0.1>,328) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,330) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,334) fake_sched.h:43: return __running_cpu;
      (<0.1>,338) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,344) fake_sched.h:43: return __running_cpu;
      (<0.1>,347) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,349) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,351) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,352) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,355) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,363) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,367) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,369) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,371) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,372) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,377) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,378) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,379) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,380) fake_defs.h:237: switch (size) {
      (<0.1>,382) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,384) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,385) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,387) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,390) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,391) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,392) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,393) tree.c:3189: if (lazy)
      (<0.1>,400) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,401) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,404) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,405) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,406) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,408) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,411) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,412) tree.c:3197: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,419) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,420) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,421) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,422) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,430)
      (<0.1>,431) tree.c:3077: static void __call_rcu_core(struct rcu_state *rsp, struct rcu_data *rdp,
      (<0.1>,432) tree.c:3077: static void __call_rcu_core(struct rcu_state *rsp, struct rcu_data *rdp,
      (<0.1>,433) tree.c:3086: if (!rcu_is_watching())
      (<0.1>,440) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,442) fake_sched.h:43: return __running_cpu;
      (<0.1>,448) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,449) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,450) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,455) tree.c:1060: ret = __rcu_is_watching();
      (<0.1>,457) tree.c:1062: return ret;
      (<0.1>,461) tree.c:3090: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,464) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,466) fake_sched.h:43: return __running_cpu;
      (<0.1>,470) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,480) tree.c:3205: local_irq_restore(flags);
      (<0.1>,483) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,485) fake_sched.h:43: return __running_cpu;
      (<0.1>,489) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,491) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,495) fake_sched.h:43: return __running_cpu;
      (<0.1>,499) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,508) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,510) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,512) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,513) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,516) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,518) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,519) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,522) update.c:365: if (checktiny &&
      (<0.1>,525) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,527) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,532) fake_sync.h:278: might_sleep();
      (<0.1>,536) fake_sched.h:43: return __running_cpu;
      (<0.1>,540) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,543) tree.c:755: local_irq_save(flags);
      (<0.1>,546) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,548) fake_sched.h:43: return __running_cpu;
      (<0.1>,552) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,554) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,558) fake_sched.h:43: return __running_cpu;
      (<0.1>,562) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,574) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,576) fake_sched.h:43: return __running_cpu;
      (<0.1>,580) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,581) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,583) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,584) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,585) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,586) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,587) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,588) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,589) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,593) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,595) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,596) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,597) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,613)
      (<0.1>,615) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,617) fake_sched.h:43: return __running_cpu;
      (<0.1>,621) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,624) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,625) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,626) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,630) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,631) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,632) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,639) fake_sched.h:43: return __running_cpu;
      (<0.1>,642) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,644) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,646) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,647) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,650) tree_plugin.h:2457: }
      (<0.1>,653) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,656) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,657) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,658) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,662) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,663) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,664) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,666) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,671) fake_sched.h:43: return __running_cpu;
      (<0.1>,674) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,676) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,678) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,679) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,682) tree_plugin.h:2457: }
      (<0.1>,685) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,688) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,689) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,690) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,694) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,695) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,696) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,698) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,705) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,708) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,709) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,710) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,712) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,713) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,715) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,716) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,717) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,718) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,732) tree_plugin.h:2879: }
      (<0.1>,734) tree.c:758: local_irq_restore(flags);
      (<0.1>,737) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,739) fake_sched.h:43: return __running_cpu;
      (<0.1>,743) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,745) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,749) fake_sched.h:43: return __running_cpu;
      (<0.1>,753) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,759) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,762) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:892: local_irq_save(flags);
    (<0.0>,35) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,63) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,65) fake_sched.h:43: return __running_cpu;
    (<0.0>,69) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,70) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,72) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,75) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,76) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,77) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,78) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,82) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,84) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,85) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,86) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,97)
    (<0.0>,98) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,100) fake_sched.h:43: return __running_cpu;
    (<0.0>,104) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,111) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,112) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,113) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,115) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,116) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,118) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,119) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,120) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,121) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,131) tree_plugin.h:2883: }
    (<0.0>,133) tree.c:895: local_irq_restore(flags);
    (<0.0>,136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,138) fake_sched.h:43: return __running_cpu;
    (<0.0>,142) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,144) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,148) fake_sched.h:43: return __running_cpu;
    (<0.0>,152) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,159) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,207)
    (<0.0>,208) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,211) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,214)
    (<0.0>,215) tree.c:625: return &rsp->node[0];
    (<0.0>,219) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,227) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,233) fake_sched.h:43: return __running_cpu;
    (<0.0>,237) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,241) fake_sched.h:43: return __running_cpu;
    (<0.0>,245) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,250) fake_sched.h:43: return __running_cpu;
    (<0.0>,254) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,270) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,272) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,273) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,274) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,276) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,278) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,279) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,280) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,281) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,282) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,283) tree.c:942: if (oldval)
    (<0.0>,291) tree_plugin.h:2883: }
    (<0.0>,297) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,306) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,307) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,308) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,312) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,313) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,314) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,316) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,321) fake_sched.h:43: return __running_cpu;
    (<0.0>,324) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,326) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,329) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,331) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,333) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,336) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,337) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,338) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,342) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,343) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,344) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,346) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,351) fake_sched.h:43: return __running_cpu;
    (<0.0>,354) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,356) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,359) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,361) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,363) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,366) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,367) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,368) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,372) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,373) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,374) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,376) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,381) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,386) fake_sched.h:43: return __running_cpu;
    (<0.0>,391) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,399) fake_sched.h:43: return __running_cpu;
    (<0.0>,405) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,411) fake_sched.h:43: return __running_cpu;
    (<0.0>,418) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,431) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,432) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,433) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,437) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,438) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,439) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,441) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,445) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,447) fake_sched.h:43: return __running_cpu;
    (<0.0>,450) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,452) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,474)
    (<0.0>,475)
    (<0.0>,476) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,478) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,479) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,480) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,482) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,484) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,485) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,486) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,521)
    (<0.0>,522)
    (<0.0>,523) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,526) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,539)
    (<0.0>,540) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,545) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,546) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,547) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,548) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,550) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,552) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,553) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,555) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,560) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,561) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,566) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,567) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,569) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,571) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,573) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,574) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,576) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,579) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,580) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,581) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,589) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,592) tree_plugin.h:2923: return false;
    (<0.0>,595) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,598) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,600) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,603) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,605) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,609) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,612)
    (<0.0>,613) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,615) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,618) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,625) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,626) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,637)
    (<0.0>,638)
    (<0.0>,639) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,652)
    (<0.0>,653) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,658) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,659) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,660) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,661) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,663) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,665) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,666) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,668) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,671) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,672) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,673) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,674) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,679) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,680) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,681) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,682) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,684) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,686) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,692) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,693) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,694) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,700) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,716)
    (<0.0>,717) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,720)
    (<0.0>,721) tree.c:625: return &rsp->node[0];
    (<0.0>,725) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,726) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,731) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,732) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,733) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,734) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,736) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,739) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,744) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,745) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,746) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,750) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,751) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,753) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,756) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,757) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,761) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,762) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,763) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,774) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,775) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,776) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,780) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,783) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,786) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,789) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,790) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,793) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,795) tree.c:666: }
    (<0.0>,798) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,800) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,802) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,803) tree.c:3523: return 1;
    (<0.0>,805) tree.c:3548: }
    (<0.0>,809) tree.c:3561: return 1;
    (<0.0>,811) tree.c:3563: }
    (<0.0>,818) fake_sched.h:43: return __running_cpu;
    (<0.0>,822) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,826) tree.c:2891: if (user)
    (<0.0>,834) fake_sched.h:43: return __running_cpu;
    (<0.0>,838) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,840) fake_sched.h:43: return __running_cpu;
    (<0.0>,844) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,850) fake_sched.h:43: return __running_cpu;
    (<0.0>,854) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,864) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,867) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,868) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,869) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,873) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,874) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,875) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,877) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,881) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,893) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,895) fake_sched.h:43: return __running_cpu;
    (<0.0>,898) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,900) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,902) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,903) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,905) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,912) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,913) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,915) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,921) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,922) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,923) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,924) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,925) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,929)
    (<0.0>,930) tree.c:2530: rcu_check_quiescent_state(struct rcu_state *rsp, struct rcu_data *rdp)
    (<0.0>,931) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,932) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,957)
    (<0.0>,958) tree.c:1899: static void note_gp_changes(struct rcu_state *rsp, struct rcu_data *rdp)
    (<0.0>,959) tree.c:1905: local_irq_save(flags);
    (<0.0>,962) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,964) fake_sched.h:43: return __running_cpu;
    (<0.0>,968) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,970) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,974) fake_sched.h:43: return __running_cpu;
    (<0.0>,978) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,983) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,985) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,986) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,987) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,989) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,990) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,995) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,996) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,997) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,998) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1000) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1002) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1003) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1005) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1008) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1009) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1010) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1013) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1015) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1016) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1021) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1022) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1023) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1024) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1026) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1028) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1029) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1031) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1034) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1035) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1036) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1039) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1043) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1044) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1045) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1046) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1048) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1050) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1054) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1057) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1058) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1066) tree.c:1911: local_irq_restore(flags);
    (<0.0>,1069) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1071) fake_sched.h:43: return __running_cpu;
    (<0.0>,1075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1077) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1081) fake_sched.h:43: return __running_cpu;
    (<0.0>,1085) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1092) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1094) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1099) tree.c:3016: local_irq_save(flags);
    (<0.0>,1102) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1104) fake_sched.h:43: return __running_cpu;
    (<0.0>,1108) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1110) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1114) fake_sched.h:43: return __running_cpu;
    (<0.0>,1118) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1123) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1124) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1135)
    (<0.0>,1136)
    (<0.0>,1137) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,1150)
    (<0.0>,1151) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1156) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1157) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1158) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1161) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1163) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1164) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1166) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1169) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1170) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1171) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1172) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1177) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1178) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1179) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1180) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1182) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1184) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1185) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1187) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1190) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1191) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1192) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1198) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,1214)
    (<0.0>,1215) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1218)
    (<0.0>,1219) tree.c:625: return &rsp->node[0];
    (<0.0>,1223) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1224) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1229) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1230) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1231) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1232) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1234) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1236) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1237) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1239) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1242) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1243) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1244) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1248) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1249) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1251) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1254) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1255) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1259) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1260) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1261) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1262) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1264) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1266) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1267) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1269) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1272) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1273) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1274) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1278) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1281) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1284) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1287) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1288) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1291) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,1293) tree.c:666: }
    (<0.0>,1296) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,1299)
    (<0.0>,1300) tree.c:625: return &rsp->node[0];
    (<0.0>,1306)
    (<0.0>,1307) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,1311) fake_sync.h:115: preempt_disable();
    (<0.0>,1313) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1314) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1321) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,1327) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1329) fake_sched.h:43: return __running_cpu;
    (<0.0>,1332) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1334) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1336) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1337) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1340)
    (<0.0>,1341) tree.c:625: return &rsp->node[0];
    (<0.0>,1345) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1346) tree.c:2334: bool ret = false;
    (<0.0>,1347) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1348) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1349) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1357)
    (<0.0>,1358) tree.c:1811: static bool rcu_advance_cbs(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,1359) tree.c:1811: static bool rcu_advance_cbs(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,1360) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1363) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1366) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1369) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1370) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1373) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1375) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1378) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1380) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1381) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1383) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1386) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1391) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1393) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1394) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1397) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1399) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1402) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1404) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1407) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1408) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1411) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1414) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1416) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1419) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1420) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1422) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1425) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1426) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1428) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1431) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1432) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1434) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1437) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1439) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1441) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1442) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1444) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1446) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1449) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1451) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1454) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1455) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1458) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1461) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1463) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1466) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1467) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1469) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1472) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1473) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1475) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1478) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1479) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1481) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1484) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1486) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1488) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1489) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1491) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1493) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1496) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1497) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1498) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1507)
    (<0.0>,1508) tree.c:1741: static bool rcu_accelerate_cbs(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,1509) tree.c:1741: static bool rcu_accelerate_cbs(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,1510) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1513) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1516) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1519) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1520) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1523) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1524) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1529)
    (<0.0>,1530) tree.c:1567: static unsigned long rcu_cbs_completed(struct rcu_state *rsp,
    (<0.0>,1531) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1534)
    (<0.0>,1535) tree.c:625: return &rsp->node[0];
    (<0.0>,1539) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1542) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1544) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1545) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1547) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1550) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1552) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1554) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1556) tree.c:1585: }
    (<0.0>,1558) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1559) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1561) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1564) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1566) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1569) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1570) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1573) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1576) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1580) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1582) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1584) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1587) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1589) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1592) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1593) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1596) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1599) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1603) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1605) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1607) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1610) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1612) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1616) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1619) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1622) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1623) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1625) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1628) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1629) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1630) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1632) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1635) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1637) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1639) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1641) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1644) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1647) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1648) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1650) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1653) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1654) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1655) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1657) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1660) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1662) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1664) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1666) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1669) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1672) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1673) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1675) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1678) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1679) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1680) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1682) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1685) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1687) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1689) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1691) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1694) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1695) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1714)
    (<0.0>,1715) tree.c:1608: rcu_start_future_gp(struct rcu_node *rnp, struct rcu_data *rdp,
    (<0.0>,1716) tree.c:1608: rcu_start_future_gp(struct rcu_node *rnp, struct rcu_data *rdp,
    (<0.0>,1717) tree.c:1613: bool ret = false;
    (<0.0>,1718) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1720) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1723)
    (<0.0>,1724) tree.c:625: return &rsp->node[0];
    (<0.0>,1728) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1729) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1731) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1732) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1737)
    (<0.0>,1738)
    (<0.0>,1739) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1742)
    (<0.0>,1743) tree.c:625: return &rsp->node[0];
    (<0.0>,1747) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1750) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1752) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1753) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1755) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1758) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1760) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1762) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1764) tree.c:1585: }
    (<0.0>,1766) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1767) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1768) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1769) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1775)
    (<0.0>,1776) tree.c:1591: static void trace_rcu_future_gp(struct rcu_node *rnp, struct rcu_data *rdp,
    (<0.0>,1777) tree.c:1591: static void trace_rcu_future_gp(struct rcu_node *rnp, struct rcu_data *rdp,
    (<0.0>,1778) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1782) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1784) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1787) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1790) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1792) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1793) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1795) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1798) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1803) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1804) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1805) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1806) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1808) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1810) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1811) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1813) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1816) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1817) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1818) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1819) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1824) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1825) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1826) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1834) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1837) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1838) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1839) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1842) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1843) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1846) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1848) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1849) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1854)
    (<0.0>,1855)
    (<0.0>,1856) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1859)
    (<0.0>,1860) tree.c:625: return &rsp->node[0];
    (<0.0>,1864) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1867) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1869) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1870) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1872) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1875) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1877) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1879) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1881) tree.c:1585: }
    (<0.0>,1883) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1884) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1886) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1889) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1890) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1892) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1895) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1899) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1900) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1902) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1905) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1908) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1910) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1912) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1915) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1916) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1918) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1921) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1926) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1928) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1930) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1933) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1934) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1936) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1939) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1944) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1946) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1948) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1951) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1953) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1956) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1959) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1961) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1964) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1966) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
  (<0>,6646) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6647) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6649) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6652) tree.c:639: return READ_ONCE(*fp);
  (<0>,6653) tree.c:639: return READ_ONCE(*fp);
  (<0>,6654) tree.c:639: return READ_ONCE(*fp);
  (<0>,6658) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
  (<0>,6660) tree.c:666: }
  (<0>,6663) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
  (<0>,6665) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
  (<0>,6667) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
  (<0>,6668) tree.c:3523: return 1;
  (<0>,6670) tree.c:3548: }
  (<0>,6674) tree.c:3561: return 1;
  (<0>,6676) tree.c:3563: }
  (<0>,6683) fake_sched.h:43: return __running_cpu;
  (<0>,6687) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
  (<0>,6691) tree.c:2891: if (user)
  (<0>,6699) fake_sched.h:43: return __running_cpu;
  (<0>,6703) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
  (<0>,6705) fake_sched.h:43: return __running_cpu;
  (<0>,6709) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6715) fake_sched.h:43: return __running_cpu;
  (<0>,6719) fake_sched.h:183: if (need_softirq[get_cpu()]) {
  (<0>,6729) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,6732) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6733) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6734) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6738) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6739) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6740) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6742) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6746) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,6758) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6760) fake_sched.h:43: return __running_cpu;
  (<0>,6763) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6765) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6767) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6768) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6770) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6777) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6778) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6780) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6786) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6787) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6788) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6789) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6790) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6794)
  (<0>,6795)
  (<0>,6796) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,6797) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,6822)
  (<0>,6823)
  (<0>,6824) tree.c:1905: local_irq_save(flags);
  (<0>,6827) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6829) fake_sched.h:43: return __running_cpu;
  (<0>,6833) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6835) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6839) fake_sched.h:43: return __running_cpu;
  (<0>,6843) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6848) tree.c:1906: rnp = rdp->mynode;
  (<0>,6850) tree.c:1906: rnp = rdp->mynode;
  (<0>,6851) tree.c:1906: rnp = rdp->mynode;
  (<0>,6852) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6854) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6855) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6860) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6861) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6862) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6863) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6865) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6867) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6868) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6870) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6873) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6874) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6875) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6878) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,6880) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,6881) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,6886) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,6887) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,6888) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,6889) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6891) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6893) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6894) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6896) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6899) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,6900) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,6901) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,6904) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,6908) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,6909) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,6910) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,6911) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6913) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6914) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6915) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6916) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6919) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,6922) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,6923) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,6931) tree.c:1911: local_irq_restore(flags);
  (<0>,6934) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6936) fake_sched.h:43: return __running_cpu;
  (<0>,6940) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6942) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6946) fake_sched.h:43: return __running_cpu;
  (<0>,6950) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6957) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,6959) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,6964) tree.c:3016: local_irq_save(flags);
  (<0>,6967) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6969) fake_sched.h:43: return __running_cpu;
  (<0>,6973) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6975) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6979) fake_sched.h:43: return __running_cpu;
  (<0>,6983) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6988) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6989) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7000)
  (<0>,7001)
  (<0>,7002) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7015)
  (<0>,7016) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7021) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7022) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7023) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7024) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7026) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7028) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7029) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7031) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7034) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7035) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7036) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7037) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7042) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7043) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7044) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7045) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7047) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7049) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7050) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7052) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7055) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7056) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7057) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7063) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,7079)
  (<0>,7080) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7083)
  (<0>,7084) tree.c:625: return &rsp->node[0];
  (<0>,7088) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7089) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7094) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7095) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7096) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7097) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7099) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7101) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7102) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7104) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7107) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7108) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7109) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7113) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7114) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7116) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7119) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7120) tree.c:639: return READ_ONCE(*fp);
  (<0>,7124) tree.c:639: return READ_ONCE(*fp);
  (<0>,7125) tree.c:639: return READ_ONCE(*fp);
  (<0>,7126) tree.c:639: return READ_ONCE(*fp);
  (<0>,7127) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7129) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7131) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7132) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7134) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7137) tree.c:639: return READ_ONCE(*fp);
  (<0>,7138) tree.c:639: return READ_ONCE(*fp);
  (<0>,7139) tree.c:639: return READ_ONCE(*fp);
  (<0>,7143) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
  (<0>,7145) tree.c:666: }
  (<0>,7148) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
  (<0>,7151)
  (<0>,7152) tree.c:625: return &rsp->node[0];
  (<0>,7158)
  (<0>,7159) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,7163) fake_sync.h:115: preempt_disable();
  (<0>,7165) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1967) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1969) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1970) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1972) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1975) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1976) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1977) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1983)
    (<0.0>,1984)
    (<0.0>,1985)
    (<0.0>,1986) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1990) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1992) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1993) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1994) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2005)
    (<0.0>,2006) tree.c:2297: rcu_start_gp_advanced(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,2007) tree.c:2297: rcu_start_gp_advanced(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,2008) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2010) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2013) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2014) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2025)
    (<0.0>,2026)
    (<0.0>,2027) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2040)
    (<0.0>,2041) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2046) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2047) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2048) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2053) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2054) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2056) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2059) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2060) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2061) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2062) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2067) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2068) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2069) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2070) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2072) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2074) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2075) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2077) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2080) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2081) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2082) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2088) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2104)
    (<0.0>,2105) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2108)
    (<0.0>,2109) tree.c:625: return &rsp->node[0];
    (<0.0>,2113) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2114) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2119) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2120) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2121) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2122) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2124) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2126) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2127) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2129) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2132) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2133) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2134) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2138) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2139) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2141) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2144) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2145) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2149) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2150) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2151) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2152) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2154) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2156) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2157) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2162) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2163) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2164) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2168) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2170) tree.c:666: }
    (<0.0>,2175) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2180) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2181) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2182) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2183) fake_defs.h:237: switch (size) {
    (<0.0>,2185) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2187) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2188) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2190) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2193) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2194) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2195) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2198) tree.c:2318: return true;
    (<0.0>,2200) tree.c:2319: }
    (<0.0>,2203) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2206) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2207) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2211) tree.c:1689: if (c_out != NULL)
    (<0.0>,2214) tree.c:1691: return ret;
    (<0.0>,2218) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,2219) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2222) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2223) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2229) tree.c:1798: return ret;
    (<0.0>,2231) tree.c:1798: return ret;
    (<0.0>,2233) tree.c:1799: }
    (<0.0>,2235) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,2237) tree.c:1843: }
    (<0.0>,2241) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,2242) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2243) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2244) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2255)
    (<0.0>,2256)
    (<0.0>,2257)
    (<0.0>,2258) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2260) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2263) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2264) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2275)
    (<0.0>,2276)
    (<0.0>,2277) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2290)
    (<0.0>,2291) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2296) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2297) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2298) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2299) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2301) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2303) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2304) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2306) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2309) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2310) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2311) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2312) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2317) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2318) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2320) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2322) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2324) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2325) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2327) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2330) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2331) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2332) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2338) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2354)
    (<0.0>,2355) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2358)
    (<0.0>,2359) tree.c:625: return &rsp->node[0];
    (<0.0>,2363) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2364) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2369) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2370) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2371) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2372) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2374) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2376) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2377) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2379) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2382) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2383) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2384) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2388) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2389) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2391) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2394) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2395) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2399) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2400) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2401) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2407) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2409) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2412) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2413) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2414) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2418) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2420) tree.c:666: }
    (<0.0>,2425) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2430) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2431) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2432) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2433) fake_defs.h:237: switch (size) {
    (<0.0>,2435) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2437) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2438) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2440) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2443) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2444) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2445) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2448) tree.c:2318: return true;
    (<0.0>,2450) tree.c:2319: }
    (<0.0>,2454) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2455) tree.c:2346: return ret;
    (<0.0>,2459) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,2463) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2464) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2465) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2468)
    (<0.0>,2469) tree.c:625: return &rsp->node[0];
    (<0.0>,2474) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2478)
    (<0.0>,2479)
    (<0.0>,2480) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2481) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2482) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2484) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,2487) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2489) fake_sched.h:43: return __running_cpu;
    (<0.0>,2493) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2495) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2499) fake_sched.h:43: return __running_cpu;
    (<0.0>,2503) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2511) tree.c:3021: if (needwake)
    (<0.0>,2514) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,2522)
    (<0.0>,2523) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2524) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2526) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2533) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2536)
    (<0.0>,2537) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2539) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2542) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2549) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2552) tree_plugin.h:2457: }
    (<0.0>,2556) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2559) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2560) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2561) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2565) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2566) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2567) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2569) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2573) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,2585) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2587) fake_sched.h:43: return __running_cpu;
    (<0.0>,2590) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2592) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2594) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2595) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2597) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2604) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2605) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2607) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2613) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2614) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2615) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2616) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2617) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2621)
    (<0.0>,2622)
    (<0.0>,2623) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2624) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2649)
    (<0.0>,2650)
    (<0.0>,2651) tree.c:1905: local_irq_save(flags);
    (<0.0>,2654) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2656) fake_sched.h:43: return __running_cpu;
    (<0.0>,2660) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2662) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2666) fake_sched.h:43: return __running_cpu;
    (<0.0>,2670) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2675) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2677) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2678) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2679) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2681) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2682) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2687) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2688) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2689) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2690) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2695) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2697) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2700) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2701) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2702) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2705) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2707) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2708) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2713) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2714) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2715) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2716) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2718) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2720) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2721) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2723) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2726) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2727) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2728) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2731) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2735) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2736) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2737) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2740) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2742) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2743) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2746) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2749) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2750) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2758) tree.c:1911: local_irq_restore(flags);
    (<0.0>,2761) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2763) fake_sched.h:43: return __running_cpu;
    (<0.0>,2767) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2769) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2773) fake_sched.h:43: return __running_cpu;
    (<0.0>,2777) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2784) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2786) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2791) tree.c:3016: local_irq_save(flags);
    (<0.0>,2794) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2802) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2806) fake_sched.h:43: return __running_cpu;
    (<0.0>,2810) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2815) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2816) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2827)
    (<0.0>,2828)
    (<0.0>,2829) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2842)
    (<0.0>,2843) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2849) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2850) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2855) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2858) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2863) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2864) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2870) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2871) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2872) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2877) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2879) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2882) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2883) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2890) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2906)
    (<0.0>,2907) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2910)
    (<0.0>,2911) tree.c:625: return &rsp->node[0];
    (<0.0>,2915) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2916) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2921) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2922) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2923) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2924) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2926) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2929) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2934) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2935) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2936) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2940) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2941) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2943) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2946) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2947) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2951) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2952) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2953) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2954) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2956) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2959) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2964) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2965) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2966) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2970) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2973) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2976) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2979) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2980) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2983) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2985) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2988) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2991) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2994) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2995) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2997) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3000) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3004) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3006) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3008) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3011) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3014) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3017) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3018) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3020) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3023) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3027) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3029) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3031) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3034) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,3036) tree.c:666: }
    (<0.0>,3039) tree.c:3024: local_irq_restore(flags);
    (<0.0>,3042) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3044) fake_sched.h:43: return __running_cpu;
    (<0.0>,3048) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3050) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3054) fake_sched.h:43: return __running_cpu;
    (<0.0>,3058) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3064) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,3067)
    (<0.0>,3068) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3070) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3073) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3080) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3083) tree_plugin.h:2457: }
    (<0.0>,3087) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3090) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3091) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3092) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3096) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3097) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3098) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3100) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3108) fake_sched.h:43: return __running_cpu;
    (<0.0>,3112) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,3122) fake_sched.h:43: return __running_cpu;
    (<0.0>,3126) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3127) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3129) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3130) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3131) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3133) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3135) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3136) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3137) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3138) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3139) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3140) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3142) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3150) tree_plugin.h:2879: }
    (<0.0>,3156) fake_sched.h:43: return __running_cpu;
    (<0.0>,3160) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,3163) tree.c:755: local_irq_save(flags);
    (<0.0>,3166) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3168) fake_sched.h:43: return __running_cpu;
    (<0.0>,3172) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3174) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3178) fake_sched.h:43: return __running_cpu;
    (<0.0>,3182) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3194) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3196) fake_sched.h:43: return __running_cpu;
    (<0.0>,3200) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3201) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3203) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3204) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3205) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3206) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3207) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3208) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3209) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3213) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3215) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3216) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3217) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3233)
    (<0.0>,3235) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3237) fake_sched.h:43: return __running_cpu;
    (<0.0>,3241) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3244) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3245) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3246) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3250) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3251) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3252) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3254) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3259) fake_sched.h:43: return __running_cpu;
    (<0.0>,3262) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3264) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3266) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3267) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3270) tree_plugin.h:2457: }
    (<0.0>,3273) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3276) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3277) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3278) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3282) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3283) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3284) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3286) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3291) fake_sched.h:43: return __running_cpu;
    (<0.0>,3294) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3296) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3298) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3299) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3302) tree_plugin.h:2457: }
    (<0.0>,3305) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3308) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3309) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3310) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3314) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3315) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3316) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3318) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3325) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3328) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3329) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3330) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3332) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3333) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3335) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3336) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3337) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3338) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3352) tree_plugin.h:2879: }
    (<0.0>,3354) tree.c:758: local_irq_restore(flags);
    (<0.0>,3357) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3359) fake_sched.h:43: return __running_cpu;
    (<0.0>,3363) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3365) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3369) fake_sched.h:43: return __running_cpu;
    (<0.0>,3373) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3379) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3382) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3387) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3392) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3393) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3394) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3395) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3397) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3399) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3400) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3405) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3406) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3407) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3414) fake_sched.h:43: return __running_cpu;
    (<0.0>,3418)
    (<0.0>,3419) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3422) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3427) tree.c:892: local_irq_save(flags);
    (<0.0>,3430) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3432) fake_sched.h:43: return __running_cpu;
    (<0.0>,3436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3442) fake_sched.h:43: return __running_cpu;
    (<0.0>,3446) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3458) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3460) fake_sched.h:43: return __running_cpu;
    (<0.0>,3464) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3465) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3467) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3468) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3469) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3470) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3471) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3472) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3473) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3477) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3479) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3480) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3481) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3492)
    (<0.0>,3493) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3495) fake_sched.h:43: return __running_cpu;
    (<0.0>,3499) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3503) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3506) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3507) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3508) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3510) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3511) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3513) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3514) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3515) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3516) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3526) tree_plugin.h:2883: }
    (<0.0>,3528) tree.c:895: local_irq_restore(flags);
    (<0.0>,3531) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3533) fake_sched.h:43: return __running_cpu;
    (<0.0>,3537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3539) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3543) fake_sched.h:43: return __running_cpu;
    (<0.0>,3547) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3554) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3556) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3557) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,3602)
    (<0.0>,3603) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3606)
    (<0.0>,3607) tree.c:625: return &rsp->node[0];
    (<0.0>,3611) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3613) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3614) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3615) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3620) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3621) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3622) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3623) fake_defs.h:237: switch (size) {
    (<0.0>,3625) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3627) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3628) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3630) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3633) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3634) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3635) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3636) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,3639)
    (<0.0>,3640) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,3644) fake_sync.h:99: local_irq_disable();
    (<0.0>,3647) fake_sched.h:43: return __running_cpu;
    (<0.0>,3651) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,3655) fake_sched.h:43: return __running_cpu;
    (<0.0>,3659) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3664) fake_sched.h:43: return __running_cpu;
    (<0.0>,3668) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3671) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3672) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3679) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3684) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3685) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3686) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3691) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3697) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3698) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3699) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3710)
    (<0.0>,3716)
    (<0.0>,3721) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3726) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3727) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3728) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3729) fake_defs.h:237: switch (size) {
    (<0.0>,3731) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3733) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3734) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3736) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3739) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3740) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3741) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3742) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3755)
    (<0.0>,3756) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3761) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3762) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3763) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3774) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3775) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3776) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3777) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3782) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3783) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3784) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3785) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3787) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3789) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3790) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3792) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3795) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3796) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3797) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3805) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3806) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3819)
    (<0.0>,3820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3825) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3826) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3827) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3830) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3833) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3835) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3838) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3839) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3840) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3841) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3846) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3847) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3849) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3854) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3859) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3860) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3868) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3869) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3870) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3873) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,3888)
    (<0.0>,3889) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3890) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3891) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3892) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3894) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3915) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3916) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3917) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3918) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3920) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3922) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3923) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3925) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3928) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3929) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3930) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3931) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3932) update.c:466: if (till_stall_check < 3) {
    (<0.0>,3935) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,3939) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,3944) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,3946) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3947) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3949) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3950) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3955) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3956) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3957) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3958) fake_defs.h:237: switch (size) {
    (<0.0>,3960) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3962) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3963) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3965) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3968) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3969) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3970) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3971) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3972) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3975) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3977) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3978) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3983) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3984) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3985) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3986) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3988) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3990) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3991) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3993) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3996) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3997) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3998) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3999) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4001) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4005) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4007) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4009) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4010) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4012) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4013) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4014) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4018) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4021)
    (<0.0>,4022) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4026)
    (<0.0>,4027) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4028) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4029) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4033) fake_sched.h:43: return __running_cpu;
    (<0.0>,4037) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4039) fake_sched.h:43: return __running_cpu;
    (<0.0>,4043) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4050) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4053) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4056) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4057) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4059) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4060) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4062) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4067) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4068) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4072)
    (<0.0>,4073) tree.c:1920: static void rcu_gp_slow(struct rcu_state *rsp, int delay)
    (<0.0>,4074) tree.c:1922: if (delay > 0 &&
    (<0.0>,4078) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4081)
    (<0.0>,4082) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4086) fake_sync.h:99: local_irq_disable();
    (<0.0>,4089) fake_sched.h:43: return __running_cpu;
    (<0.0>,4093) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4097) fake_sched.h:43: return __running_cpu;
    (<0.0>,4101) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4106) fake_sched.h:43: return __running_cpu;
    (<0.0>,4110) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4113) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4114) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4121) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4123) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4124) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4126) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4129) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4131) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4132) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4133) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4135) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4136) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4138) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4139) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4143) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4145) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4151) tree.c:1983: if (!oldmask) /* First online CPU for this rcu_node. */
    (<0.0>,4154) tree.c:1984: rcu_init_new_rnp(rnp);
    (<0.0>,4159)
    (<0.0>,4160) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4161) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4163) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4165) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4166) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4167) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4169) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4170) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4171) tree.c:3752: if (rnp == NULL)
    (<0.0>,4177) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4179) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4182) tree.c:2007: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4185)
    (<0.0>,4186) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4190)
    (<0.0>,4191) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4192) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4193) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4197) fake_sched.h:43: return __running_cpu;
    (<0.0>,4201) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4203) fake_sched.h:43: return __running_cpu;
    (<0.0>,4207) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4215) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4217) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4219) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4220) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4222) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4227) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4230) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4232) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4233) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4235) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4240) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4241) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4245)
    (<0.0>,4246)
    (<0.0>,4247) tree.c:1922: if (delay > 0 &&
    (<0.0>,4251) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4254)
    (<0.0>,4255) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4259) fake_sync.h:99: local_irq_disable();
    (<0.0>,4262) fake_sched.h:43: return __running_cpu;
    (<0.0>,4266) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4270) fake_sched.h:43: return __running_cpu;
    (<0.0>,4274) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4279) fake_sched.h:43: return __running_cpu;
    (<0.0>,4283) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4286) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4287) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4295) fake_sched.h:43: return __running_cpu;
    (<0.0>,4298) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4300) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4302) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4303) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,4309)
    (<0.0>,4310) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4312) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4317) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4318) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4320) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4324) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4325) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4326) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4328) tree.c:2028: rnp->qsmask = 0;
    (<0.0>,4330) tree.c:2028: rnp->qsmask = 0;
    (<0.0>,4332) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4334) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4335) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4336) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4341) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4342) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4343) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4344) fake_defs.h:237: switch (size) {
    (<0.0>,4346) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4348) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4349) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4351) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4354) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4355) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4356) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4357) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4359) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4360) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4362) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4367) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4368) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4370) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4371) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4373) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4377) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4378) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4379) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4382) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4383) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4385) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4388) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4389) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4390) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4412)
    (<0.0>,4413) tree.c:1851: static bool __note_gp_changes(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,4414) tree.c:1851: static bool __note_gp_changes(struct rcu_state *rsp, struct rcu_node *rnp,
    (<0.0>,4415) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4417) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4418) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4420) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4423) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4427) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4428) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4429) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4430) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4432) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4433) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4434) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4435) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4438) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4441) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4442) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4450) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4451) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4452) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4461)
    (<0.0>,4462)
    (<0.0>,4463)
    (<0.0>,4464) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4467) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4470) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4473) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4474) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4477) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4478) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4483)
    (<0.0>,4484)
    (<0.0>,4485) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4488)
    (<0.0>,4489) tree.c:625: return &rsp->node[0];
    (<0.0>,4493) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4496) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4498) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4499) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4501) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4504) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4506) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4508) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4510) tree.c:1585: }
    (<0.0>,4512) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4513) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4515) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4518) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4520) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4523) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4524) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4527) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4530) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4534) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4536) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4538) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4541) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4543) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4546) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4547) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4550) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4553) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4556) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4558) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4561) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4562) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4567) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4569) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4573) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4576) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4579) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4580) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4582) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4585) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4586) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4587) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4589) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4592) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4594) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4596) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4598) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4601) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4604) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4605) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4607) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4610) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4611) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4612) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4614) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4617) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4619) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4621) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4623) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4626) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4627) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4646)
    (<0.0>,4647)
    (<0.0>,4648)
    (<0.0>,4649) tree.c:1613: bool ret = false;
    (<0.0>,4650) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4652) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4655)
    (<0.0>,4656) tree.c:625: return &rsp->node[0];
    (<0.0>,4660) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4661) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4663) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4664) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4669)
    (<0.0>,4670)
    (<0.0>,4671) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4674)
    (<0.0>,4675) tree.c:625: return &rsp->node[0];
    (<0.0>,4679) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4682) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4684) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4685) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4687) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4690) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4692) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4694) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4696) tree.c:1585: }
    (<0.0>,4698) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4699) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4700) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4701) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4707)
    (<0.0>,4708)
    (<0.0>,4709)
    (<0.0>,4710) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4714) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4716) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4719) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4722) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4724) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4725) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4727) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4730) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4732) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4735) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4737) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4738) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4739) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4740) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4746)
    (<0.0>,4747)
    (<0.0>,4748)
    (<0.0>,4749) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4754) tree.c:1689: if (c_out != NULL)
    (<0.0>,4757) tree.c:1691: return ret;
    (<0.0>,4761) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4762) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4765) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4766) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4772) tree.c:1798: return ret;
    (<0.0>,4774) tree.c:1798: return ret;
    (<0.0>,4776) tree.c:1799: }
    (<0.0>,4779) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4781) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4783) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4784) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4786) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4789) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4791) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4792) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4794) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4797) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4799) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4800) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4802) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4808) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4809) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4812) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4816) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4818) fake_sched.h:43: return __running_cpu;
    (<0.0>,4822) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4823) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4825) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4826) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4828) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4831) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4832) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,4835)
    (<0.0>,4836) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4838) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4839) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4841) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4851)
    (<0.0>,4856) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4860) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4861) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4862) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4863) fake_defs.h:237: switch (size) {
    (<0.0>,4865) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4866) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4867) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4868) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4871) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4874) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4875) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4878) tree.c:1896: return ret;
    (<0.0>,4882) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,4885) tree_plugin.h:1231: }
    (<0.0>,4889) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4892)
    (<0.0>,4893) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4897)
    (<0.0>,4898) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4899) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4900) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4904) fake_sched.h:43: return __running_cpu;
    (<0.0>,4908) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4910) fake_sched.h:43: return __running_cpu;
    (<0.0>,4914) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4929) fake_sched.h:43: return __running_cpu;
    (<0.0>,4935) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,4943) fake_sched.h:43: return __running_cpu;
    (<0.0>,4947) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,4960) fake_sched.h:43: return __running_cpu;
    (<0.0>,4964) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,4967) tree.c:755: local_irq_save(flags);
    (<0.0>,4970) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4972) fake_sched.h:43: return __running_cpu;
    (<0.0>,4976) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4978) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,4982) fake_sched.h:43: return __running_cpu;
    (<0.0>,4986) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4998) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5000) fake_sched.h:43: return __running_cpu;
    (<0.0>,5004) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5005) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5007) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5008) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5009) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5010) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5011) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5012) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5013) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5017) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5019) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5020) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5021) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5037)
    (<0.0>,5039) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5041) fake_sched.h:43: return __running_cpu;
    (<0.0>,5045) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5048) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5049) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5050) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5054) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5055) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5056) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5058) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5063) fake_sched.h:43: return __running_cpu;
    (<0.0>,5066) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5068) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5070) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5071) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5074) tree_plugin.h:2457: }
    (<0.0>,5077) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5080) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5081) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5082) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5086) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5087) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5088) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5090) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5095) fake_sched.h:43: return __running_cpu;
    (<0.0>,5098) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5100) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5102) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5103) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5106) tree_plugin.h:2457: }
    (<0.0>,5109) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5112) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5113) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5114) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5118) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5119) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5120) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5122) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5129) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5132) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5133) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5134) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5136) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5137) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5139) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5140) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5141) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5142) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5156) tree_plugin.h:2879: }
    (<0.0>,5158) tree.c:758: local_irq_restore(flags);
    (<0.0>,5161) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5163) fake_sched.h:43: return __running_cpu;
    (<0.0>,5167) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5169) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5173) fake_sched.h:43: return __running_cpu;
    (<0.0>,5177) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5183) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5186) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5191) fake_sched.h:43: return __running_cpu;
    (<0.0>,5195)
    (<0.0>,5196) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5199) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5204) tree.c:892: local_irq_save(flags);
    (<0.0>,5207) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5209) fake_sched.h:43: return __running_cpu;
    (<0.0>,5213) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5215) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5219) fake_sched.h:43: return __running_cpu;
    (<0.0>,5223) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5235) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5237) fake_sched.h:43: return __running_cpu;
    (<0.0>,5241) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5242) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5244) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5245) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5246) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5247) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5248) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5249) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5250) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,5254) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5256) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5257) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5258) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5269)
    (<0.0>,5270) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5272) fake_sched.h:43: return __running_cpu;
    (<0.0>,5276) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5280) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5283) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5284) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5285) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5287) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5288) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5290) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5291) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5292) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5293) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5303) tree_plugin.h:2883: }
    (<0.0>,5305) tree.c:895: local_irq_restore(flags);
    (<0.0>,5308) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5310) fake_sched.h:43: return __running_cpu;
    (<0.0>,5314) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5316) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5320) fake_sched.h:43: return __running_cpu;
    (<0.0>,5324) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5338) fake_sched.h:43: return __running_cpu;
    (<0.0>,5342) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,5351) fake_sched.h:43: return __running_cpu;
    (<0.0>,5358) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,5367) fake_sched.h:43: return __running_cpu;
    (<0.0>,5371) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5373) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5379) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5380) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5381) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5386) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5387) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5388) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5389) fake_defs.h:237: switch (size) {
    (<0.0>,5391) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5393) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5394) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5396) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5399) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5400) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5401) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5403) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5405) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5407) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5408) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5410) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5415) tree.c:2046: return true;
    (<0.0>,5417) tree.c:2047: }
    (<0.0>,5421) tree.c:2214: first_gp_fqs = true;
    (<0.0>,5422) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5423) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5424) tree.c:2216: if (j > HZ) {
    (<0.0>,5427) tree.c:2220: ret = 0;
    (<0.0>,5429) tree.c:2222: if (!ret) {
    (<0.0>,5432) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5433) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5435) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5437) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5439) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5440) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5443) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5444) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5449) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5450) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5451) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5452) fake_defs.h:237: switch (size) {
    (<0.0>,5454) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5456) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5457) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5459) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5462) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5463) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5464) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5468) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5470) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5474) fake_sched.h:43: return __running_cpu;
    (<0.0>,5478) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5482) fake_sched.h:43: return __running_cpu;
    (<0.0>,5486) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5491) fake_sched.h:43: return __running_cpu;
    (<0.0>,5495) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5506) fake_sched.h:43: return __running_cpu;
    (<0.0>,5510) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5511) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5513) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5514) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5515) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5517) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5519) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5520) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5521) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5522) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5523) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5524) tree.c:942: if (oldval)
    (<0.0>,5532) tree_plugin.h:2883: }
    (<0.0>,5538) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,5547) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5548) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5549) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5553) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5554) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5555) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5557) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5562) fake_sched.h:43: return __running_cpu;
    (<0.0>,5565) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5567) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5570) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5572) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5574) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5577) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5578) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5579) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5583) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5584) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5585) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5587) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5592) fake_sched.h:43: return __running_cpu;
    (<0.0>,5595) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5597) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5600) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5602) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5604) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5607) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5608) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5609) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5613) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5614) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5615) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5617) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5622) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,5627) fake_sched.h:43: return __running_cpu;
    (<0.0>,5632) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,5640) fake_sched.h:43: return __running_cpu;
    (<0.0>,5646) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,5660) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5661) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5662) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5666) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5667) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5668) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5670) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5674) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5676) fake_sched.h:43: return __running_cpu;
    (<0.0>,5679) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5681) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5703)
    (<0.0>,5704)
    (<0.0>,5705) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5707) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5708) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5709) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5711) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5713) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5714) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5715) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5750)
    (<0.0>,5751)
    (<0.0>,5752) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,5755) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,5768)
    (<0.0>,5769) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5774) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5775) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5776) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5777) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5779) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5781) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5782) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5784) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5787) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5788) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5789) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5790) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5795) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5796) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5797) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5798) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5800) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5802) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5803) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5805) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5808) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5809) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5810) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5816) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,5841)
    (<0.0>,5842) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,5847) tree.c:1472: j = jiffies;
    (<0.0>,5848) tree.c:1472: j = jiffies;
    (<0.0>,5849) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5854) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5855) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5856) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5857) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5859) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5861) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5862) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5864) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5867) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5868) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5869) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5870) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5872) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5877) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5878) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5879) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5880) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5882) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5884) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5885) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5887) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5890) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5891) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5892) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5893) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5895) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5900) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5901) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5902) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5903) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5905) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5907) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5908) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5910) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5913) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5914) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5915) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5916) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5918) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5923) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5924) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5925) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5926) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5930) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5933) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5936) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5937) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5938) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5939) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5940) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5941) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5945) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5946) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5952) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,5955) tree_plugin.h:2923: return false;
    (<0.0>,5958) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,5961) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5963) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5966) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,5968) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,5972) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,5975)
    (<0.0>,5976) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5978) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5981) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,5988) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,5989) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6000)
    (<0.0>,6001)
    (<0.0>,6002) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,6015)
    (<0.0>,6016) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6021) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6022) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6023) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6024) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6026) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6028) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6029) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6031) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6034) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6035) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6036) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6037) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6042) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6043) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6044) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6045) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6047) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6050) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6052) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6055) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6056) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6057) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6063) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,6065) tree.c:666: }
    (<0.0>,6068) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6073) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6074) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6075) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6076) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6078) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6080) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6081) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6083) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6086) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6087) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6088) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6089) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6091) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6094) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6099) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6100) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6101) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6102) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6104) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6106) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6107) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6109) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6112) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6113) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6114) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6115) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6117) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6120) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6124) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6125) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6126) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6127) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6129) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6130) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6131) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6132) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6135) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6138) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6139) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6147) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,6150) tree_plugin.h:2452: return false;
    (<0.0>,6154) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6156) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6158) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6159) tree.c:3547: return 0;
    (<0.0>,6161) tree.c:3548: }
    (<0.0>,6166) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6169) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6170) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6171) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6175) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6176) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6177) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6179) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6183) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,6185) fake_sched.h:43: return __running_cpu;
    (<0.0>,6188) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,6190) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,6212)
    (<0.0>,6213)
    (<0.0>,6214) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6216) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6217) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,6218) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,6220) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,6222) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,6223) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,6224) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,6259)
    (<0.0>,6260)
    (<0.0>,6261) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,6264) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,6277)
    (<0.0>,6278) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6283) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6284) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6285) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6286) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6288) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6290) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6291) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6293) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6296) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6297) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6298) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6299) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6304) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6305) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6306) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6307) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6309) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6311) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6312) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6314) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6317) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6318) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6327) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,6330) tree_plugin.h:2923: return false;
    (<0.0>,6333) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,6336) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,6338) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,6341) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6343) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6347) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,6350)
    (<0.0>,6351) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6353) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6356) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6363) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6364) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6375)
    (<0.0>,6376)
    (<0.0>,6377) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,6390)
    (<0.0>,6391) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6396) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6397) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6398) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6399) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6401) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6403) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6409) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6410) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6411) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6412) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6417) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6418) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6419) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6420) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6422) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6424) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6425) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6427) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6430) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6431) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6432) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6438) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,6454)
    (<0.0>,6455) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6458)
    (<0.0>,6459) tree.c:625: return &rsp->node[0];
    (<0.0>,6463) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6464) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6469) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6470) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6471) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6472) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6474) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6476) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6477) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6479) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6482) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6483) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6484) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6488) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,6489) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6491) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6494) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,6495) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6499) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6500) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6501) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6502) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6504) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6506) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6507) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6509) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6512) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6513) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6514) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,6518) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6521) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,6524) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,6527) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,6528) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,6531) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6533) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6536) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6539) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6542) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6543) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6545) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6548) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6552) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6554) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6556) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6559) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6562) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6565) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6566) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6568) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6571) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,6575) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6577) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6579) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,6582) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,6584) tree.c:666: }
    (<0.0>,6587) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6592) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6593) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6594) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6595) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6597) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6599) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6600) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6602) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6605) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6606) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6607) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6608) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6610) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,6613) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6618) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6619) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6620) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6621) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6623) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6625) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6626) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6628) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6631) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6632) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6633) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6634) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6636) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,6639) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6643) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6644) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6645) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6646) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6648) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6649) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6650) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6651) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6654) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6657) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6658) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,6666) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,6669) tree_plugin.h:2452: return false;
    (<0.0>,6673) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6675) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6677) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,6678) tree.c:3547: return 0;
    (<0.0>,6680) tree.c:3548: }
    (<0.0>,6685) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6688) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6689) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6690) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6694) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6695) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6696) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6698) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,6702) tree.c:3562: return 0;
    (<0.0>,6704) tree.c:3563: }
    (<0.0>,6708) tree.c:2891: if (user)
    (<0.0>,6716) fake_sched.h:43: return __running_cpu;
    (<0.0>,6720) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6722) fake_sched.h:43: return __running_cpu;
    (<0.0>,6726) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6732) fake_sched.h:43: return __running_cpu;
    (<0.0>,6736) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,6747) fake_sched.h:43: return __running_cpu;
    (<0.0>,6751) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,6752) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,6754) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,6755) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,6756) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,6758) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,6760) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,6761) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6762) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6763) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6764) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6765) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,6767) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,6775) tree_plugin.h:2879: }
    (<0.0>,6781) fake_sched.h:43: return __running_cpu;
    (<0.0>,6785) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,6788) tree.c:755: local_irq_save(flags);
    (<0.0>,6791) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6793) fake_sched.h:43: return __running_cpu;
    (<0.0>,6797) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6799) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6803) fake_sched.h:43: return __running_cpu;
    (<0.0>,6807) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6819) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,6821) fake_sched.h:43: return __running_cpu;
    (<0.0>,6825) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,6826) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,6828) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,6829) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,6830) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6831) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6832) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6833) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6834) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,6838) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,6840) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,6841) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6842) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,6858)
    (<0.0>,6860) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,6862) fake_sched.h:43: return __running_cpu;
    (<0.0>,6866) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,6869) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6870) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6871) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6875) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6876) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6877) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6879) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6884) fake_sched.h:43: return __running_cpu;
    (<0.0>,6887) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6889) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6891) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6892) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6895) tree_plugin.h:2457: }
    (<0.0>,6898) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6901) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6902) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6903) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6907) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6908) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6909) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6911) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6916) fake_sched.h:43: return __running_cpu;
    (<0.0>,6919) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6921) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6923) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,6924) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6927) tree_plugin.h:2457: }
    (<0.0>,6930) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6933) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6934) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6935) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6939) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6940) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6941) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6943) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,6950) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6953) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6954) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6955) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6957) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6958) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,6960) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6961) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6962) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6963) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,6977) tree_plugin.h:2879: }
    (<0.0>,6979) tree.c:758: local_irq_restore(flags);
    (<0.0>,6982) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6984) fake_sched.h:43: return __running_cpu;
    (<0.0>,6988) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6990) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6994) fake_sched.h:43: return __running_cpu;
    (<0.0>,6998) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7004) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,7007) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,7012) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7028)
    (<0.0>,7029) tree.c:2053: static bool rcu_gp_fqs_check_wake(struct rcu_state *rsp, int *gfp)
    (<0.0>,7030) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7033)
    (<0.0>,7034) tree.c:625: return &rsp->node[0];
    (<0.0>,7038) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7039) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7044) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7045) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7046) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7047) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7052) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7054) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7057) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7058) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7059) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7061) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7062) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7063) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7064) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7068) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7073) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7074) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7075) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7076) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7078) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7080) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7081) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7083) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7086) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7087) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7088) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7091) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7094) tree_plugin.h:749: return 0;
    (<0.0>,7098) tree.c:2064: return true;
    (<0.0>,7100) tree.c:2067: }
    (<0.0>,7105) fake_sched.h:43: return __running_cpu;
    (<0.0>,7109)
    (<0.0>,7110) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,7113) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,7118) tree.c:892: local_irq_save(flags);
    (<0.0>,7121) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7123) fake_sched.h:43: return __running_cpu;
    (<0.0>,7127) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7129) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7133) fake_sched.h:43: return __running_cpu;
    (<0.0>,7137) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7149) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7151) fake_sched.h:43: return __running_cpu;
    (<0.0>,7155) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7156) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7158) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7159) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7160) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7161) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7162) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7163) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7164) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,7168) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,7170) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,7171) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,7172) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,7183)
    (<0.0>,7184) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7186) fake_sched.h:43: return __running_cpu;
    (<0.0>,7190) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7194) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7197) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7198) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7199) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7201) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7202) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7204) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7205) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7206) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7207) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7217) tree_plugin.h:2883: }
    (<0.0>,7219) tree.c:895: local_irq_restore(flags);
    (<0.0>,7222) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7224) fake_sched.h:43: return __running_cpu;
    (<0.0>,7228) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7230) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7234) fake_sched.h:43: return __running_cpu;
    (<0.0>,7238) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7245) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7246) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7247) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7248) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,7250) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,7251) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7256) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7257) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7258) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7259) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7261) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7263) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7264) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7266) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7269) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7270) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7271) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7274) tree.c:2237: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7277) tree_plugin.h:749: return 0;
    (<0.0>,7282) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,7284) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,7285) tree.c:2280: rcu_gp_cleanup(rsp);
    (<0.0>,7325)
    (<0.0>,7326) tree.c:2109: bool needgp = false;
    (<0.0>,7327) tree.c:2110: int nocb = 0;
    (<0.0>,7328) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7331)
    (<0.0>,7332) tree.c:625: return &rsp->node[0];
    (<0.0>,7336) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7338) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7339) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7340) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7345) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7346) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7347) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7348) fake_defs.h:237: switch (size) {
    (<0.0>,7350) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7352) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7353) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7355) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7358) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7359) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7360) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7361) tree.c:2116: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,7364)
    (<0.0>,7365) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,7369) fake_sync.h:99: local_irq_disable();
    (<0.0>,7372) fake_sched.h:43: return __running_cpu;
    (<0.0>,7376) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,7380) fake_sched.h:43: return __running_cpu;
    (<0.0>,7384) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7389) fake_sched.h:43: return __running_cpu;
    (<0.0>,7393) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,7396) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7397) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7404) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7405) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7407) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7409) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7410) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,7411) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,7413) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,7416) tree.c:2129: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,7419)
    (<0.0>,7420) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,7424)
    (<0.0>,7425) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7426) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7427) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7431) fake_sched.h:43: return __running_cpu;
    (<0.0>,7435) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,7437) fake_sched.h:43: return __running_cpu;
    (<0.0>,7441) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7448) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7451) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7453) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7454) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7456) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7461) tree.c:2141: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,7464)
    (<0.0>,7465) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,7469) fake_sync.h:99: local_irq_disable();
    (<0.0>,7472) fake_sched.h:43: return __running_cpu;
    (<0.0>,7476) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,7480) fake_sched.h:43: return __running_cpu;
    (<0.0>,7484) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7489) fake_sched.h:43: return __running_cpu;
    (<0.0>,7493) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,7496) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7497) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7504) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7507) tree_plugin.h:749: return 0;
    (<0.0>,7513) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7514) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7517) tree_plugin.h:749: return 0;
    (<0.0>,7522) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7523) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7524) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7525) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7527) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7532) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7533) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7535) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7539) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7540) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7541) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7543) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7545) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7546) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7547) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7552) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7553) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7554) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7555) fake_defs.h:237: switch (size) {
    (<0.0>,7557) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7559) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7560) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7562) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7565) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7566) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7567) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,7569) fake_sched.h:43: return __running_cpu;
    (<0.0>,7572) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7574) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7576) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7577) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,7578) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,7580) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,7583) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,7584) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,7585) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,7607)
    (<0.0>,7608)
    (<0.0>,7609)
    (<0.0>,7610) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,7612) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,7613) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,7615) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,7618) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,7619) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,7620) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,7628)
    (<0.0>,7629)
    (<0.0>,7630)
    (<0.0>,7631) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7634) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7637) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7640) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7641) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7644) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,7646) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,7649) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7651) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7652) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7654) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7657) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7661) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,7663) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,7666) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,7667) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,7670) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,7672) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,7674) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,7676) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,7679) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7681) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7682) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7684) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7687) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,7692) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7694) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7695) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7698) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7701) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7702) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7704) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7707) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,7709) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7711) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7713) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7714) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,7717) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,7719) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,7722) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7724) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7727) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7728) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7731) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7735) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,7736) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,7737) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,7746)
    (<0.0>,7747)
    (<0.0>,7748)
    (<0.0>,7749) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7752) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7755) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7758) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7759) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,7762) tree.c:1750: return false;
    (<0.0>,7764) tree.c:1799: }
    (<0.0>,7766) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,7768) tree.c:1843: }
    (<0.0>,7771) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,7772) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,7774) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,7775) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,7777) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,7781) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7783) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7784) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7786) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7789) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7793) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7794) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7795) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7796) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7798) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7799) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7800) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7801) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7804) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7807) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7808) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,7816) tree.c:1896: return ret;
    (<0.0>,7820) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,7824) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,7826) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,7827) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,7834)
    (<0.0>,7835) tree.c:1700: static int rcu_future_gp_cleanup(struct rcu_state *rsp, struct rcu_node *rnp)
    (<0.0>,7836) tree.c:1702: int c = rnp->completed;
    (<0.0>,7838) tree.c:1702: int c = rnp->completed;
    (<0.0>,7840) tree.c:1702: int c = rnp->completed;
    (<0.0>,7842) fake_sched.h:43: return __running_cpu;
    (<0.0>,7845) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7847) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7849) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7850) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,7853) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,7856) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,7857) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,7861) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,7864) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,7865) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,7866) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,7867) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,7868) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,7870) tree.c:1709: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,7878)
    (<0.0>,7879)
    (<0.0>,7880)
    (<0.0>,7881) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,7885) tree.c:1710: return needmore;
    (<0.0>,7887) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,7889) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,7890) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,7893) tree_plugin.h:2426: return NULL;
    (<0.0>,7895) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,7896) tree.c:2151: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,7899)
    (<0.0>,7900) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,7904)
    (<0.0>,7905) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7906) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7907) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7911) fake_sched.h:43: return __running_cpu;
    (<0.0>,7915) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,7917) fake_sched.h:43: return __running_cpu;
    (<0.0>,7921) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7928) tree.c:2152: rcu_nocb_gp_cleanup(sq);
    (<0.0>,7931) tree_plugin.h:2418: }
    (<0.0>,7941) fake_sched.h:43: return __running_cpu;
    (<0.0>,7947) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,7955) fake_sched.h:43: return __running_cpu;
    (<0.0>,7959) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,7972) fake_sched.h:43: return __running_cpu;
    (<0.0>,7976) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,7979) tree.c:755: local_irq_save(flags);
    (<0.0>,7982) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7984) fake_sched.h:43: return __running_cpu;
    (<0.0>,7988) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7990) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7994) fake_sched.h:43: return __running_cpu;
    (<0.0>,7998) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8010) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8012) fake_sched.h:43: return __running_cpu;
    (<0.0>,8016) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8017) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8019) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8020) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8021) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8022) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8023) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8024) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8025) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,8029) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,8031) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,8032) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8033) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8049)
    (<0.0>,8051) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8053) fake_sched.h:43: return __running_cpu;
    (<0.0>,8057) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8060) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8061) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8062) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8066) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8067) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8068) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8070) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8075) fake_sched.h:43: return __running_cpu;
    (<0.0>,8078) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8080) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8082) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8083) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8086) tree_plugin.h:2457: }
    (<0.0>,8089) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8092) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8093) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8094) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8098) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8099) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8100) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8102) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8107) fake_sched.h:43: return __running_cpu;
    (<0.0>,8110) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8112) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8114) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8115) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8118) tree_plugin.h:2457: }
    (<0.0>,8121) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8124) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8125) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8126) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8130) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8131) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8132) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8134) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8141) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8144) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8145) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8146) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8148) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8149) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8151) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8152) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8153) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8154) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8168) tree_plugin.h:2879: }
    (<0.0>,8170) tree.c:758: local_irq_restore(flags);
    (<0.0>,8173) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8175) fake_sched.h:43: return __running_cpu;
    (<0.0>,8179) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8181) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8185) fake_sched.h:43: return __running_cpu;
    (<0.0>,8189) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8195) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8198) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8203) fake_sched.h:43: return __running_cpu;
    (<0.0>,8207)
    (<0.0>,8208) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8211) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8216) tree.c:892: local_irq_save(flags);
    (<0.0>,8219) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8221) fake_sched.h:43: return __running_cpu;
    (<0.0>,8225) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8227) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8231) fake_sched.h:43: return __running_cpu;
    (<0.0>,8235) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8247) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8249) fake_sched.h:43: return __running_cpu;
    (<0.0>,8253) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8254) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8256) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8257) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8258) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8259) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8260) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8261) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8262) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,8266) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8268) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8269) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8270) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8281)
    (<0.0>,8282) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8284) fake_sched.h:43: return __running_cpu;
    (<0.0>,8288) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8292) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8295) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8296) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8297) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8299) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8300) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8302) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8303) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8304) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8305) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8315) tree_plugin.h:2883: }
    (<0.0>,8317) tree.c:895: local_irq_restore(flags);
    (<0.0>,8320) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8322) fake_sched.h:43: return __running_cpu;
    (<0.0>,8326) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8328) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8332) fake_sched.h:43: return __running_cpu;
    (<0.0>,8336) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8350) fake_sched.h:43: return __running_cpu;
    (<0.0>,8354) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,8363) fake_sched.h:43: return __running_cpu;
    (<0.0>,8370) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,8379) fake_sched.h:43: return __running_cpu;
    (<0.0>,8383) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,8385) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,8391) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8392) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8393) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8398) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8399) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8400) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8401) fake_defs.h:237: switch (size) {
    (<0.0>,8403) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8405) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8406) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8408) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8411) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8412) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8413) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8414) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,8415) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,8419)
    (<0.0>,8420)
    (<0.0>,8421) tree.c:1922: if (delay > 0 &&
    (<0.0>,8426) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8428) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8430) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8431) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8433) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8438) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,8441)
    (<0.0>,8442) tree.c:625: return &rsp->node[0];
    (<0.0>,8446) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,8447) tree.c:2158: raw_spin_lock_irq_rcu_node(rnp); /* Order GP before ->completed update. */
    (<0.0>,8450)
    (<0.0>,8451) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8455) fake_sync.h:99: local_irq_disable();
    (<0.0>,8458) fake_sched.h:43: return __running_cpu;
    (<0.0>,8462) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8466) fake_sched.h:43: return __running_cpu;
    (<0.0>,8470) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8475) fake_sched.h:43: return __running_cpu;
    (<0.0>,8479) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8482) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8483) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8490) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,8491) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,8495)
    (<0.0>,8496) tree_plugin.h:2422: }
    (<0.0>,8499) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8501) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8502) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8503) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8508) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8509) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8510) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8511) fake_defs.h:237: switch (size) {
    (<0.0>,8513) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8515) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8516) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8518) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8521) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8522) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8523) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8526) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,8528) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,8530) fake_sched.h:43: return __running_cpu;
    (<0.0>,8533) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8535) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8537) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8538) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,8539) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,8540) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,8548)
    (<0.0>,8549)
    (<0.0>,8550)
    (<0.0>,8551) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8554) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8557) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8560) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8561) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8564) tree.c:1818: return false;
    (<0.0>,8566) tree.c:1843: }
    (<0.0>,8569) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,8573) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,8574) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8577) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8578) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,8589)
    (<0.0>,8590)
    (<0.0>,8591) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,8604)
    (<0.0>,8605) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8610) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8611) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8612) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8613) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8615) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8617) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8618) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8620) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8623) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8624) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8625) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8626) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8631) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8632) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8633) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8634) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8636) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8638) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8639) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8641) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8644) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8645) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8646) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,8652) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,8668)
    (<0.0>,8669) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8672)
    (<0.0>,8673) tree.c:625: return &rsp->node[0];
    (<0.0>,8677) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,8678) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8683) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8684) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8685) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8686) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8688) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8690) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8691) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8693) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8696) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8697) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8698) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8702) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,8703) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8705) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8708) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,8709) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8713) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8714) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8715) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8716) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8718) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8720) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8721) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8723) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8726) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8727) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8728) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,8732) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,8734) tree.c:666: }
    (<0.0>,8739) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8744) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8745) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8746) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8747) fake_defs.h:237: switch (size) {
    (<0.0>,8749) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,8751) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,8752) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,8754) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,8757) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8758) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8759) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,8763) tree.c:2174: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,8766)
    (<0.0>,8767) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8771)
    (<0.0>,8772) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8773) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8774) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8778) fake_sched.h:43: return __running_cpu;
    (<0.0>,8782) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,8784) fake_sched.h:43: return __running_cpu;
    (<0.0>,8788) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8796) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,8798) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,8803) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,8805) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,8809) fake_sched.h:43: return __running_cpu;
    (<0.0>,8813) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8817) fake_sched.h:43: return __running_cpu;
    (<0.0>,8821) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8826) fake_sched.h:43: return __running_cpu;
    (<0.0>,8830) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8841) fake_sched.h:43: return __running_cpu;
    (<0.0>,8845) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8846) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,8848) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,8849) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,8850) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,8852) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,8854) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,8855) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8856) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8857) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8858) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8859) tree.c:942: if (oldval)
    (<0.0>,8867) tree_plugin.h:2883: }
    (<0.0>,8873) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,8882) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8883) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8884) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8888) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8889) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8890) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8892) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8897) fake_sched.h:43: return __running_cpu;
    (<0.0>,8900) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8902) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8905) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8907) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8909) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8912) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8913) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8914) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8918) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8919) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8920) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8922) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8927) fake_sched.h:43: return __running_cpu;
    (<0.0>,8930) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8932) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8935) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8937) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,8939) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8942) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8943) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8944) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8948) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8949) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8950) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8952) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,8957) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,8962) fake_sched.h:43: return __running_cpu;
    (<0.0>,8967) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,8975) fake_sched.h:43: return __running_cpu;
    (<0.0>,8981) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,8995) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,8996) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,8997) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9001) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9002) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9003) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9005) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9009) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9011) fake_sched.h:43: return __running_cpu;
    (<0.0>,9014) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9016) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9038)
    (<0.0>,9039)
    (<0.0>,9040) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9042) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9043) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9044) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9046) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9048) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9049) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,9050) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,9085)
    (<0.0>,9086)
    (<0.0>,9087) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,9090) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,9103)
    (<0.0>,9104) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9109) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9110) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9111) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9112) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9114) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9116) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9117) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9119) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9122) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9123) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9124) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9125) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9130) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9131) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9132) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9133) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9135) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9137) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9138) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9140) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9143) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9144) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9145) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9153) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,9156) tree_plugin.h:2923: return false;
    (<0.0>,9159) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,9162) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9164) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9167) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,9169) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,9173) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,9176)
    (<0.0>,9177) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9179) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9182) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,9185) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,9188) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,9195) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,9197) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,9199) tree.c:3516: rdp->n_rp_cb_ready++;
    (<0.0>,9200) tree.c:3517: return 1;
    (<0.0>,9202) tree.c:3548: }
    (<0.0>,9206) tree.c:3561: return 1;
    (<0.0>,9208) tree.c:3563: }
    (<0.0>,9215) fake_sched.h:43: return __running_cpu;
    (<0.0>,9219) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,9223) tree.c:2891: if (user)
    (<0.0>,9231) fake_sched.h:43: return __running_cpu;
    (<0.0>,9235) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9237) fake_sched.h:43: return __running_cpu;
    (<0.0>,9241) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9247) fake_sched.h:43: return __running_cpu;
    (<0.0>,9251) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,9261) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,9264) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9265) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9266) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9270) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9271) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9272) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9274) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9278) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,9290) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,9292) fake_sched.h:43: return __running_cpu;
    (<0.0>,9295) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,9297) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,9299) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,9300) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9302) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9309) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9310) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9312) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9318) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9319) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9320) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9321) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,9322) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,9326)
    (<0.0>,9327)
    (<0.0>,9328) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,9329) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,9354)
    (<0.0>,9355)
    (<0.0>,9356) tree.c:1905: local_irq_save(flags);
    (<0.0>,9359) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9361) fake_sched.h:43: return __running_cpu;
    (<0.0>,9365) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9367) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9371) fake_sched.h:43: return __running_cpu;
    (<0.0>,9375) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9380) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,9382) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,9383) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,9384) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9386) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9387) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9392) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9393) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9394) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9395) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9397) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9399) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9400) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9405) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9406) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9407) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9410) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9412) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9413) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9418) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9419) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9420) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9421) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9423) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9425) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9426) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9428) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9431) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9432) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9433) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9436) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9440) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9441) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9442) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9443) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9445) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9446) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9447) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9448) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9451) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9454) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9455) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9463) tree.c:1911: local_irq_restore(flags);
    (<0.0>,9466) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9468) fake_sched.h:43: return __running_cpu;
    (<0.0>,9472) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9474) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9478) fake_sched.h:43: return __running_cpu;
    (<0.0>,9482) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9489) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,9491) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,9496) tree.c:3016: local_irq_save(flags);
    (<0.0>,9499) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9501) fake_sched.h:43: return __running_cpu;
    (<0.0>,9505) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9507) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9511) fake_sched.h:43: return __running_cpu;
    (<0.0>,9515) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9520) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9521) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9532)
    (<0.0>,9533)
    (<0.0>,9534) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,9547)
    (<0.0>,9548) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9553) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9554) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9555) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9556) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9558) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9560) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9561) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9563) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9566) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9567) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9569) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9574) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9575) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9576) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9577) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9579) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9581) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9582) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9584) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9587) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9588) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9589) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9595) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,9611)
    (<0.0>,9612) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9615)
    (<0.0>,9616) tree.c:625: return &rsp->node[0];
    (<0.0>,9620) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9621) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9626) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9627) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9628) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9629) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9631) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9633) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9634) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9636) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9639) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9640) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9641) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9645) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9646) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9648) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9651) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9652) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9656) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9657) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9658) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9659) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9661) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9663) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9664) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9666) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9669) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9670) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9671) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9675) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,9677) tree.c:666: }
    (<0.0>,9680) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,9683)
    (<0.0>,9684) tree.c:625: return &rsp->node[0];
    (<0.0>,9690)
    (<0.0>,9691) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,9695) fake_sync.h:115: preempt_disable();
    (<0.0>,9697) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,9698) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,9705) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,9711) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9713) fake_sched.h:43: return __running_cpu;
    (<0.0>,9716) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9718) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9720) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,9721) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9724)
    (<0.0>,9725) tree.c:625: return &rsp->node[0];
    (<0.0>,9729) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9730) tree.c:2334: bool ret = false;
    (<0.0>,9731) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,9732) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,9733) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,9741)
    (<0.0>,9742)
    (<0.0>,9743)
    (<0.0>,9744) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9747) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9750) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9753) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9754) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9757) tree.c:1818: return false;
    (<0.0>,9759) tree.c:1843: }
    (<0.0>,9762) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,9766) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,9767) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,9768) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,9769) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,9780)
    (<0.0>,9781)
    (<0.0>,9782)
    (<0.0>,9783) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9785) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9788) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9789) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9800)
    (<0.0>,9801)
    (<0.0>,9802) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,9815)
    (<0.0>,9816) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9821) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9822) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9823) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9824) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9826) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9834) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9835) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9836) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9837) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9842) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9843) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9844) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9845) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9847) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9849) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9850) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9852) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9855) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9856) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9857) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9863) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,9879)
    (<0.0>,9880) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9883)
    (<0.0>,9884) tree.c:625: return &rsp->node[0];
    (<0.0>,9888) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9889) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9894) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9895) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9896) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9897) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9899) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9901) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9902) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9904) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9907) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9908) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9909) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9913) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9914) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9916) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9919) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9920) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9924) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9925) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9926) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9927) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9929) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9932) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9934) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9937) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9938) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9939) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9943) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,9945) tree.c:666: }
    (<0.0>,9950) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9955) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9956) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9957) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9958) fake_defs.h:237: switch (size) {
    (<0.0>,9960) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9962) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9963) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9965) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9968) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9969) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9970) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9973) tree.c:2318: return true;
    (<0.0>,9975) tree.c:2319: }
    (<0.0>,9979) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,9980) tree.c:2346: return ret;
    (<0.0>,9984) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,9988) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,9989) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,9990) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,9993)
    (<0.0>,9994) tree.c:625: return &rsp->node[0];
    (<0.0>,9999) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,10003)
    (<0.0>,10004)
    (<0.0>,10005) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10006) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,7166) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,7173) tree.c:3019: needwake = rcu_start_gp(rsp);
  (<0>,7179) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
  (<0>,7181) fake_sched.h:43: return __running_cpu;
  (<0>,7184) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
  (<0>,7186) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
  (<0>,7188) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
  (<0>,7189) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7192)
  (<0>,7193) tree.c:625: return &rsp->node[0];
  (<0>,7197) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7198) tree.c:2334: bool ret = false;
  (<0>,7199) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
  (<0>,7200) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
  (<0>,7201) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
  (<0>,7209)
  (<0>,7210)
  (<0>,7211)
  (<0>,7212) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7215) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7218) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7221) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7222) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7225) tree.c:1818: return false;
  (<0>,7227) tree.c:1843: }
  (<0>,7230) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
  (<0>,7234) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
  (<0>,7235) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
  (<0>,7236) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
  (<0>,7237) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
  (<0>,7248)
  (<0>,7249)
  (<0>,7250)
  (<0>,7251) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7253) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7256) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7257) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7268)
  (<0>,7269)
  (<0>,7270) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7283)
  (<0>,7284) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7289) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7290) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7291) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7292) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7294) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7296) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7297) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7299) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7302) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7303) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7304) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7305) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7310) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7311) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7312) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7313) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7315) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7317) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7318) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7320) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7323) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7324) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7325) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7331) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,7347)
  (<0>,7348) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7351)
  (<0>,7352) tree.c:625: return &rsp->node[0];
  (<0>,7356) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7357) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7362) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7363) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7364) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7365) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7367) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7369) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7370) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7372) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7375) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7376) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7377) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7381) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7382) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7384) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7387) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7388) tree.c:639: return READ_ONCE(*fp);
  (<0>,7392) tree.c:639: return READ_ONCE(*fp);
  (<0>,7393) tree.c:639: return READ_ONCE(*fp);
  (<0>,7394) tree.c:639: return READ_ONCE(*fp);
  (<0>,7395) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7397) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7399) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7400) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7402) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7405) tree.c:639: return READ_ONCE(*fp);
  (<0>,7406) tree.c:639: return READ_ONCE(*fp);
  (<0>,7407) tree.c:639: return READ_ONCE(*fp);
  (<0>,7411) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
  (<0>,7413) tree.c:666: }
  (<0>,7418) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
  (<0>,7423) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
  (<0>,7424) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
  (<0>,7425) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
  (<0>,7426) fake_defs.h:237: switch (size) {
  (<0>,7428) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
  (<0>,7430) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
  (<0>,7431) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
  (<0>,7433) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
  (<0>,7436) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
  (<0>,7437) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
  (<0>,7438) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
  (<0>,7441) tree.c:2318: return true;
  (<0>,7443) tree.c:2319: }
  (<0>,7447) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
  (<0>,7448) tree.c:2346: return ret;
  (<0>,7452) tree.c:3019: needwake = rcu_start_gp(rsp);
  (<0>,7456) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
  (<0>,7457) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
  (<0>,7458) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
  (<0>,7461)
  (<0>,7462) tree.c:625: return &rsp->node[0];
  (<0>,7467) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
  (<0>,7471)
  (<0>,7472)
  (<0>,7473) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,7474) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,7477) fake_sync.h:93: local_irq_restore(flags);
  (<0>,7480) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7482) fake_sched.h:43: return __running_cpu;
  (<0>,7486) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7488) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7492) fake_sched.h:43: return __running_cpu;
  (<0>,7496) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7504) tree.c:3021: if (needwake)
  (<0>,7507) tree.c:3022: rcu_gp_kthread_wake(rsp);
  (<0>,7515)
  (<0>,7516) tree.c:1722: if (current == rsp->gp_kthread ||
  (<0>,7517) tree.c:1722: if (current == rsp->gp_kthread ||
  (<0>,7519) tree.c:1722: if (current == rsp->gp_kthread ||
  (<0>,7522) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,7527) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,7528) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,7529) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,7530) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7532) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7534) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7535) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7537) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7540) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,7541) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,7542) tree.c:1723: !READ_ONCE(rsp->gp_flags) ||
  (<0>,7545) tree.c:1724: !rsp->gp_kthread)
  (<0>,7547) tree.c:1724: !rsp->gp_kthread)
  (<0>,7555) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7558)
  (<0>,7559) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7561) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7564) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7571) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,7574) tree_plugin.h:2457: }
  (<0>,7578) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7581) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7582) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7583) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7587) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7588) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7589) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7591) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7595) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,7607) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7609) fake_sched.h:43: return __running_cpu;
  (<0>,7612) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7614) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7616) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7617) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7619) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7626) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7627) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7629) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7635) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7636) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7637) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7638) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7639) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7643)
  (<0>,7644)
  (<0>,7645) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,7646) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,7671)
  (<0>,7672)
  (<0>,7673) tree.c:1905: local_irq_save(flags);
  (<0>,7676) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7678) fake_sched.h:43: return __running_cpu;
  (<0>,7682) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7684) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7688) fake_sched.h:43: return __running_cpu;
  (<0>,7692) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7697) tree.c:1906: rnp = rdp->mynode;
  (<0>,7699) tree.c:1906: rnp = rdp->mynode;
  (<0>,7700) tree.c:1906: rnp = rdp->mynode;
  (<0>,7701) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7703) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7704) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7709) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7710) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7711) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7712) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7714) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7716) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7717) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7719) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7722) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7723) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7724) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7727) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7729) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7730) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7735) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7736) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7737) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7738) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7740) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7742) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7743) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7745) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7748) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7749) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7750) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7753) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7757) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7758) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7759) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7760) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7762) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7763) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7764) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7765) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7768) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7771) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7772) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7780) tree.c:1911: local_irq_restore(flags);
  (<0>,7783) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7785) fake_sched.h:43: return __running_cpu;
  (<0>,7789) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7791) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7795) fake_sched.h:43: return __running_cpu;
  (<0>,7799) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7806) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7808) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7813) tree.c:3016: local_irq_save(flags);
  (<0>,7816) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7818) fake_sched.h:43: return __running_cpu;
  (<0>,7822) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7824) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7828) fake_sched.h:43: return __running_cpu;
  (<0>,7832) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7837) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7838) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7849)
  (<0>,7850)
  (<0>,7851) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7864)
  (<0>,7865) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7870) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7871) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7872) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7873) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7875) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7877) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7878) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7880) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7883) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7885) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7886) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7891) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7892) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7893) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7894) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7896) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7898) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7899) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7901) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7904) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7905) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7906) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7912) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,7928)
  (<0>,7929) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7932)
  (<0>,7933) tree.c:625: return &rsp->node[0];
  (<0>,7937) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7938) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7943) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7944) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7945) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7946) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7948) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7950) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7951) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7953) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7956) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7957) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7958) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7962) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7963) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7965) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7968) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7969) tree.c:639: return READ_ONCE(*fp);
  (<0>,7973) tree.c:639: return READ_ONCE(*fp);
  (<0>,7974) tree.c:639: return READ_ONCE(*fp);
  (<0>,7975) tree.c:639: return READ_ONCE(*fp);
  (<0>,7976) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7978) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7980) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7981) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7983) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7986) tree.c:639: return READ_ONCE(*fp);
  (<0>,7987) tree.c:639: return READ_ONCE(*fp);
  (<0>,7988) tree.c:639: return READ_ONCE(*fp);
  (<0>,7992) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7995) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7998) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,8001) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,8002) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,8005) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8007) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8010) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8013) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8016) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8017) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8019) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8022) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8026) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8028) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8030) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8033) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8036) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8039) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8040) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8042) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8045) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,8049) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8051) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8053) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,8056) tree.c:665: return false; /* No grace period needed. */
  (<0>,8058) tree.c:666: }
  (<0>,8061) tree.c:3024: local_irq_restore(flags);
  (<0>,8064) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8066) fake_sched.h:43: return __running_cpu;
  (<0>,8070) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8072) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8076) fake_sched.h:43: return __running_cpu;
  (<0>,8080) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8086) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,8089)
  (<0>,8090) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,8092) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,8095) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,8102) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,8105) tree_plugin.h:2457: }
  (<0>,8109) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8112) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8113) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8114) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8118) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8119) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8120) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8122) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8130) fake_sched.h:43: return __running_cpu;
  (<0>,8134) fake_sched.h:185: need_softirq[get_cpu()] = 0;
  (<0>,8144) fake_sched.h:43: return __running_cpu;
  (<0>,8148) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8149) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8151) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8152) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8153) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8155) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8157) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8158) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8159) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8160) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8161) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8162) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,8164) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,8172) tree_plugin.h:2879: }
      (<0.1>,763) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,10007) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10009) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,10012)
    (<0.0>,10014) fake_sched.h:43: return __running_cpu;
    (<0.0>,10018) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10020) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10024) fake_sched.h:43: return __running_cpu;
    (<0.0>,10028) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10036) tree.c:3021: if (needwake)
    (<0.0>,10039) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,10047)
    (<0.0>,10048) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,10049) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,10051) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,10058) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,10061)
    (<0.0>,10062) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10064) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10067) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10070) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10073) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10080) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,10081) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,10090)
    (<0.0>,10091)
    (<0.0>,10094) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10095) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10096) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10097) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10099) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10101) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10102) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10104) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10107) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10108) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10109) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10118) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,10120) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,10129) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,10130) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,10152)
    (<0.0>,10153)
    (<0.0>,10154) tree.c:2767: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,10157)
    (<0.0>,10158) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10160) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10163) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10166) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10169) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10176) tree.c:2779: local_irq_save(flags);
    (<0.0>,10179)
    (<0.0>,10181) fake_sched.h:43: return __running_cpu;
    (<0.0>,10185) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10187) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10191) fake_sched.h:43: return __running_cpu;
    (<0.0>,10195) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10200) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10201) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10202) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10203) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10204) tree.c:2781: bl = rdp->blimit;
    (<0.0>,10206) tree.c:2781: bl = rdp->blimit;
    (<0.0>,10207) tree.c:2781: bl = rdp->blimit;
    (<0.0>,10210) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,10212) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,10213) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,10214) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10217) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10218) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10219) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10221) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10222) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,10225) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,10226) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,10227) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10230) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10231) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10232) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10234) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10237) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10239) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10242) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10243) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10246) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10249) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10251) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10253) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10256) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10259) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10261) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10263) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10266) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10268) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10271) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10272) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10275) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10278) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10280) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10282) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10285) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10288) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10290) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10292) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10295) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10297) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10300) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10301) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10304) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10307) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10309) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10311) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10314) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10317) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10319) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10321) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10324) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10326) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10329) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10330) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10333) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10336) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10338) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10340) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10343) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10346) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10348) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10350) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10353) tree.c:2790: local_irq_restore(flags);
    (<0.0>,10356)
    (<0.0>,10358) fake_sched.h:43: return __running_cpu;
    (<0.0>,10362) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10364) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10368) fake_sched.h:43: return __running_cpu;
    (<0.0>,10372) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10377) tree.c:2793: count = count_lazy = 0;
    (<0.0>,10378) tree.c:2793: count = count_lazy = 0;
    (<0.0>,10380) tree.c:2794: while (list) {
    (<0.0>,10383) tree.c:2795: next = list->next;
    (<0.0>,10385) tree.c:2795: next = list->next;
    (<0.0>,10386) tree.c:2795: next = list->next;
    (<0.0>,10389) tree.c:2797: debug_rcu_head_unqueue(list);
    (<0.0>,10392)
    (<0.0>,10394) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,10396) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,10397) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,10403)
    (<0.0>,10404)
    (<0.0>,10405) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,10408) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,10410) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,10413) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,10416) rcu.h:118: head->func(head);
    (<0.0>,10419) rcu.h:118: head->func(head);
    (<0.0>,10420) rcu.h:118: head->func(head);
    (<0.0>,10426)
    (<0.0>,10427) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10428) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10429) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10433) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10434) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10435) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,10436) update.c:341: complete(&rcu->completion);
    (<0.0>,10440)
    (<0.0>,10441) fake_sync.h:288: x->done++;
    (<0.0>,10443) fake_sync.h:288: x->done++;
    (<0.0>,10445) fake_sync.h:288: x->done++;
    (<0.0>,10450) rcu.h:120: return false;
    (<0.0>,10452) rcu.h:122: }
    (<0.0>,10455) tree.c:2800: list = next;
    (<0.0>,10456) tree.c:2800: list = next;
    (<0.0>,10457) tree.c:2802: if (++count >= bl &&
    (<0.0>,10459) tree.c:2802: if (++count >= bl &&
    (<0.0>,10460) tree.c:2802: if (++count >= bl &&
    (<0.0>,10464) tree.c:2794: while (list) {
    (<0.0>,10467) tree.c:2808: local_irq_save(flags);
    (<0.0>,10470)
    (<0.0>,10472) fake_sched.h:43: return __running_cpu;
    (<0.0>,10476) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10478) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10482) fake_sched.h:43: return __running_cpu;
    (<0.0>,10486) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10493) tree.c:2814: if (list != NULL) {
    (<0.0>,10497) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,10498) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,10500) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,10502) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,10504) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10506) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10507) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10509) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10510) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10515) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10516) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10517) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10518) fake_defs.h:237: switch (size) {
    (<0.0>,10520) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,10522) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,10523) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,10525) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,10528) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10529) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10530) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,10531) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,10532) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,10534) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,10536) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,10537) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,10539) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,10542) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,10544) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,10547) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,10549) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,10552) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,10554) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,10555) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,10557) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,10558) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,10563) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10565) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10568) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10570) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10577) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10578) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10580) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10583) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10585) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10591) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10592) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10593) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,10594) tree.c:2840: local_irq_restore(flags);
    (<0.0>,10597)
    (<0.0>,10599) fake_sched.h:43: return __running_cpu;
    (<0.0>,10603) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10605) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10609) fake_sched.h:43: return __running_cpu;
    (<0.0>,10613) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10618) tree.c:2843: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,10621)
    (<0.0>,10622) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10624) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10627) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10638) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,10641)
    (<0.0>,10645) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10648) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10649) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10650) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10654) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10655) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10656) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10658) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,10662) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,10674)
    (<0.0>,10676) fake_sched.h:43: return __running_cpu;
    (<0.0>,10679) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10681) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10683) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,10684) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10686) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10693) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10694) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10696) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10702) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10703) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10704) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,10705) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10706) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,10710)
    (<0.0>,10711)
    (<0.0>,10712) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,10713) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,10738)
    (<0.0>,10739)
    (<0.0>,10740) tree.c:1905: local_irq_save(flags);
    (<0.0>,10743)
    (<0.0>,10745) fake_sched.h:43: return __running_cpu;
    (<0.0>,10749) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10751) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10755) fake_sched.h:43: return __running_cpu;
    (<0.0>,10759) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10764) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10766) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10767) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,10768) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10770) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10771) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10776) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10777) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10778) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10779) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10781) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10783) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10784) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10786) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10789) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10790) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10791) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,10794) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10796) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10797) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10802) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10803) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10804) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10805) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10807) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10809) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10810) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10812) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10815) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10816) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10817) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,10820) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10824) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10825) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10826) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10830) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10835) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10838) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10839) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,10847) tree.c:1911: local_irq_restore(flags);
    (<0.0>,10850)
    (<0.0>,10852) fake_sched.h:43: return __running_cpu;
    (<0.0>,10856) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10858) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10862) fake_sched.h:43: return __running_cpu;
    (<0.0>,10866) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10873) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,10875) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,10880) tree.c:3016: local_irq_save(flags);
    (<0.0>,10883)
    (<0.0>,10885) fake_sched.h:43: return __running_cpu;
    (<0.0>,10889) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10891) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10895) fake_sched.h:43: return __running_cpu;
    (<0.0>,10899) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10904) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10905) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10916)
    (<0.0>,10917)
    (<0.0>,10918) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,10931)
    (<0.0>,10932) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10937) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10938) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10939) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10940) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10944) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10945) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10947) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10950) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10951) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10952) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10953) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10958) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10959) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10960) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10963) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10965) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10966) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10968) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10971) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10972) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10973) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10979) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,10995)
    (<0.0>,10996) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10999)
    (<0.0>,11000) tree.c:625: return &rsp->node[0];
    (<0.0>,11004) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11005) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11010) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11011) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11012) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11013) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11015) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11017) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11018) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11020) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11023) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11024) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11025) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11029) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11030) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11032) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11035) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11036) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11040) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11041) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11042) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11043) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11045) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11047) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11048) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11050) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11053) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11054) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11055) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11059) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11062) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11065) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11068) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11069) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11072) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11074) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11077) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11080) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11083) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11084) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11086) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11089) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11093) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11095) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11097) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11100) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11103) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11106) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11107) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11109) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11112) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11116) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11118) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11120) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11123) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,11125) tree.c:666: }
    (<0.0>,11128) tree.c:3024: local_irq_restore(flags);
    (<0.0>,11131)
    (<0.0>,11133) fake_sched.h:43: return __running_cpu;
    (<0.0>,11137) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11139) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11143) fake_sched.h:43: return __running_cpu;
    (<0.0>,11147) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11153) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11156)
    (<0.0>,11157) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11159) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11162) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11169) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11172)
    (<0.0>,11176) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11179) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11180) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11181) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11185) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11186) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11187) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11189) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11197) fake_sched.h:43: return __running_cpu;
    (<0.0>,11201) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,11211) fake_sched.h:43: return __running_cpu;
    (<0.0>,11215) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11216) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,11218) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,11219) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,11220) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,11222) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,11224) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,11225) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11226) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11227) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11228) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11229) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,11231) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,11239)
    (<0.0>,11245) fake_sched.h:43: return __running_cpu;
    (<0.0>,11249)
    (<0.0>,11252) tree.c:755: local_irq_save(flags);
    (<0.0>,11255)
    (<0.0>,11257) fake_sched.h:43: return __running_cpu;
    (<0.0>,11261) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11263) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11267) fake_sched.h:43: return __running_cpu;
    (<0.0>,11271) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11283)
    (<0.0>,11285) fake_sched.h:43: return __running_cpu;
    (<0.0>,11289) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11290) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,11292) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,11293) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,11294) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11295) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11296) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11297) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11298) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,11302) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,11304) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,11305) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11306) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11322)
    (<0.0>,11324)
    (<0.0>,11326) fake_sched.h:43: return __running_cpu;
    (<0.0>,11330) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11333) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11334) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11335) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11339) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11340) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11341) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11343) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11348) fake_sched.h:43: return __running_cpu;
    (<0.0>,11351) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11353) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11355) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11356) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11359)
    (<0.0>,11362) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11365) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11366) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11367) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11371) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11372) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11373) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11375) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11380) fake_sched.h:43: return __running_cpu;
    (<0.0>,11383) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11385) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11387) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11388) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11391)
    (<0.0>,11394) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11397) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11398) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11399) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11403) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11404) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11405) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11407) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11414) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11417) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11418) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11419) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11421) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11422) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,11424) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11425) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11426) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11427) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11441)
    (<0.0>,11443) tree.c:758: local_irq_restore(flags);
    (<0.0>,11446)
    (<0.0>,11448) fake_sched.h:43: return __running_cpu;
    (<0.0>,11452) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11454) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11458) fake_sched.h:43: return __running_cpu;
    (<0.0>,11462) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11468) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11471) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,11476) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11481) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11482) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11483) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11484) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11486) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11488) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11489) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11491) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11494) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11495) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11496) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,11503) fake_sched.h:43: return __running_cpu;
    (<0.0>,11507)
    (<0.0>,11508) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,11511) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,11516) tree.c:892: local_irq_save(flags);
    (<0.0>,11519)
    (<0.0>,11521) fake_sched.h:43: return __running_cpu;
    (<0.0>,11525) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11527) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11531) fake_sched.h:43: return __running_cpu;
    (<0.0>,11535) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11547)
    (<0.0>,11549) fake_sched.h:43: return __running_cpu;
    (<0.0>,11553) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11554) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,11556) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,11557) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,11558) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,11559) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,11560) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,11561) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,11562) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,11566) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,11568) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,11569) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,11570) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,11581)
    (<0.0>,11582)
    (<0.0>,11584) fake_sched.h:43: return __running_cpu;
    (<0.0>,11588) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11592) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11595) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11596) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11597) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11599) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11600) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,11602) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11603) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11604) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11605) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11615)
    (<0.0>,11617) tree.c:895: local_irq_restore(flags);
    (<0.0>,11620)
    (<0.0>,11622) fake_sched.h:43: return __running_cpu;
    (<0.0>,11626) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11628) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11632) fake_sched.h:43: return __running_cpu;
    (<0.0>,11636) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11643) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,11645) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,11646) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,11691)
    (<0.0>,11692) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11695)
    (<0.0>,11696) tree.c:625: return &rsp->node[0];
    (<0.0>,11700) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11702) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11703) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11704) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11709) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11710) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11711) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11712) fake_defs.h:237: switch (size) {
    (<0.0>,11714) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11716) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11717) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11719) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11722) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11723) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11724) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,11725) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,11728)
    (<0.0>,11729) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,11733)
    (<0.0>,11736) fake_sched.h:43: return __running_cpu;
    (<0.0>,11740) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,11744) fake_sched.h:43: return __running_cpu;
    (<0.0>,11748) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11753) fake_sched.h:43: return __running_cpu;
    (<0.0>,11757) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,11760) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,11761) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,11768) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11773) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11774) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11775) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11776) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11778) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11780) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11781) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11783) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11786) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11787) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11788) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,11799)
    (<0.0>,11805)
    (<0.0>,11810) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11815) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11816) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11817) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11818) fake_defs.h:237: switch (size) {
    (<0.0>,11820) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11822) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11823) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11825) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,11828) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11829) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11830) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,11831) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11844)
    (<0.0>,11845) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11850) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11851) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11852) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11855) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11857) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11858) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11860) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11863) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11864) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11865) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11866) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11871) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11872) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11873) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11878) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11879) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11881) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11885) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11886) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11894) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11895) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11908)
    (<0.0>,11909) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11914) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11915) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11916) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11917) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11919) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11921) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11922) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11924) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11927) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11928) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11929) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11930) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11935) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11936) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11937) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11938) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11940) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11943) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11945) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11948) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11949) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11950) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11957) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11958) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11959) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,11962) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,11977)
    (<0.0>,11978) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,11979) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,11980) tree.c:1240: rsp->gp_start = j;
    (<0.0>,11981) tree.c:1240: rsp->gp_start = j;
    (<0.0>,11983) tree.c:1240: rsp->gp_start = j;
    (<0.0>,12004) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12005) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12006) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12007) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12009) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12011) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12012) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12014) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12017) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12018) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12019) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12020) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12021) update.c:466: if (till_stall_check < 3) {
    (<0.0>,12024) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,12028) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,12033) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,12035) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12036) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12038) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12039) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12044) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12045) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12046) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12047) fake_defs.h:237: switch (size) {
    (<0.0>,12049) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12051) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12052) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12054) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12057) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12058) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12059) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12060) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12061) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12064) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12066) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12067) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12072) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12073) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12074) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12075) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12077) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12079) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12080) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12082) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12085) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12086) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12087) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12088) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12090) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12094) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12096) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12098) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12099) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12101) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12102) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12103) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12107) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,12110)
    (<0.0>,12111) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12115)
    (<0.0>,12116) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12117) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12122) fake_sched.h:43: return __running_cpu;
    (<0.0>,12126) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12128) fake_sched.h:43: return __running_cpu;
    (<0.0>,12132) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12139) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12142) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12145) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12146) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12148) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12149) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12151) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12156) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,12157) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,12161)
    (<0.0>,12162)
    (<0.0>,12163) tree.c:1922: if (delay > 0 &&
    (<0.0>,12167) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,12170)
    (<0.0>,12171) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12175)
    (<0.0>,12178) fake_sched.h:43: return __running_cpu;
    (<0.0>,12182) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12186) fake_sched.h:43: return __running_cpu;
    (<0.0>,12190) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12195) fake_sched.h:43: return __running_cpu;
    (<0.0>,12199) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12202) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12203) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12210) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12212) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12213) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12215) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12218) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,12220) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,12223) tree.c:1973: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,12226)
    (<0.0>,12227) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12231)
    (<0.0>,12232) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12233) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12238) fake_sched.h:43: return __running_cpu;
    (<0.0>,12242) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12244) fake_sched.h:43: return __running_cpu;
    (<0.0>,12248) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12256) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12258) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12260) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12261) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12263) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12268) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12271) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12273) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12274) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12276) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12281) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,12282) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,12286)
    (<0.0>,12287)
    (<0.0>,12288) tree.c:1922: if (delay > 0 &&
    (<0.0>,12292) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,12295)
    (<0.0>,12296) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12300)
    (<0.0>,12303) fake_sched.h:43: return __running_cpu;
    (<0.0>,12307) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12311) fake_sched.h:43: return __running_cpu;
    (<0.0>,12315) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12320) fake_sched.h:43: return __running_cpu;
    (<0.0>,12324) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12327) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12328) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12336) fake_sched.h:43: return __running_cpu;
    (<0.0>,12339) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12341) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12343) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12344) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,12350)
    (<0.0>,12351) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12353) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12358) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12359) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12361) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12365) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12366) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12367) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12369) tree.c:2028: rnp->qsmask = 0;
    (<0.0>,12371) tree.c:2028: rnp->qsmask = 0;
    (<0.0>,12373) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12375) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12376) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12377) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12382) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12383) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12384) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12385) fake_defs.h:237: switch (size) {
    (<0.0>,12387) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12389) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12390) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12392) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12395) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12396) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12397) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12398) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12400) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12401) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12403) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12408) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12409) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12411) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12412) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12414) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12418) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12419) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12420) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,12423) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,12424) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,12426) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,12429) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,12430) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,12431) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,12453)
    (<0.0>,12454)
    (<0.0>,12455)
    (<0.0>,12456) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,12458) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,12459) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,12461) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,12464) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12468) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12469) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12470) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12471) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12473) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12474) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12475) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12476) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12479) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12482) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12483) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12491) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,12492) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,12493) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,12502)
    (<0.0>,12503)
    (<0.0>,12504)
    (<0.0>,12505) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,12508) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,12511) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,12514) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,12515) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,12518) tree.c:1750: return false;
    (<0.0>,12520) tree.c:1799: }
    (<0.0>,12523) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,12525) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12527) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12528) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12530) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,12533) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,12535) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,12536) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,12538) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,12541) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,12543) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,12544) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,12546) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,12552) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,12553) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,12556) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,12560) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,12562) fake_sched.h:43: return __running_cpu;
    (<0.0>,12566) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,12567) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,12569) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,12570) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,12572) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,12575) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,12576) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,12579)
    (<0.0>,12580) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,12582) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,12583) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,12585) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,12595)
    (<0.0>,12600) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12604) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12605) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12606) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12607) fake_defs.h:237: switch (size) {
    (<0.0>,12609) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,12610) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,12611) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,12612) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,12615) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12618) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12619) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,12622) tree.c:1896: return ret;
    (<0.0>,12626) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,12629)
    (<0.0>,12633) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,12636)
    (<0.0>,12637) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12641)
    (<0.0>,12642) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12643) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12648) fake_sched.h:43: return __running_cpu;
    (<0.0>,12652) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12654) fake_sched.h:43: return __running_cpu;
    (<0.0>,12658) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12673) fake_sched.h:43: return __running_cpu;
    (<0.0>,12679) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,12687) fake_sched.h:43: return __running_cpu;
    (<0.0>,12691) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,12704) fake_sched.h:43: return __running_cpu;
    (<0.0>,12708)
    (<0.0>,12711) tree.c:755: local_irq_save(flags);
    (<0.0>,12714)
    (<0.0>,12716) fake_sched.h:43: return __running_cpu;
    (<0.0>,12720) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12722) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12726) fake_sched.h:43: return __running_cpu;
    (<0.0>,12730) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12742)
    (<0.0>,12744) fake_sched.h:43: return __running_cpu;
    (<0.0>,12748) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12749) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12751) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12752) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,12753) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12754) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12755) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12756) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12757) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,12761) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,12763) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,12764) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12765) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,12781)
    (<0.0>,12783)
    (<0.0>,12785) fake_sched.h:43: return __running_cpu;
    (<0.0>,12789) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12792) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12793) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12794) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12798) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12799) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12800) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12802) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12807) fake_sched.h:43: return __running_cpu;
    (<0.0>,12810) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12812) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12814) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12815) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12818)
    (<0.0>,12821) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12824) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12825) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12826) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12830) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12831) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12832) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12834) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12839) fake_sched.h:43: return __running_cpu;
    (<0.0>,12842) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12844) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12846) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12847) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12850)
    (<0.0>,12853) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12856) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12857) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12858) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12862) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12863) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12864) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12866) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12873) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12876) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12877) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12878) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12880) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12881) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12883) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12884) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12885) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12886) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12900)
    (<0.0>,12902) tree.c:758: local_irq_restore(flags);
    (<0.0>,12905)
    (<0.0>,12907) fake_sched.h:43: return __running_cpu;
    (<0.0>,12911) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12913) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12917) fake_sched.h:43: return __running_cpu;
    (<0.0>,12921) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12927) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12930) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12935) fake_sched.h:43: return __running_cpu;
    (<0.0>,12939)
    (<0.0>,12940) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12943) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12948) tree.c:892: local_irq_save(flags);
    (<0.0>,12951)
    (<0.0>,12953) fake_sched.h:43: return __running_cpu;
    (<0.0>,12957) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12959) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12963) fake_sched.h:43: return __running_cpu;
    (<0.0>,12967) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12979)
    (<0.0>,12981) fake_sched.h:43: return __running_cpu;
    (<0.0>,12985) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12986) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12988) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12989) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12990) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12991) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12992) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12993) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12994) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,12998) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,13000) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,13001) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,13002) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,13013)
    (<0.0>,13014)
    (<0.0>,13016) fake_sched.h:43: return __running_cpu;
    (<0.0>,13020) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13024) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13027) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13028) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13029) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13031) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13032) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13034) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13035) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13036) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13037) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13047)
    (<0.0>,13049) tree.c:895: local_irq_restore(flags);
    (<0.0>,13052)
    (<0.0>,13054) fake_sched.h:43: return __running_cpu;
    (<0.0>,13058) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13060) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13064) fake_sched.h:43: return __running_cpu;
    (<0.0>,13068) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13082) fake_sched.h:43: return __running_cpu;
    (<0.0>,13086) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,13095) fake_sched.h:43: return __running_cpu;
    (<0.0>,13102) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,13111) fake_sched.h:43: return __running_cpu;
    (<0.0>,13115) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,13117) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,13123) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13124) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13125) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13130) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13131) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13132) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13133) fake_defs.h:237: switch (size) {
    (<0.0>,13135) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13137) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13138) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13140) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13143) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13144) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13145) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13147) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13149) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13151) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13152) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13154) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13159) tree.c:2046: return true;
    (<0.0>,13161) tree.c:2047: }
    (<0.0>,13165) tree.c:2214: first_gp_fqs = true;
    (<0.0>,13166) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,13167) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,13168) tree.c:2216: if (j > HZ) {
    (<0.0>,13171) tree.c:2220: ret = 0;
    (<0.0>,13173) tree.c:2222: if (!ret) {
    (<0.0>,13176) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13177) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13179) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13181) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13183) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13184) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13187) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13188) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13193) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13194) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13195) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13196) fake_defs.h:237: switch (size) {
    (<0.0>,13198) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13200) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13201) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13203) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13206) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13207) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13208) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13212) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,13214) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,13218) fake_sched.h:43: return __running_cpu;
    (<0.0>,13222) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,13226) fake_sched.h:43: return __running_cpu;
    (<0.0>,13230) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13235) fake_sched.h:43: return __running_cpu;
    (<0.0>,13239) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,13250) fake_sched.h:43: return __running_cpu;
    (<0.0>,13254) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13255) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,13257) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,13258) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,13259) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,13261) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,13263) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,13264) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13265) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13266) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13267) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13268) tree.c:942: if (oldval)
    (<0.0>,13276)
    (<0.0>,13282)
    (<0.0>,13291) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13292) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13293) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13297) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13298) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13299) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13301) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13306) fake_sched.h:43: return __running_cpu;
    (<0.0>,13309) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13311) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13314) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13316) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13318) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13321) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13322) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13323) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13327) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13328) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13329) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13331) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13336) fake_sched.h:43: return __running_cpu;
    (<0.0>,13339) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13341) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13344) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13346) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13348) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13351) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13352) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13353) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13357) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13358) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13359) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13361) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13366) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,13371) fake_sched.h:43: return __running_cpu;
    (<0.0>,13376) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,13384) fake_sched.h:43: return __running_cpu;
    (<0.0>,13390) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,13404) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13405) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13406) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13410) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13411) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13412) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13414) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13418) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13420) fake_sched.h:43: return __running_cpu;
    (<0.0>,13423) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13425) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13447)
    (<0.0>,13448)
    (<0.0>,13449) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13451) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13452) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13453) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13455) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13457) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13458) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,13459) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,13494)
    (<0.0>,13495)
    (<0.0>,13496) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,13499) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,13512)
    (<0.0>,13513) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13518) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13519) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13520) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13521) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13523) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13525) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13526) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13528) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13531) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13532) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13533) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13534) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13539) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13540) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13541) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13542) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13544) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13546) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13547) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13549) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13552) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13553) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13554) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13560) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,13585)
    (<0.0>,13586) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,13591) tree.c:1472: j = jiffies;
    (<0.0>,13592) tree.c:1472: j = jiffies;
    (<0.0>,13593) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13598) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13599) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13600) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13601) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13603) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13605) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13606) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13608) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13611) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13612) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13613) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13614) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,13616) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13621) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13622) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13623) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13624) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13626) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13628) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13629) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13631) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13634) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13635) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13636) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13637) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,13639) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13644) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13645) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13646) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13647) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13649) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13651) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13652) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13654) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13657) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13658) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13659) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13660) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,13662) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13667) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13668) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13669) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13670) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13672) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13674) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13675) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13677) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13680) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13681) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13682) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13683) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,13684) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,13685) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,13689) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,13690) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,13696) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,13699)
    (<0.0>,13702) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,13705) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,13707) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,13710) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,13712) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,13716) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,13719)
    (<0.0>,13720) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,13722) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,13725) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,13732) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,13733) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,13744)
    (<0.0>,13745)
    (<0.0>,13746) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,13759)
    (<0.0>,13760) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13765) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13766) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13767) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13770) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13772) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13773) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13775) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13778) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13779) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13780) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13781) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13786) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13787) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13788) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13789) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13791) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13793) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13794) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13796) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13799) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13800) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13801) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,13807) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,13809) tree.c:666: }
    (<0.0>,13812) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13817) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13818) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13819) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13820) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13822) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13824) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13825) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13830) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13831) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13832) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13833) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13835) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,13838) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13843) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13844) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13845) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13846) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13848) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13850) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13856) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13857) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13858) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13859) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13861) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,13864) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13868) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13869) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13870) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13871) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13873) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13875) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13879) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13882) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13883) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,13891) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,13894)
    (<0.0>,13898) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,13900) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,13902) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,13903) tree.c:3547: return 0;
    (<0.0>,13905) tree.c:3548: }
    (<0.0>,13910) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13913) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13914) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13915) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13919) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13920) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13921) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13923) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,13927) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13929) fake_sched.h:43: return __running_cpu;
    (<0.0>,13932) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13934) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,13956)
    (<0.0>,13957)
    (<0.0>,13958) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13960) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13961) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,13962) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13964) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13966) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,13967) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,13968) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,14003)
    (<0.0>,14004)
    (<0.0>,14005) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,14008) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,14021)
    (<0.0>,14022) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14027) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14028) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14029) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14030) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14032) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14034) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14035) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14037) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14040) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14041) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14042) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14043) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14048) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14049) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14050) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14053) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14055) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14056) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14058) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14061) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14062) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14063) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14071) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,14074)
    (<0.0>,14077) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,14080) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14082) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14085) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14087) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14091) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,14094)
    (<0.0>,14095) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,14097) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,14100) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,14107) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,14108) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,14119)
    (<0.0>,14120)
    (<0.0>,14121) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,14134)
    (<0.0>,14135) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14140) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14141) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14142) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14143) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14145) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14147) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14148) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14150) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14153) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14154) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14155) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14156) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14161) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14162) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14163) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14164) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14166) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14168) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14169) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14171) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14174) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14175) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14176) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14182) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,14198)
    (<0.0>,14199) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,14202)
    (<0.0>,14203) tree.c:625: return &rsp->node[0];
    (<0.0>,14207) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,14208) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14213) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14214) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14215) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14216) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14218) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14220) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14221) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14223) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14226) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14227) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14228) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14232) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,14233) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,14235) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,14238) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,14239) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14243) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14244) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14245) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14246) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14248) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14250) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14251) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14253) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14256) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14257) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14258) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,14262) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,14265) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,14268) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,14271) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,14272) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,14275) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14277) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14280) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14283) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14286) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14287) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14289) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14292) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14296) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14298) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14300) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14303) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14306) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14309) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14310) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14312) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14315) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,14319) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14321) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14323) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,14326) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,14328) tree.c:666: }
    (<0.0>,14331) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14336) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14337) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14338) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14339) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14341) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14343) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14344) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14346) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14349) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14350) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14351) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14352) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14354) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,14357) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14362) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14363) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14364) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14365) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14367) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14369) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14370) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14372) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14375) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14376) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14377) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14378) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14380) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,14383) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14387) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14388) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14389) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14390) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14392) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14393) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14394) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14395) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14398) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14401) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14402) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,14410) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,14413)
    (<0.0>,14417) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,14419) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,14421) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,14422) tree.c:3547: return 0;
    (<0.0>,14424) tree.c:3548: }
    (<0.0>,14429) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14432) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14433) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14434) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14438) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14439) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14440) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14442) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14446) tree.c:3562: return 0;
    (<0.0>,14448) tree.c:3563: }
    (<0.0>,14452) tree.c:2891: if (user)
    (<0.0>,14460) fake_sched.h:43: return __running_cpu;
    (<0.0>,14464) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,14466) fake_sched.h:43: return __running_cpu;
    (<0.0>,14470) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14476) fake_sched.h:43: return __running_cpu;
    (<0.0>,14480) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,14491) fake_sched.h:43: return __running_cpu;
    (<0.0>,14495) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14496) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,14498) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,14499) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,14500) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,14502) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,14504) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,14505) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14506) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14507) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14508) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14509) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,14511) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,14519)
    (<0.0>,14525) fake_sched.h:43: return __running_cpu;
    (<0.0>,14529)
    (<0.0>,14532) tree.c:755: local_irq_save(flags);
    (<0.0>,14535)
    (<0.0>,14537) fake_sched.h:43: return __running_cpu;
    (<0.0>,14541) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14543) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14547) fake_sched.h:43: return __running_cpu;
    (<0.0>,14551) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14563)
    (<0.0>,14565) fake_sched.h:43: return __running_cpu;
    (<0.0>,14569) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14570) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,14572) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,14573) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,14574) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14575) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14576) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14577) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14578) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,14582) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,14584) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,14585) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,14586) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,14602)
    (<0.0>,14604)
    (<0.0>,14606) fake_sched.h:43: return __running_cpu;
    (<0.0>,14610) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14613) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14614) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14615) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14619) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14620) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14621) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14623) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14628) fake_sched.h:43: return __running_cpu;
    (<0.0>,14631) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14633) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14635) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14636) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,14639)
    (<0.0>,14642) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14645) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14646) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14647) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14651) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14652) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14653) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14655) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14660) fake_sched.h:43: return __running_cpu;
    (<0.0>,14663) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14665) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14667) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,14668) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,14671)
    (<0.0>,14674) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14677) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14678) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14679) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14683) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14684) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14685) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14687) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,14694) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14697) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14698) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14699) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14701) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14702) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,14704) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14705) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14706) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14707) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14721)
    (<0.0>,14723) tree.c:758: local_irq_restore(flags);
    (<0.0>,14726)
    (<0.0>,14728) fake_sched.h:43: return __running_cpu;
    (<0.0>,14732) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14734) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14738) fake_sched.h:43: return __running_cpu;
    (<0.0>,14742) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14748) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,14751) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,14756) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,14772)
    (<0.0>,14773)
    (<0.0>,14774) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,14777)
    (<0.0>,14778) tree.c:625: return &rsp->node[0];
    (<0.0>,14782) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,14783) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14788) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14789) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14790) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14791) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14793) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14795) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14796) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14798) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14801) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14802) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14803) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14805) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14806) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,14807) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,14808) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,14812) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14817) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14818) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14819) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14820) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14822) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14824) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14825) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14830) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14831) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14832) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14835) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,14838)
    (<0.0>,14842) tree.c:2064: return true;
    (<0.0>,14844) tree.c:2067: }
    (<0.0>,14849) fake_sched.h:43: return __running_cpu;
    (<0.0>,14853)
    (<0.0>,14854) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,14857) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,14862) tree.c:892: local_irq_save(flags);
    (<0.0>,14865)
    (<0.0>,14867) fake_sched.h:43: return __running_cpu;
    (<0.0>,14871) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14873) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14877) fake_sched.h:43: return __running_cpu;
    (<0.0>,14881) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14893)
    (<0.0>,14895) fake_sched.h:43: return __running_cpu;
    (<0.0>,14899) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14900) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14902) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14903) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,14904) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14905) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14906) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14907) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,14908) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,14912) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,14914) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,14915) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,14916) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,14927)
    (<0.0>,14928)
    (<0.0>,14930) fake_sched.h:43: return __running_cpu;
    (<0.0>,14934) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,14938) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14941) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14942) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14943) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14945) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14946) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,14948) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14949) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14950) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14951) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,14961)
    (<0.0>,14963) tree.c:895: local_irq_restore(flags);
    (<0.0>,14966)
    (<0.0>,14968) fake_sched.h:43: return __running_cpu;
    (<0.0>,14972) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14974) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14978) fake_sched.h:43: return __running_cpu;
    (<0.0>,14982) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14989) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,14990) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,14991) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,14992) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,14994) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,14995) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15000) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15001) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15002) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15003) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15005) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15007) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15008) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15010) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15013) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15014) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15015) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,15018) tree.c:2237: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15021)
    (<0.0>,15026) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,15028) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,15029) tree.c:2280: rcu_gp_cleanup(rsp);
    (<0.0>,15069)
    (<0.0>,15070) tree.c:2109: bool needgp = false;
    (<0.0>,15071) tree.c:2110: int nocb = 0;
    (<0.0>,15072) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15075)
    (<0.0>,15076) tree.c:625: return &rsp->node[0];
    (<0.0>,15080) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15082) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15083) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15084) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15089) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15090) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15091) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15092) fake_defs.h:237: switch (size) {
    (<0.0>,15094) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15096) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15097) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15099) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15102) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15103) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15104) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,15105) tree.c:2116: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,15108)
    (<0.0>,15109) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,15113)
    (<0.0>,15116) fake_sched.h:43: return __running_cpu;
    (<0.0>,15120) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,15124) fake_sched.h:43: return __running_cpu;
    (<0.0>,15128) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15133) fake_sched.h:43: return __running_cpu;
    (<0.0>,15137) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,15140) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,15141) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,15148) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,15149) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,15151) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,15153) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,15154) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,15155) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,15157) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,15160) tree.c:2129: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,15163)
    (<0.0>,15164) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,15168)
    (<0.0>,15169) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,15170) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,15175) fake_sched.h:43: return __running_cpu;
    (<0.0>,15179) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,15181) fake_sched.h:43: return __running_cpu;
    (<0.0>,15185) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15192) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,15195) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,15197) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,15198) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,15200) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,15205) tree.c:2141: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,15208)
    (<0.0>,15209) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,15213)
    (<0.0>,15216) fake_sched.h:43: return __running_cpu;
    (<0.0>,15220) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,15224) fake_sched.h:43: return __running_cpu;
    (<0.0>,15228) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15233) fake_sched.h:43: return __running_cpu;
    (<0.0>,15237) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,15240) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,15241) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,15248) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15251)
    (<0.0>,15257) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15258) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15261)
    (<0.0>,15266) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15267) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15268) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,15269) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15271) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15276) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15277) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15279) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15283) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15284) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15285) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,15287) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15289) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15290) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15291) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15296) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15297) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15298) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15299) fake_defs.h:237: switch (size) {
    (<0.0>,15301) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15303) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15304) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15306) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,15309) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15310) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15311) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,15313) fake_sched.h:43: return __running_cpu;
    (<0.0>,15316) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15318) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15320) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15321) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,15322) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,15324) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,15327) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,15328) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,15329) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,15351)
    (<0.0>,15352)
    (<0.0>,15353)
    (<0.0>,15354) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,15356) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,15357) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,15359) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,15362) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,15363) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,15364) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,15372)
    (<0.0>,15373)
    (<0.0>,15374)
    (<0.0>,15375) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15378) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15381) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15384) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15385) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,15388) tree.c:1818: return false;
    (<0.0>,15390) tree.c:1843: }
    (<0.0>,15393) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,15394) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,15396) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,15397) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,15399) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,15403) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15405) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15406) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15408) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15411) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15415) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15416) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15417) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15418) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15420) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15421) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15422) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15423) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15426) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15429) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15430) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,15438) tree.c:1896: return ret;
    (<0.0>,15442) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,15446) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,15448) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,15449) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,15456)
    (<0.0>,15457)
    (<0.0>,15458) tree.c:1702: int c = rnp->completed;
    (<0.0>,15460) tree.c:1702: int c = rnp->completed;
    (<0.0>,15462) tree.c:1702: int c = rnp->completed;
    (<0.0>,15464) fake_sched.h:43: return __running_cpu;
    (<0.0>,15467) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15469) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15471) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15472) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,15475) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,15478) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,15479) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,15483) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,15486) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,15487) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,15488) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,15489) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,15490) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,15492) tree.c:1709: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,15500)
    (<0.0>,15501)
    (<0.0>,15502)
    (<0.0>,15503)
    (<0.0>,15507) tree.c:1710: return needmore;
    (<0.0>,15509) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,15511) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,15512) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,15515)
    (<0.0>,15517) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,15518) tree.c:2151: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,15521)
    (<0.0>,15522) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,15526)
    (<0.0>,15527) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,15528) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,15533) fake_sched.h:43: return __running_cpu;
    (<0.0>,15537) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,15539) fake_sched.h:43: return __running_cpu;
    (<0.0>,15543) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15550) tree.c:2152: rcu_nocb_gp_cleanup(sq);
    (<0.0>,15553)
    (<0.0>,15563) fake_sched.h:43: return __running_cpu;
    (<0.0>,15569) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,15577) fake_sched.h:43: return __running_cpu;
    (<0.0>,15581) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,15594) fake_sched.h:43: return __running_cpu;
    (<0.0>,15598)
    (<0.0>,15601) tree.c:755: local_irq_save(flags);
    (<0.0>,15604)
    (<0.0>,15606) fake_sched.h:43: return __running_cpu;
    (<0.0>,15610) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15612) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15616) fake_sched.h:43: return __running_cpu;
    (<0.0>,15620) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15632)
    (<0.0>,15634) fake_sched.h:43: return __running_cpu;
    (<0.0>,15638) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15639) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15641) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15642) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15643) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15644) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15645) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15646) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15647) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,15651) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,15653) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,15654) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,15655) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,15671)
    (<0.0>,15673)
    (<0.0>,15675) fake_sched.h:43: return __running_cpu;
    (<0.0>,15679) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15682) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15683) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15684) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15688) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15689) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15690) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15692) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15697) fake_sched.h:43: return __running_cpu;
    (<0.0>,15700) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15702) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15704) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15705) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15708)
    (<0.0>,15711) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15714) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15715) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15716) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15720) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15721) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15722) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15724) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15729) fake_sched.h:43: return __running_cpu;
    (<0.0>,15732) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15734) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15736) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15737) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15740)
    (<0.0>,15743) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15746) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15747) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15748) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15752) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15753) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15754) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15756) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15763) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15766) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15767) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15768) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15770) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15771) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15773) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15774) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15775) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15776) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15790)
    (<0.0>,15792) tree.c:758: local_irq_restore(flags);
    (<0.0>,15795)
    (<0.0>,15797) fake_sched.h:43: return __running_cpu;
    (<0.0>,15801) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15803) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15807) fake_sched.h:43: return __running_cpu;
    (<0.0>,15811) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15817) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,15820) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,15825) fake_sched.h:43: return __running_cpu;
    (<0.0>,15829)
    (<0.0>,15830) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,15833) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,15838) tree.c:892: local_irq_save(flags);
    (<0.0>,15841)
    (<0.0>,15843) fake_sched.h:43: return __running_cpu;
    (<0.0>,15847) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15849) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15853) fake_sched.h:43: return __running_cpu;
    (<0.0>,15857) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15869)
    (<0.0>,15871) fake_sched.h:43: return __running_cpu;
    (<0.0>,15875) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15876) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,15878) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,15879) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,15880) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,15881) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,15882) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,15883) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,15884) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,15888) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,15890) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,15891) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,15892) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,15903)
    (<0.0>,15904)
    (<0.0>,15906) fake_sched.h:43: return __running_cpu;
    (<0.0>,15910) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15914) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15917) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15918) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15919) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15921) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15922) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,15924) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15925) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15926) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15927) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15937)
    (<0.0>,15939) tree.c:895: local_irq_restore(flags);
    (<0.0>,15942)
    (<0.0>,15944) fake_sched.h:43: return __running_cpu;
    (<0.0>,15948) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15950) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15954) fake_sched.h:43: return __running_cpu;
    (<0.0>,15958) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15972) fake_sched.h:43: return __running_cpu;
    (<0.0>,15976) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,15985) fake_sched.h:43: return __running_cpu;
    (<0.0>,15992) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,16001) fake_sched.h:43: return __running_cpu;
    (<0.0>,16005) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,16007) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,16013) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16014) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16015) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16020) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16021) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16022) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16023) fake_defs.h:237: switch (size) {
    (<0.0>,16025) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16027) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16028) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16030) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16033) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16034) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16035) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,16036) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,16037) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,16041)
    (<0.0>,16042)
    (<0.0>,16043) tree.c:1922: if (delay > 0 &&
    (<0.0>,16048) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,16050) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,16052) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,16053) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,16055) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,16060) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,16063)
    (<0.0>,16064) tree.c:625: return &rsp->node[0];
    (<0.0>,16068) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,16069) tree.c:2158: raw_spin_lock_irq_rcu_node(rnp); /* Order GP before ->completed update. */
    (<0.0>,16072)
    (<0.0>,16073) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,16077)
    (<0.0>,16080) fake_sched.h:43: return __running_cpu;
    (<0.0>,16084) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,16088) fake_sched.h:43: return __running_cpu;
    (<0.0>,16092) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,16097) fake_sched.h:43: return __running_cpu;
    (<0.0>,16101) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,16104) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,16105) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,16112) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,16113) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,16117)
    (<0.0>,16118)
    (<0.0>,16121) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16123) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16124) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16125) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16130) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16131) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16132) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16133) fake_defs.h:237: switch (size) {
    (<0.0>,16135) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16137) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16138) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16140) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,16143) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16144) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16145) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,16148) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,16150) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,16152) fake_sched.h:43: return __running_cpu;
    (<0.0>,16155) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16157) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16159) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,16160) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,16161) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,16162) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,16170)
    (<0.0>,16171)
    (<0.0>,16172)
    (<0.0>,16173) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,16176) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,16179) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,16182) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,16183) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,16186) tree.c:1818: return false;
    (<0.0>,16188) tree.c:1843: }
    (<0.0>,16191) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,16195) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,16196) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,16199) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,16200) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,16211)
    (<0.0>,16212)
    (<0.0>,16213) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,16226)
    (<0.0>,16227) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16232) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16233) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16234) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16235) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16237) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16239) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16240) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16242) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16245) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16246) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16247) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16248) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16253) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16254) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16255) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16256) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16258) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16260) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16261) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16263) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16266) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16267) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16268) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16274) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,16290)
    (<0.0>,16291) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16294)
    (<0.0>,16295) tree.c:625: return &rsp->node[0];
    (<0.0>,16299) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16300) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16305) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16306) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16307) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16308) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16310) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16312) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16313) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16315) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16318) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16319) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16320) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16324) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16325) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16327) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16330) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16331) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16335) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16336) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16337) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16338) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16340) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16342) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16343) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16345) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16348) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16349) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16350) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16354) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,16357) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,16360) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,16363) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,16364) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,16367) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16369) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16372) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16375) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16378) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16379) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16381) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16384) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16388) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16390) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16392) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16395) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16398) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16401) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16402) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16404) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16407) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,16411) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16413) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16415) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,16418) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,16420) tree.c:666: }
    (<0.0>,16423) tree.c:2174: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,16426)
    (<0.0>,16427) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,16431)
    (<0.0>,16432) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,16433) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,16438) fake_sched.h:43: return __running_cpu;
    (<0.0>,16442) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,16444) fake_sched.h:43: return __running_cpu;
    (<0.0>,16448) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,16456) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,16458) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,16463) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,16465) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,16469) fake_sched.h:43: return __running_cpu;
    (<0.0>,16473) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,16477) fake_sched.h:43: return __running_cpu;
    (<0.0>,16481) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,16486) fake_sched.h:43: return __running_cpu;
    (<0.0>,16490) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,16501) fake_sched.h:43: return __running_cpu;
    (<0.0>,16505) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,16506) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,16508) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,16509) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,16510) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,16512) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,16514) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,16515) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16516) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16517) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16518) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,16519) tree.c:942: if (oldval)
    (<0.0>,16527)
    (<0.0>,16533)
    (<0.0>,16542) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16543) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16544) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16548) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16549) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16550) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16552) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16557) fake_sched.h:43: return __running_cpu;
    (<0.0>,16560) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16562) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16565) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16567) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16569) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16572) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16573) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16574) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16578) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16579) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16580) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16582) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16587) fake_sched.h:43: return __running_cpu;
    (<0.0>,16590) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16592) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16595) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16597) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,16599) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16602) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16603) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16604) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16608) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16609) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16610) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16612) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,16617) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,16622) fake_sched.h:43: return __running_cpu;
    (<0.0>,16627) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,16635) fake_sched.h:43: return __running_cpu;
    (<0.0>,16641) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,16655) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16656) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16657) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16661) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16662) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16663) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16665) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,16669) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,16671) fake_sched.h:43: return __running_cpu;
    (<0.0>,16674) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,16676) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,16698)
    (<0.0>,16699)
    (<0.0>,16700) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,16702) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,16703) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,16704) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,16706) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,16708) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,16709) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,16710) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,16745)
    (<0.0>,16746)
    (<0.0>,16747) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,16750) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,16763)
    (<0.0>,16764) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16769) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16770) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16771) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16772) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16774) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16776) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16777) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16779) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16782) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16783) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16784) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16785) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16790) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16791) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16792) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16793) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16795) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16797) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16798) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16800) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16803) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16804) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16805) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16813) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,16816)
    (<0.0>,16819) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,16822) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,16824) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,16827) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,16829) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,16833) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,16836)
    (<0.0>,16837) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16839) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16842) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,16849) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,16850) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,16861)
    (<0.0>,16862)
    (<0.0>,16863) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,16876)
    (<0.0>,16877) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16882) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16883) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16885) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16887) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16889) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16890) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16892) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16895) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16896) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16897) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16898) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16903) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16904) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16905) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16906) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16908) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16910) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16911) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16913) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16916) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16917) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16918) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,16924) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,16940)
    (<0.0>,16941) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16944)
    (<0.0>,16945) tree.c:625: return &rsp->node[0];
    (<0.0>,16949) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16950) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16955) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16956) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16957) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16960) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16962) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16963) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16965) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16968) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16969) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16970) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16974) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,16975) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16977) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16980) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,16981) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16985) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16986) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16987) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16988) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16990) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16992) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16993) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16995) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16998) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,16999) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17000) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17004) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,17007) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,17010) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17013) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17014) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17017) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17019) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17022) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17025) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17028) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17029) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17031) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17034) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17038) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17040) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17042) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17045) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17048) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17051) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17052) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17054) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17057) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17061) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17063) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17065) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17068) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,17070) tree.c:666: }
    (<0.0>,17073) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17078) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17079) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17080) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17081) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17083) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17085) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17086) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17088) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17091) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17092) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17093) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17094) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17096) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17099) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17104) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17105) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17106) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17107) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17109) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17111) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17112) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17114) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17117) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17118) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17119) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17120) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17122) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17125) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17129) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17130) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17131) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17132) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17134) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17135) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17136) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17137) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17140) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17143) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17144) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17152) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,17155)
    (<0.0>,17159) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17161) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17163) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17164) tree.c:3547: return 0;
    (<0.0>,17166) tree.c:3548: }
    (<0.0>,17171) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17174) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17175) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17176) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17180) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17181) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17182) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17184) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17188) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,17190) fake_sched.h:43: return __running_cpu;
    (<0.0>,17193) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,17195) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,17217)
    (<0.0>,17218)
    (<0.0>,17219) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,17221) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,17222) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,17223) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,17225) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,17227) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,17228) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,17229) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,17264)
    (<0.0>,17265)
    (<0.0>,17266) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,17269) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,17282)
    (<0.0>,17283) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17288) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17289) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17290) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17291) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17293) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17295) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17296) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17298) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17301) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17302) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17303) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17304) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17309) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17310) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17311) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17312) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17314) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17316) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17317) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17319) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17322) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17323) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17324) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17332) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,17335)
    (<0.0>,17338) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,17341) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,17343) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,17346) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,17348) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,17352) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,17355)
    (<0.0>,17356) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,17358) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,17361) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,17368) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,17369) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,17380)
    (<0.0>,17381)
    (<0.0>,17382) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,17395)
    (<0.0>,17396) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17401) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17402) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17403) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17408) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17409) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17411) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17414) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17415) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17416) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17417) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17422) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17423) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17424) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17425) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17427) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17429) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17430) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17432) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17435) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17436) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17437) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,17443) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,17459)
    (<0.0>,17460) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,17463)
    (<0.0>,17464) tree.c:625: return &rsp->node[0];
    (<0.0>,17468) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,17469) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17474) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17475) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17476) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17477) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17479) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17481) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17482) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17484) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17487) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17488) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17489) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17493) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,17494) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,17496) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,17499) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,17500) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17504) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17505) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17506) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17507) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17509) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17511) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17512) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17514) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17517) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17518) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17519) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,17523) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,17526) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,17529) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17532) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17533) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,17536) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17538) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17541) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17544) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17547) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17548) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17550) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17553) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17557) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17559) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17561) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17564) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17567) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17570) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17571) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17573) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17576) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,17580) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17582) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17584) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,17587) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,17589) tree.c:666: }
    (<0.0>,17592) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17597) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17598) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17599) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17600) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17602) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17604) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17605) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17607) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17610) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17611) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17612) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17613) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17615) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
    (<0.0>,17618) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17623) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17624) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17625) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17626) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17628) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17630) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17631) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17633) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17636) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17637) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17638) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17639) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17641) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
    (<0.0>,17644) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17648) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17649) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17650) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17651) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17653) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17654) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17655) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17656) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,17659) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17662) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17663) tree.c:3534: unlikely(READ_ONCE(rdp->gpwrap))) { /* outside lock */
    (<0.0>,17671) tree.c:3540: if (rcu_nocb_need_deferred_wakeup(rdp)) {
    (<0.0>,17674)
    (<0.0>,17678) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17680) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17682) tree.c:3546: rdp->n_rp_need_nothing++;
    (<0.0>,17683) tree.c:3547: return 0;
    (<0.0>,17685) tree.c:3548: }
    (<0.0>,17690) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17693) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17694) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17695) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17699) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17700) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17701) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17703) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,17707) tree.c:3562: return 0;
    (<0.0>,17709) tree.c:3563: }
    (<0.0>,17713) tree.c:2891: if (user)
    (<0.0>,17721) fake_sched.h:43: return __running_cpu;
    (<0.0>,17725) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,17727) fake_sched.h:43: return __running_cpu;
    (<0.0>,17731) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,17737) fake_sched.h:43: return __running_cpu;
    (<0.0>,17741) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,17752) fake_sched.h:43: return __running_cpu;
    (<0.0>,17756) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,17757) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,17759) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,17760) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,17761) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,17763) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,17765) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,17766) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17767) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17768) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17769) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17770) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,17772) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,17780)
    (<0.0>,17786) fake_sched.h:43: return __running_cpu;
    (<0.0>,17790)
    (<0.0>,17793) tree.c:755: local_irq_save(flags);
    (<0.0>,17796)
    (<0.0>,17798) fake_sched.h:43: return __running_cpu;
    (<0.0>,17802) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,17804) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,17808) fake_sched.h:43: return __running_cpu;
    (<0.0>,17812) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,17824)
    (<0.0>,17826) fake_sched.h:43: return __running_cpu;
    (<0.0>,17830) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,17831) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,17833) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,17834) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,17835) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17836) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17837) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17838) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17839) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,17843) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,17845) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,17846) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,17847) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,17863)
    (<0.0>,17865)
    (<0.0>,17867) fake_sched.h:43: return __running_cpu;
    (<0.0>,17871) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,17874) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17875) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17876) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17880) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17881) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17882) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17884) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17889) fake_sched.h:43: return __running_cpu;
    (<0.0>,17892) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17894) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17896) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17897) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,17900)
    (<0.0>,17903) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17906) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17907) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17908) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17912) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17913) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17914) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17916) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17921) fake_sched.h:43: return __running_cpu;
    (<0.0>,17924) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17926) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17928) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,17929) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,17932)
    (<0.0>,17935) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17938) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17939) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17940) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17944) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17945) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17946) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17948) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,17955) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17958) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17959) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17960) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17962) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17963) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,17965) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17966) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17967) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17968) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,17982)
    (<0.0>,17984) tree.c:758: local_irq_restore(flags);
    (<0.0>,17987)
    (<0.0>,17989) fake_sched.h:43: return __running_cpu;
    (<0.0>,17993) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,17995) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,17999) fake_sched.h:43: return __running_cpu;
    (<0.0>,18003) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,18009) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,18012) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,18017) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18022) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18023) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18024) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18025) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18027) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18029) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18030) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18032) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18035) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18036) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18037) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18044) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18049) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18050) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18051) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18052) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18054) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18056) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18057) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18059) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18062) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18063) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18064) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18071) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18076) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18077) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18078) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18079) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18081) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18083) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18084) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18086) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18089) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18090) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18091) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18098) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18103) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18104) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18105) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18106) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18108) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18110) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18111) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18113) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18116) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18117) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18118) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18125) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18130) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18131) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18132) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18133) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18135) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18137) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18138) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18140) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,18143) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18144) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,18145) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
      (<0.1>,764) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,767) fake_sync.h:281: while (!x->done)
      (<0.1>,769) fake_sync.h:281: while (!x->done)
      (<0.1>,776) fake_sched.h:43: return __running_cpu;
      (<0.1>,780)
      (<0.1>,781) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,784) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,789) tree.c:892: local_irq_save(flags);
      (<0.1>,792)
      (<0.1>,794) fake_sched.h:43: return __running_cpu;
      (<0.1>,798) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,804) fake_sched.h:43: return __running_cpu;
      (<0.1>,808) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,820)
      (<0.1>,822) fake_sched.h:43: return __running_cpu;
      (<0.1>,826) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,827) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,829) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,830) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,831) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,832) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,833) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,834) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,835) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,839) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,841) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,842) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,843) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,854)
      (<0.1>,855)
      (<0.1>,857) fake_sched.h:43: return __running_cpu;
      (<0.1>,861) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,865) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,868) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,869) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,870) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,872) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,873) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,875) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,876) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,877) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,878) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,888)
      (<0.1>,890) tree.c:895: local_irq_restore(flags);
      (<0.1>,893)
      (<0.1>,895) fake_sched.h:43: return __running_cpu;
      (<0.1>,899) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,905) fake_sched.h:43: return __running_cpu;
      (<0.1>,909) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,917) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,919) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,924)
      (<0.1>,927) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,929) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,931) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,932) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,940) litmus.c:91: y = 1;
  (<0>,8177) litmus.c:69: r_y = y;
  (<0>,8178) litmus.c:69: r_y = y;
  (<0>,8194) fake_sched.h:43: return __running_cpu;
  (<0>,8200) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
  (<0>,8206) fake_sched.h:43: return __running_cpu;
  (<0>,8213) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
  (<0>,8215) fake_sched.h:43: return __running_cpu;
  (<0>,8222) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
  (<0>,8230) fake_sched.h:43: return __running_cpu;
  (<0>,8234) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
  (<0>,8247) fake_sched.h:43: return __running_cpu;
  (<0>,8251)
  (<0>,8254) tree.c:755: local_irq_save(flags);
  (<0>,8257)
  (<0>,8259) fake_sched.h:43: return __running_cpu;
  (<0>,8263) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8265) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8269) fake_sched.h:43: return __running_cpu;
  (<0>,8273) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8285)
  (<0>,8287) fake_sched.h:43: return __running_cpu;
  (<0>,8291) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8292) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8294) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8295) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8296) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8297) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8298) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8299) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8300) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,8304) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,8306) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,8307) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,8308) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,8324)
  (<0>,8326)
  (<0>,8328) fake_sched.h:43: return __running_cpu;
  (<0>,8332) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8335) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8336) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8337) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8341) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8342) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8343) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8345) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8350) fake_sched.h:43: return __running_cpu;
  (<0>,8353) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8355) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8357) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8358) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,8361)
  (<0>,8364) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8367) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8368) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8369) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8373) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8374) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8375) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8377) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8382) fake_sched.h:43: return __running_cpu;
  (<0>,8385) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8387) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8389) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8390) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,8393)
  (<0>,8396) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8399) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8400) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8401) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8405) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8406) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8407) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8409) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8416) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8419) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8420) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8421) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8423) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8424) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8426) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8427) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8428) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8429) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8443)
  (<0>,8445) tree.c:758: local_irq_restore(flags);
  (<0>,8448)
  (<0>,8450) fake_sched.h:43: return __running_cpu;
  (<0>,8454) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8456) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8460) fake_sched.h:43: return __running_cpu;
  (<0>,8464) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8470) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8473) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8478) fake_sched.h:43: return __running_cpu;
  (<0>,8482)
  (<0>,8483) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,8486) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,8491) tree.c:892: local_irq_save(flags);
  (<0>,8494)
  (<0>,8496) fake_sched.h:43: return __running_cpu;
  (<0>,8500) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8502) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8506) fake_sched.h:43: return __running_cpu;
  (<0>,8510) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8522)
  (<0>,8524) fake_sched.h:43: return __running_cpu;
  (<0>,8528) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8529) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,8531) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,8532) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,8533) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8534) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8535) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8536) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,8537) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,8541) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,8543) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,8544) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,8545) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,8556)
  (<0>,8557)
  (<0>,8559) fake_sched.h:43: return __running_cpu;
  (<0>,8563) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8567) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8570) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8571) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8572) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8574) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8575) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,8577) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8578) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8579) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8580) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8590)
  (<0>,8592) tree.c:895: local_irq_restore(flags);
  (<0>,8595)
  (<0>,8597) fake_sched.h:43: return __running_cpu;
  (<0>,8601) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8603) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8607) fake_sched.h:43: return __running_cpu;
  (<0>,8611) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8622) fake_sched.h:43: return __running_cpu;
  (<0>,8626) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
  (<0>,8630) fake_sched.h:43: return __running_cpu;
  (<0>,8634) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8639) fake_sched.h:43: return __running_cpu;
  (<0>,8643) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
  (<0>,8654) fake_sched.h:43: return __running_cpu;
  (<0>,8658) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8659) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,8661) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,8662) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,8663) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,8665) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,8667) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,8668) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8669) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8670) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8671) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8672) tree.c:942: if (oldval)
  (<0>,8680)
  (<0>,8686)
  (<0>,8695) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8696) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8697) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8701) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8702) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8703) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8705) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8710) fake_sched.h:43: return __running_cpu;
  (<0>,8713) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8715) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8718) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8720) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8722) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8725) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8726) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8727) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8731) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8732) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8733) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8735) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8740) fake_sched.h:43: return __running_cpu;
  (<0>,8743) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8745) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8748) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8750) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,8752) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8755) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8756) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8757) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8761) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8762) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8763) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8765) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,8770) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,8775) fake_sched.h:43: return __running_cpu;
  (<0>,8780) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,8788) fake_sched.h:43: return __running_cpu;
  (<0>,8794) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
  (<0>,8808) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8809) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8810) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8814) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8815) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8816) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8818) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,8822) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8824) fake_sched.h:43: return __running_cpu;
  (<0>,8827) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8829) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,8851)
  (<0>,8852)
  (<0>,8853) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8855) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8856) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,8857) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8859) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8861) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,8862) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,8863) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,8898)
  (<0>,8899)
  (<0>,8900) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
  (<0>,8903) tree.c:1469: !rcu_gp_in_progress(rsp))
  (<0>,8916)
  (<0>,8917) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8922) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8923) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8924) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8925) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8927) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8929) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8930) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8932) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8935) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8936) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8937) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8938) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8943) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8944) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8945) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8946) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8948) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8950) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8951) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8953) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,8956) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8957) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8958) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,8966) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
  (<0>,8969)
  (<0>,8972) tree.c:3503: if (rcu_scheduler_fully_active &&
  (<0>,8975) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8977) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,8980) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,8982) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,8986) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,8989)
  (<0>,8990) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,8992) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,8995) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,9002) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9003) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9014)
  (<0>,9015)
  (<0>,9016) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,9029)
  (<0>,9030) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9035) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9036) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9037) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9038) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9040) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9042) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9043) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9045) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9048) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9049) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9050) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9051) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9056) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9057) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9058) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9059) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9061) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9063) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9064) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9066) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9069) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9070) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9071) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9077) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,9093)
  (<0>,9094) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,9097)
  (<0>,9098) tree.c:625: return &rsp->node[0];
  (<0>,9102) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,9103) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9108) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9109) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9110) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9111) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9113) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9115) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9116) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9118) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9121) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9122) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9123) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9127) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9128) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9130) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9133) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9134) tree.c:639: return READ_ONCE(*fp);
  (<0>,9138) tree.c:639: return READ_ONCE(*fp);
  (<0>,9139) tree.c:639: return READ_ONCE(*fp);
  (<0>,9140) tree.c:639: return READ_ONCE(*fp);
  (<0>,9141) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9143) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9145) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9146) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9148) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9151) tree.c:639: return READ_ONCE(*fp);
  (<0>,9152) tree.c:639: return READ_ONCE(*fp);
  (<0>,9153) tree.c:639: return READ_ONCE(*fp);
  (<0>,9157) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,9160) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,9163) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9166) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9167) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9170) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9172) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9175) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9178) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9181) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9182) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9184) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9187) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9191) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9193) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9195) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9198) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9201) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9204) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9205) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9207) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9210) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9214) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9216) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9218) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9221) tree.c:665: return false; /* No grace period needed. */
  (<0>,9223) tree.c:666: }
  (<0>,9226) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,9231) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,9232) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,9233) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,9234) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9236) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9238) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9239) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9241) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9244) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,9245) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,9246) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,9247) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,9249) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,9252) tree.c:3528: rdp->n_rp_gp_completed++;
  (<0>,9254) tree.c:3528: rdp->n_rp_gp_completed++;
  (<0>,9256) tree.c:3528: rdp->n_rp_gp_completed++;
  (<0>,9257) tree.c:3529: return 1;
  (<0>,9259) tree.c:3548: }
  (<0>,9263) tree.c:3561: return 1;
  (<0>,9265) tree.c:3563: }
  (<0>,9272) fake_sched.h:43: return __running_cpu;
  (<0>,9276) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
  (<0>,9280) tree.c:2891: if (user)
  (<0>,9288) fake_sched.h:43: return __running_cpu;
  (<0>,9292) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
  (<0>,9294) fake_sched.h:43: return __running_cpu;
  (<0>,9298) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9304) fake_sched.h:43: return __running_cpu;
  (<0>,9308) fake_sched.h:183: if (need_softirq[get_cpu()]) {
  (<0>,9318)
  (<0>,9321) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9322) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9323) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9327) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9328) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9329) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9331) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,9335) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,9347)
  (<0>,9349) fake_sched.h:43: return __running_cpu;
  (<0>,9352) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9354) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9356) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,9357) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9359) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9366) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9367) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9369) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9375) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9376) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9377) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,9378) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,9379) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,9383)
  (<0>,9384)
  (<0>,9385) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,9386) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,9411)
  (<0>,9412)
  (<0>,9413) tree.c:1905: local_irq_save(flags);
  (<0>,9416)
  (<0>,9418) fake_sched.h:43: return __running_cpu;
  (<0>,9422) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9424) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9428) fake_sched.h:43: return __running_cpu;
  (<0>,9432) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9437) tree.c:1906: rnp = rdp->mynode;
  (<0>,9439) tree.c:1906: rnp = rdp->mynode;
  (<0>,9440) tree.c:1906: rnp = rdp->mynode;
  (<0>,9441) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9443) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9444) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9449) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9450) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9451) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9452) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9454) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9456) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9457) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9459) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9462) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9463) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9464) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,9467) tree.c:1910: !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */
  (<0>,9471)
  (<0>,9472) tree.h:752: bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,9477)
  (<0>,9479) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
  (<0>,9480) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
  (<0>,9483) fake_sync.h:134: return 1;
  (<0>,9485) fake_sync.h:135: }
  (<0>,9489) tree.h:752: bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,9490) tree.h:754: if (locked)
  (<0>,9496) tree.h:756: return locked;
  (<0>,9500) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,9501) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,9502) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,9524)
  (<0>,9525)
  (<0>,9526)
  (<0>,9527) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,9529) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,9530) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,9532) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,9535) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,9536) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,9537) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,9545)
  (<0>,9546)
  (<0>,9547)
  (<0>,9548) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9551) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9554) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9557) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9558) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,9561) tree.c:1818: return false;
  (<0>,9563) tree.c:1843: }
  (<0>,9566) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
  (<0>,9567) tree.c:1870: rdp->completed = rnp->completed;
  (<0>,9569) tree.c:1870: rdp->completed = rnp->completed;
  (<0>,9570) tree.c:1870: rdp->completed = rnp->completed;
  (<0>,9572) tree.c:1870: rdp->completed = rnp->completed;
  (<0>,9576) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,9578) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,9579) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,9581) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,9584) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,9586) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,9587) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,9589) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,9592) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,9594) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,9595) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,9597) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,9603) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,9604) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,9607) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,9611) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,9613) fake_sched.h:43: return __running_cpu;
  (<0>,9617) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,9618) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,9620) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,9621) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,9623) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,9626) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,9627) tree.c:1893: zero_cpu_stall_ticks(rdp);
  (<0>,9630)
  (<0>,9631) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,9633) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,9634) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,9636) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,9646)
  (<0>,9651) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,9655) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,9656) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,9657) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,9658) fake_defs.h:237: switch (size) {
  (<0>,9660) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,9661) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,9662) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,9663) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,9666) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,9669) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,9670) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,9673) tree.c:1896: return ret;
  (<0>,9677) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,9681) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,9682) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,9683) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,9685) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,9689)
  (<0>,9690)
  (<0>,9691) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,9692) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,9695) fake_sync.h:93: local_irq_restore(flags);
  (<0>,9698)
  (<0>,9700) fake_sched.h:43: return __running_cpu;
  (<0>,9704) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9706) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9710) fake_sched.h:43: return __running_cpu;
  (<0>,9714) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,9722) tree.c:1916: if (needwake)
  (<0>,9726) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,9728) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,9733) tree.c:3016: local_irq_save(flags);
  (<0>,9736)
  (<0>,9738) fake_sched.h:43: return __running_cpu;
  (<0>,9742) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9744) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,9748) fake_sched.h:43: return __running_cpu;
  (<0>,9752) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,9757) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9758) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,9769)
  (<0>,9770)
  (<0>,9771) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,9784)
  (<0>,9785) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9790) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9791) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9792) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9793) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9795) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9797) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9798) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9800) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9803) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9804) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9805) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9806) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9811) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9812) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9813) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9814) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9816) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9818) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9819) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9821) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9824) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9825) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9826) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,9832) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,9848)
  (<0>,9849) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,9852)
  (<0>,9853) tree.c:625: return &rsp->node[0];
  (<0>,9857) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,9858) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9863) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9864) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9865) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9866) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9868) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9870) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9871) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9873) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9876) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9877) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9878) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9882) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,9883) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9885) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9888) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,9889) tree.c:639: return READ_ONCE(*fp);
  (<0>,9893) tree.c:639: return READ_ONCE(*fp);
  (<0>,9894) tree.c:639: return READ_ONCE(*fp);
  (<0>,9895) tree.c:639: return READ_ONCE(*fp);
  (<0>,9896) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9898) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9900) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9901) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9903) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,9906) tree.c:639: return READ_ONCE(*fp);
  (<0>,9907) tree.c:639: return READ_ONCE(*fp);
  (<0>,9908) tree.c:639: return READ_ONCE(*fp);
  (<0>,9912) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,9915) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,9918) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9921) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9922) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,9925) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9927) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9930) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9933) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9936) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9937) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9939) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9942) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9946) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9948) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9950) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9953) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9956) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9959) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9960) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9962) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9965) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,9969) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9971) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9973) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,9976) tree.c:665: return false; /* No grace period needed. */
  (<0>,9978) tree.c:666: }
  (<0>,9981) tree.c:3024: local_irq_restore(flags);
  (<0>,9984)
  (<0>,9986) fake_sched.h:43: return __running_cpu;
  (<0>,9990) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9992) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,9996) fake_sched.h:43: return __running_cpu;
  (<0>,10000) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,10006) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,10009)
  (<0>,10010) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10012) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10015) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10022) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,10025)
  (<0>,10029) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10032) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10033) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10034) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10038) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10039) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10040) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10042) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10046) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,10058)
  (<0>,10060) fake_sched.h:43: return __running_cpu;
  (<0>,10063) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,10065) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,10067) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,10068) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10070) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10077) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10078) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10080) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10086) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10087) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10088) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,10089) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,10090) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,10094)
  (<0>,10095)
  (<0>,10096) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,10097) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,10122)
  (<0>,10123)
  (<0>,10124) tree.c:1905: local_irq_save(flags);
  (<0>,10127)
  (<0>,10129) fake_sched.h:43: return __running_cpu;
  (<0>,10133) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10135) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10139) fake_sched.h:43: return __running_cpu;
  (<0>,10143) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,10148) tree.c:1906: rnp = rdp->mynode;
  (<0>,10150) tree.c:1906: rnp = rdp->mynode;
  (<0>,10151) tree.c:1906: rnp = rdp->mynode;
  (<0>,10152) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10154) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10155) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10160) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10161) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10162) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10163) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10165) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10167) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10168) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10170) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10173) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10174) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10175) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,10178) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10180) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10181) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10186) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10187) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10188) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10189) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10191) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10193) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10194) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10196) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10199) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10200) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10201) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,10204) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10208) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10209) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10210) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10211) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10213) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10214) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10215) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10216) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10219) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10222) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10223) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,10231) tree.c:1911: local_irq_restore(flags);
  (<0>,10234)
  (<0>,10236) fake_sched.h:43: return __running_cpu;
  (<0>,10240) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10242) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10246) fake_sched.h:43: return __running_cpu;
  (<0>,10250) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,10257) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,10259) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,10264) tree.c:3016: local_irq_save(flags);
  (<0>,10267)
  (<0>,10269) fake_sched.h:43: return __running_cpu;
  (<0>,10273) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10275) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10279) fake_sched.h:43: return __running_cpu;
  (<0>,10283) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,10288) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,10289) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,10300)
  (<0>,10301)
  (<0>,10302) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,10315)
  (<0>,10316) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10321) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10322) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10323) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10324) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10326) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10328) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10329) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10331) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10334) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10335) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10336) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10337) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10342) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10343) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10344) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10345) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10347) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10349) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10350) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10352) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10355) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10356) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10357) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,10363) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,10379)
  (<0>,10380) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,10383)
  (<0>,10384) tree.c:625: return &rsp->node[0];
  (<0>,10388) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,10389) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10394) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10395) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10396) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10397) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10399) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10401) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10402) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10404) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10407) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10408) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10409) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10413) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,10414) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,10416) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,10419) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,10420) tree.c:639: return READ_ONCE(*fp);
  (<0>,10424) tree.c:639: return READ_ONCE(*fp);
  (<0>,10425) tree.c:639: return READ_ONCE(*fp);
  (<0>,10426) tree.c:639: return READ_ONCE(*fp);
  (<0>,10427) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10429) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10431) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10432) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10434) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,10437) tree.c:639: return READ_ONCE(*fp);
  (<0>,10438) tree.c:639: return READ_ONCE(*fp);
  (<0>,10439) tree.c:639: return READ_ONCE(*fp);
  (<0>,10443) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,10446) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,10449) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,10452) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,10453) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,10456) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10458) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10461) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10464) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10467) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10468) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10470) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10473) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10477) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10479) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10481) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10484) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10487) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10490) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10491) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10493) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10496) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,10500) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10502) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10504) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,10507) tree.c:665: return false; /* No grace period needed. */
  (<0>,10509) tree.c:666: }
  (<0>,10512) tree.c:3024: local_irq_restore(flags);
  (<0>,10515)
  (<0>,10517) fake_sched.h:43: return __running_cpu;
  (<0>,10521) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10523) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10527) fake_sched.h:43: return __running_cpu;
  (<0>,10531) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,10537) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,10540)
  (<0>,10541) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10543) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10546) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,10553) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,10556)
  (<0>,10560) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10563) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10564) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10565) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10569) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10570) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10571) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10573) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,10581) fake_sched.h:43: return __running_cpu;
  (<0>,10585) fake_sched.h:185: need_softirq[get_cpu()] = 0;
  (<0>,10595) fake_sched.h:43: return __running_cpu;
  (<0>,10599) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,10600) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,10602) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,10603) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,10604) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,10606) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,10608) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,10609) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10610) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10611) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10612) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10613) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,10615) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,10623)
  (<0>,10629) fake_sched.h:43: return __running_cpu;
  (<0>,10633)
  (<0>,10636) tree.c:755: local_irq_save(flags);
  (<0>,10639)
  (<0>,10641) fake_sched.h:43: return __running_cpu;
  (<0>,10645) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10647) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,10651) fake_sched.h:43: return __running_cpu;
  (<0>,10655) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,10667)
  (<0>,10669) fake_sched.h:43: return __running_cpu;
  (<0>,10673) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,10674) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,10676) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,10677) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,10678) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10679) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10680) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10681) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10682) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,10686) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,10688) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,10689) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,10690) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,10706)
  (<0>,10708)
  (<0>,10710) fake_sched.h:43: return __running_cpu;
  (<0>,10714) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,10717) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10718) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10719) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10723) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10724) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10725) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10727) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10732) fake_sched.h:43: return __running_cpu;
  (<0>,10735) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10737) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10739) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10740) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,10743)
  (<0>,10746) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10749) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10750) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10751) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10755) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10756) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10757) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10759) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10764) fake_sched.h:43: return __running_cpu;
  (<0>,10767) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10769) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10771) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,10772) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,10775)
  (<0>,10778) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10781) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10782) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10783) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10787) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10788) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10789) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10791) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,10798) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10801) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10802) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10803) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10805) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10806) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,10808) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10809) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10810) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10811) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,10825)
  (<0>,10827) tree.c:758: local_irq_restore(flags);
  (<0>,10830)
  (<0>,10832) fake_sched.h:43: return __running_cpu;
  (<0>,10836) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10838) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,10842) fake_sched.h:43: return __running_cpu;
  (<0>,10846) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,10852) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,10855) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,10860) litmus.c:138: if (pthread_join(tu, NULL))
      (<0.1>,941) litmus.c:93: fake_release_cpu(get_cpu());
      (<0.1>,942) fake_sched.h:43: return __running_cpu;
      (<0.1>,946)
      (<0.1>,949) tree.c:755: local_irq_save(flags);
      (<0.1>,952)
      (<0.1>,954) fake_sched.h:43: return __running_cpu;
      (<0.1>,958) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,960) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,964) fake_sched.h:43: return __running_cpu;
      (<0.1>,968) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,980)
      (<0.1>,982) fake_sched.h:43: return __running_cpu;
      (<0.1>,986) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,987) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,989) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,990) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,991) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,992) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,993) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,994) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,995) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,999) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1001) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1002) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1003) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1019)
      (<0.1>,1021)
      (<0.1>,1023) fake_sched.h:43: return __running_cpu;
      (<0.1>,1027) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,1030) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1031) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1037) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1038) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1040) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1045) fake_sched.h:43: return __running_cpu;
      (<0.1>,1048) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1050) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1052) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1053) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1056)
      (<0.1>,1059) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1062) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1063) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1069) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1070) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1072) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1077) fake_sched.h:43: return __running_cpu;
      (<0.1>,1080) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1082) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1084) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1085) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1088)
      (<0.1>,1091) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1094) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1095) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1101) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1102) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1104) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1111) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1116) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1118) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1119) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1121) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1122) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1123) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1124) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1138)
      (<0.1>,1140) tree.c:758: local_irq_restore(flags);
      (<0.1>,1143)
      (<0.1>,1145) fake_sched.h:43: return __running_cpu;
      (<0.1>,1149) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1151) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1155) fake_sched.h:43: return __running_cpu;
      (<0.1>,1159) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1165) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1168) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,10861) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,10864) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10867) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10871) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10872) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,10873) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
              Error: Assertion violation at (<0>,10876): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmpr015qas3/tmpv0cu0h3e.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DFORCE_FAILURE_3 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmpr015qas3/tmpglfdl_fm.ll /tmp/tmpr015qas3/tmpv0cu0h3e.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=-1 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmpr015qas3/tmpglfdl_fm.ll
Total wall-clock time: 492.84 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification failure
--------------------------------------------------------------------
Trace count: 8 (also 1 sleepset blocked)

 Error detected:
  (<0>,1)
  (<0>,7)
  (<0>,8) litmus.c:114: pthread_t tu;
  (<0>,10) litmus.c:117: set_online_cpus();
  (<0>,13) litmus.c:117: set_online_cpus();
  (<0>,15) litmus.c:117: set_online_cpus();
  (<0>,17) litmus.c:117: set_online_cpus();
  (<0>,19) litmus.c:117: set_online_cpus();
  (<0>,21) litmus.c:117: set_online_cpus();
  (<0>,23) litmus.c:117: set_online_cpus();
  (<0>,26) litmus.c:117: set_online_cpus();
  (<0>,28) litmus.c:117: set_online_cpus();
  (<0>,30) litmus.c:117: set_online_cpus();
  (<0>,32) litmus.c:117: set_online_cpus();
  (<0>,34) litmus.c:117: set_online_cpus();
  (<0>,36) litmus.c:117: set_online_cpus();
  (<0>,39) litmus.c:118: set_possible_cpus();
  (<0>,41) litmus.c:118: set_possible_cpus();
  (<0>,44) litmus.c:118: set_possible_cpus();
  (<0>,46) litmus.c:118: set_possible_cpus();
  (<0>,48) litmus.c:118: set_possible_cpus();
  (<0>,50) litmus.c:118: set_possible_cpus();
  (<0>,52) litmus.c:118: set_possible_cpus();
  (<0>,54) litmus.c:118: set_possible_cpus();
  (<0>,57) litmus.c:118: set_possible_cpus();
  (<0>,59) litmus.c:118: set_possible_cpus();
  (<0>,61) litmus.c:118: set_possible_cpus();
  (<0>,63) litmus.c:118: set_possible_cpus();
  (<0>,65) litmus.c:118: set_possible_cpus();
  (<0>,67) litmus.c:118: set_possible_cpus();
  (<0>,75) tree_plugin.h:731: pr_info("Hierarchical RCU implementation.\n");
  (<0>,79) tree_plugin.h:76: if (rcu_fanout_exact)
  (<0>,82) tree_plugin.h:87: if (rcu_fanout_leaf != RCU_FANOUT_LEAF)
  (<0>,94) tree.c:4147: ulong d;
  (<0>,95) tree.c:4159: if (jiffies_till_first_fqs == ULONG_MAX)
  (<0>,98) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,99) tree.c:4160: jiffies_till_first_fqs = d;
  (<0>,101) tree.c:4161: if (jiffies_till_next_fqs == ULONG_MAX)
  (<0>,104) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,105) tree.c:4162: jiffies_till_next_fqs = d;
  (<0>,107) tree.c:4165: if (rcu_fanout_leaf == RCU_FANOUT_LEAF &&
  (<0>,120)
  (<0>,121) tree.c:4056: static void __init rcu_init_one(struct rcu_state *rsp)
  (<0>,122) tree.c:4067: int i;
  (<0>,125) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,128) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,130) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,131) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,134) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,137) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,138) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,141) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,143) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,145) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,147) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,148) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,151) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,153) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,154) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,165)
  (<0>,166) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,167) tree.c:4032: static void __init rcu_init_levelspread(int *levelspread, const int *levelcnt)
  (<0>,170) tree.c:4041: int ccur;
  (<0>,171) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,173) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,175) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,178) tree.c:4046: ccur = levelcnt[i];
  (<0>,180) tree.c:4046: ccur = levelcnt[i];
  (<0>,182) tree.c:4046: ccur = levelcnt[i];
  (<0>,183) tree.c:4046: ccur = levelcnt[i];
  (<0>,184) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,185) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,188) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,190) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,192) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,194) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,195) tree.c:4048: cprv = ccur;
  (<0>,196) tree.c:4048: cprv = ccur;
  (<0>,198) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,200) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,202) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,207) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,208) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,210) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,211) tree.c:4085: fl_mask <<= 1;
  (<0>,215) tree.c:4085: fl_mask <<= 1;
  (<0>,216) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,218) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,220) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,223) tree.c:4090: cpustride *= levelspread[i];
  (<0>,226) tree.c:4090: cpustride *= levelspread[i];
  (<0>,227) tree.c:4090: cpustride *= levelspread[i];
  (<0>,229) tree.c:4090: cpustride *= levelspread[i];
  (<0>,230) tree.c:4091: rnp = rsp->level[i];
  (<0>,232) tree.c:4091: rnp = rsp->level[i];
  (<0>,235) tree.c:4091: rnp = rsp->level[i];
  (<0>,236) tree.c:4091: rnp = rsp->level[i];
  (<0>,237) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,239) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,240) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,243) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,246) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,250)
  (<0>,251) fake_sync.h:75: void raw_spin_lock_init(raw_spinlock_t *l)
  (<0>,252) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,258) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,262)
  (<0>,263) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,264) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,270) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,272) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,273) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,275) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,276) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,278) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,279) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,281) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,282) tree.c:4101: rnp->qsmask = 0;
  (<0>,284) tree.c:4101: rnp->qsmask = 0;
  (<0>,285) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,287) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,288) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,289) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,291) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,293) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,294) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,296) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,299) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,301) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,302) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,304) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,307) tree.c:4107: if (i == 0) {
  (<0>,310) tree.c:4108: rnp->grpnum = 0;
  (<0>,312) tree.c:4108: rnp->grpnum = 0;
  (<0>,313) tree.c:4109: rnp->grpmask = 0;
  (<0>,315) tree.c:4109: rnp->grpmask = 0;
  (<0>,316) tree.c:4110: rnp->parent = NULL;
  (<0>,318) tree.c:4110: rnp->parent = NULL;
  (<0>,320) tree.c:4117: rnp->level = i;
  (<0>,322) tree.c:4117: rnp->level = i;
  (<0>,324) tree.c:4117: rnp->level = i;
  (<0>,325) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,329)
  (<0>,330) fake_defs.h:358: static inline void INIT_LIST_HEAD(struct list_head *list)
  (<0>,331) fake_defs.h:360: list->next = list;
  (<0>,333) fake_defs.h:360: list->next = list;
  (<0>,334) fake_defs.h:361: list->prev = list;
  (<0>,335) fake_defs.h:361: list->prev = list;
  (<0>,337) fake_defs.h:361: list->prev = list;
  (<0>,339) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,342) tree_plugin.h:2431: }
  (<0>,352) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,356)
  (<0>,357) fake_sync.h:141: void spin_lock_init(raw_spinlock_t *l)
  (<0>,358) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,363) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,365) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,366) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,368) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,370) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,371) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,374) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,378) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,380) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,382) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,389) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,392) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,395) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,396) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,397) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,399) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,400) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,407)
  (<0>,408) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,409) fake_defs.h:629: int cpumask_next(int n, cpumask_var_t mask)
  (<0>,411) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,412) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,415) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,417) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,418) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,421) fake_defs.h:635: if (offset & mask)
  (<0>,422) fake_defs.h:635: if (offset & mask)
  (<0>,426) fake_defs.h:636: return cpu;
  (<0>,427) fake_defs.h:636: return cpu;
  (<0>,429) fake_defs.h:639: }
  (<0>,431) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,432) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,436) tree.c:4132: while (i > rnp->grphi)
  (<0>,437) tree.c:4132: while (i > rnp->grphi)
  (<0>,439) tree.c:4132: while (i > rnp->grphi)
  (<0>,442) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,443) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,445) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,447) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,450) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,451) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,452) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,475)
  (<0>,476) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,477) tree.c:3764: rcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,479) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,481) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,483) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,484) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,487)
  (<0>,488) tree.c:623: static struct rcu_node *rcu_get_root(struct rcu_state *rsp)
  (<0>,492) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,496) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,497) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,498) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,500) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,504)
  (<0>,505) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,506) fake_sync.h:81: void raw_spin_lock_irqsave(raw_spinlock_t *l, unsigned long flags)
  (<0>,509) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,511) fake_sched.h:43: return __running_cpu;
  (<0>,515) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,517) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,521) fake_sched.h:43: return __running_cpu;
  (<0>,525) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,531) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,532) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,539) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,540) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,542) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,544) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,548) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,550) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,551) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,554) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,556) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,557) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,559) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,561) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,566) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,567) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,569) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,571) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,575) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,576) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,577) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,578) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,579) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,581) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,584) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,585) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,586) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,591) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,592) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,593) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,595) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,598) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,599) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,600) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,604) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,605) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,606) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,607) tree.c:3776: rdp->cpu = cpu;
  (<0>,608) tree.c:3776: rdp->cpu = cpu;
  (<0>,610) tree.c:3776: rdp->cpu = cpu;
  (<0>,611) tree.c:3777: rdp->rsp = rsp;
  (<0>,612) tree.c:3777: rdp->rsp = rsp;
  (<0>,614) tree.c:3777: rdp->rsp = rsp;
  (<0>,615) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,618) tree_plugin.h:2448: }
  (<0>,623) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,624) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,625) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,627) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,631)
  (<0>,632) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,633) fake_sync.h:89: void raw_spin_unlock_irqrestore(raw_spinlock_t *l, unsigned long flags)
  (<0>,634) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,637) fake_sync.h:93: local_irq_restore(flags);
  (<0>,640) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,642) fake_sched.h:43: return __running_cpu;
  (<0>,646) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,648) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,652) fake_sched.h:43: return __running_cpu;
  (<0>,656) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,666) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,667) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,674)
  (<0>,675)
  (<0>,676) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,678) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,679) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,682) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,684) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,685) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,688) fake_defs.h:635: if (offset & mask)
  (<0>,689) fake_defs.h:635: if (offset & mask)
  (<0>,693) fake_defs.h:636: return cpu;
  (<0>,694) fake_defs.h:636: return cpu;
  (<0>,696) fake_defs.h:639: }
  (<0>,698) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,699) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,703) tree.c:4132: while (i > rnp->grphi)
  (<0>,704) tree.c:4132: while (i > rnp->grphi)
  (<0>,706) tree.c:4132: while (i > rnp->grphi)
  (<0>,709) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,710) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,712) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,714) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,717) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,718) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,719) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,742)
  (<0>,743)
  (<0>,744) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,746) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,748) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,750) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,751) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,754)
  (<0>,755) tree.c:625: return &rsp->node[0];
  (<0>,759) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,763) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,764) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,765) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,767) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,771)
  (<0>,772)
  (<0>,773) fake_sync.h:83: local_irq_save(flags);
  (<0>,776) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,778) fake_sched.h:43: return __running_cpu;
  (<0>,782) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,784) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,788) fake_sched.h:43: return __running_cpu;
  (<0>,792) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,798) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,799) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,806) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,807) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,809) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,811) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,815) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,817) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,818) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,821) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,823) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,824) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,826) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,828) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,833) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,834) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,836) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,838) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,842) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,843) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,844) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,845) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,846) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,848) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,851) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,852) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,853) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,858) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,859) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,860) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,862) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,865) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,866) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,867) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,871) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,872) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,873) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,874) tree.c:3776: rdp->cpu = cpu;
  (<0>,875) tree.c:3776: rdp->cpu = cpu;
  (<0>,877) tree.c:3776: rdp->cpu = cpu;
  (<0>,878) tree.c:3777: rdp->rsp = rsp;
  (<0>,879) tree.c:3777: rdp->rsp = rsp;
  (<0>,881) tree.c:3777: rdp->rsp = rsp;
  (<0>,882) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,885) tree_plugin.h:2448: }
  (<0>,890) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,891) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,892) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,894) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,898)
  (<0>,899)
  (<0>,900) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,901) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,904) fake_sync.h:93: local_irq_restore(flags);
  (<0>,907) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,909) fake_sched.h:43: return __running_cpu;
  (<0>,913) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,915) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,919) fake_sched.h:43: return __running_cpu;
  (<0>,923) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,933) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,934) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,941)
  (<0>,942)
  (<0>,943) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,945) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,946) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,949) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,951) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,952) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,955) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,957) fake_defs.h:639: }
  (<0>,959) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,960) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,963) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,968)
  (<0>,969) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,970) fake_defs.h:374: static inline void list_add(struct list_head *new, struct list_head *head)
  (<0>,971) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,972) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,974) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,979)
  (<0>,980) fake_defs.h:364: static inline void __list_add(struct list_head *new,
  (<0>,981) fake_defs.h:365: struct list_head *prev,
  (<0>,982) fake_defs.h:366: struct list_head *next)
  (<0>,983) fake_defs.h:368: next->prev = new;
  (<0>,985) fake_defs.h:368: next->prev = new;
  (<0>,986) fake_defs.h:369: new->next = next;
  (<0>,987) fake_defs.h:369: new->next = next;
  (<0>,989) fake_defs.h:369: new->next = next;
  (<0>,990) fake_defs.h:370: new->prev = prev;
  (<0>,991) fake_defs.h:370: new->prev = prev;
  (<0>,993) fake_defs.h:370: new->prev = prev;
  (<0>,994) fake_defs.h:371: prev->next = new;
  (<0>,995) fake_defs.h:371: prev->next = new;
  (<0>,997) fake_defs.h:371: prev->next = new;
  (<0>,1009)
  (<0>,1010) tree.c:4066: int cpustride = 1;
  (<0>,1011) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1014) tree.c:4074: if (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)
  (<0>,1017) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1019) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1020) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1023) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1026) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1027) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1030) tree.c:4080: levelcnt[i] = num_rcu_lvl[i];
  (<0>,1032) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1034) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1036) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1037) tree.c:4079: for (i = 0; i < rcu_num_lvls; i++)
  (<0>,1040) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1042) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1043) tree.c:4081: for (i = 1; i < rcu_num_lvls; i++)
  (<0>,1054)
  (<0>,1055)
  (<0>,1056) tree.c:4036: if (rcu_fanout_exact) {
  (<0>,1059) tree.c:4044: cprv = nr_cpu_ids;
  (<0>,1060) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1062) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1064) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1067) tree.c:4046: ccur = levelcnt[i];
  (<0>,1069) tree.c:4046: ccur = levelcnt[i];
  (<0>,1071) tree.c:4046: ccur = levelcnt[i];
  (<0>,1072) tree.c:4046: ccur = levelcnt[i];
  (<0>,1073) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1074) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1077) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1079) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1081) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1083) tree.c:4047: levelspread[i] = (cprv + ccur - 1) / ccur;
  (<0>,1084) tree.c:4048: cprv = ccur;
  (<0>,1085) tree.c:4048: cprv = ccur;
  (<0>,1087) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1089) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1091) tree.c:4045: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1096) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1097) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1099) tree.c:4084: rsp->flavor_mask = fl_mask;
  (<0>,1100) tree.c:4085: fl_mask <<= 1;
  (<0>,1104) tree.c:4085: fl_mask <<= 1;
  (<0>,1105) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1107) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1109) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1112) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1115) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1116) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1118) tree.c:4090: cpustride *= levelspread[i];
  (<0>,1119) tree.c:4091: rnp = rsp->level[i];
  (<0>,1121) tree.c:4091: rnp = rsp->level[i];
  (<0>,1124) tree.c:4091: rnp = rsp->level[i];
  (<0>,1125) tree.c:4091: rnp = rsp->level[i];
  (<0>,1126) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1128) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1129) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1132) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1135) tree.c:4093: raw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));
  (<0>,1139)
  (<0>,1140) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1141) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1147) tree.c:4096: raw_spin_lock_init(&rnp->fqslock);
  (<0>,1151)
  (<0>,1152) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1153) fake_sync.h:77: if (pthread_mutex_init(l, NULL))
  (<0>,1159) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1161) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1162) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1164) tree.c:4099: rnp->gpnum = rsp->gpnum;
  (<0>,1165) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1167) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1168) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1170) tree.c:4100: rnp->completed = rsp->completed;
  (<0>,1171) tree.c:4101: rnp->qsmask = 0;
  (<0>,1173) tree.c:4101: rnp->qsmask = 0;
  (<0>,1174) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1176) tree.c:4102: rnp->qsmaskinit = 0;
  (<0>,1177) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1178) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1180) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1182) tree.c:4103: rnp->grplo = j * cpustride;
  (<0>,1183) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1185) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1188) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1190) tree.c:4104: rnp->grphi = (j + 1) * cpustride - 1;
  (<0>,1191) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1193) tree.c:4105: if (rnp->grphi >= nr_cpu_ids)
  (<0>,1196) tree.c:4107: if (i == 0) {
  (<0>,1199) tree.c:4108: rnp->grpnum = 0;
  (<0>,1201) tree.c:4108: rnp->grpnum = 0;
  (<0>,1202) tree.c:4109: rnp->grpmask = 0;
  (<0>,1204) tree.c:4109: rnp->grpmask = 0;
  (<0>,1205) tree.c:4110: rnp->parent = NULL;
  (<0>,1207) tree.c:4110: rnp->parent = NULL;
  (<0>,1209) tree.c:4117: rnp->level = i;
  (<0>,1211) tree.c:4117: rnp->level = i;
  (<0>,1213) tree.c:4117: rnp->level = i;
  (<0>,1214) tree.c:4118: INIT_LIST_HEAD(&rnp->blkd_tasks);
  (<0>,1218)
  (<0>,1219) fake_defs.h:360: list->next = list;
  (<0>,1220) fake_defs.h:360: list->next = list;
  (<0>,1222) fake_defs.h:360: list->next = list;
  (<0>,1223) fake_defs.h:361: list->prev = list;
  (<0>,1224) fake_defs.h:361: list->prev = list;
  (<0>,1226) fake_defs.h:361: list->prev = list;
  (<0>,1228) tree.c:4119: rcu_init_one_nocb(rnp);
  (<0>,1231) tree_plugin.h:2431: }
  (<0>,1241) tree.c:4124: spin_lock_init(&rnp->exp_lock);
  (<0>,1245)
  (<0>,1246) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1247) fake_sync.h:143: if (pthread_mutex_init(l, NULL))
  (<0>,1252) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1254) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1255) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1257) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1259) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1260) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1263) tree.c:4092: for (j = 0; j < levelcnt[i]; j++, rnp++) {
  (<0>,1267) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1269) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1271) tree.c:4089: for (i = rcu_num_lvls - 1; i >= 0; i--) {
  (<0>,1278) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1281) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1284) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1285) tree.c:4130: rnp = rsp->level[rcu_num_lvls - 1];
  (<0>,1286) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1288) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1289) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1296)
  (<0>,1297)
  (<0>,1298) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1300) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1301) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1304) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1306) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1307) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1310) fake_defs.h:635: if (offset & mask)
  (<0>,1311) fake_defs.h:635: if (offset & mask)
  (<0>,1315) fake_defs.h:636: return cpu;
  (<0>,1316) fake_defs.h:636: return cpu;
  (<0>,1318) fake_defs.h:639: }
  (<0>,1320) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1321) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1325) tree.c:4132: while (i > rnp->grphi)
  (<0>,1326) tree.c:4132: while (i > rnp->grphi)
  (<0>,1328) tree.c:4132: while (i > rnp->grphi)
  (<0>,1331) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1332) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1334) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1336) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1339) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1340) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1341) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1364)
  (<0>,1365)
  (<0>,1366) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1368) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1370) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1372) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1373) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1376)
  (<0>,1377) tree.c:625: return &rsp->node[0];
  (<0>,1381) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1385) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1386) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1387) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1389) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1393)
  (<0>,1394)
  (<0>,1395) fake_sync.h:83: local_irq_save(flags);
  (<0>,1398) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1400) fake_sched.h:43: return __running_cpu;
  (<0>,1404) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1406) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1410) fake_sched.h:43: return __running_cpu;
  (<0>,1414) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1420) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1421) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1428) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1429) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1431) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1433) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1437) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1439) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1440) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1443) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1445) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1446) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1448) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1450) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1455) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1456) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1458) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1460) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1464) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1465) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1466) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1467) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1468) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1470) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1473) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1474) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1475) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1480) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1481) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1482) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1484) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1487) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1488) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1489) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1493) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1494) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1495) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1496) tree.c:3776: rdp->cpu = cpu;
  (<0>,1497) tree.c:3776: rdp->cpu = cpu;
  (<0>,1499) tree.c:3776: rdp->cpu = cpu;
  (<0>,1500) tree.c:3777: rdp->rsp = rsp;
  (<0>,1501) tree.c:3777: rdp->rsp = rsp;
  (<0>,1503) tree.c:3777: rdp->rsp = rsp;
  (<0>,1504) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1507) tree_plugin.h:2448: }
  (<0>,1512) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1513) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1514) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1516) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1520)
  (<0>,1521)
  (<0>,1522) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1523) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1526) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1529) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1531) fake_sched.h:43: return __running_cpu;
  (<0>,1535) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1541) fake_sched.h:43: return __running_cpu;
  (<0>,1545) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1555) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1556) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1563)
  (<0>,1564)
  (<0>,1565) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1567) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1568) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1571) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1573) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1574) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1577) fake_defs.h:635: if (offset & mask)
  (<0>,1578) fake_defs.h:635: if (offset & mask)
  (<0>,1582) fake_defs.h:636: return cpu;
  (<0>,1583) fake_defs.h:636: return cpu;
  (<0>,1585) fake_defs.h:639: }
  (<0>,1587) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1588) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1592) tree.c:4132: while (i > rnp->grphi)
  (<0>,1593) tree.c:4132: while (i > rnp->grphi)
  (<0>,1595) tree.c:4132: while (i > rnp->grphi)
  (<0>,1598) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1599) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1601) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1603) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1606) tree.c:4134: per_cpu_ptr(rsp->rda, i)->mynode = rnp;
  (<0>,1607) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1608) tree.c:4135: rcu_boot_init_percpu_data(i, rsp);
  (<0>,1631)
  (<0>,1632)
  (<0>,1633) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1635) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1637) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1639) tree.c:3767: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1640) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1643)
  (<0>,1644) tree.c:625: return &rsp->node[0];
  (<0>,1648) tree.c:3768: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1652) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1653) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1654) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1656) tree.c:3771: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,1660)
  (<0>,1661)
  (<0>,1662) fake_sync.h:83: local_irq_save(flags);
  (<0>,1665) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1667) fake_sched.h:43: return __running_cpu;
  (<0>,1671) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1673) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,1677) fake_sched.h:43: return __running_cpu;
  (<0>,1681) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,1687) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1688) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,1695) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1696) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1698) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1700) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1704) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1706) tree.c:3772: rdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);
  (<0>,1707) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1710) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1712) tree.c:3773: rdp->dynticks = &per_cpu(rcu_dynticks, cpu);
  (<0>,1713) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1715) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1717) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1722) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1723) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1725) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1727) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1731) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1732) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1733) tree.c:3774: WARN_ON_ONCE(rdp->dynticks->dynticks_nesting != DYNTICK_TASK_EXIT_IDLE);
  (<0>,1734) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1735) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1737) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1740) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1741) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1742) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1747) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1748) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1749) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1751) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1754) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1755) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1756) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1760) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1761) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1762) tree.c:3775: WARN_ON_ONCE(atomic_read(&rdp->dynticks->dynticks) != 1);
  (<0>,1763) tree.c:3776: rdp->cpu = cpu;
  (<0>,1764) tree.c:3776: rdp->cpu = cpu;
  (<0>,1766) tree.c:3776: rdp->cpu = cpu;
  (<0>,1767) tree.c:3777: rdp->rsp = rsp;
  (<0>,1768) tree.c:3777: rdp->rsp = rsp;
  (<0>,1770) tree.c:3777: rdp->rsp = rsp;
  (<0>,1771) tree.c:3778: rcu_boot_init_nocb_percpu_data(rdp);
  (<0>,1774) tree_plugin.h:2448: }
  (<0>,1779) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1780) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1781) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1783) tree.c:3779: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,1787)
  (<0>,1788)
  (<0>,1789) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1790) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,1793) fake_sync.h:93: local_irq_restore(flags);
  (<0>,1796) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1798) fake_sched.h:43: return __running_cpu;
  (<0>,1802) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1804) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,1808) fake_sched.h:43: return __running_cpu;
  (<0>,1812) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,1822) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1823) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1830)
  (<0>,1831)
  (<0>,1832) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1834) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1835) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1838) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1840) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1841) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1844) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,1846) fake_defs.h:639: }
  (<0>,1848) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1849) tree.c:4131: for_each_possible_cpu(i) {
  (<0>,1852) tree.c:4137: list_add(&rsp->flavors, &rcu_struct_flavors);
  (<0>,1857)
  (<0>,1858)
  (<0>,1859) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1860) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1861) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1863) fake_defs.h:376: __list_add(new, head, head->next);
  (<0>,1868)
  (<0>,1869)
  (<0>,1870)
  (<0>,1871) fake_defs.h:368: next->prev = new;
  (<0>,1872) fake_defs.h:368: next->prev = new;
  (<0>,1874) fake_defs.h:368: next->prev = new;
  (<0>,1875) fake_defs.h:369: new->next = next;
  (<0>,1876) fake_defs.h:369: new->next = next;
  (<0>,1878) fake_defs.h:369: new->next = next;
  (<0>,1879) fake_defs.h:370: new->prev = prev;
  (<0>,1880) fake_defs.h:370: new->prev = prev;
  (<0>,1882) fake_defs.h:370: new->prev = prev;
  (<0>,1883) fake_defs.h:371: prev->next = new;
  (<0>,1884) fake_defs.h:371: prev->next = new;
  (<0>,1886) fake_defs.h:371: prev->next = new;
  (<0>,1890) tree.c:4251: if (dump_tree)
  (<0>,1899) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1901) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1902) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1909)
  (<0>,1910)
  (<0>,1911) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1913) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1914) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1917) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,1919) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1920) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,1923) fake_defs.h:635: if (offset & mask)
  (<0>,1924) fake_defs.h:635: if (offset & mask)
  (<0>,1928) fake_defs.h:636: return cpu;
  (<0>,1929) fake_defs.h:636: return cpu;
  (<0>,1931) fake_defs.h:639: }
  (<0>,1933) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1934) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,1937) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,1945)
  (<0>,1946) tree.c:3829: int rcutree_prepare_cpu(unsigned int cpu)
  (<0>,1947) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1948) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1952) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1953) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1954) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1956) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,1960) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1961) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,1987)
  (<0>,1988) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1989) tree.c:3789: rcu_init_percpu_data(int cpu, struct rcu_state *rsp)
  (<0>,1991) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1993) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1995) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,1996) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,1999)
  (<0>,2000) tree.c:625: return &rsp->node[0];
  (<0>,2004) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2008) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2009) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2010) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2012) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2016)
  (<0>,2017)
  (<0>,2018) fake_sync.h:83: local_irq_save(flags);
  (<0>,2021) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2023) fake_sched.h:43: return __running_cpu;
  (<0>,2027) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2029) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2033) fake_sched.h:43: return __running_cpu;
  (<0>,2037) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2043) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2044) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2051) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2053) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2054) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2056) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2057) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2059) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2060) tree.c:3800: rdp->blimit = blimit;
  (<0>,2061) tree.c:3800: rdp->blimit = blimit;
  (<0>,2063) tree.c:3800: rdp->blimit = blimit;
  (<0>,2064) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2066) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2069) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2072)
  (<0>,2073) tree.c:1551: static void init_callback_list(struct rcu_data *rdp)
  (<0>,2076) tree_plugin.h:2469: return false;
  (<0>,2079) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2083)
  (<0>,2084) tree.c:1539: static void init_default_callback_list(struct rcu_data *rdp)
  (<0>,2086) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2087) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2089) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2092) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2094) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2096) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2099) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2101) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2103) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2105) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2108) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2110) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2112) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2115) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2117) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2119) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2121) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2124) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2126) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2128) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2131) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2133) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2135) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2137) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2140) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2142) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2144) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2147) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2149) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2151) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2153) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2160) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2162) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2164) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2165) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2167) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2170) tree_plugin.h:2902: }
  (<0>,2172) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2173) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2175) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2178) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2179) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2180) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2183) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2185) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2188) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2189) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2192)
  (<0>,2193) tree.h:721: static inline void raw_spin_unlock_rcu_node(struct rcu_node *rnp)
  (<0>,2197)
  (<0>,2198) fake_sync.h:120: void raw_spin_unlock(raw_spinlock_t *l)
  (<0>,2199) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2205) tree.c:3814: rnp = rdp->mynode;
  (<0>,2207) tree.c:3814: rnp = rdp->mynode;
  (<0>,2208) tree.c:3814: rnp = rdp->mynode;
  (<0>,2209) tree.c:3815: mask = rdp->grpmask;
  (<0>,2211) tree.c:3815: mask = rdp->grpmask;
  (<0>,2212) tree.c:3815: mask = rdp->grpmask;
  (<0>,2213) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2216)
  (<0>,2217) tree.h:715: static inline void raw_spin_lock_rcu_node(struct rcu_node *rnp)
  (<0>,2221) fake_sync.h:115: preempt_disable();
  (<0>,2223) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2224) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2231) tree.c:3817: if (!rdp->beenonline)
  (<0>,2233) tree.c:3817: if (!rdp->beenonline)
  (<0>,2237) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2242) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2243) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2244) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2245) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2247) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2249) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2250) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2252) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2255) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2256) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2257) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2259) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2260) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2265) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2266) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2267) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2268) fake_defs.h:237: switch (size) {
  (<0>,2270) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2272) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2273) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2275) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2278) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2279) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2280) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2282) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2284) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2285) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2287) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2288) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2290) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2291) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2293) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2294) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2296) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2297) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2301) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2302) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2305) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2306) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2308) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2309) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2311) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2317) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2318) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2319) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2321) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2325)
  (<0>,2326)
  (<0>,2327) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2328) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2331) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2334) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2336) fake_sched.h:43: return __running_cpu;
  (<0>,2340) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2342) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2346) fake_sched.h:43: return __running_cpu;
  (<0>,2350) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2360) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2363) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2364) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2365) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2369) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2370) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2371) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2373) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2377) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2378) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,2404)
  (<0>,2405)
  (<0>,2406) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2408) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2410) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2412) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,2413) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2416)
  (<0>,2417) tree.c:625: return &rsp->node[0];
  (<0>,2421) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,2425) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2426) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2427) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2429) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2433)
  (<0>,2434)
  (<0>,2435) fake_sync.h:83: local_irq_save(flags);
  (<0>,2438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2440) fake_sched.h:43: return __running_cpu;
  (<0>,2444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2446) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2450) fake_sched.h:43: return __running_cpu;
  (<0>,2454) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2460) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2461) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2468) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2470) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,2471) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2473) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2474) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2476) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,2477) tree.c:3800: rdp->blimit = blimit;
  (<0>,2478) tree.c:3800: rdp->blimit = blimit;
  (<0>,2480) tree.c:3800: rdp->blimit = blimit;
  (<0>,2481) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2483) tree.c:3801: if (!rdp->nxtlist)
  (<0>,2486) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,2489)
  (<0>,2490) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,2493) tree_plugin.h:2469: return false;
  (<0>,2496) tree.c:1555: init_default_callback_list(rdp);
  (<0>,2500)
  (<0>,2501) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2503) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,2504) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2506) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2509) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2511) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2513) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2516) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2518) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2520) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2522) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2525) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2527) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2529) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2532) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2534) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2536) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2538) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2541) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2543) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2545) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2548) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2550) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2552) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2554) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2557) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2559) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2561) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2564) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,2566) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2568) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2570) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,2577) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2579) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2581) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,2582) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2584) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,2587) tree_plugin.h:2902: }
  (<0>,2589) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2590) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2592) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2595) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2596) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2597) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2600) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2602) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2605) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,2606) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,2609)
  (<0>,2610) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2614)
  (<0>,2615) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2616) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,2622) tree.c:3814: rnp = rdp->mynode;
  (<0>,2624) tree.c:3814: rnp = rdp->mynode;
  (<0>,2625) tree.c:3814: rnp = rdp->mynode;
  (<0>,2626) tree.c:3815: mask = rdp->grpmask;
  (<0>,2628) tree.c:3815: mask = rdp->grpmask;
  (<0>,2629) tree.c:3815: mask = rdp->grpmask;
  (<0>,2630) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,2633)
  (<0>,2634) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,2638) fake_sync.h:115: preempt_disable();
  (<0>,2640) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2641) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,2648) tree.c:3817: if (!rdp->beenonline)
  (<0>,2650) tree.c:3817: if (!rdp->beenonline)
  (<0>,2654) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2659) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2660) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2661) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2662) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2664) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2666) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2667) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2669) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,2672) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2673) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2674) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2676) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2677) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2682) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2683) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2684) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2685) fake_defs.h:237: switch (size) {
  (<0>,2687) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2689) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2690) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2692) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,2695) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2696) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2697) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,2699) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2701) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,2702) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2704) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2705) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2707) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,2708) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2710) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2711) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2713) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,2714) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2718) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,2719) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2722) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2723) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2725) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,2726) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2728) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,2734) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2735) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2736) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2738) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2742)
  (<0>,2743)
  (<0>,2744) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2745) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2748) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2751) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2753) fake_sched.h:43: return __running_cpu;
  (<0>,2757) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2759) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2763) fake_sched.h:43: return __running_cpu;
  (<0>,2767) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2777) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2780) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2781) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2782) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2786) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2787) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2788) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2790) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,2794) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,2797) tree_plugin.h:1243: }
  (<0>,2799) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,2802) tree_plugin.h:2461: }
  (<0>,2805) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,2823)
  (<0>,2824) tree.c:3890: void rcu_cpu_starting(unsigned int cpu)
  (<0>,2825) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2826) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2830) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2831) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2832) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2834) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2839) fake_sched.h:43: return __running_cpu;
  (<0>,2842) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2844) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2846) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2847) tree.c:3900: rnp = rdp->mynode;
  (<0>,2849) tree.c:3900: rnp = rdp->mynode;
  (<0>,2850) tree.c:3900: rnp = rdp->mynode;
  (<0>,2851) tree.c:3901: mask = rdp->grpmask;
  (<0>,2853) tree.c:3901: mask = rdp->grpmask;
  (<0>,2854) tree.c:3901: mask = rdp->grpmask;
  (<0>,2858) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2859) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2860) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2862) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2866)
  (<0>,2867)
  (<0>,2868) fake_sync.h:83: local_irq_save(flags);
  (<0>,2871) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2873) fake_sched.h:43: return __running_cpu;
  (<0>,2877) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2879) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,2883) fake_sched.h:43: return __running_cpu;
  (<0>,2887) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,2893) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2894) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,2901) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2902) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2904) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2906) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,2907) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2908) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2910) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2912) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,2916) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2917) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2918) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2920) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,2924)
  (<0>,2925)
  (<0>,2926) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2927) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,2930) fake_sync.h:93: local_irq_restore(flags);
  (<0>,2933) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2935) fake_sched.h:43: return __running_cpu;
  (<0>,2939) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2941) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,2945) fake_sched.h:43: return __running_cpu;
  (<0>,2949) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,2958) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2961) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2962) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2963) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2967) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2968) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2969) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2971) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,2976) fake_sched.h:43: return __running_cpu;
  (<0>,2979) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2981) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2983) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,2984) tree.c:3900: rnp = rdp->mynode;
  (<0>,2986) tree.c:3900: rnp = rdp->mynode;
  (<0>,2987) tree.c:3900: rnp = rdp->mynode;
  (<0>,2988) tree.c:3901: mask = rdp->grpmask;
  (<0>,2990) tree.c:3901: mask = rdp->grpmask;
  (<0>,2991) tree.c:3901: mask = rdp->grpmask;
  (<0>,2995) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2996) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2997) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,2999) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3003)
  (<0>,3004)
  (<0>,3005) fake_sync.h:83: local_irq_save(flags);
  (<0>,3008) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3010) fake_sched.h:43: return __running_cpu;
  (<0>,3014) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3016) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3020) fake_sched.h:43: return __running_cpu;
  (<0>,3024) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3030) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3031) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3038) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3039) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3041) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3043) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,3044) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3045) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3047) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3049) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,3053) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3054) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3055) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3057) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3061)
  (<0>,3062)
  (<0>,3063) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3064) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3067) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3070) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3072) fake_sched.h:43: return __running_cpu;
  (<0>,3076) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3078) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3082) fake_sched.h:43: return __running_cpu;
  (<0>,3086) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3095) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3098) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3099) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3100) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3104) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3105) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3106) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3108) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,3114) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3115) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3122)
  (<0>,3123)
  (<0>,3124) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3126) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3127) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3130) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,3132) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3133) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,3136) fake_defs.h:635: if (offset & mask)
  (<0>,3137) fake_defs.h:635: if (offset & mask)
  (<0>,3141) fake_defs.h:636: return cpu;
  (<0>,3142) fake_defs.h:636: return cpu;
  (<0>,3144) fake_defs.h:639: }
  (<0>,3146) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3147) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,3150) tree.c:4263: rcutree_prepare_cpu(cpu);
  (<0>,3158)
  (<0>,3159) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3160) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3161) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3165) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3166) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3167) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3169) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3173) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3174) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3200)
  (<0>,3201)
  (<0>,3202) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3204) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3206) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3208) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3209) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3212)
  (<0>,3213) tree.c:625: return &rsp->node[0];
  (<0>,3217) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3221) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3222) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3223) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3225) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3229)
  (<0>,3230)
  (<0>,3231) fake_sync.h:83: local_irq_save(flags);
  (<0>,3234) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3236) fake_sched.h:43: return __running_cpu;
  (<0>,3240) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3242) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3246) fake_sched.h:43: return __running_cpu;
  (<0>,3250) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3256) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3257) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3264) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3266) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3267) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3269) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3270) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3272) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3273) tree.c:3800: rdp->blimit = blimit;
  (<0>,3274) tree.c:3800: rdp->blimit = blimit;
  (<0>,3276) tree.c:3800: rdp->blimit = blimit;
  (<0>,3277) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3279) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3282) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3285)
  (<0>,3286) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3289) tree_plugin.h:2469: return false;
  (<0>,3292) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3296)
  (<0>,3297) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3299) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3300) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3302) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3305) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3307) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3309) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3312) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3314) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3316) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3318) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3321) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3323) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3325) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3328) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3330) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3332) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3334) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3337) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3339) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3341) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3344) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3346) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3348) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3350) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3353) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3355) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3357) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3360) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3362) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3364) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3366) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3373) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3375) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3377) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3378) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3380) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3383) tree_plugin.h:2902: }
  (<0>,3385) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3386) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3388) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3391) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3392) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3393) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3396) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3398) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3401) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3402) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3405)
  (<0>,3406) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3410)
  (<0>,3411) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3412) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3418) tree.c:3814: rnp = rdp->mynode;
  (<0>,3420) tree.c:3814: rnp = rdp->mynode;
  (<0>,3421) tree.c:3814: rnp = rdp->mynode;
  (<0>,3422) tree.c:3815: mask = rdp->grpmask;
  (<0>,3424) tree.c:3815: mask = rdp->grpmask;
  (<0>,3425) tree.c:3815: mask = rdp->grpmask;
  (<0>,3426) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3429)
  (<0>,3430) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3434) fake_sync.h:115: preempt_disable();
  (<0>,3436) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3437) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3444) tree.c:3817: if (!rdp->beenonline)
  (<0>,3446) tree.c:3817: if (!rdp->beenonline)
  (<0>,3450) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3455) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3456) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3457) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3458) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3460) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3462) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3463) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3465) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3468) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3469) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3470) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3472) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3473) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3478) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3479) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3480) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3481) fake_defs.h:237: switch (size) {
  (<0>,3483) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3485) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3486) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3488) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3491) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3492) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3493) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3495) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3497) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3498) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3500) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3501) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3503) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3504) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3506) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3507) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3509) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3510) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3514) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3515) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3518) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3519) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3521) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3522) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3524) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3530) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3531) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3532) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3534) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3538)
  (<0>,3539)
  (<0>,3540) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3541) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3544) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3547) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3549) fake_sched.h:43: return __running_cpu;
  (<0>,3553) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3555) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3559) fake_sched.h:43: return __running_cpu;
  (<0>,3563) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3573) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3576) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3577) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3578) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3582) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3583) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3584) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3586) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3590) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3591) tree.c:3834: rcu_init_percpu_data(cpu, rsp);
  (<0>,3617)
  (<0>,3618)
  (<0>,3619) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3621) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3623) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3625) tree.c:3793: struct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);
  (<0>,3626) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3629)
  (<0>,3630) tree.c:625: return &rsp->node[0];
  (<0>,3634) tree.c:3794: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,3638) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3639) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3640) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3642) tree.c:3797: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,3646)
  (<0>,3647)
  (<0>,3648) fake_sync.h:83: local_irq_save(flags);
  (<0>,3651) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3653) fake_sched.h:43: return __running_cpu;
  (<0>,3657) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3659) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,3663) fake_sched.h:43: return __running_cpu;
  (<0>,3667) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,3673) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3674) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,3681) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3683) tree.c:3798: rdp->qlen_last_fqs_check = 0;
  (<0>,3684) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3686) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3687) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3689) tree.c:3799: rdp->n_force_qs_snap = rsp->n_force_qs;
  (<0>,3690) tree.c:3800: rdp->blimit = blimit;
  (<0>,3691) tree.c:3800: rdp->blimit = blimit;
  (<0>,3693) tree.c:3800: rdp->blimit = blimit;
  (<0>,3694) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3696) tree.c:3801: if (!rdp->nxtlist)
  (<0>,3699) tree.c:3802: init_callback_list(rdp);  /* Re-enable callbacks on this CPU. */
  (<0>,3702)
  (<0>,3703) tree.c:1553: if (init_nocb_callback_list(rdp))
  (<0>,3706) tree_plugin.h:2469: return false;
  (<0>,3709) tree.c:1555: init_default_callback_list(rdp);
  (<0>,3713)
  (<0>,3714) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3716) tree.c:1543: rdp->nxtlist = NULL;
  (<0>,3717) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3719) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3722) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3724) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3726) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3729) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3731) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3733) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3735) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3738) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3740) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3742) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3745) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3747) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3749) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3751) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3754) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3756) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3758) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3761) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3763) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3765) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3767) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3770) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3772) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3774) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3777) tree.c:1545: rdp->nxttail[i] = &rdp->nxtlist;
  (<0>,3779) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3781) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3783) tree.c:1544: for (i = 0; i < RCU_NEXT_SIZE; i++)
  (<0>,3790) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3792) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3794) tree.c:3803: rdp->dynticks->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,3795) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3797) tree.c:3804: rcu_sysidle_init_percpu_data(rdp->dynticks);
  (<0>,3800) tree_plugin.h:2902: }
  (<0>,3802) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3803) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3805) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3808) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3809) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3810) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3813) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3815) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3818) tree.c:3805: atomic_set(&rdp->dynticks->dynticks,
  (<0>,3819) tree.c:3807: raw_spin_unlock_rcu_node(rnp);		/* irqs remain disabled. */
  (<0>,3822)
  (<0>,3823) tree.h:723: raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3827)
  (<0>,3828) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3829) fake_sync.h:122: if (pthread_mutex_unlock(l))
  (<0>,3835) tree.c:3814: rnp = rdp->mynode;
  (<0>,3837) tree.c:3814: rnp = rdp->mynode;
  (<0>,3838) tree.c:3814: rnp = rdp->mynode;
  (<0>,3839) tree.c:3815: mask = rdp->grpmask;
  (<0>,3841) tree.c:3815: mask = rdp->grpmask;
  (<0>,3842) tree.c:3815: mask = rdp->grpmask;
  (<0>,3843) tree.c:3816: raw_spin_lock_rcu_node(rnp);		/* irqs already disabled. */
  (<0>,3846)
  (<0>,3847) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,3851) fake_sync.h:115: preempt_disable();
  (<0>,3853) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3854) fake_sync.h:116: if (pthread_mutex_lock(l))
  (<0>,3861) tree.c:3817: if (!rdp->beenonline)
  (<0>,3863) tree.c:3817: if (!rdp->beenonline)
  (<0>,3867) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3872) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3873) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3874) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3875) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3877) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3879) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3880) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3882) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,3885) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3886) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3887) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3889) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3890) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3895) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3896) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3897) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3898) fake_defs.h:237: switch (size) {
  (<0>,3900) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3902) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3903) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3905) fake_defs.h:240: case 4: *(volatile u32 *)p = *(u32 *)res; break;
  (<0>,3908) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3909) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3910) tree.c:3818: WRITE_ONCE(rsp->ncpus, READ_ONCE(rsp->ncpus) + 1);
  (<0>,3912) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3914) tree.c:3819: rdp->beenonline = true;	 /* We have now been online. */
  (<0>,3915) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3917) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3918) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3920) tree.c:3820: rdp->gpnum = rnp->completed; /* Make CPU later note any new GP. */
  (<0>,3921) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3923) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3924) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3926) tree.c:3821: rdp->completed = rnp->completed;
  (<0>,3927) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3931) tree.c:3822: rdp->cpu_no_qs.b.norm = true;
  (<0>,3932) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3935) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3936) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3938) tree.c:3823: rdp->rcu_qs_ctr_snap = per_cpu(rcu_qs_ctr, cpu);
  (<0>,3939) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3941) tree.c:3824: rdp->core_needs_qs = false;
  (<0>,3947) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3948) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3949) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3951) tree.c:3826: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,3955)
  (<0>,3956)
  (<0>,3957) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3958) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,3961) fake_sync.h:93: local_irq_restore(flags);
  (<0>,3964) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3966) fake_sched.h:43: return __running_cpu;
  (<0>,3970) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3972) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,3976) fake_sched.h:43: return __running_cpu;
  (<0>,3980) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,3990) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3993) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3994) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3995) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,3999) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4000) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4001) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4003) tree.c:3833: for_each_rcu_flavor(rsp)
  (<0>,4007) tree.c:3836: rcu_prepare_kthreads(cpu);
  (<0>,4010) tree_plugin.h:1243: }
  (<0>,4012) tree.c:3837: rcu_spawn_all_nocb_kthreads(cpu);
  (<0>,4015) tree_plugin.h:2461: }
  (<0>,4018) tree.c:4264: rcu_cpu_starting(cpu);
  (<0>,4036)
  (<0>,4037) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4038) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4039) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4043) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4044) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4045) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4047) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4052) fake_sched.h:43: return __running_cpu;
  (<0>,4055) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4057) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4059) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4060) tree.c:3900: rnp = rdp->mynode;
  (<0>,4062) tree.c:3900: rnp = rdp->mynode;
  (<0>,4063) tree.c:3900: rnp = rdp->mynode;
  (<0>,4064) tree.c:3901: mask = rdp->grpmask;
  (<0>,4066) tree.c:3901: mask = rdp->grpmask;
  (<0>,4067) tree.c:3901: mask = rdp->grpmask;
  (<0>,4071) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4072) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4073) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4075) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4079)
  (<0>,4080)
  (<0>,4081) fake_sync.h:83: local_irq_save(flags);
  (<0>,4084) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4086) fake_sched.h:43: return __running_cpu;
  (<0>,4090) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4092) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4096) fake_sched.h:43: return __running_cpu;
  (<0>,4100) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4106) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4107) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4114) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4115) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4117) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4119) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4120) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4121) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4123) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4125) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4129) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4130) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4131) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4133) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4137)
  (<0>,4138)
  (<0>,4139) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4140) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4143) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4146) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4148) fake_sched.h:43: return __running_cpu;
  (<0>,4152) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4154) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4158) fake_sched.h:43: return __running_cpu;
  (<0>,4162) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4171) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4174) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4175) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4176) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4180) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4181) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4182) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4184) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4189) fake_sched.h:43: return __running_cpu;
  (<0>,4192) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4194) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4196) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4197) tree.c:3900: rnp = rdp->mynode;
  (<0>,4199) tree.c:3900: rnp = rdp->mynode;
  (<0>,4200) tree.c:3900: rnp = rdp->mynode;
  (<0>,4201) tree.c:3901: mask = rdp->grpmask;
  (<0>,4203) tree.c:3901: mask = rdp->grpmask;
  (<0>,4204) tree.c:3901: mask = rdp->grpmask;
  (<0>,4208) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4209) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4210) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4212) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4216)
  (<0>,4217)
  (<0>,4218) fake_sync.h:83: local_irq_save(flags);
  (<0>,4221) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4223) fake_sched.h:43: return __running_cpu;
  (<0>,4227) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4229) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4233) fake_sched.h:43: return __running_cpu;
  (<0>,4237) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4243) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4244) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4251) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4252) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4254) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4256) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4257) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4258) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4260) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4262) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4266) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4267) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4268) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4270) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4274)
  (<0>,4275)
  (<0>,4276) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4277) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4280) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4283) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4285) fake_sched.h:43: return __running_cpu;
  (<0>,4289) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4291) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4295) fake_sched.h:43: return __running_cpu;
  (<0>,4299) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4308) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4311) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4312) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4313) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4317) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4318) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4319) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4321) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4327) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4328) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4335)
  (<0>,4336)
  (<0>,4337) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4339) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4340) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4343) fake_defs.h:633: for (cpu = n + 1, offset = 1 << (n + 1); \
  (<0>,4345) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4346) fake_defs.h:634: offset <= mask; cpu++, offset <<= 1) {
  (<0>,4349) fake_defs.h:638: return nr_cpu_ids + 1;
  (<0>,4351) fake_defs.h:639: }
  (<0>,4353) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4354) tree.c:4262: for_each_online_cpu(cpu) {
  (<0>,4358) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4360) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4363) litmus.c:123: set_cpu(i);
  (<0>,4366)
  (<0>,4367) fake_sched.h:54: void set_cpu(int cpu)
  (<0>,4368) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4370) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4388)
  (<0>,4389) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4390) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4391) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4395) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4396) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4397) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4399) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4404) fake_sched.h:43: return __running_cpu;
  (<0>,4407) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4409) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4411) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4412) tree.c:3900: rnp = rdp->mynode;
  (<0>,4414) tree.c:3900: rnp = rdp->mynode;
  (<0>,4415) tree.c:3900: rnp = rdp->mynode;
  (<0>,4416) tree.c:3901: mask = rdp->grpmask;
  (<0>,4418) tree.c:3901: mask = rdp->grpmask;
  (<0>,4419) tree.c:3901: mask = rdp->grpmask;
  (<0>,4423) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4424) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4425) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4427) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4431)
  (<0>,4432)
  (<0>,4433) fake_sync.h:83: local_irq_save(flags);
  (<0>,4436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4438) fake_sched.h:43: return __running_cpu;
  (<0>,4442) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4444) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4448) fake_sched.h:43: return __running_cpu;
  (<0>,4452) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4458) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4459) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4466) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4467) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4469) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4471) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4472) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4473) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4475) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4477) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4481) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4482) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4483) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4485) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4489)
  (<0>,4490)
  (<0>,4491) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4492) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4495) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4498) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4500) fake_sched.h:43: return __running_cpu;
  (<0>,4504) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4506) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4510) fake_sched.h:43: return __running_cpu;
  (<0>,4514) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4523) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4526) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4527) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4528) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4532) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4533) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4534) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4536) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4541) fake_sched.h:43: return __running_cpu;
  (<0>,4544) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4546) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4548) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4549) tree.c:3900: rnp = rdp->mynode;
  (<0>,4551) tree.c:3900: rnp = rdp->mynode;
  (<0>,4552) tree.c:3900: rnp = rdp->mynode;
  (<0>,4553) tree.c:3901: mask = rdp->grpmask;
  (<0>,4555) tree.c:3901: mask = rdp->grpmask;
  (<0>,4556) tree.c:3901: mask = rdp->grpmask;
  (<0>,4560) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4561) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4562) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4564) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4568)
  (<0>,4569)
  (<0>,4570) fake_sync.h:83: local_irq_save(flags);
  (<0>,4573) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4575) fake_sched.h:43: return __running_cpu;
  (<0>,4579) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4581) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4585) fake_sched.h:43: return __running_cpu;
  (<0>,4589) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4595) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4596) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,4603) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4604) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4606) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4608) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,4609) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4610) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4612) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4614) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,4618) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4619) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4620) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4622) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,4626)
  (<0>,4627)
  (<0>,4628) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4629) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,4632) fake_sync.h:93: local_irq_restore(flags);
  (<0>,4635) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4637) fake_sched.h:43: return __running_cpu;
  (<0>,4641) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4643) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4647) fake_sched.h:43: return __running_cpu;
  (<0>,4651) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4660) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4663) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4664) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4665) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4669) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4670) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4671) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4673) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4680) tree.c:753: unsigned long flags;
  (<0>,4683) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4685) fake_sched.h:43: return __running_cpu;
  (<0>,4689) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4691) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4695) fake_sched.h:43: return __running_cpu;
  (<0>,4699) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4711) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4713) fake_sched.h:43: return __running_cpu;
  (<0>,4717) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4718) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4720) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4721) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,4722) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4723) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4724) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4725) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4726) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,4730) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4732) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,4733) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4734) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,4750)
  (<0>,4752) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4754) fake_sched.h:43: return __running_cpu;
  (<0>,4758) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,4761) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4762) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4763) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4767) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4768) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4769) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4771) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4776) fake_sched.h:43: return __running_cpu;
  (<0>,4779) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4781) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4783) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4784) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4787) tree_plugin.h:2457: }
  (<0>,4790) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4793) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4794) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4795) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4799) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4800) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4801) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4803) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4808) fake_sched.h:43: return __running_cpu;
  (<0>,4811) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4813) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4815) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4816) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,4819) tree_plugin.h:2457: }
  (<0>,4822) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4825) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4826) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4827) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4831) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4832) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4833) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4835) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,4842) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4845) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4846) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4847) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4849) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4850) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,4852) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4853) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4854) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4855) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,4869) tree_plugin.h:2879: }
  (<0>,4871) tree.c:758: local_irq_restore(flags);
  (<0>,4874) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4876) fake_sched.h:43: return __running_cpu;
  (<0>,4880) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4882) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,4886) fake_sched.h:43: return __running_cpu;
  (<0>,4890) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,4897) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4899) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4901) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,4904) litmus.c:123: set_cpu(i);
  (<0>,4907)
  (<0>,4908) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4909) fake_sched.h:56: __running_cpu = cpu;
  (<0>,4911) litmus.c:125: rcu_cpu_starting(i);
  (<0>,4929)
  (<0>,4930) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4931) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4932) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4936) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4937) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4938) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4940) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,4945) fake_sched.h:43: return __running_cpu;
  (<0>,4948) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4950) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4952) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,4953) tree.c:3900: rnp = rdp->mynode;
  (<0>,4955) tree.c:3900: rnp = rdp->mynode;
  (<0>,4956) tree.c:3900: rnp = rdp->mynode;
  (<0>,4957) tree.c:3901: mask = rdp->grpmask;
  (<0>,4959) tree.c:3901: mask = rdp->grpmask;
  (<0>,4960) tree.c:3901: mask = rdp->grpmask;
  (<0>,4964) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4965) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4966) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4968) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,4972)
  (<0>,4973)
  (<0>,4974) fake_sync.h:83: local_irq_save(flags);
  (<0>,4977) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4979) fake_sched.h:43: return __running_cpu;
  (<0>,4983) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4985) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,4989) fake_sched.h:43: return __running_cpu;
  (<0>,4993) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,4999) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5000) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5007) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5008) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5010) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5012) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5013) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5014) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5016) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5018) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5022) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5023) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5024) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5026) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5030)
  (<0>,5031)
  (<0>,5032) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5033) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5036) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5039) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5041) fake_sched.h:43: return __running_cpu;
  (<0>,5045) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5047) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5051) fake_sched.h:43: return __running_cpu;
  (<0>,5055) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5064) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5067) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5068) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5069) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5073) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5074) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5075) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5077) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5082) fake_sched.h:43: return __running_cpu;
  (<0>,5085) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5087) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5089) tree.c:3899: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5090) tree.c:3900: rnp = rdp->mynode;
  (<0>,5092) tree.c:3900: rnp = rdp->mynode;
  (<0>,5093) tree.c:3900: rnp = rdp->mynode;
  (<0>,5094) tree.c:3901: mask = rdp->grpmask;
  (<0>,5096) tree.c:3901: mask = rdp->grpmask;
  (<0>,5097) tree.c:3901: mask = rdp->grpmask;
  (<0>,5101) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5102) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5103) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5105) tree.c:3902: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5109)
  (<0>,5110)
  (<0>,5111) fake_sync.h:83: local_irq_save(flags);
  (<0>,5114) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5116) fake_sched.h:43: return __running_cpu;
  (<0>,5120) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5122) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5126) fake_sched.h:43: return __running_cpu;
  (<0>,5130) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5136) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5137) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5144) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5145) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5147) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5149) tree.c:3903: rnp->qsmaskinitnext |= mask;
  (<0>,5150) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5151) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5153) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5155) tree.c:3904: rnp->expmaskinitnext |= mask;
  (<0>,5159) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5160) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5161) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5163) tree.c:3905: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5167)
  (<0>,5168)
  (<0>,5169) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5170) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5173) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5176) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5178) fake_sched.h:43: return __running_cpu;
  (<0>,5182) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5184) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5188) fake_sched.h:43: return __running_cpu;
  (<0>,5192) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5201) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5204) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5205) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5206) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5210) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5211) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5212) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5214) tree.c:3898: for_each_rcu_flavor(rsp) {
  (<0>,5221) tree.c:755: local_irq_save(flags);
  (<0>,5224) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5226) fake_sched.h:43: return __running_cpu;
  (<0>,5230) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5232) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5236) fake_sched.h:43: return __running_cpu;
  (<0>,5240) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5252) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5254) fake_sched.h:43: return __running_cpu;
  (<0>,5258) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5259) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5261) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5262) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,5263) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5264) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5265) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5266) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5267) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,5271) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5273) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,5274) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5275) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,5291)
  (<0>,5293) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5295) fake_sched.h:43: return __running_cpu;
  (<0>,5299) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,5302) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5303) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5304) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5308) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5309) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5310) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5312) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5317) fake_sched.h:43: return __running_cpu;
  (<0>,5320) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5322) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5324) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5325) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5328) tree_plugin.h:2457: }
  (<0>,5331) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5334) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5335) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5336) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5340) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5341) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5342) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5344) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5349) fake_sched.h:43: return __running_cpu;
  (<0>,5352) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5354) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5356) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,5357) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,5360) tree_plugin.h:2457: }
  (<0>,5363) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5366) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5367) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5368) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5372) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5373) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5374) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5376) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,5383) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5386) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5387) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5388) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5390) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5391) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,5393) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5394) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5395) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5396) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,5410) tree_plugin.h:2879: }
  (<0>,5412) tree.c:758: local_irq_restore(flags);
  (<0>,5415) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5417) fake_sched.h:43: return __running_cpu;
  (<0>,5421) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5423) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5427) fake_sched.h:43: return __running_cpu;
  (<0>,5431) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5438) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5440) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5442) litmus.c:122: for (int i = 0; i < NR_CPUS; i++) {
  (<0>,5462) tree.c:3971: unsigned long flags;
  (<0>,5463) tree.c:3972: int kthread_prio_in = kthread_prio;
  (<0>,5464) tree.c:3973: struct rcu_node *rnp;
  (<0>,5467) tree.c:3983: else if (kthread_prio > 99)
  (<0>,5471) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5472) tree.c:3985: if (kthread_prio != kthread_prio_in)
  (<0>,5475) tree.c:3989: rcu_scheduler_fully_active = 1;
  (<0>,5476) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5477) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5478) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5482) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5483) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5484) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5486) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5490) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5492) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5494) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5503)
  (<0>,5504) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5505) fake_defs.h:861: struct task_struct *spawn_gp_kthread(int (*threadfn)(void *data), void *data,
  (<0>,5506) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5507) fake_defs.h:862: const char namefmt[], const char name[])
  (<0>,5508) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5511)
  (<0>,5512)
  (<0>,5519)
  (<0>,5520)
  (<0>,5527)
  (<0>,5528)
  (<0>,5535)
  (<0>,5536)
  (<0>,5543)
  (<0>,5544)
  (<0>,5551)
  (<0>,5552)
  (<0>,5559)
  (<0>,5560)
  (<0>,5567)
  (<0>,5568)
  (<0>,5575)
  (<0>,5576)
  (<0>,5583)
  (<0>,5584) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5593) fake_defs.h:868: if (pthread_create(&t, NULL, run_gp_kthread, data))
  (<0>,5599) fake_defs.h:870: task = malloc(sizeof(*task));
  (<0>,5600) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5602) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5604) fake_defs.h:871: task->pid = (unsigned long) t;
  (<0>,5605) fake_defs.h:872: task->tid = t;
  (<0>,5606) fake_defs.h:872: task->tid = t;
  (<0>,5608) fake_defs.h:872: task->tid = t;
  (<0>,5609) fake_defs.h:873: return task;
  (<0>,5610) fake_defs.h:873: return task;
  (<0>,5612) fake_defs.h:876: }
  (<0>,5614) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5615) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5618)
  (<0>,5619) tree.c:625: return &rsp->node[0];
  (<0>,5623) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5627) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5628) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5629) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5631) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5635)
  (<0>,5636)
  (<0>,5637) fake_sync.h:83: local_irq_save(flags);
  (<0>,5640) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5642) fake_sched.h:43: return __running_cpu;
  (<0>,5646) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5648) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5652) fake_sched.h:43: return __running_cpu;
  (<0>,5656) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5662) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5663) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5670) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5671) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5673) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5674) tree.c:3996: if (kthread_prio) {
  (<0>,5680) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5681) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5682) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5684) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5688)
  (<0>,5689)
  (<0>,5690) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5691) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5694) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5697) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5699) fake_sched.h:43: return __running_cpu;
  (<0>,5703) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5705) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5709) fake_sched.h:43: return __running_cpu;
  (<0>,5713) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5724) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5727) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5728) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5729) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5733) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5734) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5735) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5737) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5741) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5743) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5745) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5754)
  (<0>,5755)
  (<0>,5756)
  (<0>,5757)
  (<0>,5758) fake_defs.h:865: struct task_struct *task = NULL;
  (<0>,5759) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5762)
  (<0>,5763)
  (<0>,5770)
  (<0>,5771)
  (<0>,5778)
  (<0>,5779)
  (<0>,5786)
  (<0>,5787)
  (<0>,5794)
  (<0>,5795) fake_defs.h:867: if (!strcmp(name, "rcu_sched") || IS_ENABLED(ENABLE_RCU_BH)) {
  (<0>,5808) fake_defs.h:875: return NULL;
  (<0>,5810) fake_defs.h:876: }
  (<0>,5812) tree.c:3991: t = kthread_create(rcu_gp_kthread, rsp, "%s", rsp->name);
  (<0>,5813) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5816)
  (<0>,5817) tree.c:625: return &rsp->node[0];
  (<0>,5821) tree.c:3993: rnp = rcu_get_root(rsp);
  (<0>,5825) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5826) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5827) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5829) tree.c:3994: raw_spin_lock_irqsave_rcu_node(rnp, flags);
  (<0>,5833)
  (<0>,5834)
  (<0>,5835) fake_sync.h:83: local_irq_save(flags);
  (<0>,5838) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5840) fake_sched.h:43: return __running_cpu;
  (<0>,5844) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5846) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5850) fake_sched.h:43: return __running_cpu;
  (<0>,5854) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,5860) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5861) fake_sync.h:85: if (pthread_mutex_lock(l))
  (<0>,5868) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5869) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5871) tree.c:3995: rsp->gp_kthread = t;
  (<0>,5872) tree.c:3996: if (kthread_prio) {
  (<0>,5878) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5879) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5880) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5882) tree.c:4000: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,5886)
  (<0>,5887)
  (<0>,5888) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5889) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,5892) fake_sync.h:93: local_irq_restore(flags);
  (<0>,5895) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5897) fake_sched.h:43: return __running_cpu;
  (<0>,5901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5903) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,5907) fake_sched.h:43: return __running_cpu;
  (<0>,5911) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,5922) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5925) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5926) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5927) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5931) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5932) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5933) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5935) tree.c:3990: for_each_rcu_flavor(rsp) {
  (<0>,5949)
  (<0>,5950) litmus.c:51: void *thread_reader(void *arg)
  (<0>,5953)
  (<0>,5954) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5955) fake_sched.h:56: __running_cpu = cpu;
  (<0>,5958) fake_sched.h:43: return __running_cpu;
  (<0>,5962)
  (<0>,5963) fake_sched.h:82: void fake_acquire_cpu(int cpu)
  (<0>,5966) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
  (<0>,5971) tree.c:890: unsigned long flags;
  (<0>,5974) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5976) fake_sched.h:43: return __running_cpu;
  (<0>,5980) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5982) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,5986) fake_sched.h:43: return __running_cpu;
  (<0>,5990) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6002) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6004) fake_sched.h:43: return __running_cpu;
  (<0>,6008) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6009) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6011) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6012) tree.c:867: oldval = rdtp->dynticks_nesting;
  (<0>,6013) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6014) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6015) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6016) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
  (<0>,6017) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
  (<0>,6021) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6023) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
  (<0>,6024) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6025) tree.c:873: rcu_eqs_exit_common(oldval, user);
  (<0>,6036)
  (<0>,6037) tree.c:830: static void rcu_eqs_exit_common(long long oldval, int user)
  (<0>,6039) fake_sched.h:43: return __running_cpu;
  (<0>,6043) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6047) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6050) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6051) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6052) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6054) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6055) tree.c:836: atomic_inc(&rdtp->dynticks);
  (<0>,6057) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6058) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6059) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6060) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6070) tree_plugin.h:2883: }
  (<0>,6072) tree.c:895: local_irq_restore(flags);
  (<0>,6075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6077) fake_sched.h:43: return __running_cpu;
  (<0>,6081) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6083) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,6087) fake_sched.h:43: return __running_cpu;
  (<0>,6091) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6106) litmus.c:57: r_x = x;
  (<0>,6107) litmus.c:57: r_x = x;
  (<0>,6111) fake_sched.h:43: return __running_cpu;
  (<0>,6115) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
  (<0>,6119) fake_sched.h:43: return __running_cpu;
  (<0>,6123) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6128) fake_sched.h:43: return __running_cpu;
  (<0>,6132) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
  (<0>,6143) fake_sched.h:43: return __running_cpu;
  (<0>,6147) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,6148) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6150) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6151) tree.c:938: oldval = rdtp->dynticks_nesting;
  (<0>,6152) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6154) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6156) tree.c:939: rdtp->dynticks_nesting++;
  (<0>,6157) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6158) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6159) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6160) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,6161) tree.c:942: if (oldval)
  (<0>,6169) tree_plugin.h:2883: }
  (<0>,6175) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
  (<0>,6184) tree_plugin.h:1669: struct rcu_state *rsp;
  (<0>,6185) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6186) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6190) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6191) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6192) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6194) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6199) fake_sched.h:43: return __running_cpu;
  (<0>,6202) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6204) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6207) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6209) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6211) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6214) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6215) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6216) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6220) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6221) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6222) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6224) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6229) fake_sched.h:43: return __running_cpu;
  (<0>,6232) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6234) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6237) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6239) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
  (<0>,6241) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6244) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6245) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6246) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6250) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6251) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6252) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6254) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
  (<0>,6259) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
  (<0>,6264) fake_sched.h:43: return __running_cpu;
  (<0>,6269) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
  (<0>,6277) fake_sched.h:43: return __running_cpu;
  (<0>,6283) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
  (<0>,6289) fake_sched.h:43: return __running_cpu;
  (<0>,6296) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
  (<0>,6309) tree.c:3557: struct rcu_state *rsp;
  (<0>,6310) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6311) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6315) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6316) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6317) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6319) tree.c:3559: for_each_rcu_flavor(rsp)
  (<0>,6323) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6325) fake_sched.h:43: return __running_cpu;
  (<0>,6328) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6330) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
  (<0>,6352)
  (<0>,6353) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6354) tree.c:3489: static int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6356) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6357) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
  (<0>,6358) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6360) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6362) tree.c:3493: rdp->n_rcu_pending++;
  (<0>,6363) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6364) tree.c:3496: check_cpu_stall(rsp, rdp);
  (<0>,6399)
  (<0>,6400) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6401) tree.c:1459: static void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6404) tree.c:1469: !rcu_gp_in_progress(rsp))
  (<0>,6417)
  (<0>,6418) tree.c:237: static int rcu_gp_in_progress(struct rcu_state *rsp)
  (<0>,6423) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6424) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6425) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6426) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6428) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6430) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6431) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6433) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6436) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6437) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6438) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6439) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6444) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6445) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6446) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6447) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6449) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6451) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6452) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6454) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6457) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6458) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6459) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6467) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
  (<0>,6470) tree_plugin.h:2923: return false;
  (<0>,6473) tree.c:3503: if (rcu_scheduler_fully_active &&
  (<0>,6476) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6478) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
  (<0>,6481) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6483) tree.c:3507: } else if (rdp->core_needs_qs &&
  (<0>,6487) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
  (<0>,6490)
  (<0>,6491) tree.c:614: cpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)
  (<0>,6493) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6496) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,6503) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6504) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,6515)
  (<0>,6516) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6517) tree.c:648: cpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)
  (<0>,6530)
  (<0>,6531) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6536) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6537) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6538) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6539) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6541) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6543) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6544) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6546) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6549) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6550) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6551) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6552) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6557) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6560) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6562) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6564) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6565) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6567) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6570) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6571) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6572) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,6578) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,6594)
  (<0>,6595) tree.c:633: static int rcu_future_needs_gp(struct rcu_state *rsp)
  (<0>,6598)
  (<0>,6599) tree.c:625: return &rsp->node[0];
  (<0>,6603) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,6604) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6609) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6610) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6611) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6612) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6614) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6616) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6617) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6619) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6622) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6623) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6624) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6628) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,6629) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6631) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6634) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,6635) tree.c:639: return READ_ONCE(*fp);
  (<0>,6639) tree.c:639: return READ_ONCE(*fp);
  (<0>,6640) tree.c:639: return READ_ONCE(*fp);
  (<0>,6641) tree.c:639: return READ_ONCE(*fp);
  (<0>,6642) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6644) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6646) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6647) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6649) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6652) tree.c:639: return READ_ONCE(*fp);
  (<0>,6653) tree.c:639: return READ_ONCE(*fp);
  (<0>,6654) tree.c:639: return READ_ONCE(*fp);
  (<0>,6658) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6661) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,6664) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6667) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6668) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,6671) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6673) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6676) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6679) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6682) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6683) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6685) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6688) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6692) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6694) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6696) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6699) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6702) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6705) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6706) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6708) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6711) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,6715) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6717) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6719) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,6722) tree.c:665: return false; /* No grace period needed. */
  (<0>,6724) tree.c:666: }
  (<0>,6727) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6732) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6733) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6734) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6735) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6737) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6739) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6740) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6742) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6745) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6746) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6747) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6748) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6750) tree.c:3527: if (READ_ONCE(rnp->completed) != rdp->completed) { /* outside lock */
  (<0>,6753) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6758) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6759) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6760) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6761) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6763) fake_defs.h:266: __READ_ONCE_SIZE;
      (<0.1>,1)
    (<0.0>,1)
    (<0.0>,3)
    (<0.0>,4) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,6) litmus.c:99: struct rcu_state *rsp = arg;
    (<0.0>,7) litmus.c:101: set_cpu(cpu0);
    (<0.0>,10)
    (<0.0>,11) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,12) fake_sched.h:56: __running_cpu = cpu;
    (<0.0>,14) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,16) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,17) litmus.c:102: current = rsp->gp_kthread; /* rcu_gp_kthread must not wake itself */
    (<0.0>,19) fake_sched.h:43: return __running_cpu;
    (<0.0>,23)
    (<0.0>,24) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,2)
      (<0.1>,3) litmus.c:84: set_cpu(cpu0);
      (<0.1>,6)
      (<0.1>,7) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,8) fake_sched.h:56: __running_cpu = cpu;
      (<0.1>,11) fake_sched.h:43: return __running_cpu;
      (<0.1>,15)
      (<0.1>,16) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,19) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,24) tree.c:892: local_irq_save(flags);
      (<0.1>,27) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,29) fake_sched.h:43: return __running_cpu;
      (<0.1>,33) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,35) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,39) fake_sched.h:43: return __running_cpu;
      (<0.1>,43) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,55) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,57) fake_sched.h:43: return __running_cpu;
      (<0.1>,61) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,62) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,64) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,65) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,66) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,67) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,68) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,69) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,70) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,74) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,76) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,77) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,78) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,89)
      (<0.1>,90) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,92) fake_sched.h:43: return __running_cpu;
      (<0.1>,96) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,100) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,103) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,104) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,105) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,107) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,110) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,111) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,112) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,113) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,123) tree_plugin.h:2883: }
      (<0.1>,125) tree.c:895: local_irq_restore(flags);
      (<0.1>,128) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,130) fake_sched.h:43: return __running_cpu;
      (<0.1>,134) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,140) fake_sched.h:43: return __running_cpu;
      (<0.1>,144) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,151) litmus.c:87: x = 1;
      (<0.1>,163) tree.c:3255: ret = num_online_cpus() <= 1;
      (<0.1>,165) tree.c:3257: return ret;
      (<0.1>,172) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,175) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,176) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,177) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,178) update.c:147: return rcu_expedited || atomic_read(&rcu_expedited_nesting) ||
      (<0.1>,181) update.c:148: rcu_scheduler_active == RCU_SCHEDULER_INIT;
      (<0.1>,198)
      (<0.1>,199)
      (<0.1>,200)
      (<0.1>,201)
      (<0.1>,202) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,204) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,205) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,208) update.c:352: if (checktiny &&
      (<0.1>,211) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,213) update.c:358: init_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,218) rcupdate.h:476: }
      (<0.1>,220) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,222) update.c:359: init_completion(&rs_array[i].completion);
      (<0.1>,227)
      (<0.1>,228) fake_sync.h:272: x->done = 0;
      (<0.1>,230) fake_sync.h:272: x->done = 0;
      (<0.1>,234) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,236) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,238) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,239) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,241) update.c:360: (crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);
      (<0.1>,247)
      (<0.1>,248)
      (<0.1>,249) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,250) tree.c:3213: __call_rcu(head, func, &rcu_sched_state, -1, 0);
      (<0.1>,281)
      (<0.1>,282)
      (<0.1>,283)
      (<0.1>,284)
      (<0.1>,286)
      (<0.1>,287) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,294) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,295) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,301) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,302) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,303) tree.c:3146: WARN_ON_ONCE((unsigned long)head & 0x1); /* Misaligned rcu_head! */
      (<0.1>,304) tree.c:3147: if (debug_rcu_head_queue(head)) {
      (<0.1>,307) rcu.h:92: return 0;
      (<0.1>,311) tree.c:3153: head->func = func;
      (<0.1>,312) tree.c:3153: head->func = func;
      (<0.1>,315) tree.c:3153: head->func = func;
      (<0.1>,316) tree.c:3154: head->next = NULL;
      (<0.1>,318) tree.c:3154: head->next = NULL;
      (<0.1>,319) tree.c:3162: local_irq_save(flags);
      (<0.1>,322) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,324) fake_sched.h:43: return __running_cpu;
      (<0.1>,328) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,330) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,334) fake_sched.h:43: return __running_cpu;
      (<0.1>,338) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,344) fake_sched.h:43: return __running_cpu;
      (<0.1>,347) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,349) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,351) tree.c:3163: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,352) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,355) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,363) tree.c:3166: if (unlikely(rdp->nxttail[RCU_NEXT_TAIL] == NULL) || cpu != -1) {
      (<0.1>,367) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,369) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,371) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,372) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,377) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,378) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,379) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,380) fake_defs.h:237: switch (size) {
      (<0.1>,382) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,384) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,385) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,387) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
      (<0.1>,390) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,391) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,392) tree.c:3188: WRITE_ONCE(rdp->qlen, rdp->qlen + 1);
      (<0.1>,393) tree.c:3189: if (lazy)
      (<0.1>,400) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,401) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,404) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,405) tree.c:3194: *rdp->nxttail[RCU_NEXT_TAIL] = head;
      (<0.1>,406) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,408) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,411) tree.c:3195: rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
      (<0.1>,412) tree.c:3197: if (__is_kfree_rcu_offset((unsigned long)func))
      (<0.1>,419) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,420) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,421) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,422) tree.c:3204: __call_rcu_core(rsp, rdp, head, flags);
      (<0.1>,430)
      (<0.1>,431)
      (<0.1>,432)
      (<0.1>,433) tree.c:3086: if (!rcu_is_watching())
      (<0.1>,440) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,442) fake_sched.h:43: return __running_cpu;
      (<0.1>,448) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,449) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,450) tree.c:1046: return atomic_read(&rcu_dynticks[get_cpu()].dynticks) & 0x1;
      (<0.1>,455) tree.c:1060: ret = __rcu_is_watching();
      (<0.1>,457) tree.c:1062: return ret;
      (<0.1>,461) tree.c:3090: if (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))
      (<0.1>,464) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,466) fake_sched.h:43: return __running_cpu;
      (<0.1>,470) fake_sched.h:165: return !!local_irq_depth[get_cpu()];
      (<0.1>,480) tree.c:3205: local_irq_restore(flags);
      (<0.1>,483) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,485) fake_sched.h:43: return __running_cpu;
      (<0.1>,489) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,491) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,495) fake_sched.h:43: return __running_cpu;
      (<0.1>,499) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,508) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,510) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,512) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,513) update.c:351: for (i = 0; i < n; i++) {
      (<0.1>,516) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,518) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,519) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,522) update.c:365: if (checktiny &&
      (<0.1>,525) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,527) update.c:369: wait_for_completion(&rs_array[i].completion);
      (<0.1>,532) fake_sync.h:278: might_sleep();
      (<0.1>,536) fake_sched.h:43: return __running_cpu;
      (<0.1>,540) fake_sched.h:96: rcu_idle_enter();
      (<0.1>,543) tree.c:755: local_irq_save(flags);
      (<0.1>,546) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,548) fake_sched.h:43: return __running_cpu;
      (<0.1>,552) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,554) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,558) fake_sched.h:43: return __running_cpu;
      (<0.1>,562) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,574) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,576) fake_sched.h:43: return __running_cpu;
      (<0.1>,580) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,581) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,583) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,584) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,585) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,586) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,587) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,588) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,589) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,593) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,595) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,596) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,597) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,613)
      (<0.1>,615) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,617) fake_sched.h:43: return __running_cpu;
      (<0.1>,621) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,624) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,625) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,626) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,630) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,631) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,632) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,634) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,639) fake_sched.h:43: return __running_cpu;
      (<0.1>,642) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,644) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,646) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,647) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,650) tree_plugin.h:2457: }
      (<0.1>,653) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,656) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,657) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,658) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,662) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,663) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,664) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,666) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,671) fake_sched.h:43: return __running_cpu;
      (<0.1>,674) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,676) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,678) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,679) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,682) tree_plugin.h:2457: }
      (<0.1>,685) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,688) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,689) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,690) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,694) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,695) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,696) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,698) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,705) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,708) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,709) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,710) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,712) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,713) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,715) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,716) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,717) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,718) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,732) tree_plugin.h:2879: }
      (<0.1>,734) tree.c:758: local_irq_restore(flags);
      (<0.1>,737) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,739) fake_sched.h:43: return __running_cpu;
      (<0.1>,743) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,745) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,749) fake_sched.h:43: return __running_cpu;
      (<0.1>,753) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,759) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,762) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,27) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,32) tree.c:892: local_irq_save(flags);
    (<0.0>,35) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,37) fake_sched.h:43: return __running_cpu;
    (<0.0>,41) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,43) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,47) fake_sched.h:43: return __running_cpu;
    (<0.0>,51) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,63) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,65) fake_sched.h:43: return __running_cpu;
    (<0.0>,69) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,70) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,72) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,73) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,74) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,75) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,76) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,77) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,78) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,82) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,84) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,85) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,86) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,97)
    (<0.0>,98) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,100) fake_sched.h:43: return __running_cpu;
    (<0.0>,104) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,108) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,111) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,112) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,113) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,115) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,116) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,118) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,119) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,120) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,121) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,131) tree_plugin.h:2883: }
    (<0.0>,133) tree.c:895: local_irq_restore(flags);
    (<0.0>,136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,138) fake_sched.h:43: return __running_cpu;
    (<0.0>,142) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,144) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,148) fake_sched.h:43: return __running_cpu;
    (<0.0>,152) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,159) litmus.c:106: rcu_gp_kthread(rsp);
    (<0.0>,207)
    (<0.0>,208) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,210) tree.c:2186: struct rcu_state *rsp = arg;
    (<0.0>,211) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,214)
    (<0.0>,215) tree.c:625: return &rsp->node[0];
    (<0.0>,219) tree.c:2187: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,227) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,229) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,233) fake_sched.h:43: return __running_cpu;
    (<0.0>,237) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,241) fake_sched.h:43: return __running_cpu;
    (<0.0>,245) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,250) fake_sched.h:43: return __running_cpu;
    (<0.0>,254) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,265) fake_sched.h:43: return __running_cpu;
    (<0.0>,269) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,270) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,272) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,273) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,274) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,276) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,278) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,279) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,280) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,281) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,282) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,283) tree.c:942: if (oldval)
    (<0.0>,291) tree_plugin.h:2883: }
    (<0.0>,297) tree.c:2858: trace_rcu_utilization(TPS("Start scheduler-tick"));
    (<0.0>,306) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,307) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,308) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,312) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,313) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,314) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,316) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,321) fake_sched.h:43: return __running_cpu;
    (<0.0>,324) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,326) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,329) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,331) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,333) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,336) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,337) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,338) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,342) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,343) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,344) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,346) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,351) fake_sched.h:43: return __running_cpu;
    (<0.0>,354) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,356) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,359) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,361) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,363) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,366) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,367) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,368) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,372) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,373) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,374) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,376) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,381) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,386) fake_sched.h:43: return __running_cpu;
    (<0.0>,391) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,399) fake_sched.h:43: return __running_cpu;
    (<0.0>,405) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,411) fake_sched.h:43: return __running_cpu;
    (<0.0>,418) tree.c:272: rcu_bh_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,431) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,432) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,433) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,437) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,438) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,439) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,441) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,445) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,447) fake_sched.h:43: return __running_cpu;
    (<0.0>,450) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,452) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,474)
    (<0.0>,475)
    (<0.0>,476) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,478) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,479) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,480) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,482) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,484) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,485) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,486) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,521)
    (<0.0>,522)
    (<0.0>,523) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,526) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,539)
    (<0.0>,540) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,545) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,546) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,547) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,548) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,550) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,552) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,553) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,555) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,560) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,561) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,566) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,567) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,569) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,571) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,573) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,574) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,576) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,579) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,580) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,581) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,589) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,592) tree_plugin.h:2923: return false;
    (<0.0>,595) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,598) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,600) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,603) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,605) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,609) tree.c:3515: if (cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,612)
    (<0.0>,613) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,615) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,618) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,625) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,626) tree.c:3521: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,637)
    (<0.0>,638)
    (<0.0>,639) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,652)
    (<0.0>,653) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,658) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,659) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,660) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,661) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,663) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,665) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,666) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,668) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,671) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,672) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,673) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,674) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,679) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,680) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,681) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,682) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,684) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,686) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,692) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,693) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,694) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,700) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,716)
    (<0.0>,717) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,720)
    (<0.0>,721) tree.c:625: return &rsp->node[0];
    (<0.0>,725) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,726) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,731) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,732) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,733) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,734) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,736) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,739) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,744) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,745) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,746) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,750) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,751) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,753) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,756) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,757) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,761) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,762) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,763) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,774) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,775) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,776) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,780) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,783) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,786) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,789) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,790) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,793) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,795) tree.c:666: }
    (<0.0>,798) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,800) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,802) tree.c:3522: rdp->n_rp_cpu_needs_gp++;
    (<0.0>,803) tree.c:3523: return 1;
    (<0.0>,805) tree.c:3548: }
    (<0.0>,809) tree.c:3561: return 1;
    (<0.0>,811) tree.c:3563: }
    (<0.0>,818) fake_sched.h:43: return __running_cpu;
    (<0.0>,822) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,826) tree.c:2891: if (user)
    (<0.0>,834) fake_sched.h:43: return __running_cpu;
    (<0.0>,838) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,840) fake_sched.h:43: return __running_cpu;
    (<0.0>,844) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,850) fake_sched.h:43: return __running_cpu;
    (<0.0>,854) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,864) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
    (<0.0>,867) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,868) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,869) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,873) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,874) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,875) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,877) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,881) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,893) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,895) fake_sched.h:43: return __running_cpu;
    (<0.0>,898) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,900) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,902) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,903) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,905) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,912) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,913) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,915) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,921) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,922) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,923) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,924) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,925) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,929)
    (<0.0>,930)
    (<0.0>,931) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,932) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,957)
    (<0.0>,958)
    (<0.0>,959) tree.c:1905: local_irq_save(flags);
    (<0.0>,962) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,964) fake_sched.h:43: return __running_cpu;
    (<0.0>,968) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,970) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,974) fake_sched.h:43: return __running_cpu;
    (<0.0>,978) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,983) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,985) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,986) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,987) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,989) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,990) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,995) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,996) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,997) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,998) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1000) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1002) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1003) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1005) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1008) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1009) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1010) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,1013) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1015) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1016) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1021) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1022) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1023) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1024) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1026) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1028) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1029) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1031) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1034) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1035) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1036) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,1039) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1043) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1044) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1045) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1046) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1048) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1050) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1054) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1057) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1058) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,1066) tree.c:1911: local_irq_restore(flags);
    (<0.0>,1069) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1071) fake_sched.h:43: return __running_cpu;
    (<0.0>,1075) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1077) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,1081) fake_sched.h:43: return __running_cpu;
    (<0.0>,1085) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,1092) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1094) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,1099) tree.c:3016: local_irq_save(flags);
    (<0.0>,1102) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1104) fake_sched.h:43: return __running_cpu;
    (<0.0>,1108) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1110) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,1114) fake_sched.h:43: return __running_cpu;
    (<0.0>,1118) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,1123) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1124) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,1135)
    (<0.0>,1136)
    (<0.0>,1137) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,1150)
    (<0.0>,1151) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1156) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1157) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1158) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1161) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1163) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1164) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1166) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1169) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1170) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1171) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1172) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1177) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1178) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1179) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1180) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1182) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1184) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1185) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1187) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1190) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1191) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1192) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,1198) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,1214)
    (<0.0>,1215) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1218)
    (<0.0>,1219) tree.c:625: return &rsp->node[0];
    (<0.0>,1223) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1224) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1229) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1230) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1231) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1232) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1234) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1236) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1237) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1239) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1242) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1243) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1244) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1248) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,1249) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1251) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1254) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,1255) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1259) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1260) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1261) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1262) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1264) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1266) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1267) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1269) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1272) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1273) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1274) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,1278) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1281) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1284) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1287) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1288) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,1291) tree.c:659: return true;  /* Yes, CPU has newly registered callbacks. */
    (<0.0>,1293) tree.c:666: }
    (<0.0>,1296) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,1299)
    (<0.0>,1300) tree.c:625: return &rsp->node[0];
    (<0.0>,1306)
    (<0.0>,1307) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,1311) fake_sync.h:115: preempt_disable();
    (<0.0>,1313) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1314) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,1321) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,1327) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1329) fake_sched.h:43: return __running_cpu;
    (<0.0>,1332) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1334) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1336) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,1337) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1340)
    (<0.0>,1341) tree.c:625: return &rsp->node[0];
    (<0.0>,1345) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,1346) tree.c:2334: bool ret = false;
    (<0.0>,1347) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1348) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1349) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,1357)
    (<0.0>,1358)
    (<0.0>,1359)
    (<0.0>,1360) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1363) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1366) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1369) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1370) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1373) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1375) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,1378) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1380) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1381) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1383) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1386) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,1391) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1393) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1394) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,1397) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1399) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1402) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1404) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1407) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1408) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1411) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1414) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1416) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1419) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1420) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1422) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1425) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1426) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1428) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1431) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1432) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1434) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1437) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1439) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1441) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1442) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1444) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1446) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1449) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1451) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1454) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1455) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1458) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,1461) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1463) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1466) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1467) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1469) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1472) tree.c:1837: rdp->nxttail[j] = rdp->nxttail[i];
    (<0.0>,1473) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1475) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1478) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1479) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1481) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1484) tree.c:1838: rdp->nxtcompleted[j] = rdp->nxtcompleted[i];
    (<0.0>,1486) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1488) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1489) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1491) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1493) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,1496) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1497) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1498) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,1507)
    (<0.0>,1508)
    (<0.0>,1509)
    (<0.0>,1510) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1513) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1516) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1519) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1520) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,1523) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1524) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1529)
    (<0.0>,1530)
    (<0.0>,1531) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1534)
    (<0.0>,1535) tree.c:625: return &rsp->node[0];
    (<0.0>,1539) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1542) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1544) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1545) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1547) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1550) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1552) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1554) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1556) tree.c:1585: }
    (<0.0>,1558) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,1559) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1561) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1564) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1566) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1569) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1570) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1573) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1576) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1580) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1582) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1584) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1587) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1589) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1592) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1593) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1596) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1599) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,1603) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1605) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1607) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,1610) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1612) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,1616) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1619) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1622) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1623) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1625) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1628) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1629) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1630) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1632) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1635) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1637) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1639) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1641) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1644) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1647) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1648) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1650) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1653) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1654) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1655) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1657) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1660) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1662) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1664) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1666) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1669) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1672) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1673) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1675) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1678) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,1679) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1680) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1682) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1685) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,1687) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1689) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1691) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,1694) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1695) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,1714)
    (<0.0>,1715)
    (<0.0>,1716)
    (<0.0>,1717) tree.c:1613: bool ret = false;
    (<0.0>,1718) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1720) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1723)
    (<0.0>,1724) tree.c:625: return &rsp->node[0];
    (<0.0>,1728) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,1729) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1731) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1732) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1737)
    (<0.0>,1738)
    (<0.0>,1739) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1742)
    (<0.0>,1743) tree.c:625: return &rsp->node[0];
    (<0.0>,1747) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1750) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1752) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1753) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1755) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1758) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1760) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1762) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1764) tree.c:1585: }
    (<0.0>,1766) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,1767) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1768) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1769) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,1775)
    (<0.0>,1776)
    (<0.0>,1777)
    (<0.0>,1778) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1782) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1784) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1787) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,1790) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1792) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1793) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1795) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,1798) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1803) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1804) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1805) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1806) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1808) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1810) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1811) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1813) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1816) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1817) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1818) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1819) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1824) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1825) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1826) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1831) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1834) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,1837) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1838) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1839) tree.c:1641: READ_ONCE(rnp_root->gpnum) != READ_ONCE(rnp_root->completed)) {
    (<0.0>,1842) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1843) tree.c:1652: if (rnp != rnp_root)
    (<0.0>,1846) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1848) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1849) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1854)
    (<0.0>,1855)
    (<0.0>,1856) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1859)
    (<0.0>,1860) tree.c:625: return &rsp->node[0];
    (<0.0>,1864) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1867) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1869) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1870) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1872) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,1875) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1877) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1879) tree.c:1578: return rnp->completed + 1;
    (<0.0>,1881) tree.c:1585: }
    (<0.0>,1883) tree.c:1661: c = rcu_cbs_completed(rdp->rsp, rnp_root);
    (<0.0>,1884) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1886) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1889) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1890) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1892) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1895) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1899) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1900) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1902) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1905) tree.c:1664: rdp->nxtcompleted[i] = c;
    (<0.0>,1908) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1910) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1912) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1915) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1916) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1918) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1921) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1926) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1928) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1930) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1933) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1934) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1936) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1939) tree.c:1663: if (ULONG_CMP_LT(c, rdp->nxtcompleted[i]))
    (<0.0>,1944) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1946) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1948) tree.c:1662: for (i = RCU_DONE_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,1951) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1953) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1956) tree.c:1670: if (rnp_root->need_future_gp[c & 0x1]) {
    (<0.0>,1959) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1961) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1964) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1966) tree.c:1676: rnp_root->need_future_gp[c & 0x1]++;
    (<0.0>,1967) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1969) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1970) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1972) tree.c:1679: if (rnp_root->gpnum != rnp_root->completed) {
    (<0.0>,1975) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1976) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1977) tree.c:1682: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedroot"));
    (<0.0>,1983)
    (<0.0>,1984)
    (<0.0>,1985)
    (<0.0>,1986) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,1990) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1992) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1993) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,1994) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2005)
    (<0.0>,2006)
    (<0.0>,2007)
    (<0.0>,2008) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2010) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2013) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2014) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2025)
    (<0.0>,2026)
    (<0.0>,2027) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2040)
    (<0.0>,2041) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2046) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2047) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2048) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2053) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2054) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2056) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2059) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2060) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2061) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2062) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2067) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2068) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2069) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2070) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2072) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2074) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2075) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2077) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2080) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2081) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2082) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2088) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2104)
    (<0.0>,2105) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2108)
    (<0.0>,2109) tree.c:625: return &rsp->node[0];
    (<0.0>,2113) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2114) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2119) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2120) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2121) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2122) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2124) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2126) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2127) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2129) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2132) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2133) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2134) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2138) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2139) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2141) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2144) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2145) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2149) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2150) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2151) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2152) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2154) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2156) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2157) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2162) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2163) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2164) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2168) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2170) tree.c:666: }
    (<0.0>,2175) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2180) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2181) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2182) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2183) fake_defs.h:237: switch (size) {
    (<0.0>,2185) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2187) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2188) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2190) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2193) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2194) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2195) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2198) tree.c:2318: return true;
    (<0.0>,2200) tree.c:2319: }
    (<0.0>,2203) tree.c:1683: ret = rcu_start_gp_advanced(rdp->rsp, rnp_root, rdp);
    (<0.0>,2206) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2207) tree.c:1686: if (rnp != rnp_root)
    (<0.0>,2211) tree.c:1689: if (c_out != NULL)
    (<0.0>,2214) tree.c:1691: return ret;
    (<0.0>,2218) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,2219) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2222) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2223) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,2229) tree.c:1798: return ret;
    (<0.0>,2231) tree.c:1798: return ret;
    (<0.0>,2233) tree.c:1799: }
    (<0.0>,2235) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,2237) tree.c:1843: }
    (<0.0>,2241) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,2242) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2243) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2244) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2255)
    (<0.0>,2256)
    (<0.0>,2257)
    (<0.0>,2258) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2260) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2263) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2264) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2275)
    (<0.0>,2276)
    (<0.0>,2277) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2290)
    (<0.0>,2291) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2296) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2297) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2298) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2299) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2301) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2303) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2304) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2306) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2309) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2310) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2311) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2312) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2317) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2318) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2320) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2322) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2324) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2325) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2327) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2330) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2331) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2332) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2338) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2354)
    (<0.0>,2355) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2358)
    (<0.0>,2359) tree.c:625: return &rsp->node[0];
    (<0.0>,2363) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2364) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2369) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2370) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2371) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2372) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2374) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2376) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2377) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2379) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2382) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2383) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2384) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2388) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2389) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2391) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2394) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2395) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2399) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2400) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2401) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2407) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2409) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2412) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2413) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2414) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2418) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,2420) tree.c:666: }
    (<0.0>,2425) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2430) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2431) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2432) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2433) fake_defs.h:237: switch (size) {
    (<0.0>,2435) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2437) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2438) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2440) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,2443) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2444) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2445) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,2448) tree.c:2318: return true;
    (<0.0>,2450) tree.c:2319: }
    (<0.0>,2454) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,2455) tree.c:2346: return ret;
    (<0.0>,2459) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,2463) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2464) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2465) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2468)
    (<0.0>,2469) tree.c:625: return &rsp->node[0];
    (<0.0>,2474) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,2478)
    (<0.0>,2479)
    (<0.0>,2480) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2481) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,2484) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,2487) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2489) fake_sched.h:43: return __running_cpu;
    (<0.0>,2493) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2495) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2499) fake_sched.h:43: return __running_cpu;
    (<0.0>,2503) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2511) tree.c:3021: if (needwake)
    (<0.0>,2514) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,2522)
    (<0.0>,2523) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2524) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2526) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,2533) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,2536)
    (<0.0>,2537) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2539) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2542) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,2549) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,2552) tree_plugin.h:2457: }
    (<0.0>,2556) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2559) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2560) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2561) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2565) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2566) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2567) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2569) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,2573) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,2585) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2587) fake_sched.h:43: return __running_cpu;
    (<0.0>,2590) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2592) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2594) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,2595) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2597) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2604) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2605) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2607) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2613) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2614) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2615) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,2616) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2617) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,2621)
    (<0.0>,2622)
    (<0.0>,2623) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2624) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,2649)
    (<0.0>,2650)
    (<0.0>,2651) tree.c:1905: local_irq_save(flags);
    (<0.0>,2654) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2656) fake_sched.h:43: return __running_cpu;
    (<0.0>,2660) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2662) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2666) fake_sched.h:43: return __running_cpu;
    (<0.0>,2670) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2675) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2677) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2678) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,2679) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2681) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2682) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2687) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2688) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2689) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2690) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2695) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2697) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2700) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2701) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2702) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,2705) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2707) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2708) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2713) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2714) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2715) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2716) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2718) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2720) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2721) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2723) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2726) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2727) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2728) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,2731) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2735) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2736) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2737) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2738) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2740) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2741) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2742) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2743) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2746) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2749) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2750) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,2758) tree.c:1911: local_irq_restore(flags);
    (<0.0>,2761) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2763) fake_sched.h:43: return __running_cpu;
    (<0.0>,2767) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2769) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,2773) fake_sched.h:43: return __running_cpu;
    (<0.0>,2777) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,2784) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2786) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,2791) tree.c:3016: local_irq_save(flags);
    (<0.0>,2794) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2796) fake_sched.h:43: return __running_cpu;
    (<0.0>,2800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2802) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,2806) fake_sched.h:43: return __running_cpu;
    (<0.0>,2810) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,2815) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2816) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,2827)
    (<0.0>,2828)
    (<0.0>,2829) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,2842)
    (<0.0>,2843) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2849) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2850) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2855) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2858) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2862) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2863) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2864) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2869) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2870) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2871) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2872) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2877) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2879) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2882) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2883) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2884) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,2890) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,2906)
    (<0.0>,2907) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2910)
    (<0.0>,2911) tree.c:625: return &rsp->node[0];
    (<0.0>,2915) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,2916) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2921) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2922) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2923) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2924) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2926) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2928) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2929) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2931) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2934) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2935) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2936) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2940) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,2941) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2943) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2946) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,2947) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2951) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2952) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2953) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2954) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2956) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2959) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2961) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,2964) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2965) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2966) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,2970) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2973) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,2976) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2979) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2980) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,2983) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2985) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,2988) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2991) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2994) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2995) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,2997) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3000) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3004) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3006) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3008) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3011) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3014) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3017) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3018) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3020) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3023) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,3027) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3029) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3031) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,3034) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,3036) tree.c:666: }
    (<0.0>,3039) tree.c:3024: local_irq_restore(flags);
    (<0.0>,3042) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3044) fake_sched.h:43: return __running_cpu;
    (<0.0>,3048) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3050) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3054) fake_sched.h:43: return __running_cpu;
    (<0.0>,3058) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3064) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,3067)
    (<0.0>,3068) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3070) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3073) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,3080) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3083) tree_plugin.h:2457: }
    (<0.0>,3087) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3090) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3091) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3092) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3096) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3097) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3098) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3100) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,3108) fake_sched.h:43: return __running_cpu;
    (<0.0>,3112) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,3122) fake_sched.h:43: return __running_cpu;
    (<0.0>,3126) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3127) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3129) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3130) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,3131) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3133) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3135) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,3136) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3137) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3138) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3139) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3140) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3142) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,3150) tree_plugin.h:2879: }
    (<0.0>,3156) fake_sched.h:43: return __running_cpu;
    (<0.0>,3160) fake_sched.h:96: rcu_idle_enter();
    (<0.0>,3163) tree.c:755: local_irq_save(flags);
    (<0.0>,3166) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3168) fake_sched.h:43: return __running_cpu;
    (<0.0>,3172) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3174) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3178) fake_sched.h:43: return __running_cpu;
    (<0.0>,3182) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3194) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3196) fake_sched.h:43: return __running_cpu;
    (<0.0>,3200) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3201) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3203) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3204) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,3205) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3206) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3207) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3208) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3209) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,3213) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3215) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,3216) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3217) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,3233)
    (<0.0>,3235) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3237) fake_sched.h:43: return __running_cpu;
    (<0.0>,3241) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3244) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3245) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3246) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3250) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3251) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3252) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3254) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3259) fake_sched.h:43: return __running_cpu;
    (<0.0>,3262) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3264) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3266) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3267) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3270) tree_plugin.h:2457: }
    (<0.0>,3273) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3276) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3277) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3278) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3282) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3283) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3284) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3286) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3291) fake_sched.h:43: return __running_cpu;
    (<0.0>,3294) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3296) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3298) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,3299) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,3302) tree_plugin.h:2457: }
    (<0.0>,3305) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3308) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3309) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3310) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3314) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3315) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3316) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3318) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,3325) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3328) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3329) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3330) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3332) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3333) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,3335) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3336) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3337) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3338) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3352) tree_plugin.h:2879: }
    (<0.0>,3354) tree.c:758: local_irq_restore(flags);
    (<0.0>,3357) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3359) fake_sched.h:43: return __running_cpu;
    (<0.0>,3363) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3365) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3369) fake_sched.h:43: return __running_cpu;
    (<0.0>,3373) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3379) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3382) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,3387) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3392) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3393) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3394) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3395) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3397) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3399) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3400) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3402) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3405) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3406) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3407) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,3414) fake_sched.h:43: return __running_cpu;
    (<0.0>,3418)
    (<0.0>,3419) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3422) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,3427) tree.c:892: local_irq_save(flags);
    (<0.0>,3430) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3432) fake_sched.h:43: return __running_cpu;
    (<0.0>,3436) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3438) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,3442) fake_sched.h:43: return __running_cpu;
    (<0.0>,3446) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3458) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3460) fake_sched.h:43: return __running_cpu;
    (<0.0>,3464) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3465) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3467) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3468) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,3469) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3470) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3471) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3472) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,3473) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,3477) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3479) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,3480) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3481) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,3492)
    (<0.0>,3493) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3495) fake_sched.h:43: return __running_cpu;
    (<0.0>,3499) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,3503) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3506) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3507) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3508) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3510) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3511) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,3513) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3514) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3515) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3516) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,3526) tree_plugin.h:2883: }
    (<0.0>,3528) tree.c:895: local_irq_restore(flags);
    (<0.0>,3531) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3533) fake_sched.h:43: return __running_cpu;
    (<0.0>,3537) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3539) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,3543) fake_sched.h:43: return __running_cpu;
    (<0.0>,3547) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,3554) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3556) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,3557) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,3602)
    (<0.0>,3603) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3606)
    (<0.0>,3607) tree.c:625: return &rsp->node[0];
    (<0.0>,3611) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,3613) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3614) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3615) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3620) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3621) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3622) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3623) fake_defs.h:237: switch (size) {
    (<0.0>,3625) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3627) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3628) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3630) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3633) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3634) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3635) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,3636) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,3639)
    (<0.0>,3640) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,3644) fake_sync.h:99: local_irq_disable();
    (<0.0>,3647) fake_sched.h:43: return __running_cpu;
    (<0.0>,3651) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,3655) fake_sched.h:43: return __running_cpu;
    (<0.0>,3659) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,3664) fake_sched.h:43: return __running_cpu;
    (<0.0>,3668) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,3671) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3672) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,3679) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3684) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3685) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3686) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3687) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3689) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3691) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3692) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3694) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3697) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3698) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3699) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,3710)
    (<0.0>,3716)
    (<0.0>,3721) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3726) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3727) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3728) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3729) fake_defs.h:237: switch (size) {
    (<0.0>,3731) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3733) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3734) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3736) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,3739) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3740) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3741) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,3742) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3755)
    (<0.0>,3756) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3761) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3762) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3763) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3764) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3766) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3768) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3769) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3771) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3774) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3775) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3776) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3777) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3782) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3783) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3784) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3785) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3787) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3789) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3790) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3792) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3795) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3796) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3797) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3805) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3806) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3819)
    (<0.0>,3820) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3825) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3826) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3827) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3830) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3833) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3835) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3838) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3839) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3840) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3841) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3846) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3847) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3848) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3849) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3851) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3854) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3856) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3859) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3860) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3861) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,3868) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3869) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3870) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,3873) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,3888)
    (<0.0>,3889) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3890) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,3891) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3892) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3894) tree.c:1240: rsp->gp_start = j;
    (<0.0>,3915) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3916) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3917) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3918) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3920) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3922) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3923) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3925) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3928) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3929) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3930) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3931) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,3932) update.c:466: if (till_stall_check < 3) {
    (<0.0>,3935) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,3939) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,3944) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,3946) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3947) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3949) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3950) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3955) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3956) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3957) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3958) fake_defs.h:237: switch (size) {
    (<0.0>,3960) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3962) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3963) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3965) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,3968) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3969) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3970) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,3971) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3972) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3975) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3977) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,3978) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3983) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3984) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3985) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3986) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3988) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3990) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3991) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3993) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,3996) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3997) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3998) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,3999) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4001) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,4005) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4007) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4009) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4010) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4012) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4013) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4014) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,4018) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4021)
    (<0.0>,4022) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4026)
    (<0.0>,4027) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4028) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4033) fake_sched.h:43: return __running_cpu;
    (<0.0>,4037) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4039) fake_sched.h:43: return __running_cpu;
    (<0.0>,4043) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4050) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4053) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4056) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4057) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4059) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4060) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4062) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4067) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4068) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,4072)
    (<0.0>,4073)
    (<0.0>,4074) tree.c:1922: if (delay > 0 &&
    (<0.0>,4078) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4081)
    (<0.0>,4082) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4086) fake_sync.h:99: local_irq_disable();
    (<0.0>,4089) fake_sched.h:43: return __running_cpu;
    (<0.0>,4093) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4097) fake_sched.h:43: return __running_cpu;
    (<0.0>,4101) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4106) fake_sched.h:43: return __running_cpu;
    (<0.0>,4110) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4113) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4114) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4121) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4123) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4124) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4126) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,4129) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4131) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4132) tree.c:1978: oldmask = rnp->qsmaskinit;
    (<0.0>,4133) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4135) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4136) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4138) tree.c:1979: rnp->qsmaskinit = rnp->qsmaskinitnext;
    (<0.0>,4139) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4143) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4145) tree.c:1982: if (!oldmask != !rnp->qsmaskinit) {
    (<0.0>,4151) tree.c:1983: if (!oldmask) /* First online CPU for this rcu_node. */
    (<0.0>,4154) tree.c:1984: rcu_init_new_rnp(rnp);
    (<0.0>,4159)
    (<0.0>,4160) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4161) tree.c:3747: struct rcu_node *rnp = rnp_leaf;
    (<0.0>,4163) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4165) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4166) tree.c:3750: mask = rnp->grpmask;
    (<0.0>,4167) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4169) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4170) tree.c:3751: rnp = rnp->parent;
    (<0.0>,4171) tree.c:3752: if (rnp == NULL)
    (<0.0>,4177) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4179) tree.c:2000: if (rnp->wait_blkd_tasks &&
    (<0.0>,4182) tree.c:2007: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4185)
    (<0.0>,4186) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4190)
    (<0.0>,4191) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4192) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4197) fake_sched.h:43: return __running_cpu;
    (<0.0>,4201) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4203) fake_sched.h:43: return __running_cpu;
    (<0.0>,4207) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4215) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4217) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4219) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4220) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4222) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,4227) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4230) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4232) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4233) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4235) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,4240) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4241) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,4245)
    (<0.0>,4246)
    (<0.0>,4247) tree.c:1922: if (delay > 0 &&
    (<0.0>,4251) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,4254)
    (<0.0>,4255) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4259) fake_sync.h:99: local_irq_disable();
    (<0.0>,4262) fake_sched.h:43: return __running_cpu;
    (<0.0>,4266) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,4270) fake_sched.h:43: return __running_cpu;
    (<0.0>,4274) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,4279) fake_sched.h:43: return __running_cpu;
    (<0.0>,4283) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,4286) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4287) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,4295) fake_sched.h:43: return __running_cpu;
    (<0.0>,4298) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4300) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4302) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,4303) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,4309)
    (<0.0>,4310) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4312) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4317) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4318) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4320) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4324) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4325) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4326) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,4328) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4330) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4331) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4333) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,4335) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4337) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4338) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4339) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4344) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4345) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4346) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4347) fake_defs.h:237: switch (size) {
    (<0.0>,4349) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4351) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4352) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4354) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
  (<0>,6765) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6766) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6768) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6771) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6772) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6773) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6774) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6776) tree.c:3533: if (READ_ONCE(rnp->gpnum) != rdp->gpnum ||
  (<0>,6779) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6781) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6783) tree.c:3535: rdp->n_rp_gp_started++;
  (<0>,6784) tree.c:3536: return 1;
  (<0>,6786) tree.c:3548: }
  (<0>,6790) tree.c:3561: return 1;
  (<0>,6792) tree.c:3563: }
  (<0>,6799) fake_sched.h:43: return __running_cpu;
  (<0>,6803) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
  (<0>,6807) tree.c:2891: if (user)
  (<0>,6815) fake_sched.h:43: return __running_cpu;
  (<0>,6819) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
  (<0>,6821) fake_sched.h:43: return __running_cpu;
  (<0>,6825) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,6831) fake_sched.h:43: return __running_cpu;
  (<0>,6835) fake_sched.h:183: if (need_softirq[get_cpu()]) {
  (<0>,6845) tree.c:3044: trace_rcu_utilization(TPS("Start RCU core"));
  (<0>,6848) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6849) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6850) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6854) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6855) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6856) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6858) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,6862) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,6874) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6876) fake_sched.h:43: return __running_cpu;
  (<0>,6879) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6881) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6883) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,6884) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6886) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6893) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6894) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6896) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6902) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6903) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6904) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,6905) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6906) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,6910)
  (<0>,6911)
  (<0>,6912) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,6913) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,6938)
  (<0>,6939)
  (<0>,6940) tree.c:1905: local_irq_save(flags);
  (<0>,6943) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6945) fake_sched.h:43: return __running_cpu;
  (<0>,6949) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6951) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,6955) fake_sched.h:43: return __running_cpu;
  (<0>,6959) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,6964) tree.c:1906: rnp = rdp->mynode;
  (<0>,6966) tree.c:1906: rnp = rdp->mynode;
  (<0>,6967) tree.c:1906: rnp = rdp->mynode;
  (<0>,6968) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6970) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6971) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6976) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6977) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6978) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6979) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6981) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6983) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6984) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6986) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,6989) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6990) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6991) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,6994) tree.c:1910: !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */
  (<0>,6998)
  (<0>,6999) tree.h:750: static inline bool raw_spin_trylock_rcu_node(struct rcu_node *rnp)
  (<0>,7004) fake_sync.h:129: preempt_disable();
  (<0>,7006) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
    (<0.0>,4355) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,4357) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4358) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4359) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,4360) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4362) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4363) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4365) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4370) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4371) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4373) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4374) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4376) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4380) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4381) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4382) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,4385) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4386) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4388) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,4391) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4392) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4393) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,4415)
    (<0.0>,4416)
    (<0.0>,4417)
    (<0.0>,4418) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4420) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4421) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4423) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,4426) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4430) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4431) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4432) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4433) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4435) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4436) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4437) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4438) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,4441) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4444) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4445) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4453) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4454) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4455) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4464)
    (<0.0>,4465)
    (<0.0>,4466)
    (<0.0>,4467) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4470) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4473) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4476) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4477) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,4480) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4481) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4486)
    (<0.0>,4487)
    (<0.0>,4488) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4491)
    (<0.0>,4492) tree.c:625: return &rsp->node[0];
    (<0.0>,4496) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4499) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4501) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4502) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4504) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4507) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4509) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4511) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4513) tree.c:1585: }
    (<0.0>,4515) tree.c:1766: c = rcu_cbs_completed(rsp, rnp);
    (<0.0>,4516) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4518) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4521) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4523) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4526) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4527) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4530) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4533) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4537) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4539) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4541) tree.c:1767: for (i = RCU_NEXT_TAIL - 1; i > RCU_DONE_TAIL; i--)
    (<0.0>,4544) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4546) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4549) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4550) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4553) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4556) tree.c:1768: if (rdp->nxttail[i] != rdp->nxttail[i - 1] &&
    (<0.0>,4559) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4561) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4564) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4565) tree.c:1769: !ULONG_CMP_GE(rdp->nxtcompleted[i], c))
    (<0.0>,4570) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4572) tree.c:1778: if (++i >= RCU_NEXT_TAIL)
    (<0.0>,4576) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4579) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4582) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4583) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4585) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4588) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4589) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4590) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4592) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4595) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4597) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4599) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4601) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4604) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4607) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4608) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4610) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4613) tree.c:1787: rdp->nxttail[i] = rdp->nxttail[RCU_NEXT_TAIL];
    (<0.0>,4614) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4615) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4617) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4620) tree.c:1788: rdp->nxtcompleted[i] = c;
    (<0.0>,4622) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4624) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4626) tree.c:1786: for (; i <= RCU_NEXT_TAIL; i++) {
    (<0.0>,4629) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4630) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4649)
    (<0.0>,4650)
    (<0.0>,4651)
    (<0.0>,4652) tree.c:1613: bool ret = false;
    (<0.0>,4653) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4655) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4658)
    (<0.0>,4659) tree.c:625: return &rsp->node[0];
    (<0.0>,4663) tree.c:1614: struct rcu_node *rnp_root = rcu_get_root(rdp->rsp);
    (<0.0>,4664) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4666) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4667) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4672)
    (<0.0>,4673)
    (<0.0>,4674) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4677)
    (<0.0>,4678) tree.c:625: return &rsp->node[0];
    (<0.0>,4682) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4685) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4687) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4688) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4690) tree.c:1577: if (rcu_get_root(rsp) == rnp && rnp->gpnum == rnp->completed)
    (<0.0>,4693) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4695) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4697) tree.c:1584: return rnp->completed + 2;
    (<0.0>,4699) tree.c:1585: }
    (<0.0>,4701) tree.c:1620: c = rcu_cbs_completed(rdp->rsp, rnp);
    (<0.0>,4702) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4703) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4704) tree.c:1621: trace_rcu_future_gp(rnp, rdp, c, TPS("Startleaf"));
    (<0.0>,4710)
    (<0.0>,4711)
    (<0.0>,4712)
    (<0.0>,4713) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4717) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4719) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4722) tree.c:1622: if (rnp->need_future_gp[c & 0x1]) {
    (<0.0>,4725) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4727) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4728) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4730) tree.c:1640: if (rnp->gpnum != rnp->completed ||
    (<0.0>,4733) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4735) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4738) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4740) tree.c:1642: rnp->need_future_gp[c & 0x1]++;
    (<0.0>,4741) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4742) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4743) tree.c:1643: trace_rcu_future_gp(rnp, rdp, c, TPS("Startedleaf"));
    (<0.0>,4749)
    (<0.0>,4750)
    (<0.0>,4751)
    (<0.0>,4752) tree.c:1594: trace_rcu_future_grace_period(rdp->rsp->name, rnp->gpnum,
    (<0.0>,4757) tree.c:1689: if (c_out != NULL)
    (<0.0>,4760) tree.c:1691: return ret;
    (<0.0>,4764) tree.c:1791: ret = rcu_start_future_gp(rnp, rdp, NULL);
    (<0.0>,4765) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4768) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4769) tree.c:1794: if (!*rdp->nxttail[RCU_WAIT_TAIL])
    (<0.0>,4775) tree.c:1798: return ret;
    (<0.0>,4777) tree.c:1798: return ret;
    (<0.0>,4779) tree.c:1799: }
    (<0.0>,4782) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,4784) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4786) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4787) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4789) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,4792) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4794) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4795) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4797) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,4800) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4802) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4803) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4805) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4811) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,4812) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4815) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4819) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,4821) fake_sched.h:43: return __running_cpu;
    (<0.0>,4825) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4826) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4828) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,4829) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4831) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4834) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,4835) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,4837) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,4839) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,4841) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,4843) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,4844) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,4847)
    (<0.0>,4848) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4850) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,4851) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4853) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,4863)
    (<0.0>,4868) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4872) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4873) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4874) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4875) fake_defs.h:237: switch (size) {
    (<0.0>,4877) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4878) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4879) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4880) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,4883) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4886) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4887) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,4890) tree.c:1896: return ret;
    (<0.0>,4894) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,4897) tree_plugin.h:1231: }
    (<0.0>,4901) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,4904)
    (<0.0>,4905) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,4909)
    (<0.0>,4910) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4911) fake_sync.h:107: if (pthread_mutex_unlock(l))
  (<0>,7007) fake_sync.h:130: if (pthread_mutex_trylock(l)) {
  (<0>,7010) fake_sync.h:134: return 1;
  (<0>,7012) fake_sync.h:135: }
  (<0>,7016) tree.h:752: bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
  (<0>,7017) tree.h:754: if (locked)
  (<0>,7023) tree.h:756: return locked;
  (<0>,7027) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7028) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7029) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7051)
  (<0>,7052)
  (<0>,7053)
  (<0>,7054) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7056) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7057) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7059) tree.c:1858: if (rdp->completed == rnp->completed &&
  (<0>,7062) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7066) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7067) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7068) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7069) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7071) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7072) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7073) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7074) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7077) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7080) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7081) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7089) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7090) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7091) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7100)
  (<0>,7101)
  (<0>,7102)
  (<0>,7103) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7106) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7109) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7112) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7113) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
  (<0>,7116) tree.c:1750: return false;
  (<0>,7118) tree.c:1799: }
  (<0>,7121) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
  (<0>,7123) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7125) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7126) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7128) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
  (<0>,7131) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7133) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7134) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7136) tree.c:1880: rdp->gpnum = rnp->gpnum;
  (<0>,7139) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7141) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7142) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7144) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7150) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
  (<0>,7151) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7154) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7158) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
  (<0>,7160) fake_sched.h:43: return __running_cpu;
  (<0>,7164) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7165) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7167) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
  (<0>,7168) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7170) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7173) tree.c:1888: rdp->core_needs_qs = need_gp;
  (<0>,7174) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
  (<0>,7176) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
  (<0>,7178) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
  (<0>,7180) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
  (<0>,7182) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
  (<0>,7183) tree.c:1893: zero_cpu_stall_ticks(rdp);
  (<0>,7186)
  (<0>,7187) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,7189) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
  (<0>,7190) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,7192) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
  (<0>,7202)
  (<0>,7207) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7211) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7212) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7213) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7214) fake_defs.h:237: switch (size) {
  (<0>,7216) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7217) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7218) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7219) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
  (<0>,7222) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7225) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7226) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
  (<0>,7229) tree.c:1896: return ret;
  (<0>,7233) tree.c:1914: needwake = __note_gp_changes(rsp, rnp, rdp);
  (<0>,7237) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7238) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7239) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7241) tree.c:1915: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
  (<0>,7245)
  (<0>,7246)
  (<0>,7247) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,7248) fake_sync.h:91: if (pthread_mutex_unlock(l))
  (<0>,7251) fake_sync.h:93: local_irq_restore(flags);
  (<0>,7254) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7256) fake_sched.h:43: return __running_cpu;
  (<0>,7260) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7262) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7266) fake_sched.h:43: return __running_cpu;
  (<0>,7270) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7278) tree.c:1916: if (needwake)
  (<0>,7282) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7284) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7287) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,7291) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
  (<0>,7295) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7297) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7299) fake_sched.h:43: return __running_cpu;
  (<0>,7303) tree.c:2547: rdp->rcu_qs_ctr_snap == __this_cpu_read(rcu_qs_ctr))
  (<0>,7308) tree.c:3016: local_irq_save(flags);
  (<0>,7311) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7313) fake_sched.h:43: return __running_cpu;
  (<0>,7317) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7319) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7323) fake_sched.h:43: return __running_cpu;
  (<0>,7327) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7332) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7333) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7344)
  (<0>,7345)
  (<0>,7346) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7359)
  (<0>,7360) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7365) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7366) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7367) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7368) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7370) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7372) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7373) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7375) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7378) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7379) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7380) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7381) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7386) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7387) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7388) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7389) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7391) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7393) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7394) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7396) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7399) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7400) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7401) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7407) tree.c:653: return false;  /* No, a grace period is already in progress. */
  (<0>,7409) tree.c:666: }
  (<0>,7412) tree.c:3024: local_irq_restore(flags);
  (<0>,7415) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7417) fake_sched.h:43: return __running_cpu;
  (<0>,7421) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7423) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7427) fake_sched.h:43: return __running_cpu;
  (<0>,7431) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7437) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7440)
  (<0>,7441) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7443) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7446) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7453) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,7456) tree_plugin.h:2457: }
  (<0>,7460) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7463) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7464) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7465) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7469) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7470) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7471) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7473) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7477) tree.c:3046: __rcu_process_callbacks(rsp);
  (<0>,7489) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7491) fake_sched.h:43: return __running_cpu;
  (<0>,7494) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7496) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7498) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
  (<0>,7499) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7501) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7508) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7509) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7511) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7517) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7518) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7519) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
  (<0>,7520) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7521) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
  (<0>,7525)
  (<0>,7526)
  (<0>,7527) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,7528) tree.c:2533: note_gp_changes(rsp, rdp);
  (<0>,7553)
  (<0>,7554)
  (<0>,7555) tree.c:1905: local_irq_save(flags);
  (<0>,7558) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7560) fake_sched.h:43: return __running_cpu;
  (<0>,7564) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7566) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7570) fake_sched.h:43: return __running_cpu;
  (<0>,7574) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7579) tree.c:1906: rnp = rdp->mynode;
  (<0>,7581) tree.c:1906: rnp = rdp->mynode;
  (<0>,7582) tree.c:1906: rnp = rdp->mynode;
  (<0>,7583) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7585) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7586) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7591) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7592) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7593) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7594) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7596) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7598) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7599) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7601) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7604) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7605) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7606) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
  (<0>,7609) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7611) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7612) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7617) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7618) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7619) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7620) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7622) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7624) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7625) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7627) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7630) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7631) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7632) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
  (<0>,7635) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7639) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7640) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7641) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7642) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7644) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7645) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7646) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7647) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7650) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7653) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7654) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
  (<0>,7662) tree.c:1911: local_irq_restore(flags);
  (<0>,7665) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7667) fake_sched.h:43: return __running_cpu;
  (<0>,7671) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7673) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7677) fake_sched.h:43: return __running_cpu;
  (<0>,7681) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7688) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7690) tree.c:2539: if (!rdp->core_needs_qs)
  (<0>,7695) tree.c:3016: local_irq_save(flags);
  (<0>,7698) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7700) fake_sched.h:43: return __running_cpu;
  (<0>,7704) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7706) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,7710) fake_sched.h:43: return __running_cpu;
  (<0>,7714) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,7719) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7720) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
  (<0>,7731)
  (<0>,7732)
  (<0>,7733) tree.c:652: if (rcu_gp_in_progress(rsp))
  (<0>,7746)
  (<0>,7747) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7752) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7753) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7754) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7755) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7757) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7759) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7760) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7762) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7765) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7766) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7767) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7768) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7773) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7774) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7775) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7776) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7778) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7780) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7781) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7783) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7786) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7787) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7788) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
  (<0>,7794) tree.c:654: if (rcu_future_needs_gp(rsp))
  (<0>,7810)
  (<0>,7811) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7814)
  (<0>,7815) tree.c:625: return &rsp->node[0];
  (<0>,7819) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
  (<0>,7820) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7825) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7826) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7827) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7828) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7830) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7832) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7833) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7835) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7838) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7839) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7840) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7844) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
  (<0>,7845) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7847) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7850) tree.c:637: int *fp = &rnp->need_future_gp[idx];
  (<0>,7851) tree.c:639: return READ_ONCE(*fp);
  (<0>,7855) tree.c:639: return READ_ONCE(*fp);
  (<0>,7856) tree.c:639: return READ_ONCE(*fp);
  (<0>,7857) tree.c:639: return READ_ONCE(*fp);
  (<0>,7858) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7860) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7862) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7863) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7865) fake_defs.h:266: __READ_ONCE_SIZE;
  (<0>,7868) tree.c:639: return READ_ONCE(*fp);
  (<0>,7869) tree.c:639: return READ_ONCE(*fp);
  (<0>,7870) tree.c:639: return READ_ONCE(*fp);
  (<0>,7874) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7877) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
  (<0>,7880) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7883) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7884) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
  (<0>,7887) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7889) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7892) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7895) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7898) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7899) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7901) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7904) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7908) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7910) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7912) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7915) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7918) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7921) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7922) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7924) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7927) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
  (<0>,7931) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7933) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7935) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
  (<0>,7938) tree.c:665: return false; /* No grace period needed. */
  (<0>,7940) tree.c:666: }
  (<0>,7943) tree.c:3024: local_irq_restore(flags);
  (<0>,7946) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7948) fake_sched.h:43: return __running_cpu;
  (<0>,7952) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7954) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,7958) fake_sched.h:43: return __running_cpu;
  (<0>,7962) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,7968) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
  (<0>,7971)
  (<0>,7972) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7974) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7977) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
  (<0>,7984) tree.c:3032: do_nocb_deferred_wakeup(rdp);
  (<0>,7987) tree_plugin.h:2457: }
  (<0>,7991) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7994) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7995) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,7996) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8000) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8001) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8002) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8004) tree.c:3045: for_each_rcu_flavor(rsp)
  (<0>,8012) fake_sched.h:43: return __running_cpu;
  (<0>,8016) fake_sched.h:185: need_softirq[get_cpu()] = 0;
  (<0>,8026) fake_sched.h:43: return __running_cpu;
  (<0>,8030) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8031) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8033) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8034) tree.c:800: oldval = rdtp->dynticks_nesting;
  (<0>,8035) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8037) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8039) tree.c:801: rdtp->dynticks_nesting--;
  (<0>,8040) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8041) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8042) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8043) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8044) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,8046) tree.c:804: if (rdtp->dynticks_nesting)
  (<0>,8054) tree_plugin.h:2879: }
      (<0.1>,763) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,4912) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,4916) fake_sched.h:43: return __running_cpu;
    (<0.0>,4920) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,4922) fake_sched.h:43: return __running_cpu;
    (<0.0>,4926) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,4941) fake_sched.h:43: return __running_cpu;
    (<0.0>,4947) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,4953) fake_sched.h:43: return __running_cpu;
    (<0.0>,4960) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,4962) fake_sched.h:43: return __running_cpu;
    (<0.0>,4969) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
    (<0.0>,4977) fake_sched.h:43: return __running_cpu;
    (<0.0>,4981) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,4994) fake_sched.h:43: return __running_cpu;
    (<0.0>,4998)
    (<0.0>,5001) tree.c:755: local_irq_save(flags);
    (<0.0>,5004)
    (<0.0>,5006) fake_sched.h:43: return __running_cpu;
    (<0.0>,5010) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5012) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5016) fake_sched.h:43: return __running_cpu;
    (<0.0>,5020) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5032)
    (<0.0>,5034) fake_sched.h:43: return __running_cpu;
    (<0.0>,5038) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5039) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5041) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5042) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,5043) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5044) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5045) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5046) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5047) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,5051) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5053) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,5054) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5055) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,5071)
    (<0.0>,5073)
    (<0.0>,5075) fake_sched.h:43: return __running_cpu;
    (<0.0>,5079) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5082) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5083) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5084) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5088) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5089) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5090) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5092) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5097) fake_sched.h:43: return __running_cpu;
    (<0.0>,5100) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5102) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5104) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5105) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5108)
    (<0.0>,5111) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5114) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5115) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5116) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5120) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5121) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5122) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5124) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5129) fake_sched.h:43: return __running_cpu;
    (<0.0>,5132) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5134) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5136) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,5137) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,5140)
    (<0.0>,5143) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5146) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5147) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5148) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5152) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5153) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5154) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5156) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,5163) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5166) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5167) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5168) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5170) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5171) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,5173) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5174) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5175) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5176) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5190)
    (<0.0>,5192) tree.c:758: local_irq_restore(flags);
    (<0.0>,5195)
    (<0.0>,5197) fake_sched.h:43: return __running_cpu;
    (<0.0>,5201) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5203) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5207) fake_sched.h:43: return __running_cpu;
    (<0.0>,5211) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5217) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5220) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,5225) fake_sched.h:43: return __running_cpu;
    (<0.0>,5229)
    (<0.0>,5230) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5233) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,5238) tree.c:892: local_irq_save(flags);
    (<0.0>,5241)
    (<0.0>,5243) fake_sched.h:43: return __running_cpu;
    (<0.0>,5247) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5249) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,5253) fake_sched.h:43: return __running_cpu;
    (<0.0>,5257) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5269)
    (<0.0>,5271) fake_sched.h:43: return __running_cpu;
    (<0.0>,5275) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5276) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5278) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5279) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,5280) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5281) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5282) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5283) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,5284) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,5288) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5290) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,5291) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5292) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,5303)
    (<0.0>,5304)
    (<0.0>,5306) fake_sched.h:43: return __running_cpu;
    (<0.0>,5310) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5314) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5317) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5318) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5319) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5321) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5322) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,5324) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5325) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5326) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5327) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5337)
    (<0.0>,5339) tree.c:895: local_irq_restore(flags);
    (<0.0>,5342)
    (<0.0>,5344) fake_sched.h:43: return __running_cpu;
    (<0.0>,5348) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5350) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,5354) fake_sched.h:43: return __running_cpu;
    (<0.0>,5358) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,5372) fake_sched.h:43: return __running_cpu;
    (<0.0>,5376) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,5385) fake_sched.h:43: return __running_cpu;
    (<0.0>,5392) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,5401) fake_sched.h:43: return __running_cpu;
    (<0.0>,5405) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5407) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,5413) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5414) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5415) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5420) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5421) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5422) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5423) fake_defs.h:237: switch (size) {
    (<0.0>,5425) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5427) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5428) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5430) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5433) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5434) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5435) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,5437) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5439) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5441) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5442) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5444) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,5449) tree.c:2046: return true;
    (<0.0>,5451) tree.c:2047: }
    (<0.0>,5455) tree.c:2214: first_gp_fqs = true;
    (<0.0>,5456) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5457) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,5458) tree.c:2216: if (j > HZ) {
    (<0.0>,5461) tree.c:2220: ret = 0;
    (<0.0>,5463) tree.c:2222: if (!ret) {
    (<0.0>,5466) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5467) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5469) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5471) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,5473) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5474) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5477) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5478) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5483) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5484) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5485) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5486) fake_defs.h:237: switch (size) {
    (<0.0>,5488) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5490) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5491) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5493) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,5496) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5497) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5498) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,5502) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5504) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,5508) fake_sched.h:43: return __running_cpu;
    (<0.0>,5512) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,5516) fake_sched.h:43: return __running_cpu;
    (<0.0>,5520) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,5525) fake_sched.h:43: return __running_cpu;
    (<0.0>,5529) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,5540) fake_sched.h:43: return __running_cpu;
    (<0.0>,5544) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,5545) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5547) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5548) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,5549) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5551) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5553) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,5554) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5555) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5556) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5557) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,5558) tree.c:942: if (oldval)
    (<0.0>,5566)
    (<0.0>,5572)
    (<0.0>,5581) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5582) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5583) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5587) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5588) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5589) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5591) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5596) fake_sched.h:43: return __running_cpu;
    (<0.0>,5599) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5601) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5604) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5606) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5608) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5611) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5612) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5613) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5617) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5618) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5619) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5621) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5626) fake_sched.h:43: return __running_cpu;
    (<0.0>,5629) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5631) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5634) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5636) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,5638) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5641) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5642) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5643) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5647) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5648) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5649) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5651) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,5656) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,5661) fake_sched.h:43: return __running_cpu;
    (<0.0>,5666) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,5674) fake_sched.h:43: return __running_cpu;
    (<0.0>,5680) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,5694) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5695) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5696) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5700) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5701) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5702) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5704) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,5708) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5710) fake_sched.h:43: return __running_cpu;
    (<0.0>,5713) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5715) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,5737)
    (<0.0>,5738)
    (<0.0>,5739) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5741) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5742) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,5743) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5745) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5747) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,5748) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5749) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,5784)
    (<0.0>,5785)
    (<0.0>,5786) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,5789) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,5802)
    (<0.0>,5803) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5808) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5809) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5810) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5811) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5813) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5815) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5816) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5818) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5821) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5822) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5823) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5824) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5829) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5830) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5831) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5832) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5834) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5836) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5837) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5839) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5842) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5843) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5844) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,5850) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,5875)
    (<0.0>,5876) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,5881) tree.c:1472: j = jiffies;
    (<0.0>,5882) tree.c:1472: j = jiffies;
    (<0.0>,5883) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5888) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5889) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5890) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5891) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5893) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5895) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5896) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5898) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5901) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5902) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5903) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5904) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,5906) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5911) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5912) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5913) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5914) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5916) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5918) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5919) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5921) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5924) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5925) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5926) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5927) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,5929) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5934) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5935) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5936) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5937) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5939) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5941) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5944) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5947) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5948) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5949) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5950) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,5952) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5957) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5958) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5959) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5960) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5962) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5964) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5965) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5967) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,5970) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5971) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5972) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5973) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,5974) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5975) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,5979) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5980) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,5986) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,5989)
    (<0.0>,5992) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,5995) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,5997) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,6000) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,6004) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,6008) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6010) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,6013) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,6017) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,6020) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6022) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6024) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,6025) tree.c:3511: return 1;
    (<0.0>,6027) tree.c:3548: }
    (<0.0>,6031) tree.c:3561: return 1;
    (<0.0>,6033) tree.c:3563: }
    (<0.0>,6040) fake_sched.h:43: return __running_cpu;
    (<0.0>,6044) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,6048) tree.c:2891: if (user)
    (<0.0>,6056) fake_sched.h:43: return __running_cpu;
    (<0.0>,6060) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,6062) fake_sched.h:43: return __running_cpu;
    (<0.0>,6066) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6072) fake_sched.h:43: return __running_cpu;
    (<0.0>,6076) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,6086)
    (<0.0>,6089) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6090) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6091) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6095) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6096) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6097) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6099) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6103) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,6115)
    (<0.0>,6117) fake_sched.h:43: return __running_cpu;
    (<0.0>,6120) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6122) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6124) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6125) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6127) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6134) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6135) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6137) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6143) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6144) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6145) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6146) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6147) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6151)
    (<0.0>,6152)
    (<0.0>,6153) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6154) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6179)
    (<0.0>,6180)
    (<0.0>,6181) tree.c:1905: local_irq_save(flags);
    (<0.0>,6184)
    (<0.0>,6186) fake_sched.h:43: return __running_cpu;
    (<0.0>,6190) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6192) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6196) fake_sched.h:43: return __running_cpu;
    (<0.0>,6200) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6205) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6207) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6208) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6209) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6211) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6212) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6217) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6218) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6219) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6220) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6222) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6224) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6225) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6227) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6230) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6231) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6232) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6235) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6237) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6238) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6243) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6244) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6245) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6246) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6248) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6250) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6251) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6253) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6256) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6257) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6258) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6261) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6265) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6266) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6267) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6268) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6270) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6271) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6272) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6273) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6276) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6279) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6280) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6288) tree.c:1911: local_irq_restore(flags);
    (<0.0>,6291)
    (<0.0>,6293) fake_sched.h:43: return __running_cpu;
    (<0.0>,6297) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6299) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6303) fake_sched.h:43: return __running_cpu;
    (<0.0>,6307) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6314) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6316) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6319) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,6323) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,6327) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6329) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6330) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6331) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,6349)
    (<0.0>,6350)
    (<0.0>,6351)
    (<0.0>,6352) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6354) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6355) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,6359) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6360) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6361) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6363) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,6367)
    (<0.0>,6368)
    (<0.0>,6369) fake_sync.h:83: local_irq_save(flags);
    (<0.0>,6372)
    (<0.0>,6374) fake_sched.h:43: return __running_cpu;
    (<0.0>,6378) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6380) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6384) fake_sched.h:43: return __running_cpu;
    (<0.0>,6388) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6394) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,6395) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,6402) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,6406) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,6410) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6412) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6413) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6415) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6418) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6420) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6421) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6423) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,6426) tree.c:2491: rdp->gpwrap) {
    (<0.0>,6428) tree.c:2491: rdp->gpwrap) {
    (<0.0>,6431) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6433) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6434) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,6435) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6437) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6438) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,6445) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6446) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6447) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6449) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,6453)
    (<0.0>,6454)
    (<0.0>,6455) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,6456) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,6459) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,6462)
    (<0.0>,6464) fake_sched.h:43: return __running_cpu;
    (<0.0>,6468) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6470) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6474) fake_sched.h:43: return __running_cpu;
    (<0.0>,6478) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6490) tree.c:3016: local_irq_save(flags);
    (<0.0>,6493)
    (<0.0>,6495) fake_sched.h:43: return __running_cpu;
    (<0.0>,6499) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6501) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6505) fake_sched.h:43: return __running_cpu;
    (<0.0>,6509) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6514) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6515) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6526)
    (<0.0>,6527)
    (<0.0>,6528) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,6541)
    (<0.0>,6542) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6547) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6548) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6549) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6550) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6552) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6554) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6555) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6557) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6560) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6561) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6562) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6563) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6569) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6570) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6571) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6573) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6575) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6576) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6578) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6581) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6582) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6583) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6589) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,6591) tree.c:666: }
    (<0.0>,6594) tree.c:3024: local_irq_restore(flags);
    (<0.0>,6597)
    (<0.0>,6599) fake_sched.h:43: return __running_cpu;
    (<0.0>,6603) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6605) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6609) fake_sched.h:43: return __running_cpu;
    (<0.0>,6613) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6619) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,6622)
    (<0.0>,6623) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6625) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6628) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,6635) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,6638)
    (<0.0>,6642) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6645) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6646) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6647) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6651) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6652) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6653) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6655) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,6659) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,6671)
    (<0.0>,6673) fake_sched.h:43: return __running_cpu;
    (<0.0>,6676) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6678) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6680) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,6681) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6683) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6690) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6691) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6693) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6699) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6700) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6701) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,6702) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6703) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,6707)
    (<0.0>,6708)
    (<0.0>,6709) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6710) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,6735)
    (<0.0>,6736)
    (<0.0>,6737) tree.c:1905: local_irq_save(flags);
    (<0.0>,6740)
    (<0.0>,6742) fake_sched.h:43: return __running_cpu;
    (<0.0>,6746) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6748) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6752) fake_sched.h:43: return __running_cpu;
    (<0.0>,6756) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6761) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6763) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6764) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,6765) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6767) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6768) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6773) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6774) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6775) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6776) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6778) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6780) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6781) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6783) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6786) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6787) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6788) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,6791) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6793) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6794) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6799) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6800) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6801) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6802) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6804) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6806) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6807) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6809) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6812) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6813) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6814) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,6817) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6821) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6822) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6823) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6824) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6826) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6827) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6828) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6829) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6832) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6835) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6836) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,6844) tree.c:1911: local_irq_restore(flags);
    (<0.0>,6847)
    (<0.0>,6849) fake_sched.h:43: return __running_cpu;
    (<0.0>,6853) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6855) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,6859) fake_sched.h:43: return __running_cpu;
    (<0.0>,6863) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,6870) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6872) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,6877) tree.c:3016: local_irq_save(flags);
    (<0.0>,6880)
    (<0.0>,6882) fake_sched.h:43: return __running_cpu;
    (<0.0>,6886) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6888) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,6892) fake_sched.h:43: return __running_cpu;
    (<0.0>,6896) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,6901) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6902) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,6913)
    (<0.0>,6914)
    (<0.0>,6915) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,6928)
    (<0.0>,6929) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6934) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6935) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6936) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6937) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6939) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6941) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6944) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6947) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6948) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6949) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6950) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6955) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6956) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6957) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6960) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6962) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6963) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6965) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,6968) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6969) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6970) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,6976) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,6992)
    (<0.0>,6993) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,6996)
    (<0.0>,6997) tree.c:625: return &rsp->node[0];
    (<0.0>,7001) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7002) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7007) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7008) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7009) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7010) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7012) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7014) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7015) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7017) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7020) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7021) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7022) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7026) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,7027) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7029) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7032) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,7033) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7037) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7038) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7039) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7040) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7042) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7044) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7045) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7047) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7050) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7051) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7052) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,7056) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7059) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,7062) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7065) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7066) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,7069) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7071) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7074) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7077) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7080) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7081) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7083) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7086) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7090) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7092) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7094) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7097) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7100) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7103) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7104) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7106) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7109) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,7113) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7115) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7117) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,7120) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,7122) tree.c:666: }
    (<0.0>,7125) tree.c:3024: local_irq_restore(flags);
    (<0.0>,7128)
    (<0.0>,7130) fake_sched.h:43: return __running_cpu;
    (<0.0>,7134) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7136) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7140) fake_sched.h:43: return __running_cpu;
    (<0.0>,7144) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7150) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,7153)
    (<0.0>,7154) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7156) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7159) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,7166) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7169)
    (<0.0>,7173) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7176) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7177) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7178) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7182) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7183) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7184) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7186) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,7194) fake_sched.h:43: return __running_cpu;
    (<0.0>,7198) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,7208) fake_sched.h:43: return __running_cpu;
    (<0.0>,7212) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7213) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7215) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7216) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,7217) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7219) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7221) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,7222) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7223) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7224) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7225) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7226) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,7228) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,7236)
    (<0.0>,7242) fake_sched.h:43: return __running_cpu;
    (<0.0>,7246)
    (<0.0>,7249) tree.c:755: local_irq_save(flags);
    (<0.0>,7252)
    (<0.0>,7254) fake_sched.h:43: return __running_cpu;
    (<0.0>,7258) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7260) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7264) fake_sched.h:43: return __running_cpu;
    (<0.0>,7268) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7280)
    (<0.0>,7282) fake_sched.h:43: return __running_cpu;
    (<0.0>,7286) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7287) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7289) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7290) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,7291) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7292) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7293) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7294) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7295) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,7299) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,7301) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,7302) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,7303) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,7319)
    (<0.0>,7321)
    (<0.0>,7323) fake_sched.h:43: return __running_cpu;
    (<0.0>,7327) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7330) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7331) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7332) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7336) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7337) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7338) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7340) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7345) fake_sched.h:43: return __running_cpu;
    (<0.0>,7348) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7350) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7352) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7353) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7356)
    (<0.0>,7359) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7362) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7363) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7364) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7368) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7369) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7370) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7372) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7377) fake_sched.h:43: return __running_cpu;
    (<0.0>,7380) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7382) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7384) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,7385) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,7388)
    (<0.0>,7391) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7394) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7395) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7396) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7400) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7401) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7402) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7404) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,7411) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7414) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7415) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7416) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7418) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7419) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,7421) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7422) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7423) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7424) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7438)
    (<0.0>,7440) tree.c:758: local_irq_restore(flags);
    (<0.0>,7443)
    (<0.0>,7445) fake_sched.h:43: return __running_cpu;
    (<0.0>,7449) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7451) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7455) fake_sched.h:43: return __running_cpu;
    (<0.0>,7459) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7465) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,7468) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,7473) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7489)
    (<0.0>,7490)
    (<0.0>,7491) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7494)
    (<0.0>,7495) tree.c:625: return &rsp->node[0];
    (<0.0>,7499) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7500) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7505) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7506) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7507) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7508) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7510) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7512) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7513) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7515) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7518) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7519) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7520) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7522) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7523) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,7524) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7525) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,7529) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7534) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7535) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7536) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7537) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7539) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7541) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7542) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7544) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7547) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7548) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7549) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7552) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7555)
    (<0.0>,7559) tree.c:2064: return true;
    (<0.0>,7561) tree.c:2067: }
    (<0.0>,7566) fake_sched.h:43: return __running_cpu;
    (<0.0>,7570)
    (<0.0>,7571) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,7574) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,7579) tree.c:892: local_irq_save(flags);
    (<0.0>,7582)
    (<0.0>,7584) fake_sched.h:43: return __running_cpu;
    (<0.0>,7588) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7590) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,7594) fake_sched.h:43: return __running_cpu;
    (<0.0>,7598) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7610)
    (<0.0>,7612) fake_sched.h:43: return __running_cpu;
    (<0.0>,7616) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7617) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7619) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7620) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,7621) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7622) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7623) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7624) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,7625) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,7629) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,7631) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,7632) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,7633) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,7644)
    (<0.0>,7645)
    (<0.0>,7647) fake_sched.h:43: return __running_cpu;
    (<0.0>,7651) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,7655) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7658) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7659) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7660) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7662) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7663) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,7665) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7666) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7667) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7668) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,7678)
    (<0.0>,7680) tree.c:895: local_irq_restore(flags);
    (<0.0>,7683)
    (<0.0>,7685) fake_sched.h:43: return __running_cpu;
    (<0.0>,7689) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7691) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,7695) fake_sched.h:43: return __running_cpu;
    (<0.0>,7699) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7706) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7707) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7708) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,7709) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,7711) tree.c:2233: rsp->gp_state = RCU_GP_DOING_FQS;
    (<0.0>,7712) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7717) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7718) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7719) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7720) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7722) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7724) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7725) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7727) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,7730) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7731) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7732) tree.c:2236: if (!READ_ONCE(rnp->qsmask) &&
    (<0.0>,7735) tree.c:2237: !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,7738)
    (<0.0>,7743) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,7745) tree.c:2279: rsp->gp_state = RCU_GP_CLEANUP;
    (<0.0>,7746) tree.c:2280: rcu_gp_cleanup(rsp);
    (<0.0>,7786)
    (<0.0>,7787) tree.c:2109: bool needgp = false;
    (<0.0>,7788) tree.c:2110: int nocb = 0;
    (<0.0>,7789) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7792)
    (<0.0>,7793) tree.c:625: return &rsp->node[0];
    (<0.0>,7797) tree.c:2112: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,7799) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7800) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7801) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7806) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7807) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7808) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7809) fake_defs.h:237: switch (size) {
    (<0.0>,7811) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7813) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7814) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7816) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,7819) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7820) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7821) tree.c:2115: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,7822) tree.c:2116: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,7825)
    (<0.0>,7826) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,7830)
    (<0.0>,7833) fake_sched.h:43: return __running_cpu;
    (<0.0>,7837) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,7841) fake_sched.h:43: return __running_cpu;
    (<0.0>,7845) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7850) fake_sched.h:43: return __running_cpu;
    (<0.0>,7854) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,7857) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7858) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7865) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7866) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7868) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7870) tree.c:2117: gp_duration = jiffies - rsp->gp_start;
    (<0.0>,7871) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,7872) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,7874) tree.c:2118: if (gp_duration > rsp->gp_max)
    (<0.0>,7877) tree.c:2129: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,7880)
    (<0.0>,7881) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,7885)
    (<0.0>,7886) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7887) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,7892) fake_sched.h:43: return __running_cpu;
    (<0.0>,7896) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,7898) fake_sched.h:43: return __running_cpu;
    (<0.0>,7902) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,7909) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7912) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7914) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7915) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7917) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,7922) tree.c:2141: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,7925)
    (<0.0>,7926) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,7930)
    (<0.0>,7933) fake_sched.h:43: return __running_cpu;
    (<0.0>,7937) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,7941) fake_sched.h:43: return __running_cpu;
    (<0.0>,7945) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,7950) fake_sched.h:43: return __running_cpu;
    (<0.0>,7954) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,7957) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7958) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,7965) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7968)
    (<0.0>,7974) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7975) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7978)
    (<0.0>,7983) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7984) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7985) tree.c:2142: WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));
    (<0.0>,7986) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7988) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7993) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7994) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,7996) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8000) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8001) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8002) tree.c:2143: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,8004) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8006) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8007) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8008) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8013) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8014) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8015) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8016) fake_defs.h:237: switch (size) {
    (<0.0>,8018) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8020) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8021) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8023) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8026) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8027) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8028) tree.c:2144: WRITE_ONCE(rnp->completed, rsp->gpnum);
    (<0.0>,8030) fake_sched.h:43: return __running_cpu;
    (<0.0>,8033) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8035) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8037) tree.c:2145: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8038) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8039) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8041) tree.c:2146: if (rnp == rdp->mynode)
    (<0.0>,8044) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8045) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8046) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8068)
    (<0.0>,8069)
    (<0.0>,8070)
    (<0.0>,8071) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8073) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8074) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8076) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,8079) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8080) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8081) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8089)
    (<0.0>,8090)
    (<0.0>,8091)
    (<0.0>,8092) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8095) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8098) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8101) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8102) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8105) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8107) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8110) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8112) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8113) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8115) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8118) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8122) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8124) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8127) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8128) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8131) tree.c:1827: rdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[i];
    (<0.0>,8133) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8135) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8137) tree.c:1824: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++) {
    (<0.0>,8140) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8142) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8143) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8145) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8148) tree.c:1825: if (ULONG_CMP_LT(rnp->completed, rdp->nxtcompleted[i]))
    (<0.0>,8153) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8155) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8156) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8159) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8162) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8163) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8165) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8168) tree.c:1831: rdp->nxttail[j] = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,8170) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8172) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8174) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8175) tree.c:1830: for (j = RCU_WAIT_TAIL; j < i; j++)
    (<0.0>,8178) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,8180) tree.c:1834: for (j = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++, j++) {
    (<0.0>,8183) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8185) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8188) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8189) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8192) tree.c:1835: if (rdp->nxttail[j] == rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,8196) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8197) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8198) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8207)
    (<0.0>,8208)
    (<0.0>,8209)
    (<0.0>,8210) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8213) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8216) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8219) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8220) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,8223) tree.c:1750: return false;
    (<0.0>,8225) tree.c:1799: }
    (<0.0>,8227) tree.c:1842: return rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,8229) tree.c:1843: }
    (<0.0>,8232) tree.c:1867: ret = rcu_advance_cbs(rsp, rnp, rdp);
    (<0.0>,8233) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8235) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8236) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8238) tree.c:1870: rdp->completed = rnp->completed;
    (<0.0>,8242) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8244) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8245) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8247) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8250) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8254) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8255) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8256) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8257) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8259) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8260) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8261) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8262) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,8265) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8268) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8269) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,8277) tree.c:1896: return ret;
    (<0.0>,8281) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8285) tree.c:2147: needgp = __note_gp_changes(rsp, rnp, rdp) || needgp;
    (<0.0>,8287) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8288) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8295)
    (<0.0>,8296)
    (<0.0>,8297) tree.c:1702: int c = rnp->completed;
    (<0.0>,8299) tree.c:1702: int c = rnp->completed;
    (<0.0>,8301) tree.c:1702: int c = rnp->completed;
    (<0.0>,8303) fake_sched.h:43: return __running_cpu;
    (<0.0>,8306) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8308) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8310) tree.c:1704: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8311) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,8314) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,8317) tree.c:1706: rnp->need_future_gp[c & 0x1] = 0;
    (<0.0>,8318) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8322) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8325) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8326) tree.c:1707: needmore = rnp->need_future_gp[(c + 1) & 0x1];
    (<0.0>,8327) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,8328) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,8329) tree.c:1708: trace_rcu_future_gp(rnp, rdp, c,
    (<0.0>,8331) tree.c:1709: needmore ? TPS("CleanupMore") : TPS("Cleanup"));
    (<0.0>,8339)
    (<0.0>,8340)
    (<0.0>,8341)
    (<0.0>,8342)
    (<0.0>,8346) tree.c:1710: return needmore;
    (<0.0>,8348) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8350) tree.c:2149: nocb += rcu_future_gp_cleanup(rsp, rnp);
    (<0.0>,8351) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,8354)
    (<0.0>,8356) tree.c:2150: sq = rcu_nocb_gp_get(rnp);
    (<0.0>,8357) tree.c:2151: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,8360)
    (<0.0>,8361) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8365)
    (<0.0>,8366) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8367) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,8372) fake_sched.h:43: return __running_cpu;
    (<0.0>,8376) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,8378) fake_sched.h:43: return __running_cpu;
    (<0.0>,8382) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8389) tree.c:2152: rcu_nocb_gp_cleanup(sq);
    (<0.0>,8392)
    (<0.0>,8402) fake_sched.h:43: return __running_cpu;
    (<0.0>,8408) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,8416) fake_sched.h:43: return __running_cpu;
    (<0.0>,8420) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,8433) fake_sched.h:43: return __running_cpu;
    (<0.0>,8437)
    (<0.0>,8440) tree.c:755: local_irq_save(flags);
    (<0.0>,8443)
    (<0.0>,8445) fake_sched.h:43: return __running_cpu;
    (<0.0>,8449) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8451) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8455) fake_sched.h:43: return __running_cpu;
    (<0.0>,8459) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8471)
    (<0.0>,8473) fake_sched.h:43: return __running_cpu;
    (<0.0>,8477) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8478) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8480) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8481) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,8482) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8483) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8484) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8485) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8486) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,8490) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,8492) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,8493) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8494) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,8510)
    (<0.0>,8512)
    (<0.0>,8514) fake_sched.h:43: return __running_cpu;
    (<0.0>,8518) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8521) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8522) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8523) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8527) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8528) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8529) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8531) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8536) fake_sched.h:43: return __running_cpu;
    (<0.0>,8539) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8541) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8543) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8544) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8547)
    (<0.0>,8550) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8553) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8554) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8555) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8559) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8560) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8561) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8563) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8568) fake_sched.h:43: return __running_cpu;
    (<0.0>,8571) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8573) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8575) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8576) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,8579)
    (<0.0>,8582) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8585) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8586) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8587) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8591) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8592) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8593) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8595) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,8602) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8605) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8606) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8607) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8609) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8610) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,8612) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8613) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8614) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8615) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8629)
    (<0.0>,8631) tree.c:758: local_irq_restore(flags);
    (<0.0>,8634)
    (<0.0>,8636) fake_sched.h:43: return __running_cpu;
    (<0.0>,8640) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8642) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8646) fake_sched.h:43: return __running_cpu;
    (<0.0>,8650) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8656) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8659) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,8664) fake_sched.h:43: return __running_cpu;
    (<0.0>,8668)
    (<0.0>,8669) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8672) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,8677) tree.c:892: local_irq_save(flags);
    (<0.0>,8680)
    (<0.0>,8682) fake_sched.h:43: return __running_cpu;
    (<0.0>,8686) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8688) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,8692) fake_sched.h:43: return __running_cpu;
    (<0.0>,8696) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8708)
    (<0.0>,8710) fake_sched.h:43: return __running_cpu;
    (<0.0>,8714) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8715) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8717) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8718) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,8719) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8720) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8721) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8722) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,8723) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,8727) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8729) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,8730) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8731) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,8742)
    (<0.0>,8743)
    (<0.0>,8745) fake_sched.h:43: return __running_cpu;
    (<0.0>,8749) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,8753) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8756) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8757) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8758) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8760) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8761) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,8763) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8764) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8765) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8766) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,8776)
    (<0.0>,8778) tree.c:895: local_irq_restore(flags);
    (<0.0>,8781)
    (<0.0>,8783) fake_sched.h:43: return __running_cpu;
    (<0.0>,8787) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8789) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,8793) fake_sched.h:43: return __running_cpu;
    (<0.0>,8797) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,8811) fake_sched.h:43: return __running_cpu;
    (<0.0>,8815) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,8824) fake_sched.h:43: return __running_cpu;
    (<0.0>,8831) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,8840) fake_sched.h:43: return __running_cpu;
    (<0.0>,8844) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,8846) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,8852) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8853) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8854) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8859) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8860) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8861) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8862) fake_defs.h:237: switch (size) {
    (<0.0>,8864) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8866) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8867) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8869) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8872) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8873) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8874) tree.c:2154: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,8875) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,8876) tree.c:2155: rcu_gp_slow(rsp, gp_cleanup_delay);
    (<0.0>,8880)
    (<0.0>,8881)
    (<0.0>,8882) tree.c:1922: if (delay > 0 &&
    (<0.0>,8887) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8889) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8891) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8892) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8894) tree.c:2140: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,8899) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,8902)
    (<0.0>,8903) tree.c:625: return &rsp->node[0];
    (<0.0>,8907) tree.c:2157: rnp = rcu_get_root(rsp);
    (<0.0>,8908) tree.c:2158: raw_spin_lock_irq_rcu_node(rnp); /* Order GP before ->completed update. */
    (<0.0>,8911)
    (<0.0>,8912) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,8916)
    (<0.0>,8919) fake_sched.h:43: return __running_cpu;
    (<0.0>,8923) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,8927) fake_sched.h:43: return __running_cpu;
    (<0.0>,8931) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,8936) fake_sched.h:43: return __running_cpu;
    (<0.0>,8940) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,8943) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8944) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,8951) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,8952) tree.c:2159: rcu_nocb_gp_set(rnp, nocb);
    (<0.0>,8956)
    (<0.0>,8957)
    (<0.0>,8960) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8962) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8963) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8964) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8969) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8970) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8971) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8972) fake_defs.h:237: switch (size) {
    (<0.0>,8974) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8976) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8977) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8979) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,8982) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8983) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8984) tree.c:2162: WRITE_ONCE(rsp->completed, rsp->gpnum);
    (<0.0>,8987) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,8989) tree.c:2164: rsp->gp_state = RCU_GP_IDLE;
    (<0.0>,8991) fake_sched.h:43: return __running_cpu;
    (<0.0>,8994) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8996) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8998) tree.c:2165: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,8999) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9000) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9001) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9009)
    (<0.0>,9010)
    (<0.0>,9011)
    (<0.0>,9012) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9015) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9018) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9021) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9022) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,9025) tree.c:1818: return false;
    (<0.0>,9027) tree.c:1843: }
    (<0.0>,9030) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9034) tree.c:2167: needgp = rcu_advance_cbs(rsp, rnp, rdp) || needgp;
    (<0.0>,9035) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9038) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9039) tree.c:2168: if (needgp || cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,9050)
    (<0.0>,9051)
    (<0.0>,9052) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,9065)
    (<0.0>,9066) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9071) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9072) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9073) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9074) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9076) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9078) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9079) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9081) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9084) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9085) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9086) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9087) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9092) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9093) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9094) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9095) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9097) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9099) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9100) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9102) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9105) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9106) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9107) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9113) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,9129)
    (<0.0>,9130) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9133)
    (<0.0>,9134) tree.c:625: return &rsp->node[0];
    (<0.0>,9138) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,9139) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9144) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9145) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9146) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9147) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9149) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9151) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9152) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9154) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9157) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9158) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9159) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9163) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,9164) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9166) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9169) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,9170) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9174) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9175) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9176) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9177) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9179) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9181) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9182) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9184) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9187) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9188) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9189) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,9193) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,9195) tree.c:666: }
    (<0.0>,9200) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9205) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9206) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9207) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9208) fake_defs.h:237: switch (size) {
    (<0.0>,9210) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9212) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9213) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9215) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,9218) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9219) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9220) tree.c:2169: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,9224) tree.c:2174: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,9227)
    (<0.0>,9228) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,9232)
    (<0.0>,9233) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,9234) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,9239) fake_sched.h:43: return __running_cpu;
    (<0.0>,9243) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9245) fake_sched.h:43: return __running_cpu;
    (<0.0>,9249) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9257) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,9259) tree.c:2281: rsp->gp_state = RCU_GP_CLEANED;
    (<0.0>,9264) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,9266) tree.c:2197: rsp->gp_state = RCU_GP_WAIT_GPS;
    (<0.0>,9270) fake_sched.h:43: return __running_cpu;
    (<0.0>,9274) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,9278) fake_sched.h:43: return __running_cpu;
    (<0.0>,9282) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9287) fake_sched.h:43: return __running_cpu;
    (<0.0>,9291) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,9302) fake_sched.h:43: return __running_cpu;
    (<0.0>,9306) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,9307) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,9309) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,9310) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,9311) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,9313) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,9315) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,9316) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9317) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9318) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9319) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,9320) tree.c:942: if (oldval)
    (<0.0>,9328)
    (<0.0>,9334)
    (<0.0>,9343) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9344) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9345) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9349) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9350) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9351) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9353) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9358) fake_sched.h:43: return __running_cpu;
    (<0.0>,9361) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9363) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9366) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9368) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9370) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9373) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9374) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9375) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9379) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9380) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9381) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9383) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9388) fake_sched.h:43: return __running_cpu;
    (<0.0>,9391) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9393) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9396) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9398) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,9400) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9403) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9404) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9405) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9409) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9410) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9411) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9413) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,9418) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,9423) fake_sched.h:43: return __running_cpu;
    (<0.0>,9428) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,9436) fake_sched.h:43: return __running_cpu;
    (<0.0>,9442) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,9456) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9457) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9458) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9462) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9463) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9464) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9466) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,9470) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9472) fake_sched.h:43: return __running_cpu;
    (<0.0>,9475) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9477) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,9499)
    (<0.0>,9500)
    (<0.0>,9501) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9503) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9504) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,9505) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9507) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9509) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,9510) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,9511) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,9546)
    (<0.0>,9547)
    (<0.0>,9548) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,9551) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,9564)
    (<0.0>,9565) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9570) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9571) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9572) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9573) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9575) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9577) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9578) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9580) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9583) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9584) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9585) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9586) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9591) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9592) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9593) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9594) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9596) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9598) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9599) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9601) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9604) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9605) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9606) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,9614) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,9617)
    (<0.0>,9620) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,9623) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9625) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9628) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9632) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,9636) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,9638) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,9641) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,9645) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,9648) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,9650) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,9652) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,9653) tree.c:3511: return 1;
    (<0.0>,9655) tree.c:3548: }
    (<0.0>,9659) tree.c:3561: return 1;
    (<0.0>,9661) tree.c:3563: }
    (<0.0>,9668) fake_sched.h:43: return __running_cpu;
    (<0.0>,9672) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,9676) tree.c:2891: if (user)
    (<0.0>,9684) fake_sched.h:43: return __running_cpu;
    (<0.0>,9688) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,9690) fake_sched.h:43: return __running_cpu;
    (<0.0>,9694) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9700) fake_sched.h:43: return __running_cpu;
    (<0.0>,9704) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,9714)
    (<0.0>,9717) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9718) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9719) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9723) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9724) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9725) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9727) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,9731) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,9743)
    (<0.0>,9745) fake_sched.h:43: return __running_cpu;
    (<0.0>,9748) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,9750) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,9752) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,9753) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9755) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9762) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9763) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9765) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9771) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9772) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9773) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,9774) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,9775) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,9779)
    (<0.0>,9780)
    (<0.0>,9781) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,9782) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,9807)
    (<0.0>,9808)
    (<0.0>,9809) tree.c:1905: local_irq_save(flags);
    (<0.0>,9812)
    (<0.0>,9814) fake_sched.h:43: return __running_cpu;
    (<0.0>,9818) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9820) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,9824) fake_sched.h:43: return __running_cpu;
    (<0.0>,9828) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,9833) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,9835) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,9836) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,9837) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9839) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9840) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9845) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9846) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9847) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9848) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9850) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9852) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9853) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9855) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9858) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9859) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9860) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,9863) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9865) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9866) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9871) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9872) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9873) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9878) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9879) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9881) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9884) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9885) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9886) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,9889) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9893) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9894) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9895) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9896) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9898) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9899) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9900) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9901) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,9904) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9907) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9908) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,9916) tree.c:1911: local_irq_restore(flags);
    (<0.0>,9919)
    (<0.0>,9921) fake_sched.h:43: return __running_cpu;
    (<0.0>,9925) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9927) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,9931) fake_sched.h:43: return __running_cpu;
    (<0.0>,9935) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,9942) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,9944) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,9947) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,9951) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,9955) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,9957) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,9958) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,9959) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,9977)
    (<0.0>,9978)
    (<0.0>,9979)
    (<0.0>,9980) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,9982) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,9983) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,9987) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,9988) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,9989) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,9991) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,9995)
    (<0.0>,9996)
    (<0.0>,9997) fake_sync.h:83: local_irq_save(flags);
    (<0.0>,10000)
    (<0.0>,10002) fake_sched.h:43: return __running_cpu;
    (<0.0>,10006) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10008) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10012) fake_sched.h:43: return __running_cpu;
    (<0.0>,10016) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10022) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,10023) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,10030) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,10034) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,10038) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10040) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10041) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10043) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10046) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10048) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10049) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10051) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,10054) tree.c:2499: rdp->cpu_no_qs.b.norm = true;	/* need qs for new gp. */
    (<0.0>,10058) tree.c:2499: rdp->cpu_no_qs.b.norm = true;	/* need qs for new gp. */
    (<0.0>,10060) fake_sched.h:43: return __running_cpu;
    (<0.0>,10064) tree.c:2500: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,10065) tree.c:2500: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,10067) tree.c:2500: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,10071) tree.c:2501: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,10072) tree.c:2501: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,10073) tree.c:2501: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,10075) tree.c:2501: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,10079)
    (<0.0>,10080)
    (<0.0>,10081) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10082) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10085) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,10088)
    (<0.0>,10090) fake_sched.h:43: return __running_cpu;
    (<0.0>,10094) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10096) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10100) fake_sched.h:43: return __running_cpu;
    (<0.0>,10104) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10116) tree.c:3016: local_irq_save(flags);
    (<0.0>,10119)
    (<0.0>,10121) fake_sched.h:43: return __running_cpu;
    (<0.0>,10125) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10127) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10131) fake_sched.h:43: return __running_cpu;
    (<0.0>,10135) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10140) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10141) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10152)
    (<0.0>,10153)
    (<0.0>,10154) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,10167)
    (<0.0>,10168) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10173) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10174) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10175) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10176) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10178) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10180) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10181) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10183) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10186) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10187) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10188) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10189) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10194) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10195) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10196) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10197) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10199) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10201) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10202) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10204) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10207) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10208) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10209) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10215) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,10231)
    (<0.0>,10232) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10235)
    (<0.0>,10236) tree.c:625: return &rsp->node[0];
    (<0.0>,10240) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10241) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10246) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10247) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10248) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10249) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10251) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10253) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10254) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10256) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10259) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10260) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10261) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10265) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10266) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10268) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10271) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10272) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10276) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10277) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10278) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10279) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10281) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10283) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10284) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10286) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10289) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10290) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10291) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10295) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,10297) tree.c:666: }
    (<0.0>,10300) tree.c:3018: raw_spin_lock_rcu_node(rcu_get_root(rsp)); /* irqs disabled. */
    (<0.0>,10303)
    (<0.0>,10304) tree.c:625: return &rsp->node[0];
    (<0.0>,10310)
    (<0.0>,10311) tree.h:717: raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,10315)
    (<0.0>,10317) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,10318) fake_sync.h:116: if (pthread_mutex_lock(l))
    (<0.0>,10325) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,10331)
    (<0.0>,10333) fake_sched.h:43: return __running_cpu;
    (<0.0>,10336) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10338) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10340) tree.c:2332: struct rcu_data *rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,10341) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10344)
    (<0.0>,10345) tree.c:625: return &rsp->node[0];
    (<0.0>,10349) tree.c:2333: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10350) tree.c:2334: bool ret = false;
    (<0.0>,10351) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10352) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10353) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10361)
    (<0.0>,10362)
    (<0.0>,10363)
    (<0.0>,10364) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10367) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10370) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10373) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10374) tree.c:1817: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10377) tree.c:1818: return false;
    (<0.0>,10379) tree.c:1843: }
    (<0.0>,10382) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10386) tree.c:2344: ret = rcu_advance_cbs(rsp, rnp, rdp) || ret;
    (<0.0>,10387) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10388) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10389) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10400)
    (<0.0>,10401)
    (<0.0>,10402)
    (<0.0>,10403) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10405) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10408) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10409) tree.c:2300: if (!rsp->gp_kthread || !cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,10420)
    (<0.0>,10421)
    (<0.0>,10422) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,10435)
    (<0.0>,10436) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10441) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10442) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10443) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10444) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10446) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10448) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10449) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10451) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10454) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10455) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10456) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10457) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10462) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10463) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10464) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10465) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10467) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10469) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10470) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10472) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10475) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10476) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10477) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,10483) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,10499)
    (<0.0>,10500) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10503)
    (<0.0>,10504) tree.c:625: return &rsp->node[0];
    (<0.0>,10508) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,10509) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10514) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10515) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10516) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10517) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10519) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10521) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10522) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10524) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10527) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10528) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10529) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10533) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,10534) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10536) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10539) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,10540) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10544) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10545) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10546) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10547) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10549) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10551) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10552) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10554) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10557) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10558) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10559) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,10563) tree.c:655: return true;  /* Yes, a no-CBs CPU needs one. */
    (<0.0>,10565) tree.c:666: }
    (<0.0>,10570) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10575) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10576) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10577) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10578) fake_defs.h:237: switch (size) {
    (<0.0>,10580) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,10582) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,10583) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,10585) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,10588) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10589) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10590) tree.c:2309: WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
    (<0.0>,10593) tree.c:2318: return true;
    (<0.0>,10595) tree.c:2319: }
    (<0.0>,10599) tree.c:2345: ret = rcu_start_gp_advanced(rsp, rnp, rdp) || ret;
    (<0.0>,10600) tree.c:2346: return ret;
    (<0.0>,10604) tree.c:3019: needwake = rcu_start_gp(rsp);
    (<0.0>,10608) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,10609) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,10610) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,10613)
    (<0.0>,10614) tree.c:625: return &rsp->node[0];
    (<0.0>,10619) tree.c:3020: raw_spin_unlock_irqrestore_rcu_node(rcu_get_root(rsp), flags);
    (<0.0>,10623)
    (<0.0>,10624)
    (<0.0>,10625) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10626) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,10629) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,10632)
    (<0.0>,10634) fake_sched.h:43: return __running_cpu;
    (<0.0>,10638) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10640) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10644) fake_sched.h:43: return __running_cpu;
    (<0.0>,10648) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10656) tree.c:3021: if (needwake)
    (<0.0>,10659) tree.c:3022: rcu_gp_kthread_wake(rsp);
    (<0.0>,10667)
    (<0.0>,10668) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,10669) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,10671) tree.c:1722: if (current == rsp->gp_kthread ||
    (<0.0>,10678) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,10681)
    (<0.0>,10682) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10684) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10687) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10690) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10693) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10700) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,10701) tree.c:3029: invoke_rcu_callbacks(rsp, rdp);
    (<0.0>,10710)
    (<0.0>,10711)
    (<0.0>,10714) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10715) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10716) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10717) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10719) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10721) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10722) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10724) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,10727) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10728) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10729) tree.c:3059: if (unlikely(!READ_ONCE(rcu_scheduler_fully_active)))
    (<0.0>,10738) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,10740) tree.c:3061: if (likely(!rsp->boost)) {
    (<0.0>,10749) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,10750) tree.c:3062: rcu_do_batch(rsp, rdp);
    (<0.0>,10772)
    (<0.0>,10773)
    (<0.0>,10774) tree.c:2767: if (!cpu_has_callbacks_ready_to_invoke(rdp)) {
    (<0.0>,10777)
    (<0.0>,10778) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10780) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10783) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,10786) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10789) tree.c:617: rdp->nxttail[RCU_DONE_TAIL] != NULL;
    (<0.0>,10796) tree.c:2779: local_irq_save(flags);
    (<0.0>,10799)
    (<0.0>,10801) fake_sched.h:43: return __running_cpu;
    (<0.0>,10805) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10807) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,10811) fake_sched.h:43: return __running_cpu;
    (<0.0>,10815) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,10820) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10821) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10822) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10823) tree.c:2780: WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
    (<0.0>,10824) tree.c:2781: bl = rdp->blimit;
    (<0.0>,10826) tree.c:2781: bl = rdp->blimit;
    (<0.0>,10827) tree.c:2781: bl = rdp->blimit;
    (<0.0>,10830) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,10832) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,10833) tree.c:2783: list = rdp->nxtlist;
    (<0.0>,10834) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10837) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10838) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10839) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10841) tree.c:2784: rdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10842) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,10845) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,10846) tree.c:2785: *rdp->nxttail[RCU_DONE_TAIL] = NULL;
    (<0.0>,10847) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10850) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10851) tree.c:2786: tail = rdp->nxttail[RCU_DONE_TAIL];
    (<0.0>,10852) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10854) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10857) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10859) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10862) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10863) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10866) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10869) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10871) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10873) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10876) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10879) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10881) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10883) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10886) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10888) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10891) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10892) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10895) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10898) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10900) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10902) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10905) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10908) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10910) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10912) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10915) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10917) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10920) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10921) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10924) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10927) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10929) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10931) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10934) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10937) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10939) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10941) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10944) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10946) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10949) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10950) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10953) tree.c:2788: if (rdp->nxttail[i] == rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,10956) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10958) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10960) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10963) tree.c:2789: rdp->nxttail[i] = &rdp->nxtlist;
    (<0.0>,10966) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10968) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10970) tree.c:2787: for (i = RCU_NEXT_SIZE - 1; i >= 0; i--)
    (<0.0>,10973) tree.c:2790: local_irq_restore(flags);
    (<0.0>,10976)
    (<0.0>,10978) fake_sched.h:43: return __running_cpu;
    (<0.0>,10982) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10984) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,10988) fake_sched.h:43: return __running_cpu;
    (<0.0>,10992) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,10997) tree.c:2793: count = count_lazy = 0;
    (<0.0>,10998) tree.c:2793: count = count_lazy = 0;
    (<0.0>,11000) tree.c:2794: while (list) {
    (<0.0>,11003) tree.c:2795: next = list->next;
    (<0.0>,11005) tree.c:2795: next = list->next;
    (<0.0>,11006) tree.c:2795: next = list->next;
    (<0.0>,11009) tree.c:2797: debug_rcu_head_unqueue(list);
    (<0.0>,11012)
    (<0.0>,11014) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11016) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11017) tree.c:2798: if (__rcu_reclaim(rsp->name, list))
    (<0.0>,11023)
    (<0.0>,11024)
    (<0.0>,11025) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11028) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11030) rcu.h:108: unsigned long offset = (unsigned long)head->func;
    (<0.0>,11033) rcu.h:111: if (__is_kfree_rcu_offset(offset)) {
    (<0.0>,11036) rcu.h:118: head->func(head);
    (<0.0>,11039) rcu.h:118: head->func(head);
    (<0.0>,11040) rcu.h:118: head->func(head);
    (<0.0>,11046)
    (<0.0>,11047) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11048) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11049) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11053) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11054) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11055) update.c:340: rcu = container_of(head, struct rcu_synchronize, head);
    (<0.0>,11056) update.c:341: complete(&rcu->completion);
    (<0.0>,11060)
    (<0.0>,11061) fake_sync.h:288: x->done++;
    (<0.0>,11063) fake_sync.h:288: x->done++;
    (<0.0>,11065) fake_sync.h:288: x->done++;
    (<0.0>,11070) rcu.h:120: return false;
    (<0.0>,11072) rcu.h:122: }
    (<0.0>,11075) tree.c:2800: list = next;
    (<0.0>,11076) tree.c:2800: list = next;
    (<0.0>,11077) tree.c:2802: if (++count >= bl &&
    (<0.0>,11079) tree.c:2802: if (++count >= bl &&
    (<0.0>,11080) tree.c:2802: if (++count >= bl &&
    (<0.0>,11084) tree.c:2794: while (list) {
    (<0.0>,11087) tree.c:2808: local_irq_save(flags);
    (<0.0>,11090)
    (<0.0>,11092) fake_sched.h:43: return __running_cpu;
    (<0.0>,11096) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11098) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11102) fake_sched.h:43: return __running_cpu;
    (<0.0>,11106) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11113) tree.c:2814: if (list != NULL) {
    (<0.0>,11117) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11118) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11120) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11122) tree.c:2824: rdp->qlen_lazy -= count_lazy;
    (<0.0>,11124) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11126) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11127) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11129) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11130) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11135) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11136) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11137) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11138) fake_defs.h:237: switch (size) {
    (<0.0>,11140) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11142) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11143) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11145) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,11148) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11149) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11150) tree.c:2825: WRITE_ONCE(rdp->qlen, rdp->qlen - count);
    (<0.0>,11151) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11152) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11154) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11156) tree.c:2826: rdp->n_cbs_invoked += count;
    (<0.0>,11157) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,11159) tree.c:2829: if (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)
    (<0.0>,11162) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11164) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11167) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11169) tree.c:2833: if (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {
    (<0.0>,11172) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11174) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11175) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11177) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11178) tree.c:2836: } else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)
    (<0.0>,11183) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11185) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11188) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11190) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11197) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11198) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11200) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11203) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11205) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11211) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11212) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11213) tree.c:2838: WARN_ON_ONCE((rdp->nxtlist == NULL) != (rdp->qlen == 0));
    (<0.0>,11214) tree.c:2840: local_irq_restore(flags);
    (<0.0>,11217)
    (<0.0>,11219) fake_sched.h:43: return __running_cpu;
    (<0.0>,11223) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11225) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11229) fake_sched.h:43: return __running_cpu;
    (<0.0>,11233) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11238) tree.c:2843: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11241)
    (<0.0>,11242) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11244) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11247) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11258) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11261)
    (<0.0>,11265) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11268) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11269) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11270) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11274) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11275) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11276) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11278) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11282) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,11294)
    (<0.0>,11296) fake_sched.h:43: return __running_cpu;
    (<0.0>,11299) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11301) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11303) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,11304) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11306) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11313) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11314) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11316) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11322) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11323) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11324) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,11325) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11326) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,11330)
    (<0.0>,11331)
    (<0.0>,11332) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,11333) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,11358)
    (<0.0>,11359)
    (<0.0>,11360) tree.c:1905: local_irq_save(flags);
    (<0.0>,11363)
    (<0.0>,11365) fake_sched.h:43: return __running_cpu;
    (<0.0>,11369) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11371) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11375) fake_sched.h:43: return __running_cpu;
    (<0.0>,11379) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11384) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11386) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11387) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,11388) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11390) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11391) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11396) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11397) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11398) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11399) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11401) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11403) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11409) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11410) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11411) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,11414) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11416) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11417) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11422) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11423) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11424) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11425) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11427) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11429) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11430) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11432) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11435) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11436) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11437) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,11440) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11444) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11445) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11446) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11447) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11449) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11450) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11451) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11452) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11455) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11458) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11459) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,11467) tree.c:1911: local_irq_restore(flags);
    (<0.0>,11470)
    (<0.0>,11472) fake_sched.h:43: return __running_cpu;
    (<0.0>,11476) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11478) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11482) fake_sched.h:43: return __running_cpu;
    (<0.0>,11486) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11493) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,11495) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,11500) tree.c:3016: local_irq_save(flags);
    (<0.0>,11503)
    (<0.0>,11505) fake_sched.h:43: return __running_cpu;
    (<0.0>,11509) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11511) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11515) fake_sched.h:43: return __running_cpu;
    (<0.0>,11519) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11524) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11525) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,11536)
    (<0.0>,11537)
    (<0.0>,11538) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,11551)
    (<0.0>,11552) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11557) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11558) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11559) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11560) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11562) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11564) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11565) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11567) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11570) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11571) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11572) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11573) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11578) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11579) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11580) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11581) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11583) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11585) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11586) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11588) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11591) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11592) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11593) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,11599) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,11615)
    (<0.0>,11616) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11619)
    (<0.0>,11620) tree.c:625: return &rsp->node[0];
    (<0.0>,11624) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,11625) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11630) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11631) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11632) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11633) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11635) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11637) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11638) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11640) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11643) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11644) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11645) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11649) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,11650) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11652) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11655) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,11656) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11660) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11661) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11662) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11663) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11665) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11667) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11668) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11670) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,11673) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11674) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11675) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,11679) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11682) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,11685) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11688) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11689) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,11692) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11694) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11697) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11700) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11703) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11704) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11706) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11709) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11713) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11715) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11717) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11720) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11723) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11726) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11727) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11729) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11732) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,11736) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11738) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11740) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,11743) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,11745) tree.c:666: }
    (<0.0>,11748) tree.c:3024: local_irq_restore(flags);
    (<0.0>,11751)
    (<0.0>,11753) fake_sched.h:43: return __running_cpu;
    (<0.0>,11757) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11759) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,11763) fake_sched.h:43: return __running_cpu;
    (<0.0>,11767) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,11773) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,11776)
    (<0.0>,11777) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11779) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11782) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,11789) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11792)
    (<0.0>,11796) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11799) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11800) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11801) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11805) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11806) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11807) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11809) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,11817) fake_sched.h:43: return __running_cpu;
    (<0.0>,11821) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,11831) fake_sched.h:43: return __running_cpu;
    (<0.0>,11835) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11836) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,11838) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,11839) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,11840) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,11842) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,11844) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,11845) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11846) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11847) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11848) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11849) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,11851) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,11859)
    (<0.0>,11865) fake_sched.h:43: return __running_cpu;
    (<0.0>,11869)
    (<0.0>,11872) tree.c:755: local_irq_save(flags);
    (<0.0>,11875)
    (<0.0>,11877) fake_sched.h:43: return __running_cpu;
    (<0.0>,11881) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11883) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,11887) fake_sched.h:43: return __running_cpu;
    (<0.0>,11891) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,11903)
    (<0.0>,11905) fake_sched.h:43: return __running_cpu;
    (<0.0>,11909) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11910) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,11912) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,11913) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,11914) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11915) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11916) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11917) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,11918) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,11922) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,11924) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,11925) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11926) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,11942)
    (<0.0>,11944)
    (<0.0>,11946) fake_sched.h:43: return __running_cpu;
    (<0.0>,11950) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,11953) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11954) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11955) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11959) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11960) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11961) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11963) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11968) fake_sched.h:43: return __running_cpu;
    (<0.0>,11971) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11973) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11975) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,11976) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,11979)
    (<0.0>,11982) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11985) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11986) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11987) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11991) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11992) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11993) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,11995) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12000) fake_sched.h:43: return __running_cpu;
    (<0.0>,12003) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12005) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12007) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12008) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,12011)
    (<0.0>,12014) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12017) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12018) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12019) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12023) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12024) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12025) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12027) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,12034) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12037) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12038) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12039) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12041) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12042) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,12044) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12045) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12046) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12047) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12061)
    (<0.0>,12063) tree.c:758: local_irq_restore(flags);
    (<0.0>,12066)
    (<0.0>,12068) fake_sched.h:43: return __running_cpu;
    (<0.0>,12072) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12074) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12078) fake_sched.h:43: return __running_cpu;
    (<0.0>,12082) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12088) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12091) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,12096) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12101) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12102) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12103) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12104) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12106) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12108) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12109) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12111) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12114) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12115) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12116) tree.c:2198: swait_event_interruptible(rsp->gp_wq,
    (<0.0>,12123) fake_sched.h:43: return __running_cpu;
    (<0.0>,12127)
    (<0.0>,12128) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12131) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,12136) tree.c:892: local_irq_save(flags);
    (<0.0>,12139)
    (<0.0>,12141) fake_sched.h:43: return __running_cpu;
    (<0.0>,12145) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12147) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,12151) fake_sched.h:43: return __running_cpu;
    (<0.0>,12155) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12167)
    (<0.0>,12169) fake_sched.h:43: return __running_cpu;
    (<0.0>,12173) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12174) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12176) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12177) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,12178) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12179) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12180) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12181) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,12182) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,12186) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,12188) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,12189) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,12190) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,12201)
    (<0.0>,12202)
    (<0.0>,12204) fake_sched.h:43: return __running_cpu;
    (<0.0>,12208) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,12212) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12215) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12216) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12217) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12219) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12220) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,12222) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12223) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12224) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12225) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,12235)
    (<0.0>,12237) tree.c:895: local_irq_restore(flags);
    (<0.0>,12240)
    (<0.0>,12242) fake_sched.h:43: return __running_cpu;
    (<0.0>,12246) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12248) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,12252) fake_sched.h:43: return __running_cpu;
    (<0.0>,12256) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12263) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,12265) tree.c:2201: rsp->gp_state = RCU_GP_DONE_GPS;
    (<0.0>,12266) tree.c:2203: if (rcu_gp_init(rsp))
    (<0.0>,12311)
    (<0.0>,12312) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12315)
    (<0.0>,12316) tree.c:625: return &rsp->node[0];
    (<0.0>,12320) tree.c:1934: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,12322) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12323) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12324) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12329) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12330) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12331) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12332) fake_defs.h:237: switch (size) {
    (<0.0>,12334) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12336) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12337) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12339) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12342) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12343) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12344) tree.c:1936: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,12345) tree.c:1937: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,12348)
    (<0.0>,12349) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12353)
    (<0.0>,12356) fake_sched.h:43: return __running_cpu;
    (<0.0>,12360) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12364) fake_sched.h:43: return __running_cpu;
    (<0.0>,12368) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12373) fake_sched.h:43: return __running_cpu;
    (<0.0>,12377) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12380) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12381) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12388) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12393) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12394) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12395) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12396) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12398) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12400) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12401) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12403) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12406) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12407) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12408) tree.c:1938: if (!READ_ONCE(rsp->gp_flags)) {
    (<0.0>,12419)
    (<0.0>,12425)
    (<0.0>,12430) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12435) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12436) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12437) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12438) fake_defs.h:237: switch (size) {
    (<0.0>,12440) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12442) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12443) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12445) fake_defs.h:239: case 2: *(volatile u16 *)p = *(u16 *)res; break;
    (<0.0>,12448) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12449) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12450) tree.c:1943: WRITE_ONCE(rsp->gp_flags, 0); /* Clear all flags: New grace period. */
    (<0.0>,12451) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12464)
    (<0.0>,12465) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12470) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12471) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12472) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12473) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12475) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12477) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12478) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12480) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12483) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12484) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12485) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12486) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12491) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12492) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12493) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12494) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12496) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12498) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12499) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12501) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12504) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12505) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12506) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12514) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12515) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12528)
    (<0.0>,12529) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12534) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12535) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12536) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12537) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12539) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12541) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12542) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12544) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12547) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12548) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12549) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12550) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12555) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12556) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12557) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12558) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12560) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12562) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12563) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12565) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12568) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12569) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12570) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,12577) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12578) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12579) tree.c:1945: if (WARN_ON_ONCE(rcu_gp_in_progress(rsp))) {
    (<0.0>,12582) tree.c:1955: record_gp_stall_check_time(rsp);
    (<0.0>,12597)
    (<0.0>,12598) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,12599) tree.c:1237: unsigned long j = jiffies;
    (<0.0>,12600) tree.c:1240: rsp->gp_start = j;
    (<0.0>,12601) tree.c:1240: rsp->gp_start = j;
    (<0.0>,12603) tree.c:1240: rsp->gp_start = j;
    (<0.0>,12624) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12625) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12626) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12627) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12629) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12631) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12632) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12634) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12637) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12638) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12639) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12640) update.c:460: int till_stall_check = READ_ONCE(rcu_cpu_stall_timeout);
    (<0.0>,12641) update.c:466: if (till_stall_check < 3) {
    (<0.0>,12644) update.c:469: } else if (till_stall_check > 300) {
    (<0.0>,12648) update.c:473: return till_stall_check * HZ + RCU_STALL_DELAY_DELTA;
    (<0.0>,12653) tree.c:1242: j1 = rcu_jiffies_till_stall_check();
    (<0.0>,12655) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12656) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12658) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12659) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12664) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12665) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12666) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12667) fake_defs.h:237: switch (size) {
    (<0.0>,12669) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12671) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12672) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12674) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,12677) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12678) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12679) tree.c:1243: WRITE_ONCE(rsp->jiffies_stall, j + j1);
    (<0.0>,12680) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12681) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12684) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12686) tree.c:1244: rsp->jiffies_resched = j + j1 / 2;
    (<0.0>,12687) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12692) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12693) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12694) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12695) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12697) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12699) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12700) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12702) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,12705) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12706) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12707) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12708) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12710) tree.c:1245: rsp->n_force_qs_gpstart = READ_ONCE(rsp->n_force_qs);
    (<0.0>,12714) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12716) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12718) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12719) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12721) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12722) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12723) tree.c:1957: smp_store_release(&rsp->gpnum, rsp->gpnum + 1);
    (<0.0>,12727) tree.c:1959: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,12730)
    (<0.0>,12731) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12735)
    (<0.0>,12736) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12737) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12742) fake_sched.h:43: return __running_cpu;
    (<0.0>,12746) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12748) fake_sched.h:43: return __running_cpu;
    (<0.0>,12752) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12759) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12762) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12765) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12766) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12768) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12769) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12771) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12776) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,12777) tree.c:1968: rcu_gp_slow(rsp, gp_preinit_delay);
    (<0.0>,12781)
    (<0.0>,12782)
    (<0.0>,12783) tree.c:1922: if (delay > 0 &&
    (<0.0>,12787) tree.c:1969: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,12790)
    (<0.0>,12791) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12795)
    (<0.0>,12798) fake_sched.h:43: return __running_cpu;
    (<0.0>,12802) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12806) fake_sched.h:43: return __running_cpu;
    (<0.0>,12810) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12815) fake_sched.h:43: return __running_cpu;
    (<0.0>,12819) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12822) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12823) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12830) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12832) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12833) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12835) tree.c:1970: if (rnp->qsmaskinit == rnp->qsmaskinitnext &&
    (<0.0>,12838) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,12840) tree.c:1971: !rnp->wait_blkd_tasks) {
    (<0.0>,12843) tree.c:1973: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,12846)
    (<0.0>,12847) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12851)
    (<0.0>,12852) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12853) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,12858) fake_sched.h:43: return __running_cpu;
    (<0.0>,12862) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,12864) fake_sched.h:43: return __running_cpu;
    (<0.0>,12868) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,12876) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12878) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12880) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12881) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12883) tree.c:1967: rcu_for_each_leaf_node(rsp, rnp) {
    (<0.0>,12888) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12891) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12893) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12894) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12896) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,12901) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,12902) tree.c:2023: rcu_gp_slow(rsp, gp_init_delay);
    (<0.0>,12906)
    (<0.0>,12907)
    (<0.0>,12908) tree.c:1922: if (delay > 0 &&
    (<0.0>,12912) tree.c:2024: raw_spin_lock_irq_rcu_node(rnp);
    (<0.0>,12915)
    (<0.0>,12916) tree.h:728: raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,12920)
    (<0.0>,12923) fake_sched.h:43: return __running_cpu;
    (<0.0>,12927) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,12931) fake_sched.h:43: return __running_cpu;
    (<0.0>,12935) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,12940) fake_sched.h:43: return __running_cpu;
    (<0.0>,12944) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,12947) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12948) fake_sync.h:101: if (pthread_mutex_lock(l))
    (<0.0>,12956) fake_sched.h:43: return __running_cpu;
    (<0.0>,12959) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12961) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12963) tree.c:2025: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,12964) tree.c:2026: rcu_preempt_check_blocked_tasks(rnp);
    (<0.0>,12970)
    (<0.0>,12971) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12973) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12978) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12979) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12981) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12985) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12986) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12987) tree_plugin.h:794: WARN_ON_ONCE(rnp->qsmask);
    (<0.0>,12989) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,12991) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,12992) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,12994) tree.c:2030: rnp->qsmask = rnp->qsmaskinit;
    (<0.0>,12996) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12998) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,12999) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13000) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13005) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13006) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13007) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13008) fake_defs.h:237: switch (size) {
    (<0.0>,13010) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13012) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13013) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13015) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13018) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13019) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13020) tree.c:2032: WRITE_ONCE(rnp->gpnum, rsp->gpnum);
    (<0.0>,13021) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13023) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13024) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13026) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13031) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13032) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13034) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13035) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13037) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13041) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13042) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13043) tree.c:2033: if (WARN_ON_ONCE(rnp->completed != rsp->completed))
    (<0.0>,13046) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13047) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13049) tree.c:2035: if (rnp == rdp->mynode)
    (<0.0>,13052) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13053) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13054) tree.c:2036: (void)__note_gp_changes(rsp, rnp, rdp);
    (<0.0>,13076)
    (<0.0>,13077)
    (<0.0>,13078)
    (<0.0>,13079) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13081) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13082) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13084) tree.c:1858: if (rdp->completed == rnp->completed &&
    (<0.0>,13087) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13091) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13092) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13093) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13094) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13096) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13097) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13098) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13099) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,13102) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13105) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13106) tree.c:1859: !unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13114) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13115) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13116) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13125)
    (<0.0>,13126)
    (<0.0>,13127)
    (<0.0>,13128) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13131) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13134) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13137) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13138) tree.c:1749: if (!rdp->nxttail[RCU_NEXT_TAIL] || !*rdp->nxttail[RCU_DONE_TAIL])
    (<0.0>,13141) tree.c:1750: return false;
    (<0.0>,13143) tree.c:1799: }
    (<0.0>,13146) tree.c:1862: ret = rcu_accelerate_cbs(rsp, rnp, rdp);
    (<0.0>,13148) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13150) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13151) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13153) tree.c:1874: if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
    (<0.0>,13156) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13158) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13159) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13161) tree.c:1880: rdp->gpnum = rnp->gpnum;
    (<0.0>,13164) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13166) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13167) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13169) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13175) tree.c:1882: need_gp = !!(rnp->qsmask & rdp->grpmask);
    (<0.0>,13176) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13179) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13183) tree.c:1883: rdp->cpu_no_qs.b.norm = need_gp;
    (<0.0>,13185) fake_sched.h:43: return __running_cpu;
    (<0.0>,13189) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13190) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13192) tree.c:1884: rdp->rcu_qs_ctr_snap = __this_cpu_read(rcu_qs_ctr);
    (<0.0>,13193) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13195) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13198) tree.c:1888: rdp->core_needs_qs = need_gp;
    (<0.0>,13199) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,13201) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,13203) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,13205) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,13207) tree.c:1891: rnp->qsmask &= ~rdp->grpmask;
    (<0.0>,13208) tree.c:1893: zero_cpu_stall_ticks(rdp);
    (<0.0>,13211)
    (<0.0>,13212) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,13214) tree_plugin.h:1662: rdp->ticks_this_gp = 0;
    (<0.0>,13215) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,13217) tree_plugin.h:1663: rdp->softirq_snap = kstat_softirqs_cpu(RCU_SOFTIRQ, smp_processor_id());
    (<0.0>,13227)
    (<0.0>,13232) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13236) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13237) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13238) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13239) fake_defs.h:237: switch (size) {
    (<0.0>,13241) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13242) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13243) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13244) fake_defs.h:238: case 1: *(volatile u8 *)p = *(u8 *)res; break;
    (<0.0>,13247) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13250) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13251) tree.c:1894: WRITE_ONCE(rdp->gpwrap, false);
    (<0.0>,13254) tree.c:1896: return ret;
    (<0.0>,13258) tree.c:2037: rcu_preempt_boost_start_gp(rnp);
    (<0.0>,13261)
    (<0.0>,13265) tree.c:2041: raw_spin_unlock_irq_rcu_node(rnp);
    (<0.0>,13268)
    (<0.0>,13269) tree.h:734: raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
    (<0.0>,13273)
    (<0.0>,13274) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13275) fake_sync.h:107: if (pthread_mutex_unlock(l))
    (<0.0>,13280) fake_sched.h:43: return __running_cpu;
    (<0.0>,13284) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,13286) fake_sched.h:43: return __running_cpu;
    (<0.0>,13290) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13305) fake_sched.h:43: return __running_cpu;
    (<0.0>,13311) tree.c:253: if (!rcu_sched_data[get_cpu()].cpu_no_qs.s)
    (<0.0>,13317) fake_sched.h:43: return __running_cpu;
    (<0.0>,13324) tree.c:258: rcu_sched_data[get_cpu()].cpu_no_qs.b.norm = false;
    (<0.0>,13326) fake_sched.h:43: return __running_cpu;
    (<0.0>,13333) tree.c:259: if (!rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)
    (<0.0>,13341) fake_sched.h:43: return __running_cpu;
    (<0.0>,13345) tree.c:352: if (unlikely(raw_cpu_read(rcu_sched_qs_mask)))
    (<0.0>,13358) fake_sched.h:43: return __running_cpu;
    (<0.0>,13362)
    (<0.0>,13365) tree.c:755: local_irq_save(flags);
    (<0.0>,13368)
    (<0.0>,13370) fake_sched.h:43: return __running_cpu;
    (<0.0>,13374) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13376) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13380) fake_sched.h:43: return __running_cpu;
    (<0.0>,13384) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13396)
    (<0.0>,13398) fake_sched.h:43: return __running_cpu;
    (<0.0>,13402) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13403) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13405) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13406) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,13407) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13408) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13409) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13410) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13411) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,13415) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,13417) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,13418) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,13419) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,13435)
    (<0.0>,13437)
    (<0.0>,13439) fake_sched.h:43: return __running_cpu;
    (<0.0>,13443) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13446) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13447) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13448) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13452) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13453) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13454) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13456) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13461) fake_sched.h:43: return __running_cpu;
    (<0.0>,13464) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13466) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13468) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13469) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,13472)
    (<0.0>,13475) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13478) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13479) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13480) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13484) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13485) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13486) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13488) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13493) fake_sched.h:43: return __running_cpu;
    (<0.0>,13496) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13498) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13500) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,13501) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,13504)
    (<0.0>,13507) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13510) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13511) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13512) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13516) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13517) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13518) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13520) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,13527) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13530) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13531) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13532) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13534) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13535) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,13537) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13538) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13539) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13540) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13554)
    (<0.0>,13556) tree.c:758: local_irq_restore(flags);
    (<0.0>,13559)
    (<0.0>,13561) fake_sched.h:43: return __running_cpu;
    (<0.0>,13565) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13567) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13571) fake_sched.h:43: return __running_cpu;
    (<0.0>,13575) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13581) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,13584) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,13589) fake_sched.h:43: return __running_cpu;
    (<0.0>,13593)
    (<0.0>,13594) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,13597) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
    (<0.0>,13602) tree.c:892: local_irq_save(flags);
    (<0.0>,13605)
    (<0.0>,13607) fake_sched.h:43: return __running_cpu;
    (<0.0>,13611) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13613) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,13617) fake_sched.h:43: return __running_cpu;
    (<0.0>,13621) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13633)
    (<0.0>,13635) fake_sched.h:43: return __running_cpu;
    (<0.0>,13639) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13640) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,13642) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,13643) tree.c:867: oldval = rdtp->dynticks_nesting;
    (<0.0>,13644) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,13645) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,13646) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,13647) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
    (<0.0>,13648) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
    (<0.0>,13652) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,13654) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
    (<0.0>,13655) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,13656) tree.c:873: rcu_eqs_exit_common(oldval, user);
    (<0.0>,13667)
    (<0.0>,13668)
    (<0.0>,13670) fake_sched.h:43: return __running_cpu;
    (<0.0>,13674) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13678) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13681) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13682) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13683) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13685) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13686) tree.c:836: atomic_inc(&rdtp->dynticks);
    (<0.0>,13688) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13689) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13690) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13691) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13701)
    (<0.0>,13703) tree.c:895: local_irq_restore(flags);
    (<0.0>,13706)
    (<0.0>,13708) fake_sched.h:43: return __running_cpu;
    (<0.0>,13712) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13714) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,13718) fake_sched.h:43: return __running_cpu;
    (<0.0>,13722) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,13736) fake_sched.h:43: return __running_cpu;
    (<0.0>,13740) tree.c:377: if (unlikely(raw_cpu_read(rcu_sched_qs_mask))) {
    (<0.0>,13749) fake_sched.h:43: return __running_cpu;
    (<0.0>,13756) tree.c:382: if (unlikely(rcu_sched_data[get_cpu()].cpu_no_qs.b.exp)) {
    (<0.0>,13765) fake_sched.h:43: return __running_cpu;
    (<0.0>,13769) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,13771) tree.c:397: this_cpu_inc(rcu_qs_ctr);
    (<0.0>,13777) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13778) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13779) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13784) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13785) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13786) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13787) fake_defs.h:237: switch (size) {
    (<0.0>,13789) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13791) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13792) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13794) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13797) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13798) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13799) tree.c:2043: WRITE_ONCE(rsp->gp_activity, jiffies);
    (<0.0>,13801) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13803) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13805) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13806) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13808) tree.c:2022: rcu_for_each_node_breadth_first(rsp, rnp) {
    (<0.0>,13813) tree.c:2046: return true;
    (<0.0>,13815) tree.c:2047: }
    (<0.0>,13819) tree.c:2214: first_gp_fqs = true;
    (<0.0>,13820) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,13821) tree.c:2215: j = jiffies_till_first_fqs;
    (<0.0>,13822) tree.c:2216: if (j > HZ) {
    (<0.0>,13825) tree.c:2220: ret = 0;
    (<0.0>,13827) tree.c:2222: if (!ret) {
    (<0.0>,13830) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13831) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13833) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13835) tree.c:2223: rsp->jiffies_force_qs = jiffies + j;
    (<0.0>,13837) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13838) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13841) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13842) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13847) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13848) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13849) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13850) fake_defs.h:237: switch (size) {
    (<0.0>,13852) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13854) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13855) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13857) fake_defs.h:241: case 8: *(volatile u64 *)p = *(u64 *)res; break;
    (<0.0>,13860) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13861) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13862) tree.c:2224: WRITE_ONCE(rsp->jiffies_kick_kthreads,
    (<0.0>,13866) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,13868) tree.c:2230: rsp->gp_state = RCU_GP_WAIT_FQS;
    (<0.0>,13872) fake_sched.h:43: return __running_cpu;
    (<0.0>,13876) fake_sched.h:149: if (!local_irq_depth[get_cpu()]) {
    (<0.0>,13880) fake_sched.h:43: return __running_cpu;
    (<0.0>,13884) fake_sched.h:150: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,13889) fake_sched.h:43: return __running_cpu;
    (<0.0>,13893) fake_sched.h:153: local_irq_depth[get_cpu()] = 1;
    (<0.0>,13904) fake_sched.h:43: return __running_cpu;
    (<0.0>,13908) tree.c:937: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,13909) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,13911) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,13912) tree.c:938: oldval = rdtp->dynticks_nesting;
    (<0.0>,13913) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,13915) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,13917) tree.c:939: rdtp->dynticks_nesting++;
    (<0.0>,13918) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13919) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13920) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13921) tree.c:940: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,13922) tree.c:942: if (oldval)
    (<0.0>,13930)
    (<0.0>,13936)
    (<0.0>,13945) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13946) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13947) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13951) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13952) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13953) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13955) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13960) fake_sched.h:43: return __running_cpu;
    (<0.0>,13963) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13965) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13968) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13970) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13972) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13975) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13976) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13977) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13981) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13982) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13983) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13985) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,13990) fake_sched.h:43: return __running_cpu;
    (<0.0>,13993) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13995) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,13998) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14000) tree_plugin.h:1672: rsp->rda[get_cpu()].ticks_this_gp++;
    (<0.0>,14002) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14005) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14006) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14007) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14011) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14012) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14013) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14015) tree_plugin.h:1671: for_each_rcu_flavor(rsp)
    (<0.0>,14020) tree.c:2860: if (user || rcu_is_cpu_rrupt_from_idle()) {
    (<0.0>,14025) fake_sched.h:43: return __running_cpu;
    (<0.0>,14030) tree.c:1118: return rcu_dynticks[get_cpu()].dynticks_nesting <= 1;
    (<0.0>,14038) fake_sched.h:43: return __running_cpu;
    (<0.0>,14044) tree.c:268: if (rcu_bh_data[get_cpu()].cpu_no_qs.s) {
    (<0.0>,14058) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14059) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14060) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14064) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14065) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14066) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14068) tree.c:3559: for_each_rcu_flavor(rsp)
    (<0.0>,14072) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14074) fake_sched.h:43: return __running_cpu;
    (<0.0>,14077) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14079) tree.c:3560: if (__rcu_pending(rsp, this_cpu_ptr(rsp->rda)))
    (<0.0>,14101)
    (<0.0>,14102)
    (<0.0>,14103) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14105) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14106) tree.c:3491: struct rcu_node *rnp = rdp->mynode;
    (<0.0>,14107) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14109) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14111) tree.c:3493: rdp->n_rcu_pending++;
    (<0.0>,14112) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,14113) tree.c:3496: check_cpu_stall(rsp, rdp);
    (<0.0>,14148)
    (<0.0>,14149)
    (<0.0>,14150) tree.c:1468: if ((rcu_cpu_stall_suppress && !rcu_kick_kthreads) ||
    (<0.0>,14153) tree.c:1469: !rcu_gp_in_progress(rsp))
    (<0.0>,14166)
    (<0.0>,14167) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14172) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14173) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14174) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14175) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14177) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14179) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14180) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14182) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14185) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14186) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14187) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14188) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14193) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14194) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14195) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14196) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14198) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14200) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14201) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14203) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14206) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14207) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14208) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14214) tree.c:1471: rcu_stall_kick_kthreads(rsp);
    (<0.0>,14239)
    (<0.0>,14240) tree.c:1310: if (!rcu_kick_kthreads)
    (<0.0>,14245) tree.c:1472: j = jiffies;
    (<0.0>,14246) tree.c:1472: j = jiffies;
    (<0.0>,14247) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14252) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14253) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14254) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14255) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14257) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14259) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14260) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14262) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14265) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14266) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14267) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14268) tree.c:1491: gpnum = READ_ONCE(rsp->gpnum);
    (<0.0>,14270) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14275) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14276) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14277) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14278) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14280) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14282) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14283) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14285) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14288) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14289) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14290) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14291) tree.c:1493: js = READ_ONCE(rsp->jiffies_stall);
    (<0.0>,14293) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14298) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14299) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14300) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14301) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14303) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14305) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14306) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14308) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14311) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14312) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14313) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14314) tree.c:1495: gps = READ_ONCE(rsp->gp_start);
    (<0.0>,14316) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14321) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14322) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14323) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14324) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14326) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14328) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14329) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14331) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14334) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14335) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14336) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14337) tree.c:1497: completed = READ_ONCE(rsp->completed);
    (<0.0>,14338) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,14339) tree.c:1498: if (ULONG_CMP_GE(completed, gpnum) ||
    (<0.0>,14343) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,14344) tree.c:1499: ULONG_CMP_LT(j, js) ||
    (<0.0>,14350) tree.c:3499: if (rcu_nohz_full_cpu(rsp))
    (<0.0>,14353)
    (<0.0>,14356) tree.c:3503: if (rcu_scheduler_fully_active &&
    (<0.0>,14359) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14361) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14364) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14368) tree.c:3504: rdp->core_needs_qs && rdp->cpu_no_qs.b.norm &&
    (<0.0>,14372) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14374) tree.c:3507: } else if (rdp->core_needs_qs &&
    (<0.0>,14377) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,14381) tree.c:3508: (!rdp->cpu_no_qs.b.norm ||
    (<0.0>,14384) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14386) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14388) tree.c:3510: rdp->n_rp_report_qs++;
    (<0.0>,14389) tree.c:3511: return 1;
    (<0.0>,14391) tree.c:3548: }
    (<0.0>,14395) tree.c:3561: return 1;
    (<0.0>,14397) tree.c:3563: }
    (<0.0>,14404) fake_sched.h:43: return __running_cpu;
    (<0.0>,14408) tree.c:3071: raise_softirq(RCU_SOFTIRQ);
    (<0.0>,14412) tree.c:2891: if (user)
    (<0.0>,14420) fake_sched.h:43: return __running_cpu;
    (<0.0>,14424) fake_sched.h:158: local_irq_depth[get_cpu()] = 0;
    (<0.0>,14426) fake_sched.h:43: return __running_cpu;
    (<0.0>,14430) fake_sched.h:159: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14436) fake_sched.h:43: return __running_cpu;
    (<0.0>,14440) fake_sched.h:183: if (need_softirq[get_cpu()]) {
    (<0.0>,14450)
    (<0.0>,14453) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14454) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14455) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14459) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14460) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14461) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14463) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,14467) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,14479)
    (<0.0>,14481) fake_sched.h:43: return __running_cpu;
    (<0.0>,14484) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,14486) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,14488) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,14489) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14491) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14498) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14499) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14501) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14507) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14508) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14509) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,14510) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,14511) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,14515)
    (<0.0>,14516)
    (<0.0>,14517) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,14518) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,14543)
    (<0.0>,14544)
    (<0.0>,14545) tree.c:1905: local_irq_save(flags);
    (<0.0>,14548)
    (<0.0>,14550) fake_sched.h:43: return __running_cpu;
    (<0.0>,14554) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14556) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14560) fake_sched.h:43: return __running_cpu;
    (<0.0>,14564) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14569) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,14571) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,14572) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,14573) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14575) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14576) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14581) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14582) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14583) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14584) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14586) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14588) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14589) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14591) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14594) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14595) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14596) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,14599) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14601) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14602) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14607) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14608) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14609) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14610) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14612) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14614) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14615) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14617) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14620) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14621) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14622) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,14625) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14629) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14630) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14631) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14632) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14634) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14635) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14636) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14637) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14640) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14643) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14644) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,14652) tree.c:1911: local_irq_restore(flags);
    (<0.0>,14655)
    (<0.0>,14657) fake_sched.h:43: return __running_cpu;
    (<0.0>,14661) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14663) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14667) fake_sched.h:43: return __running_cpu;
    (<0.0>,14671) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14678) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,14680) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,14683) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,14687) tree.c:2546: if (rdp->cpu_no_qs.b.norm &&
    (<0.0>,14691) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,14693) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,14694) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,14695) tree.c:2554: rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
    (<0.0>,14713)
    (<0.0>,14714)
    (<0.0>,14715)
    (<0.0>,14716) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,14718) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,14719) tree.c:2486: rnp = rdp->mynode;
    (<0.0>,14723) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,14724) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,14725) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,14727) tree.c:2487: raw_spin_lock_irqsave_rcu_node(rnp, flags);
    (<0.0>,14731)
    (<0.0>,14732)
    (<0.0>,14733) fake_sync.h:83: local_irq_save(flags);
    (<0.0>,14736)
    (<0.0>,14738) fake_sched.h:43: return __running_cpu;
    (<0.0>,14742) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14744) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14748) fake_sched.h:43: return __running_cpu;
    (<0.0>,14752) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14758) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,14759) fake_sync.h:85: if (pthread_mutex_lock(l))
    (<0.0>,14766) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,14770) tree.c:2488: if ((rdp->cpu_no_qs.b.norm &&
    (<0.0>,14774) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,14776) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,14777) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,14779) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,14782) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,14784) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,14785) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,14787) tree.c:2490: rdp->gpnum != rnp->gpnum || rnp->completed == rnp->gpnum ||
    (<0.0>,14790) tree.c:2491: rdp->gpwrap) {
    (<0.0>,14792) tree.c:2491: rdp->gpwrap) {
    (<0.0>,14795) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,14797) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,14798) tree.c:2504: mask = rdp->grpmask;
    (<0.0>,14799) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,14801) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,14802) tree.c:2505: if ((rnp->qsmask & mask) == 0) {
    (<0.0>,14809) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,14810) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,14811) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,14813) tree.c:2506: raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
    (<0.0>,14817)
    (<0.0>,14818)
    (<0.0>,14819) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,14820) fake_sync.h:91: if (pthread_mutex_unlock(l))
    (<0.0>,14823) fake_sync.h:93: local_irq_restore(flags);
    (<0.0>,14826)
    (<0.0>,14828) fake_sched.h:43: return __running_cpu;
    (<0.0>,14832) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14834) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14838) fake_sched.h:43: return __running_cpu;
    (<0.0>,14842) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14854) tree.c:3016: local_irq_save(flags);
    (<0.0>,14857)
    (<0.0>,14859) fake_sched.h:43: return __running_cpu;
    (<0.0>,14863) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14865) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,14869) fake_sched.h:43: return __running_cpu;
    (<0.0>,14873) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,14878) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,14879) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,14890)
    (<0.0>,14891)
    (<0.0>,14892) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,14905)
    (<0.0>,14906) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14911) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14912) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14913) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14914) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14916) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14918) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14919) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14921) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14924) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14925) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14926) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14927) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14932) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14933) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14934) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14935) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14937) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14939) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14940) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14942) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,14945) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14946) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14947) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,14953) tree.c:653: return false;  /* No, a grace period is already in progress. */
    (<0.0>,14955) tree.c:666: }
    (<0.0>,14958) tree.c:3024: local_irq_restore(flags);
    (<0.0>,14961)
    (<0.0>,14963) fake_sched.h:43: return __running_cpu;
    (<0.0>,14967) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14969) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,14973) fake_sched.h:43: return __running_cpu;
    (<0.0>,14977) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,14983) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,14986)
    (<0.0>,14987) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,14989) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,14992) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,14999) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15002)
    (<0.0>,15006) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15009) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15010) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15011) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15015) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15016) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15017) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15019) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15023) tree.c:3046: __rcu_process_callbacks(rsp);
    (<0.0>,15035)
    (<0.0>,15037) fake_sched.h:43: return __running_cpu;
    (<0.0>,15040) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15042) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15044) tree.c:3008: struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
    (<0.0>,15045) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15047) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15054) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15055) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15057) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15063) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15064) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15065) tree.c:3010: WARN_ON_ONCE(rdp->beenonline == 0);
    (<0.0>,15066) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,15067) tree.c:3013: rcu_check_quiescent_state(rsp, rdp);
    (<0.0>,15071)
    (<0.0>,15072)
    (<0.0>,15073) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,15074) tree.c:2533: note_gp_changes(rsp, rdp);
    (<0.0>,15099)
    (<0.0>,15100)
    (<0.0>,15101) tree.c:1905: local_irq_save(flags);
    (<0.0>,15104)
    (<0.0>,15106) fake_sched.h:43: return __running_cpu;
    (<0.0>,15110) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15112) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15116) fake_sched.h:43: return __running_cpu;
    (<0.0>,15120) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15125) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15127) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15128) tree.c:1906: rnp = rdp->mynode;
    (<0.0>,15129) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15131) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15132) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15137) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15138) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15139) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15140) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15142) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15144) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15145) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15147) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15150) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15151) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15152) tree.c:1907: if ((rdp->gpnum == READ_ONCE(rnp->gpnum) &&
    (<0.0>,15155) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15157) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15158) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15163) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15164) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15165) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15166) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15168) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15170) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15171) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15173) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15176) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15177) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15178) tree.c:1908: rdp->completed == READ_ONCE(rnp->completed) &&
    (<0.0>,15181) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15185) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15186) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15187) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15188) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15190) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15191) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15192) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15193) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15196) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15199) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15200) tree.c:1909: !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */
    (<0.0>,15208) tree.c:1911: local_irq_restore(flags);
    (<0.0>,15211)
    (<0.0>,15213) fake_sched.h:43: return __running_cpu;
    (<0.0>,15217) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15219) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15223) fake_sched.h:43: return __running_cpu;
    (<0.0>,15227) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15234) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15236) tree.c:2539: if (!rdp->core_needs_qs)
    (<0.0>,15241) tree.c:3016: local_irq_save(flags);
    (<0.0>,15244)
    (<0.0>,15246) fake_sched.h:43: return __running_cpu;
    (<0.0>,15250) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15252) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15256) fake_sched.h:43: return __running_cpu;
    (<0.0>,15260) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15265) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15266) tree.c:3017: if (cpu_needs_another_gp(rsp, rdp)) {
    (<0.0>,15277)
    (<0.0>,15278)
    (<0.0>,15279) tree.c:652: if (rcu_gp_in_progress(rsp))
    (<0.0>,15292)
    (<0.0>,15293) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15298) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15299) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15300) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15301) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15303) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15305) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15306) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15308) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15311) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15312) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15313) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15314) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15319) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15320) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15321) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15322) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15324) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15326) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15327) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15329) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15332) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15333) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15334) tree.c:239: return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
    (<0.0>,15340) tree.c:654: if (rcu_future_needs_gp(rsp))
    (<0.0>,15356)
    (<0.0>,15357) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15360)
    (<0.0>,15361) tree.c:625: return &rsp->node[0];
    (<0.0>,15365) tree.c:635: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15366) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15371) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15372) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15373) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15374) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15376) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15378) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15379) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15381) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15384) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15385) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15386) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15390) tree.c:636: int idx = (READ_ONCE(rnp->completed) + 1) & 0x1;
    (<0.0>,15391) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,15393) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,15396) tree.c:637: int *fp = &rnp->need_future_gp[idx];
    (<0.0>,15397) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15401) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15402) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15403) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15404) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15406) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15408) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15409) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15411) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15414) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15415) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15416) tree.c:639: return READ_ONCE(*fp);
    (<0.0>,15420) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,15423) tree.c:656: if (!rdp->nxttail[RCU_NEXT_TAIL])
    (<0.0>,15426) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,15429) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,15430) tree.c:658: if (*rdp->nxttail[RCU_NEXT_READY_TAIL])
    (<0.0>,15433) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15435) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15438) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15441) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15444) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15445) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15447) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15450) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15454) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15456) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15458) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15461) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15464) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15467) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15468) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15470) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15473) tree.c:661: if (rdp->nxttail[i - 1] != rdp->nxttail[i] &&
    (<0.0>,15477) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15479) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15481) tree.c:660: for (i = RCU_WAIT_TAIL; i < RCU_NEXT_TAIL; i++)
    (<0.0>,15484) tree.c:665: return false; /* No grace period needed. */
    (<0.0>,15486) tree.c:666: }
    (<0.0>,15489) tree.c:3024: local_irq_restore(flags);
    (<0.0>,15492)
    (<0.0>,15494) fake_sched.h:43: return __running_cpu;
    (<0.0>,15498) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15500) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15504) fake_sched.h:43: return __running_cpu;
    (<0.0>,15508) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15514) tree.c:3028: if (cpu_has_callbacks_ready_to_invoke(rdp))
    (<0.0>,15517)
    (<0.0>,15518) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15520) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15523) tree.c:616: return &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL] &&
    (<0.0>,15530) tree.c:3032: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15533)
    (<0.0>,15537) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15540) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15541) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15542) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15546) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15547) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15548) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15550) tree.c:3045: for_each_rcu_flavor(rsp)
    (<0.0>,15558) fake_sched.h:43: return __running_cpu;
    (<0.0>,15562) fake_sched.h:185: need_softirq[get_cpu()] = 0;
    (<0.0>,15572) fake_sched.h:43: return __running_cpu;
    (<0.0>,15576) tree.c:799: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15577) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,15579) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,15580) tree.c:800: oldval = rdtp->dynticks_nesting;
    (<0.0>,15581) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,15583) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,15585) tree.c:801: rdtp->dynticks_nesting--;
    (<0.0>,15586) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15587) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15588) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15589) tree.c:802: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15590) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,15592) tree.c:804: if (rdtp->dynticks_nesting)
    (<0.0>,15600)
    (<0.0>,15606) fake_sched.h:43: return __running_cpu;
    (<0.0>,15610)
    (<0.0>,15613) tree.c:755: local_irq_save(flags);
    (<0.0>,15616)
    (<0.0>,15618) fake_sched.h:43: return __running_cpu;
    (<0.0>,15622) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15624) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
    (<0.0>,15628) fake_sched.h:43: return __running_cpu;
    (<0.0>,15632) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
    (<0.0>,15644)
    (<0.0>,15646) fake_sched.h:43: return __running_cpu;
    (<0.0>,15650) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15651) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15653) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15654) tree.c:728: oldval = rdtp->dynticks_nesting;
    (<0.0>,15655) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15656) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15657) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15658) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15659) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
    (<0.0>,15663) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,15665) tree.c:732: rdtp->dynticks_nesting = 0;
    (<0.0>,15666) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,15667) tree.c:733: rcu_eqs_enter_common(oldval, user);
    (<0.0>,15683)
    (<0.0>,15685)
    (<0.0>,15687) fake_sched.h:43: return __running_cpu;
    (<0.0>,15691) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
    (<0.0>,15694) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15695) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15696) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15700) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15701) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15702) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15704) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15709) fake_sched.h:43: return __running_cpu;
    (<0.0>,15712) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15714) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15716) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15717) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15720)
    (<0.0>,15723) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15726) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15727) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15728) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15732) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15733) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15734) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15736) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15741) fake_sched.h:43: return __running_cpu;
    (<0.0>,15744) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15746) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15748) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
    (<0.0>,15749) tree.c:695: do_nocb_deferred_wakeup(rdp);
    (<0.0>,15752)
    (<0.0>,15755) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15758) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15759) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15760) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15764) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15765) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15766) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15768) tree.c:693: for_each_rcu_flavor(rsp) {
    (<0.0>,15775) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15778) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15779) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15780) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15782) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15783) tree.c:700: atomic_inc(&rdtp->dynticks);
    (<0.0>,15785) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15786) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15787) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15788) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
    (<0.0>,15802)
    (<0.0>,15804) tree.c:758: local_irq_restore(flags);
    (<0.0>,15807)
    (<0.0>,15809) fake_sched.h:43: return __running_cpu;
    (<0.0>,15813) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15815) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
    (<0.0>,15819) fake_sched.h:43: return __running_cpu;
    (<0.0>,15823) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
    (<0.0>,15829) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,15832) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
    (<0.0>,15837) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,15853)
    (<0.0>,15854)
    (<0.0>,15855) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15858)
    (<0.0>,15859) tree.c:625: return &rsp->node[0];
    (<0.0>,15863) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15864) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15869) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15870) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15871) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15872) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15874) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15876) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15877) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15879) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15882) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15883) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15884) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15886) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15887) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15888) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,15889) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,15893) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15898) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15899) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15900) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15901) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15903) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15905) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15906) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15908) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15911) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15912) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15913) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15916) tree.c:2066: return false;
    (<0.0>,15918) tree.c:2067: }
    (<0.0>,15923) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,15939)
    (<0.0>,15940)
    (<0.0>,15941) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15944)
    (<0.0>,15945) tree.c:625: return &rsp->node[0];
    (<0.0>,15949) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,15950) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15955) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15956) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15957) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15958) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15960) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15962) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15963) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15965) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15968) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15969) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15970) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15972) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15973) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,15974) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,15975) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,15979) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15984) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15985) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15986) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15987) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15989) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15991) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15992) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15994) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,15997) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15998) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,15999) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16002) tree.c:2066: return false;
    (<0.0>,16004) tree.c:2067: }
    (<0.0>,16009) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16025)
    (<0.0>,16026)
    (<0.0>,16027) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16030)
    (<0.0>,16031) tree.c:625: return &rsp->node[0];
    (<0.0>,16035) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16036) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16041) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16042) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16043) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16044) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16046) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16048) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16049) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16051) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16054) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16055) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16056) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16058) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16059) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16060) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16061) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16065) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16070) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16071) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16072) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16073) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16075) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16077) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16078) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16080) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16083) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16084) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16085) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16088) tree.c:2066: return false;
    (<0.0>,16090) tree.c:2067: }
    (<0.0>,16095) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16111)
    (<0.0>,16112)
    (<0.0>,16113) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16116)
    (<0.0>,16117) tree.c:625: return &rsp->node[0];
    (<0.0>,16121) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16122) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16127) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16128) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16129) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16130) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16132) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16134) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16135) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16137) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16140) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16141) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16142) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16144) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16145) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16146) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16147) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16151) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16156) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16157) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16158) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16159) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16161) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16163) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16164) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16166) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16169) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16170) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16171) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16174) tree.c:2066: return false;
    (<0.0>,16176) tree.c:2067: }
    (<0.0>,16181) tree.c:2231: ret = swait_event_interruptible_timeout(rsp->gp_wq,
    (<0.0>,16197)
    (<0.0>,16198)
    (<0.0>,16199) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16202)
    (<0.0>,16203) tree.c:625: return &rsp->node[0];
    (<0.0>,16207) tree.c:2055: struct rcu_node *rnp = rcu_get_root(rsp);
    (<0.0>,16208) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16213) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16214) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16215) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16216) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16218) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16220) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16221) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16223) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16226) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16227) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16228) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16230) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16231) tree.c:2058: *gfp = READ_ONCE(rsp->gp_flags);
    (<0.0>,16232) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16233) tree.c:2059: if (*gfp & RCU_GP_FLAG_FQS)
    (<0.0>,16237) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16242) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16243) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16244) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16245) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16247) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16249) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16250) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16252) fake_defs.h:266: __READ_ONCE_SIZE;
    (<0.0>,16255) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16256) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16257) tree.c:2063: if (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))
    (<0.0>,16260) tree.c:2066: return false;
    (<0.0>,16262) tree.c:2067: }
      (<0.1>,764) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,767) fake_sync.h:281: while (!x->done)
      (<0.1>,769) fake_sync.h:281: while (!x->done)
      (<0.1>,776) fake_sched.h:43: return __running_cpu;
      (<0.1>,780)
      (<0.1>,781) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,784) fake_sched.h:84: if (pthread_mutex_lock(&cpu_lock[cpu]))
      (<0.1>,789) tree.c:892: local_irq_save(flags);
      (<0.1>,792)
      (<0.1>,794) fake_sched.h:43: return __running_cpu;
      (<0.1>,798) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,800) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,804) fake_sched.h:43: return __running_cpu;
      (<0.1>,808) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,820)
      (<0.1>,822) fake_sched.h:43: return __running_cpu;
      (<0.1>,826) tree.c:866: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,827) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,829) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,830) tree.c:867: oldval = rdtp->dynticks_nesting;
      (<0.1>,831) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,832) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,833) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,834) tree.c:868: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);
      (<0.1>,835) tree.c:869: if (oldval & DYNTICK_TASK_NEST_MASK) {
      (<0.1>,839) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,841) tree.c:872: rdtp->dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
      (<0.1>,842) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,843) tree.c:873: rcu_eqs_exit_common(oldval, user);
      (<0.1>,854)
      (<0.1>,855)
      (<0.1>,857) fake_sched.h:43: return __running_cpu;
      (<0.1>,861) tree.c:832: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,865) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,868) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,869) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,870) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,872) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,873) tree.c:836: atomic_inc(&rdtp->dynticks);
      (<0.1>,875) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,876) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,877) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,878) tree.c:839: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,888)
      (<0.1>,890) tree.c:895: local_irq_restore(flags);
      (<0.1>,893)
      (<0.1>,895) fake_sched.h:43: return __running_cpu;
      (<0.1>,899) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,901) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,905) fake_sched.h:43: return __running_cpu;
      (<0.1>,909) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,917) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,919) update.c:370: destroy_rcu_head_on_stack(&rs_array[i].head);
      (<0.1>,924)
      (<0.1>,927) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,929) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,931) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,932) update.c:364: for (i = 0; i < n; i++) {
      (<0.1>,940) litmus.c:91: y = 1;
  (<0>,8059) litmus.c:69: r_y = y;
  (<0>,8060) litmus.c:69: r_y = y;
  (<0>,8070) fake_sched.h:43: return __running_cpu;
  (<0>,8074)
  (<0>,8077) tree.c:755: local_irq_save(flags);
  (<0>,8080)
  (<0>,8082) fake_sched.h:43: return __running_cpu;
  (<0>,8086) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8088) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
  (<0>,8092) fake_sched.h:43: return __running_cpu;
  (<0>,8096) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
  (<0>,8108)
  (<0>,8110) fake_sched.h:43: return __running_cpu;
  (<0>,8114) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8115) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8117) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8118) tree.c:728: oldval = rdtp->dynticks_nesting;
  (<0>,8119) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8120) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8121) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8122) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8123) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
  (<0>,8127) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,8129) tree.c:732: rdtp->dynticks_nesting = 0;
  (<0>,8130) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,8131) tree.c:733: rcu_eqs_enter_common(oldval, user);
  (<0>,8147)
  (<0>,8149)
  (<0>,8151) fake_sched.h:43: return __running_cpu;
  (<0>,8155) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
  (<0>,8158) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8159) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8160) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8164) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8165) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8166) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8168) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8173) fake_sched.h:43: return __running_cpu;
  (<0>,8176) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8178) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8180) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8181) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,8184)
  (<0>,8187) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8190) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8191) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8192) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8196) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8197) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8198) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8200) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8205) fake_sched.h:43: return __running_cpu;
  (<0>,8208) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8210) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8212) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
  (<0>,8213) tree.c:695: do_nocb_deferred_wakeup(rdp);
  (<0>,8216)
  (<0>,8219) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8222) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8223) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8224) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8228) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8229) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8230) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8232) tree.c:693: for_each_rcu_flavor(rsp) {
  (<0>,8239) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8242) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8243) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8244) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8246) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8247) tree.c:700: atomic_inc(&rdtp->dynticks);
  (<0>,8249) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8250) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8251) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8252) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
  (<0>,8266)
  (<0>,8268) tree.c:758: local_irq_restore(flags);
  (<0>,8271)
  (<0>,8273) fake_sched.h:43: return __running_cpu;
  (<0>,8277) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8279) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
  (<0>,8283) fake_sched.h:43: return __running_cpu;
  (<0>,8287) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
  (<0>,8293) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8296) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8301) litmus.c:138: if (pthread_join(tu, NULL))
      (<0.1>,941) litmus.c:93: fake_release_cpu(get_cpu());
      (<0.1>,942) fake_sched.h:43: return __running_cpu;
      (<0.1>,946)
      (<0.1>,949) tree.c:755: local_irq_save(flags);
      (<0.1>,952)
      (<0.1>,954) fake_sched.h:43: return __running_cpu;
      (<0.1>,958) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,960) fake_sched.h:133: if (!local_irq_depth[get_cpu()]++) {
      (<0.1>,964) fake_sched.h:43: return __running_cpu;
      (<0.1>,968) fake_sched.h:134: if (pthread_mutex_lock(&irq_lock[get_cpu()]))
      (<0.1>,980)
      (<0.1>,982) fake_sched.h:43: return __running_cpu;
      (<0.1>,986) tree.c:727: rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,987) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,989) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,990) tree.c:728: oldval = rdtp->dynticks_nesting;
      (<0.1>,991) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,992) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,993) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,994) tree.c:729: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,995) tree.c:731: if ((oldval & DYNTICK_TASK_NEST_MASK) == DYNTICK_TASK_NEST_VALUE) {
      (<0.1>,999) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1001) tree.c:732: rdtp->dynticks_nesting = 0;
      (<0.1>,1002) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1003) tree.c:733: rcu_eqs_enter_common(oldval, user);
      (<0.1>,1019)
      (<0.1>,1021)
      (<0.1>,1023) fake_sched.h:43: return __running_cpu;
      (<0.1>,1027) tree.c:679: struct rcu_dynticks *rdtp = this_cpu_ptr(rcu_dynticks);
      (<0.1>,1030) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1031) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1032) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1036) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1037) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1038) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1040) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1045) fake_sched.h:43: return __running_cpu;
      (<0.1>,1048) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1050) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1052) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1053) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1056)
      (<0.1>,1059) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1062) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1063) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1064) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1068) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1069) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1070) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1072) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1077) fake_sched.h:43: return __running_cpu;
      (<0.1>,1080) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1082) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1084) tree.c:694: rdp = this_cpu_ptr(rsp->rda);
      (<0.1>,1085) tree.c:695: do_nocb_deferred_wakeup(rdp);
      (<0.1>,1088)
      (<0.1>,1091) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1094) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1095) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1096) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1100) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1101) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1102) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1104) tree.c:693: for_each_rcu_flavor(rsp) {
      (<0.1>,1111) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1114) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1115) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1116) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1118) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1119) tree.c:700: atomic_inc(&rdtp->dynticks);
      (<0.1>,1121) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1122) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1123) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1124) tree.c:702: WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&
      (<0.1>,1138)
      (<0.1>,1140) tree.c:758: local_irq_restore(flags);
      (<0.1>,1143)
      (<0.1>,1145) fake_sched.h:43: return __running_cpu;
      (<0.1>,1149) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1151) fake_sched.h:141: if (!--local_irq_depth[get_cpu()]) {
      (<0.1>,1155) fake_sched.h:43: return __running_cpu;
      (<0.1>,1159) fake_sched.h:142: if (pthread_mutex_unlock(&irq_lock[get_cpu()]))
      (<0.1>,1165) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
      (<0.1>,1168) fake_sched.h:97: if (pthread_mutex_unlock(&cpu_lock[cpu]))
  (<0>,8302) litmus.c:138: if (pthread_join(tu, NULL))
  (<0>,8305) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8308) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8312) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8313) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
  (<0>,8314) litmus.c:141: BUG_ON(r_x == 0 && r_y == 1);
             Error: Assertion violation at (<0>,8317): (!(r_x == 0 && r_y == 1) || CK_NOASSERT())
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp18w2d7k8/tmpxcqz0bb5.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DFORCE_FAILURE_5 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp18w2d7k8/tmp8k0qp5nb.ll /tmp/tmp18w2d7k8/tmpxcqz0bb5.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --bound=-1 --preemption-bounding=S --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp18w2d7k8/tmp8k0qp5nb.ll
Total wall-clock time: 1.52 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 472 (also 16 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp2gp1xi1c/tmpm_nv1x8p.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DLIVENESS_CHECK_1 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp2gp1xi1c/tmp_xx0lvhj.ll /tmp/tmp2gp1xi1c/tmpm_nv1x8p.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp2gp1xi1c/tmp_xx0lvhj.ll
Total wall-clock time: 16.72 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 500 (also 16 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmplakzp7s5/tmp3tn1xqlu.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DLIVENESS_CHECK_2 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmplakzp7s5/tmpgp5y_ml5.ll /tmp/tmplakzp7s5/tmp3tn1xqlu.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmplakzp7s5/tmpgp5y_ml5.ll
Total wall-clock time: 18.34 s
--------------------------------------------------------------------
--- Preparing to run tests on kernel v4.9.6 under sc
--- Expecting verification success
--------------------------------------------------------------------
Trace count: 516 (also 16 sleepset blocked)
No errors were detected.
* Nidhuggc: $ /usr/bin/clang-3.8 -o /tmp/tmp03xpcfoq/tmpqxhjkgqd.ll -S -emit-llvm -g -I v4.9.6 -std=gnu99 -DLIVENESS_CHECK_3 -DASSERT_0 litmus.c
* Nidhuggc: $ /usr/local/bin/nidhugg --unroll=5 -transform /tmp/tmp03xpcfoq/tmpqqyq5a4n.ll /tmp/tmp03xpcfoq/tmpqxhjkgqd.ll
* Nidhuggc: $ /usr/local/bin/nidhugg --sc --extfun-no-race=fprintf --extfun-no-race=memcpy --preemption-bounding=S --bound=-1 --print-progress-estimate --disable-mutex-init-requirement /tmp/tmp03xpcfoq/tmpqqyq5a4n.ll
Total wall-clock time: 18.61 s
--------------------------------------------------------------------
---  Verification proceeded as expected
--------------------------------------------------------------------
