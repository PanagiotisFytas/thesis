\chapter{Performance Evaluation}
\label{perfresults}

In this chapter, we are going to present the performance results of our parallel source-DPOR and optimal-DPOR algorithms,
implemented in Concuerror. We are going to evaluate our results on some ``standard" and synthetic benchmarks that are normally
used to test DPOR algorithms, as well a real erlang program. Finally, we will try to explain the behavior of the parallel program as reflected in those
charts, drawing related conclusions whenever possible.

\section{Tests Overview}

First we are going to give a brief overview of the programs tested:

\begin{itemize}
    \item $indexer$ $N$: This test uses a Compare and Swap (CAS) primitive instruction to check if a specific element of
    a matrix is set to 0 and if so, set it to a new value. This is implemented in Erlang by using $ETS$ tables and specifically
    the $insert\_new/2$ function. This function returns false if the key of the inserted tuple exists (the entry is set to 0)
    or it inserts the tuple if the key is not found. $N$ refers to the number of threads that are performing this function.
    \item $readers$ $N$: This benchmark uses a writer process that writes a variable and $N$ reader processes that read that variable.
    \item $ lastzero$ $N$: In this test we have $N+1$ processes that read and write on an array of $N+1$ size, which has all its 
    values initialized with zero. The first process reads the array in order to find the zero element with the highest
    index. The other $N$ processes read an array element and update the next one.
    \item $rush$ $hour$: a program that uses processes and
    ETS tables to solve the Rush Hour puzzle in parallel, using A\textasteriskcentered  search. Rush hour is a complex but self-contained (917 lines of code) program.
\end{itemize}

\section{Performance Results}

The benchmarks were performed on a multiprocessor with 64 AMD Opteron 6276(2.3 GHz) cores, 126 GB of memory, running
Linux 4.9.0-8amd64 and running the later Erlang version (Erlang/OTP 21). While running our tests, we are using the
--$keep\_going$ flag to continue exploring our state space, even after an error is found. We do this so we can evaluate
how fast the complete state space gets explored.

Table \ref{Sequential performance of source-DPOR and optimal-DPOR on four benchmarks.} contains information about the traces explored and the duration of those
explorations for the sequential versions of source-DPOR and optimal-DPOR, as well the duration of the parallel versions with one scheduler, a fragmentation value of one, and a Budget of 10000.

\bigtabular{"tables/synthetic_unbounded.tex"}{Sequential performance of source-DPOR and optimal-DPOR on four benchmarks.}

Lets also present graphs depicting the execution time and the speedup ($T_{serial} 
\over T_{parallel}$) of the source-DPOR and optimal-DPOR algorithms
for different numbers of schedulers and for various test cases.

\customgGraph{scripts/readers_15_10000_combo_time.png}{Performance of readers 15 with Budget of 10000.}
\customgGraph{scripts/readers_15_30000_combo_time.png}{Performance of readers 15 with budget of 30000.}
\customgGraph{scripts/rush_hour_10000_combo_time.png}{Performance of rush hour with Budget of 10000 for source and 30000 for optimal.}
\customgGraph{scripts/lastzero_11_10000_combo_time.png}{Performance of lastzero 11 with Budget of 10000 for source and 30000 for optimal.}
\customgGraph{scripts/indexer_17_10000_combo_time.png}{Performance of indexer 17 with Budget of 10000 for source and 30000 for optimal.}


\section{Performance Analysis}

As we can see in our charts, our parallel implementation, both of source-DPOR and of optimal-DPOR, significantly reduces the testing
time of our test cases. Namely, in the cases of rush hour and indexer, source-DPOR provides a speedup by a factor of around 1.8 for 2 schedulers and
of 3.5 for 4 schedulers and 7.1 for 8 schedulers. In these benchmarks, optimal-DPOR provides a speedup of around 1.7, 3.4 and 6.8 for 2, 4 and 8 schedulers respectively. This makes our parallel implementations highly usable in personal computers.

Additionally, source-DPOR achieves a decent scalability in test cases like rush hour, lastzero 11 and indexer 17. Particularly, with
32 schedulers we manage to speed up Concuerror by a factor of 18.6 for rush hour and 20.3 for indexer 17. However, through the scalability charts 
we can notice that with more schedulers the scalability of those test cases starts to decline. This happens because we
are using a centralized Controller, which becomes a bottleneck, since the more the schedulers, the
higher the chance the Controller will be busy and therefore, unable to distribute work. The most important reason 
behind this drop in scalability though, is the fact that with more available schedulers, the execution sequences of the frontier are partitioned more finely and therefore, the subtrees of the state space assigned to each scheduler, are also more finely grained. A more fine grained assignment leads to the schedulers exploring their
assigned subtrees faster and having more races found outside their assigned subtrees.
Consequently, the communication with the Controller is more frequent. This situation is problematic for two reasons:
the communication between a scheduler and the Controller has a non-negligible overhead and when the amount of 
communications between the schedulers and the Controller increases, the Controller becomes an even bigger bottleneck.
As such, we come to the conclusion that a test case cannot scale beyond a certain point based on the number of its interleavings.

In the case of optimal-DPOR, while we still get decent speedups and scalability, the algorithm seems to stop scaling faster than source-DPOR. For instance, optimal-DPOR with 32 schedulers achieves a speedup of 17.3 for indexer 17, while for rush hour, it has a speedup of 13.4, even though that with 24 schedulers it has a speedup of 15.3. We can notice here the scalability of the algorithm to starts to break. Therefore, the parallel optimal-DPOR algorithm performs worse than parallel source-DPOR. This behavior is expected since:

\begin{itemize}

\item In source-DPOR, schedulers would have a subtree rooted at an execution of form $E.p$ (where $p$ is a process) assigned to them. However, in optimal-DPOR schedulers are assigned subtrees rooted at an execution sequence of form $E.w$ (where $w$ is an sequence of processes). Therefore, the subtrees of the state space assigned to schedulers, in optimal-DPOR, are smaller, in general, compared to the ones from source-DPOR. This leads to optimal-DPOR encountering more backtrack entries that have a disputed ownership and therefore, optimal-DPOR has to communicate with the Controller more frequently.
\item In source-DPOR, for each disputed backtrack entry, the Controller simply checks whether that entry exists in its execution tree. However, in the case of optimal-DPOR, the Controller tries to $insert$ a sequence of processes into the execution tree. This leads to the Controller of the optimal-DPOR having a higher complexity.

\end{itemize}

Consequently, in optimal-DPOR, our bottleneck i.e., the Controller, is not only slower, but also there is an increased amount of communication with it. This causes optimal-DPOR to be less scalable than source-DPOR.

From Figure \ref{Performance of readers 15 with Budget of 10000.} we notice that the scalability 
of readers 15 breaks significantly faster,
compared to that of the other benchmarks, despite the fact that readers 15 does not have much less explored traces than lastzero 11
(Table \ref{Sequential performance of source-DPOR and optimal-DPOR on four benchmarks.}). 
While trying to figure out the reasons behind this behavior, we have produced a graph that shows how many times the schedulers communicated with the Controller and why such a communication occurred, depending on the number of the schedulers. In Graph \ref{Number of times schedulers stopped their execution with a Budget of 10000.},
we can see that our schedulers exceeding their allotted budget, plays a major factor in the communication towards the Controller.
To try and measure the impact of the budget, we tried increasing the budget for both algorithms to 30000ms. The resulting execution times can be seen in Graph \ref{Performance of readers 15 with budget of 30000.}. We notice, source-DPOR performs worse with a reduced budget, due to the fact that its schedulers become more imbalanced, since the Controller repartitions the exploration frontier less frequently.
However, optimal-DPOR starts to perform better with an increased budget. As it can be seen from Graphs 
\ref{Number of times schedulers stopped their execution with a Budget of 10000.} and \ref{Number of times schedulers stopped their execution with a Budget of 30000.}, the reason for this increase is that optimal-DPOR also finds disputed entries in readers 15. Finding disputed entries causes communication with the Controller, which also leads to the frontier getting repartitioned. Therefore, optimal-DPOR can have a larger budget, without affecting its load balance, since the extra communication makes up for it.

\customgGraph{scripts/readers1510000_combined_stack.png}{Number of times schedulers stopped their execution with a Budget of 10000.}
\customgGraph{scripts/readers1530000_combined_stack.png}{Number of times schedulers stopped their execution with a Budget of 30000.}

In the case of lastzero 11, the scalability of optimal-DPOR breaks significantly faster than source-DPOR. The main reason behind this, has to do with the fact the source-DPOR explores a much larger number of traces. Namely, optimal-DPOR explores 7168 traces, while source-DPOR 60073 (52905 sleep-set blocked interleavings). A larger number of interleavings means that the schedulers have more work to distribute among them. It is important to notice here, that source-DPOR manages to catch up to optimal-DPOR with 16 or more schedulers.

\section{Final Comments}

To sum up, both of our parallel algorithms manage to significantly reduce the execution time of Concuerror and are able to scale to a large number of workers. The performance, however,
differs on the various benchmarks, depending, mainly, on number of the interleavings that must be explored. Specifically, the higher the number of interleavings, the better the scalability of our algorithms appears to be.

Source-DPOR appears to scale better than optimal-DPOR, due to the fact optimal-DPOR has a higher communication overhead with the Controller. This leads to parallel source-DPOR outperforming parallel optimal-DPOR on benchmarks with no sleep-set blocked interleavings. This should come as no surprise, since this was also true of the sequential versions. 

What is more, our parallel implementation of source-DPOR, with enough schedulers, appears to be able to outperform the optimal version even on test cases with a large number of sleep set blocked interleavings. However, on very large test cases, I do not expect this trend to continue.
