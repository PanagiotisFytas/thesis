\chapter{Parallelizing Optimal-DPOR Algorithm}
\label{paradpor_opt}

In this chapter, we are going to present a first attempt in parallelizing the optimal-DPOR
algorithm, provide an analysis on why this attempt fails to achieve any speedup and then present 
an improved parallel optimal-DPOR algorithm.


\section{Parallel optimal-DPOR - A First Attempt}

\subsection{Basic Idea}

Parallelizing the optimal-DPOR algorithm is significantly more complicated than source-DPOR. We do know that whenever a call to 
$Explore(E, Sleep, WuT)$ returns during Algorithm \ref{optimal}, then for all maximal execution sequences
of form $E.w$, the algorithm has explored some execution sequence $E'$ which is in $[E.w]_\simeq$ \cite{AbdullaAronisJohnssonSagonasDPOR2014}. In other words, calls to $Explore(E, Sleep, WuT)$ guarantee that the complete subtree rooted
at $E$ will be explored.
However, the complete $WuT$ at some execution sequence $E$ cannot be known until we have completed
exploring all execution sequences which are ordered before $E$, according to the total order of our state space
(Definition \ref{def:Ordered}). This happens because the $insert_{[E']}(v,wut(E'))$ function can add
entries to any wakeup tree of an execution sequence that is ordered after the current execution sequence.

Therefore, when assigning an incomplete wakeup tree to a scheduler, there is no
guarantee that the scheduler will explore the complete assigned state space. This means that if a
scheduler inserts a fragment into a wakeup tree owned by a different scheduler, we cannot know if that 
fragment (or a different but equivalent fragment) was indeed explored. As a result, the concept of the ownership of a 
backtrack entry, as defined in Chapter \ref{paradpor}, cannot remain the same for the optimal-DPOR algorithm.

Another main issue with optimal-DPOR, is the fact that the $insert_{[E]}(v,wut(E))$ function may end up
 and inserting at $wut(E)$ and execution sequence that is different than $v$ (but it will lead exploring an equivalent subtree). This means that two execution sequences, while different,
may be equivalent. This can potentially lead to different schedulers inserting in their
wakeup trees execution sequences that while different, may produce equivalent interleavings and therefore, the optimality
of the algorithm could be lost.

However, we do know that any leaf of $\langle B , \prec \rangle$ remains a leaf of $insert_{[E]}(w,\langle B , \prec \rangle)$
\cite{AbdullaAronisJohnssonSagonasDPOR2014}. This means that, during the sequential algorithm, any fragment that
is inserted into a wakeup tree is a fragment that must be explored, unless it is removed during
the exploration phase of the algorithm. Therefore, when we insert a fragment into a wakeup tree,
we can explore it out of order, as long as we are careful to remove execution sequences that
would have been removed on the sequential algorithm. We can take advantage of this to create an algorithm that can explore 
many interleavings in parallel but race detects each explored interleaving sequentially.

\subsection{Algorithm}

Due to the fact that we need to have parallel exploration of interleavings but sequential planning, we need to decouple
the normal exploration loop of a scheduler into two different parts: $state$ $exploration$ and $race$ $detection-planning$.
Our workers (the schedulers) will be responsible for the first part. For the second part, we are going to use a centralized
Planner. However, in order to be able to better distribute the available work to the schedulers when the Planner is busy,
we are also going to use a Controller.

\begin{algorithm}
    \caption{Controller for Optimal-DPOR - First Attempt}
    \label{optcontrollerloop}
    \Fn{controller\_loop($Schedulers$)}{
        $E_0 \leftarrow$ an arbitrary initial execution sequence\;
        $Frontier \leftarrow[E_0]$\;
        $T \leftarrow$ an execution tree rooted at $E_0$\;
        $PlannerQueue \leftarrow empty$\;
        \While{$size(Frontier) > 0$ \textnormal{and} $ size(PlannerQueue) > 0$} {
            $Frontier \leftarrow partition(Frontier)$\;
            \While{exists an idle scheduler $S$ and an unassigned execution sequence $E$ in $Frontier$}{
                $E_c \leftarrow$ a copy of $E$\;
                $spawn(S, explore(E_c))$\;
            }
            \While{the Planner is idle and PlannerQueue $\neq$ empty }{
                $E \leftarrow PlannerQueue.pop()$\;
                $update\_trace(E, T)$\;
                $spawn(Planner, plan(E))$\;
            }
            $wait\_response(Frontier, T, PlannerQueue)$\;
        }
           
    }

\end{algorithm}

Algorithm \ref{optcontrollerloop} describes the functionality of the Controller. Similarly to the source-DPOR parallel version,
the Controller is responsible for maintaining the current Frontier (as well as partitioning it) and the current Execution Tree
and for assigning execution sequences to schedulers, for as long we have idle schedulers and available work.

Apart from that, the Controller also maintains a queue of fully explored execution sequences that need to be race detected.
When the Planner is idle and the queue is not empty, the execution sequence is updated (through $update\_trace(E,T)$)
and then is sent to the Planner so its races can be detected. When updating the execution sequence from the execution tree,
the subtrees of the execution tree which are ordered after the execution sequence (according to the ordering of our state
space Definition \ref{def:Ordered}) are inserted into the execution tree as $not\_owned$ wakeup trees. This guarantees
that no redundant fragments are going to be inserted for future explorations and therefore, the algorithm remains optimal. 

The $plan(E)$ function race detects the fully explored execution sequence $E$ according to the logic of optimal-DPOR 
(Algorithm \ref{optimal}). When the planning of the sequence is finished the results are reported back to the Controller.
The $explore(E)$ function explores the execution sequence $E$ until a maximal execution sequence
has been reached and reports back that execution sequence to the Controller.


\begin{algorithm}
    \caption{Optimal Frontier Partitioning}
    \label{optpartition}
    \Fn{partition($Frontier$)}{
        \For{all E $\in$ Frontier}{
            \While{$wakeup\_tree\_leaves(E) > 1$ }{
                $E' \leftarrow $ \textnormal{a prefix of $E$
                with $wut(E') \neq \emptyset $} \;
                $v \leftarrow \textnormal{ a leaf} \in wut(E') $\;
                $E_c' \leftarrow \textnormal{ a copy of } E'$\;
                \textnormal{mark $v$ as $not\_owned$ at $ wut(E')$}\;
                $\{Prefix, v, Suffix\} \leftarrow split\_wut\_at(v, wut(E_c'))$\;
                \textnormal{mark $Prefix$ and $Suffix$ as $not\_owned$ at $ wut(E_c')$}\;
                \textnormal{add $E_c'$ to $ Frontier$}\;
            }
        }
        \Return $Frontier$\;
    }

\end{algorithm}

Partitioning the exploration frontier (Algorithm \ref{optpartition}) has two main differences, compared to the parallel source-DPOR.
Firstly, the frontier gets partitioned completely, so we can maximize the parallelization of the exploration phase. 
Secondly, the entries that are distributed from one execution sequence, are not simply removed from the backtrack and added to
the sleep set. It is vital here to maintain the correct ordering between the interleavings (Definition \ref{def:Ordered}), because during the exploration phase of the optimal-DPOR algorithm, sequences can be removed
from the wakeup tree. Maintaining the ordering will keep this behavior intact in the parallel version. Therefore,
the given entry is marked as $not\_owned$ at the distributed sequence. The function  $split\_wut\_at(v, wut(E_c'))$
splits the copy of the wakeup tree to three parts: the $Prefix$ (the wakeup tree entries ordered before the sequence $v$), the leaf $v$
and the $Suffix$ (the wakeup entries ordered after $v$). The first processes processes of the entries of
the $Prefix$ are added to the sleep set at the new execution sequence $E_c'$ (e.g. if $p.q.r$ is a leaf in $Prefix$, then $p$ is added to
$sleep(E_c')$). The $Suffix$ entries are marked as $not\_owned$ at $E_c'$.

\begin{algorithm}
    \caption{Handling Scheduler and Planner Response}
    \label{optresponse}
    \Fn{wait\_response(Frontier, T, PlannerQueue)}{
        \textbf{receive} a message M\;
        \uIf{M is sent from a Scheduler}{  
            $E \leftarrow M$\;
            $PlannerQueue.push(E)$\;
        }
        \uElseIf{M is sent from the Planner}{
            $E \leftarrow M$\;
            $update\_execution\_tree(E, T)$\;
            \textnormal{add $E$ to $Frontier$}\;
        }
    }
\end{algorithm}

After assigning the available work to the available schedulers and the Planner, the Controller will wait for a response
either from a scheduler or the Planner (Algorithm \ref{optresponse}). When a response is received from a scheduler,
the fully explored received execution sequence will be added to the queue of the Planner and the Controller will continue
with its loop (Algorithm \ref{optcontrollerloop}). If a response is received from the Planner, the Controller will update
the execution tree $T$ by adding the new wakeup trees that were inserted by the Planner and by deleting the suffix of
the execution sequence that was just explored and has no wakeup trees. We delete this part in order to have the 
execution tree only contain the part of the state space that is either currently getting explored or is planned to be explored.
If we were to not delete those suffixes, the size of the execution tree would eventually be the size of our complete state space.

\subsection{Example}

\begin{figure*}
    \begin{minipage}{0.3\textwidth}
      \begin{lstlisting}[frame=none, numbers=none]
        p:
        i = N
        while x[i] != 0 and i > 0:
            i = i - 1
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \begin{lstlisting}[frame=none, numbers=none]
            q:
            R1 = x[0]
            R2 = x[0]
            assert(R1 == R2)
            x[1] = R2 + 1
        \end{lstlisting}
      \end{minipage}
      \begin{minipage}{0.3\textwidth}
        \begin{lstlisting}[frame=none, numbers=none]
            r:
            R3 = x[0]
            R4 = x[1]
            assert(R3 == R4)
            x[2] = R4 + 1
        \end{lstlisting}
      \end{minipage}
      \caption{$Lastzero$ 2 example}
      \label{optexample}
  \end{figure*}

In Figure \ref{optexample} we can see the pseudocode of $lastzero$ 2, were we have an array of 3 elements (initially all elements have
a zero value) and three processes. The first process ($p$) searches the array for the zero element with the highest index. 
The other two processes increase their assigned element by a value of 1.

\tracelonglong{opt1.png}{Interleavings explored by the sequential optimal-DPOR}

Figure \ref{Interleavings explored by the sequential optimal-DPOR} represents 
the traces explored during the sequential optimal-DPOR.
We use a black bold rectangle to represent a new event and a faint rectangle to denote a replayed event. 
The continuous red edges represent the races that are detected and planned. The red nodes represent the wakeup tree entries
at each trace.

\tracelonglong{opt3.png}{Initial interleaving explored by the parallel optimal-DPOR}

Figure \ref{Initial interleaving explored by the parallel optimal-DPOR} depicts the initial step of the parallel optimal-DPOR. 
An arbitrary execution sequence is explored initially and then its races are detected and planned in the form of wakeup trees.
The wakeup trees are distributed into different fragments and all unassigned fragments are assigned into the idle schedulers.
Also the execution tree is initialized with the current exploration frontier.

In this example, lets assume that Scheduler 2 finishes first the exploration of its assigned trace. The controller will
then receive the new explored trace and will add this trace to the queue of the Planner. Since the Planner is idle,
this trace will be sent to the Planner to be race detected. While race detecting this trace, no more interleavings will be 
planned. This trace is equivalent to the 3rd trace of the sequential execution 
(Figue \ref{Interleavings explored by the sequential optimal-DPOR}). Notice here that in the sequential algorithm
this trace had an additional wakeup tree. This wakeup tree was planned by the 2nd trace of the sequential algorithm,
which has yet to be race detected in our example. Therefore, traces 4 and 5 of the sequential algorithm cannot be
planned from the 3rd trace but only from the 2nd. This makes apparent the main issue of the parallelization of the optimal-DPOR:
the complete wakeup tree at some execution sequence $E$ cannot be known until we have completed
exploring all execution sequences which are ordered before $E$, according to the total order of our state space
(Definition \ref{def:Ordered})

\subsection{Performance Evaluation}

We are going to use the following two synthetic benchmarks to evaluate
our first attempt in parallelizing the optimal-DPOR algorithm:

\begin{itemize}
    \item $readers$ $N$: This benchmark uses a writer process that writes a variable and $N$ reader processes that read that variable.
    \item $ lastzero$ $N$: In this test we have $N+1$ processes that read and write on an array of $N+1$ size, which has all its 
    values initialized with zero. The first process reads the array in order to find the zero element with the highest
    index. The other $N$ processes read an array element and update the next one.
\end{itemize}

Our parallel optimal-DPOR fails to achieve any type of speedup, as depicted in Figure 
\ref{Execution time for readers 10 by optimal-DPOR.}. Table \ref{Parallel optimal-DPOR performance.}
holds information about how our implementation runs on various test cases. The exploration and planning
time rows show what percentage of the time of the sequential algorithm is spend on exploring and planning interleavings respectively.
Those measurements will help us explain the reason for this lack of speedup.

\mediumGraph{scripts/readers10_timedd.png}{Execution time for readers 10 by optimal-DPOR.}


\smalltabular{"tables/opttab.tex"}{Parallel optimal-DPOR performance.}

\subsection{Performance Analysis}

This lack of performance is justified by the following reasons.

Firstly, lets look at the exploration and planning percentages of the sequential algorithm,
presented at Table \ref{Parallel Optimal-DPOR performance.}. While the percentage of exploration time in the case of a small test
case like readers10 is around 40$\%$, we have observed that for larger test cases this figure varies between 10$\%$-30$\%$. 
This means that even on an ideal setup, with zero overhead and infinite schedulers, our speedup could never exceed a factor
of 1.429 (on a test case with 70$\%$ planning time, if we consider that the exploration phase happens instantly the 
speedup would be 1.429 = $100\over 70$, since the planning phase would still have to take place sequentially). 
This reason, by itself, makes it impossible for our algorithm to achieve good performance and scalability.

What is more, the rate with which new interleavings are planned for exploration leads to schedulers not being sufficiently utilized throughout the execution of the algorithm. Specifically, we have noticed that the execution of the DPOR algorithm can be split into three consecutive phases. In the first phase, race detecting a single interleaving generates a large amount of interleavings that need to be explored and therefore the exploration Frontier is relatively large. This means that the schedulers have enough work and are being kept utilized. During the second phase, race detecting an interleaving generates a relatively small amount of new interleavings that need to be explored. Generally, throughout this phase, not enough work is being generated and the schedulers tend to stay idle. In the third phase,
barely any new interleavings are planned, until the complete state space has been explored, at which point the algorithm terminates. 

\graph{bottleneck_lastzero.png}{Planner Queue and Frontier sizes for the execution of lastzero.}

This pattern becomes visible from Figure \ref{Planner Queue and Frontier sizes for the execution of lastzero.}.
We have run our algorithm with four schedulers for the lastzero 7, 8 and 9 benchmarks. 
To create this graph we have taken measurements regarding the size of queue of the Planner and the exploration Frontier of the schedulers. The measurements are taken after either the exploration or the planning of an interleaving has finished. In this graph, the three aforementioned phases can be clearly observed. After the first phase ends, we notice that the relative large exploration Frontier that has been created, gets ``consumed" in an extremely fast rate, due to the fact that we have four scheduler consuming from it, while only one Planner producing in it. Also, like we mentioned before, the exploration of a single trace is significantly faster than its race detection. Consequently, during the second phase there are very few traces within the exploration Frontier and therefore, the schedulers stay underutilized. We can also notice from the graph that this effect intensifies while our state space increases. The main reason behind this is the fact that as the state space of a program increases, the planning of an interleaving becomes even more slow, compared to the exploration of an interleaving. 

To make matters worse, our algorithm also has a significant overhead. As mentioned before, communication between
the controller and the workers can be a substantial bottleneck. The parallel source-DPOR deals with it by assigning
a state space to the workers, minimizing the need for communication. However, in this attempt the use of the centralized planner leads to the schedulers having to report a trace back to the controller every single
time an exploration reaches its end.

\section{Scalable Parallel optimal-DPOR}

\subsection{Basic Idea}

Like we mentioned in the previous section, trying to develop a parallel optimal-DPOR algorithm based on the parallel source-DPOR has two main issues.

Firstly, in the source-DPOR algorithm, the backtrack is a set of processes (Definition \ref{def:Source Sets}), which means it contains single steps that the algorithm is going to ``take" in the future. In the parallel algorithm, as we mentioned in Chapter \ref{paradpor}, when a backtrack entry $p$ at an execution sequence $E$ is assigned to a scheduler, that scheduler will own every trace that starts with $E.p$ i.e., the scheduler can explore the subtree that is rooted at $E$, without reporting back to the controller (unless its Budget is not exceeded). 
However, in optimal-DPOR, backtrack entries are trees that contain sequences of steps that will be explored. A scheduler could not own the tree due to the fact that in Algorithm \ref{optimal}, new fragments keep getting inserted into the wakeup trees and therefore, the trees are not complete until they are about to be explored. A wakeup tree is complete, when its root is the smallest node w.r.t the ordering of the tree ($p = min_{\prec}\{ p \in wut(E)\}$). At each point throughout the execution of the algorithm, there exists only one wakeup tree with that property. 
Nonetheless, any leaf of $\langle B , \prec \rangle$ remains a leaf of $insert_{[E]}(w,\langle B , \prec \rangle)$
\cite{AbdullaAronisJohnssonSagonasDPOR2014}. This means that, a scheduler can own a subtree rooted at a leaf sequence of the wakeup tree. We are going to modify the concept of ownership as follows:
\begin{itemize}
\item A leaf sequence can be marked as $owned$ in the wakeup tree of a scheduler. This means that this scheduler also own every node in the subtree rooted at that execution sequence. For example, if $v$ is an owned leaf sequence in $wut(E)$ of some scheduler, then that scheduler owns every execution sequence that has a prefix of $E.v$.
\item A leaf sequence is marked as $not\_owned$ when some other scheduler is responsible for the corresponding subtree.
\item All other nodes are considered $disputed$. When a leaf sequence is inserted underneath a disputed node, that leaf sequence is considered disputed.
\end{itemize}

The second issue has to do with the fact that the $insert_{[E']}(v,wut(E'))$ function may end up
rearranging the sequence of the \textbf{non-racing} events within $v$. This means that two execution sequences, while different,
may be equivalent. Therefore, the ownership of disputed nodes cannot be resolved by simply checking whether those nodes exists in the execution tree.
However, we can modify $insert_{[E']}(v,wut(E'))$ to insert execution sequence within subtrees of the execution tree, instead of wakeup trees. If a disputed sequence can be inserted into the execution tree, then this means that no other scheduler has discovered an equivalent execution sequence and therefore, ownership is claimed over that execution sequence. Otherwise, this sequence is marked as $not\_owned$.

\subsection{Algorithm}

The Controller has the same logic as with the one from parallel source-DPOR (Algorithm \ref{controllerloop}). Similarly with the source-DPOR algorithm, The Controller maintains a $Frontier$, 
which is a set of execution sequences $E$, and an execution tree $T$, which contains
as branches the execution sequences of the $Frontier$. For as long as there exists an execution sequence at the $Frontier$ 
($Frontier \neq \emptyset$), the Controller will partition its $Frontier$ to at most $N$ execution sequences. Then, the Controller
will try to assign all of its unassigned execution sequences to any idle scheduler, by spawning $explore\_loop(E_c, Budget)$
functions. Finally, it will block until it receives a response from a scheduler.

\begin{algorithm}
    \caption{Optimal Frontier Partitioning - Scalable Algorithm}
    \label{optpartition2}
    \Fn{partition($Frontier$, $N$)}{
        \For{all E $\in$ Frontier}{
            \While{$total\_owned\_leaf\_sequences(E) > 1$ \textbf{and} $size(Frontier) < N$}{
                $E' \leftarrow $ \textnormal{the smallest prefix of $E$
                that has a backtrack entry} \;
                $v \leftarrow \textnormal{ the smallest (w.r.t. $\prec$) $owned$ leaf sequence} \in wut(E') $\;
                $E_c' \leftarrow \textnormal{ a copy of } E'$\;
                \textnormal{mark $v$ as $not\_owned$ at $ wut(E')$}\;
                $\{Prefix, v, Suffix\} \leftarrow split\_wut\_at(v, wut(E_c'))$\;
                \textnormal{mark $Prefix$ and $Suffix$ as $not\_owned$ at $ wut(E_c')$}\;
                \textnormal{add $E_c'$ to $ Frontier$}\;
            }
        }
        \Return $Frontier$\;
    }

\end{algorithm}
t
The $partitioning$ phase (Algorithm \ref{optpartition2}) works pretty similarly with Algorithm \ref{partition}. The main difference here, is that when we distribute sequences from the wakeup tree, we do not simply add them on the sleep set of the original execution sequence, but we mark them as not owned. The reason behind this, can be seen at lines 20-21 of Algorithm \ref{optimal}. After optimal-DPOR has finished exploring a wakeup tree at some execution sequence $E$,
the root of the wakeup tree is added at the sleep set at $E$ and sequences that begin with that root are removed from the remaining wakeup trees at $E$. In order to keep this behavior intact, we have modified the optimal-DPOR exploration algorithm to simply add we mark execution sequences that are assigned to different schedulers are $not_owned$

\begin{algorithm}
    \caption{Scheduler Exploration Loop - Scalable optimal-DPOR}
    \label{explore_loop_opt}
    \Fn{explore\_loop($E_0$, $Budget$)}{
        $StartTime \leftarrow get\_time()$\;
        $ E \leftarrow E_0$\;
        \Repeat{$CurrentTime - StartTime > Budget \textbf{ or }(ownership(E) \neq owned \textbf{ and } WuT(E) \textnormal{ has no $owned$ sequences})$}{
            $ E' \leftarrow explore(E)$\;
            $ E' \leftarrow plan\_more\_interleavings(E') $\;
            $ E \leftarrow get\_next\_execution\_sequence(E')$\;
            $CurrentTime \leftarrow get\_time()$\;
        }
        \textnormal{\textbf{send}  $E$ to Controller} \;
    }
\end{algorithm}